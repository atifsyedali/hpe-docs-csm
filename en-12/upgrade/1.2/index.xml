<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>1.2 on Cray System Management (CSM)</title>
    <link>/docs-csm/en-12/upgrade/1.2/</link>
    <description>Recent content in 1.2 on Cray System Management (CSM)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Wed, 23 Feb 2022 03:03:04 +0000</lastBuildDate><atom:link href="/docs-csm/en-12/upgrade/1.2/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>/docs-csm/en-12/upgrade/1.2/scripts/sls/sls_utils/readme/</link>
      <pubDate>Wed, 23 Feb 2022 03:03:03 +0000</pubDate>
      
      <guid>/docs-csm/en-12/upgrade/1.2/scripts/sls/sls_utils/readme/</guid>
      <description>This is a reusable python library for safely interacting with SLS network data (in JSON format).
The library has been tested against python version 3.6 and up.</description>
    </item>
    
    <item>
      <title>Upgrade SLS Offline From 1.0.x To Csm 1.2</title>
      <link>/docs-csm/en-12/upgrade/1.2/scripts/sls/readme.sls_upgrade/</link>
      <pubDate>Wed, 23 Feb 2022 03:03:03 +0000</pubDate>
      
      <guid>/docs-csm/en-12/upgrade/1.2/scripts/sls/readme.sls_upgrade/</guid>
      <description>Upgrade SLS Offline from CSM 1.0.x to CSM 1.2 TL;DR  Get a token:  export TOKEN=$(curl -s -k -S -d grant_type=client_credentials -d client_id=admin-client -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=&amp;#39;{.data.client-secret}&amp;#39; | base64 -d` https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r &amp;#39;.access_token&amp;#39;)  Extract SLS data to a file:  curl -k -H &amp;#34;Authorization: Bearer ${TOKEN}&amp;#34; https://api-gw-service-nmn.local/apis/sls/v1/dumpstate | jq -S . &amp;gt; sls_input_file.json  Upgrade SLS data (Example 1): Upgrade, using the CHN as the system default route (will by default output to migrated_sls_file.</description>
    </item>
    
    <item>
      <title>Stage 2 - Kubernetes Upgrade From 1.19.9 To 1.20.13</title>
      <link>/docs-csm/en-12/upgrade/1.2/stage_2/</link>
      <pubDate>Wed, 23 Feb 2022 03:03:02 +0000</pubDate>
      
      <guid>/docs-csm/en-12/upgrade/1.2/stage_2/</guid>
      <description>Stage 2 - Kubernetes Upgrade from 1.19.9 to 1.20.13  NOTE: During the CSM-0.9 install the LiveCD containing the initial install files for this system should have been unmounted from the master node when rebooting into the Kubernetes cluster. The scripts run in this section will also attempt to unmount/eject it if found to ensure the USB stick does not get erased.
 Stage 2.1   Run ncn-upgrade-k8s-master.sh for ncn-m002.</description>
    </item>
    
    <item>
      <title>Stage 3 - Service Upgrades</title>
      <link>/docs-csm/en-12/upgrade/1.2/stage_3/</link>
      <pubDate>Wed, 23 Feb 2022 03:03:02 +0000</pubDate>
      
      <guid>/docs-csm/en-12/upgrade/1.2/stage_3/</guid>
      <description>Stage 3 - CSM Service Upgrades IMPORTANT:
 During this stage there will be a brief approximately 5 minute window where pods with PVCs will not be able to migrate between nodes. This is due to a redeployment of the ceph csi provisioners into namespaces to accomodate the newer charts and a better upgrade strategy.
 Run csm-service-upgrade.sh to deploy upgraded CSM applications and services:
ncn-m002# /usr/share/doc/csm/upgrade/1.2/scripts/upgrade/csm-service-upgrade.sh Once Stage 3 service upgrade is complete, proceed to Return to Main Page and Proceed to Validate CSM Health</description>
    </item>
    
    <item>
      <title>1.0.0 Or Later To 1.2.0 Upgrade Process (wip)</title>
      <link>/docs-csm/en-12/upgrade/1.2/readme/</link>
      <pubDate>Wed, 23 Feb 2022 03:03:01 +0000</pubDate>
      
      <guid>/docs-csm/en-12/upgrade/1.2/readme/</guid>
      <description>CSM 1.0.0 or later to 1.2.0 Upgrade Process (WIP) #NOTE: this is a WIP doc
Introduction This document is intended to guide an administrator through the upgrade process going from Cray Systems Management v1.0 to v1.2. When upgrading a system, this top-level README.md file should be followed top to bottom, and the content on this top level page is meant to be terse. See the additional files in the various directories under the resource_material directory for additional reference material in support of the processes/scripts mentioned explicitly on this page.</description>
    </item>
    
    <item>
      <title>Stage 0 - Prerequisites And Preflight Checks</title>
      <link>/docs-csm/en-12/upgrade/1.2/stage_0_prerequisites/</link>
      <pubDate>Wed, 23 Feb 2022 03:03:01 +0000</pubDate>
      
      <guid>/docs-csm/en-12/upgrade/1.2/stage_0_prerequisites/</guid>
      <description>Stage 0 - Prerequisites and Preflight Checks  NOTE: CSM-1.0.1 is required in order to upgrade to CSM-1.2.0
 Stage 0.1 - Install latest docs RPM   Install latest document RPM package:
  Internet Connected
ncn-m001# cd /root/ ncn-m001# wget https://storage.googleapis.com/csm-release-public/csm-1.2/docs-csm/docs-csm-latest.noarch.rpm ncn-m001# rpm -Uvh docs-csm-latest.noarch.rpm   Air Gapped (replace the PATH_TO below with the location of the rpm)
ncn-m001# cp [PATH_TO_docs-csm-*.noarch.rpm] /root ncn-m001# rpm -Uvh [PATH_TO_docs-csm-*.noarch.rpm]     Stage 0.</description>
    </item>
    
    <item>
      <title>Stage 1 - Ceph Image Upgrade</title>
      <link>/docs-csm/en-12/upgrade/1.2/stage_1/</link>
      <pubDate>Wed, 23 Feb 2022 03:03:01 +0000</pubDate>
      
      <guid>/docs-csm/en-12/upgrade/1.2/stage_1/</guid>
      <description>Stage 1 - Ceph image upgrade   Run ncn-upgrade-ceph-nodes.sh for ncn-s001. Follow output of the script carefully. The script will pause for manual interaction.
ncn-m001# /usr/share/doc/csm/upgrade/1.2/scripts/upgrade/ncn-upgrade-ceph-nodes.sh ncn-s001  NOTE: You may need to reset the root password for each node after it is rebooted
   Repeat the previous step for each other storage node, one at a time.
  After ncn-upgrade-ceph-nodes.sh has successfully run for all storage nodes, rescan ssh keys on all storage nodes</description>
    </item>
    
  </channel>
</rss>
