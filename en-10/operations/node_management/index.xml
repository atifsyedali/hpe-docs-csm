<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Node Management on Cray System Management (CSM)</title>
    <link>/docs-csm/en-10/operations/node_management/</link>
    <description>Recent content in Node Management on Cray System Management (CSM)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Thu, 14 Oct 2021 05:31:36 +0000</lastBuildDate><atom:link href="/docs-csm/en-10/operations/node_management/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Replace A Compute Blade</title>
      <link>/docs-csm/en-10/operations/node_management/replace_a_compute_blade/</link>
      <pubDate>Thu, 14 Oct 2021 05:31:06 +0000</pubDate>
      
      <guid>/docs-csm/en-10/operations/node_management/replace_a_compute_blade/</guid>
      <description>Replace a Compute Blade Replace an HPE Cray EX liquid-cooled compute blade.
Shutdown software and power off the blade   Temporarily disable endpoint discovery service (MEDS) for the compute nodes(s) being replaced. This example disables MEDS for the compute node in cabinet 1000, chassis 3, slot 0 (x1000c3s0b0). If there is more than 1 node card, in the blade specify each node card (x1000c3s0b0,x1000c3s0b1).
ncn-m001# cray hsm inventory redfishEndpoints update --enabled false x1000c3s0b0   Verify that the workload manager (WLM) is not using the affected nodes.</description>
    </item>
    
    <item>
      <title>Reset Credentials On Redfish Devices</title>
      <link>/docs-csm/en-10/operations/node_management/reset_credentials_on_redfish_devices_for_reinstallation/</link>
      <pubDate>Thu, 14 Oct 2021 05:31:06 +0000</pubDate>
      
      <guid>/docs-csm/en-10/operations/node_management/reset_credentials_on_redfish_devices_for_reinstallation/</guid>
      <description>Reset Credentials on Redfish Devices Before re-installing or upgrading the system the credentials need to be changed back to their defaults for any devices that had their credentials changed post-install. This is necessary for the installation process to properly discover and communicate with these devices.
Prerequisites Administrative privileges are required.
Procedure   Create a SCSD payload file with the default credentials for the Redfish devices that have been changed from the defaults.</description>
    </item>
    
    <item>
      <title>Swap A Compute Blade With A Different System</title>
      <link>/docs-csm/en-10/operations/node_management/swap_a_compute_blade_with_a_different_system/</link>
      <pubDate>Thu, 14 Oct 2021 05:31:06 +0000</pubDate>
      
      <guid>/docs-csm/en-10/operations/node_management/swap_a_compute_blade_with_a_different_system/</guid>
      <description>Swap a compute blade with a different system Swap an HPE Cray EX liquid-cooled compute blade between two systems.
  The two systems in this example are:
 Source system - Cray EX TDS cabinet x9000 with a healthy EX425 blade (Windom dual-injection) in chassis 3, slot 0 Destination system - Cray EX cabinet x1005 with a defective EX425 blade (Windom dual-injection) in chassis 3, slot 0    Substitute the correct xnames or other parameters in the command examples that follow.</description>
    </item>
    
    <item>
      <title>TLS Certificates For Redfish BMCs</title>
      <link>/docs-csm/en-10/operations/node_management/tls_certificates_for_redfish_bmcs/</link>
      <pubDate>Thu, 14 Oct 2021 05:31:06 +0000</pubDate>
      
      <guid>/docs-csm/en-10/operations/node_management/tls_certificates_for_redfish_bmcs/</guid>
      <description>TLS Certificates for Redfish BMCs Redfish HTTP communications are capable of using TLS certificates and Certificate Authority (CA) trust bundles to improve security. Several Hardware Management Services (HMS) have been modified to enable the HTTP transports used for Redfish communications to use a CA trust bundle.
The following services communicate with Redfish BMCs:
 State Manager Daemon (SMD) Cray Advanced Platform Monitoring and Control (CAPMC) Firmware Action Service (FAS) HMS Collector River Endpoint Discovery Service (REDS) Mountain Endpoint Discovery Service (MEDS)  Each Redfish BMC must have a TLS certificate in order to be useful.</description>
    </item>
    
    <item>
      <title>Troubleshoot Interfaces With Ip Address Issues</title>
      <link>/docs-csm/en-10/operations/node_management/troubleshoot_interfaces_with_ip_address_issues/</link>
      <pubDate>Thu, 14 Oct 2021 05:31:06 +0000</pubDate>
      
      <guid>/docs-csm/en-10/operations/node_management/troubleshoot_interfaces_with_ip_address_issues/</guid>
      <description>Troubleshoot Interfaces with IP Address Issues Correct NCNs that are failing to assigning a static IP address or detect a duplicate IP address.
The Wicked network manager tool will fail to bring an interface up if its assigned IP address already exists in the respective LAN. This can be detected by checking for signs of duplicate IP messages in the log.
Prerequisites An NCN has an interface that is failing to assign a static IP address or that has a duplicate IP address.</description>
    </item>
    
    <item>
      <title>Troubleshoot Issues With Redfish Endpoint Discovery</title>
      <link>/docs-csm/en-10/operations/node_management/troubleshoot_issues_with_redfish_endpoint_discovery/</link>
      <pubDate>Thu, 14 Oct 2021 05:31:06 +0000</pubDate>
      
      <guid>/docs-csm/en-10/operations/node_management/troubleshoot_issues_with_redfish_endpoint_discovery/</guid>
      <description>Troubleshoot Issues with Redfish Endpoint Discovery If a Redfish endpoint is in the HTTPsGetFailed status, the endpoint does not need to be fully rediscovered. The error indicates an issue in the inventory process done by the Hardware State Manager (HSM). Restart the inventory process to fix this issue.
Update the HSM inventory to resolve issues with discovering Redfish endpoints.
The following is an example of the HSM error:
ncn-m001# cray hsm inventory redfishEndpoints describe x3000c0s15b0 Domain = &amp;#34; MACAddr = &amp;#34;b42e993b70ac&amp;#34; Enabled = true Hostname = &amp;#34;10.</description>
    </item>
    
    <item>
      <title>Troubleshoot Loss Of Console Connections And Logs On Gigabyte Nodes</title>
      <link>/docs-csm/en-10/operations/node_management/troubleshoot_loss_of_console_connections_and_logs_on_gigabyte_nodes/</link>
      <pubDate>Thu, 14 Oct 2021 05:31:06 +0000</pubDate>
      
      <guid>/docs-csm/en-10/operations/node_management/troubleshoot_loss_of_console_connections_and_logs_on_gigabyte_nodes/</guid>
      <description>Troubleshoot Loss of Console Connections and Logs on Gigabyte Nodes Gigabyte console log information will no longer be collected, and if attempting to initiate a console session through the cray-conman pod, there will be an error reported. This error will occur every time the node is rebooted unless this workaround is applied.
Prerequisites Console log information is no longer being collected for Gigabyte nodes or ConMan is reporting an error.</description>
    </item>
    
    <item>
      <title>Update Compute Node Mellanox HSN NIC Firmware</title>
      <link>/docs-csm/en-10/operations/node_management/update_compute_node_mellanox_hsn_nic_firmware/</link>
      <pubDate>Thu, 14 Oct 2021 05:31:06 +0000</pubDate>
      
      <guid>/docs-csm/en-10/operations/node_management/update_compute_node_mellanox_hsn_nic_firmware/</guid>
      <description>Update Compute Node Mellanox HSN NIC Firmware This procedure updates liquid-cooled or standard rack compute node NIC mezzanine cards (NMC) firmware for Slingshot 10 Mellanox ConnectX-5 NICs. The deployed RPM on compute nodes contains the scripts and firmware images required to perform the firmware and configuration updates.
Attention: The NIC firmware update is performed while the node is running the compute image (in-band). Use the CX-5 NIC firmware that is deployed with the compute node RPMs and not from some other repository.</description>
    </item>
    
    <item>
      <title>Update The Gigabyte Server Bios Time</title>
      <link>/docs-csm/en-10/operations/node_management/update_the_gigabyte_server_bios_time/</link>
      <pubDate>Thu, 14 Oct 2021 05:31:06 +0000</pubDate>
      
      <guid>/docs-csm/en-10/operations/node_management/update_the_gigabyte_server_bios_time/</guid>
      <description>Update the Gigabyte Server BIOS Time Check and set the time for Gigabyte compute nodes.
If the console log indicates the time between the rest of the system and the compute nodes is off by several hours, it prevents the spire-agent from getting a valid certificate, causing the node boot to drop into the dracut emergency shell.
Procedure   Enter the &amp;ldquo;Del&amp;rdquo; key during the POST call.
  Update the &amp;ldquo;System Date&amp;rdquo; field to match the time on the system.</description>
    </item>
    
    <item>
      <title>Use The Physical KVM</title>
      <link>/docs-csm/en-10/operations/node_management/use_the_physical_kvm/</link>
      <pubDate>Thu, 14 Oct 2021 05:31:06 +0000</pubDate>
      
      <guid>/docs-csm/en-10/operations/node_management/use_the_physical_kvm/</guid>
      <description>Use the Physical KVM For those who prefer to stand in front of the system and use a physically connected keyboard, mouse, and monitor, Cray provides a rack-mount-extendable KVM unit installed in rack unit slot 23 (RU23) of the management cabinet. It is connected to the first non-compute node (NCN) by default.
To use it, pull it out and raise the lid.
To bring up the main menu (shown in following figure), press Prnt Scrn.</description>
    </item>
    
    <item>
      <title>Verify Accuracy Of The System Clock</title>
      <link>/docs-csm/en-10/operations/node_management/verify_accuracy_of_the_system_clock/</link>
      <pubDate>Thu, 14 Oct 2021 05:31:06 +0000</pubDate>
      
      <guid>/docs-csm/en-10/operations/node_management/verify_accuracy_of_the_system_clock/</guid>
      <description>Verify Accuracy of the System Clock When a non-compute node (NCN) is rebooted, there may be cases when the node&amp;rsquo;s time is not synchronized, or set correctly. Use this procedure to ensure that the time is set correctly on a rebooted node.
An inaccurate system clock can result in a number of problems, such as issues with Kubernetes operations, etcd, node&amp;rsquo;s responsiveness, and more.
Prerequisites This procedure requires root privileges.</description>
    </item>
    
    <item>
      <title>View Bios Logs For Liquid Cooled Nodes</title>
      <link>/docs-csm/en-10/operations/node_management/view_bios_logs_for_liquid_cooled_nodes/</link>
      <pubDate>Thu, 14 Oct 2021 05:31:06 +0000</pubDate>
      
      <guid>/docs-csm/en-10/operations/node_management/view_bios_logs_for_liquid_cooled_nodes/</guid>
      <description>View BIOS Logs for Liquid Cooled Nodes SSH to a Liquid Cooled node and view the BIOS logs. The BIOS logs for Liquid Cooled node controllers (nC) are stored in the /var/log/n0/current and var/log/n1/current directories.
The BIOS logs for Liquid Cooled nodes are helpful for troubleshooting boot-related issues.
Prerequisites This procedure requires administrative privileges.
Procedure   Log in to the node.
SSH into the node controller for the host xname.</description>
    </item>
    
    <item>
      <title>Configuration Of NCN Bonding</title>
      <link>/docs-csm/en-10/operations/node_management/configuration_of_ncn_bonding/</link>
      <pubDate>Thu, 14 Oct 2021 05:31:05 +0000</pubDate>
      
      <guid>/docs-csm/en-10/operations/node_management/configuration_of_ncn_bonding/</guid>
      <description>Configuration of NCN Bonding Non-compute nodes (NCNs) have network interface controllers (NICs) connected to the management network that are configured in a redundant manner via Link Aggregation Control Protocol (LACP) link aggregation. The link aggregation configuration can be modified by editing and applying various configuration files either through Ansible or the interfaces directly.
The bond configuration exists across three files on an NCN. These files may vary depending on the NCN in use:</description>
    </item>
    
    <item>
      <title>Configure NTP On NCNs</title>
      <link>/docs-csm/en-10/operations/node_management/configure_ntp_on_ncns/</link>
      <pubDate>Thu, 14 Oct 2021 05:31:05 +0000</pubDate>
      
      <guid>/docs-csm/en-10/operations/node_management/configure_ntp_on_ncns/</guid>
      <description>Configure NTP on NCNs The management nodes serve Network Time Protocol (NTP) at stratum 3, and all management nodes peer with each other. Currently, the PIT node booted from the LiveCD does not run NTP, but the other nodes do when they are booted. NTP is currently allowed on the Node Management Network (NMN) and Hardware Management Network (HMN).
The NTP peers are set in the data.json file, which is normally created during an initial install.</description>
    </item>
    
    <item>
      <title>Disable Nodes</title>
      <link>/docs-csm/en-10/operations/node_management/disable_nodes/</link>
      <pubDate>Thu, 14 Oct 2021 05:31:05 +0000</pubDate>
      
      <guid>/docs-csm/en-10/operations/node_management/disable_nodes/</guid>
      <description>Disable Nodes Use the Hardware State Manager (HSM) Cray CLI commands to disable nodes on the system.
Disabling nodes that are not configured correctly allows the system to successfully boot.
Prerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system.  Procedure   Disable individual nodes with HSM.
ncn-m001# cray hsm state components enabled update --enabled false XNAME   Verify the desired nodes are disabled.</description>
    </item>
    
    <item>
      <title>Dump A Non-compute Node</title>
      <link>/docs-csm/en-10/operations/node_management/dump_a_non-compute_node/</link>
      <pubDate>Thu, 14 Oct 2021 05:31:05 +0000</pubDate>
      
      <guid>/docs-csm/en-10/operations/node_management/dump_a_non-compute_node/</guid>
      <description>Dump a Non-Compute Node Trigger an NCN memory dump and send the dump for analysis. This procedure is helpful for debugging NCN crashes.
Prerequisites A non-compute node (NCN) has crashed or an admin has triggered a node crash.
Procedure   Force a dump on an NCN.
ncn-m001# echo c &amp;gt; /proc/sysrq-trigger   Wait for the node to reboot.
The NCN dump is stored in /var/crash is on local disk after the node is rebooted.</description>
    </item>
    
    <item>
      <title>Enable Nodes</title>
      <link>/docs-csm/en-10/operations/node_management/enable_nodes/</link>
      <pubDate>Thu, 14 Oct 2021 05:31:05 +0000</pubDate>
      
      <guid>/docs-csm/en-10/operations/node_management/enable_nodes/</guid>
      <description>Enable Nodes Use the Hardware State Manager (HSM) Cray CLI commands to enable nodes on the system.
Enabling nodes that are available provides an accurate system configuration and node map.
Prerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system.  Procedure   Enable individual nodes with HSM.
ncn-m001# cray hsm state components enabled update --enabled true XNAME   Verify the desired nodes are enabled.</description>
    </item>
    
    <item>
      <title>Enable Passwordless Connections To Liquid Cooled Node BMCs</title>
      <link>/docs-csm/en-10/operations/node_management/enable_passwordless_connections_to_liquid_cooled_node_bmcs/</link>
      <pubDate>Thu, 14 Oct 2021 05:31:05 +0000</pubDate>
      
      <guid>/docs-csm/en-10/operations/node_management/enable_passwordless_connections_to_liquid_cooled_node_bmcs/</guid>
      <description>Enable Passwordless Connections to Liquid Cooled Node BMCs Set the passwordless SSH keys for the root account and/or console of all liquid-cooled Baseboard Management Controllers (BMCs) on the system. This procedure will not work on BMCs for air-cooled hardware.
Warning: If admin uses SCSD to update the SSHConsoleKey value outside of ConMan, it will disrupt the ConMan connection to the console and collection of console logs. Refer to ConMan for more information.</description>
    </item>
    
    <item>
      <title>Find Node Type And Manufacturer</title>
      <link>/docs-csm/en-10/operations/node_management/find_node_type_and_manufacturer/</link>
      <pubDate>Thu, 14 Oct 2021 05:31:05 +0000</pubDate>
      
      <guid>/docs-csm/en-10/operations/node_management/find_node_type_and_manufacturer/</guid>
      <description>Find Node Type and Manufacturer There are three different vendors providing nodes for air-cooled cabinets, which are Gigabyte, Intel, and HPE. The Hardware State Manager (HSM) contains the information required to determine which type of air-cooled node is installed. The endpoint returned in the HSM command can be used to determine the manufacturer.
HPE nodes contain the /redfish/v1/Systems/1 endpoint:
ncn-m001# cray hsm inventory componentEndpoints describe XNAME | jq &#39;.RedfishURL&#39; &amp;quot;x3000c0s18b0/redfish/v1/Systems/1&amp;quot; Gigabyte nodes contain the /redfish/v1/Systems/Self endpoint:</description>
    </item>
    
    <item>
      <title>Launch A Virtual KVM On Gigabyte Servers</title>
      <link>/docs-csm/en-10/operations/node_management/launch_a_virtual_kvm_on_gigabyte_servers/</link>
      <pubDate>Thu, 14 Oct 2021 05:31:05 +0000</pubDate>
      
      <guid>/docs-csm/en-10/operations/node_management/launch_a_virtual_kvm_on_gigabyte_servers/</guid>
      <description>Launch a Virtual KVM on Gigabyte Servers This procedure shows how to launch a virtual KVM to connect to Gigabyte server. The virtual KVM can be launched on any host that is on the same network as the server&amp;rsquo;s BMC. This method of connecting to a server is frequently used during system installation.
Prerequisites  A laptop or workstation with a browser and access to the Internet The externally visible BMC IP address of the server&amp;rsquo;s integrated BMC  Procedure   Connect to the server&amp;rsquo;s BMC by entering the externally visible BMC IP address in the address bar of a web browser.</description>
    </item>
    
    <item>
      <title>Launch A Virtual KVM On Intel Servers</title>
      <link>/docs-csm/en-10/operations/node_management/launch_a_virtual_kvm_on_intel_servers/</link>
      <pubDate>Thu, 14 Oct 2021 05:31:05 +0000</pubDate>
      
      <guid>/docs-csm/en-10/operations/node_management/launch_a_virtual_kvm_on_intel_servers/</guid>
      <description>Launch a Virtual KVM on Intel Servers This procedure shows how to launch a virtual KVM to connect to an Intel server. The virtual KVM can be launched on any host that is on the same network as the server&amp;rsquo;s BMC. This method of connecting to a server is frequently used during system installation.
Prerequisites  A laptop or workstation with a browser and access to the Internet The externally visible BMC IP address of the server&amp;rsquo;s integrated BMC  Procedure   Connect to the server&amp;rsquo;s BMC by entering the externally visible BMC IP address in the address bar of a web browser.</description>
    </item>
    
    <item>
      <title>Move A Standard Rack Node</title>
      <link>/docs-csm/en-10/operations/node_management/move_a_standard_rack_node/</link>
      <pubDate>Thu, 14 Oct 2021 05:31:05 +0000</pubDate>
      
      <guid>/docs-csm/en-10/operations/node_management/move_a_standard_rack_node/</guid>
      <description>Move a Standard Rack Node Update the location-based xname for a standard rack node within the system.
Prerequisites   An authentication token has been retrieved.
ncn-m001# function get_token () { ADMIN_SECRET=$(kubectl get secrets admin-client-auth -ojsonpath=&#39;{.data.client-secret}&#39; | base64 -d) curl -s -d grant_type=client_credentials -d client_id=admin-client -d client_secret=$ADMIN_SECRET \ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token \ | python -c &#39;import sys, json; print json.load(sys.stdin)[&amp;quot;access_token&amp;quot;]&#39; }   The Cray command line interface (CLI) tool is initialized and configured on the system.</description>
    </item>
    
    <item>
      <title>Move A Standard Rack Node (same Rack/same HSN Ports)</title>
      <link>/docs-csm/en-10/operations/node_management/move_a_standard_rack_node_samerack_samehsnports/</link>
      <pubDate>Thu, 14 Oct 2021 05:31:05 +0000</pubDate>
      
      <guid>/docs-csm/en-10/operations/node_management/move_a_standard_rack_node_samerack_samehsnports/</guid>
      <description>Move a Standard Rack Node (Same Rack/Same HSN Ports) This procedure move standard rack UAN or compute node to a different location and uses the same Slingshot switch ports and management network ports.
Update the location-based xname for a standard rack node within the system.
Prerequisites   An authentication token has been retrieved.
ncn-m001# function get_token () { ADMIN_SECRET=$(kubectl get secrets admin-client-auth -ojsonpath=&amp;#39;{.data.client-secret}&amp;#39; | base64 -d) curl -s -d grant_type=client_credentials -d client_id=admin-client -d client_secret=$ADMIN_SECRET https://api-gw-service-nmn.</description>
    </item>
    
    <item>
      <title>Node Management</title>
      <link>/docs-csm/en-10/operations/node_management/node_management/</link>
      <pubDate>Thu, 14 Oct 2021 05:31:05 +0000</pubDate>
      
      <guid>/docs-csm/en-10/operations/node_management/node_management/</guid>
      <description>Node Management The HPE Cray EX system includes two types of nodes:
 Compute Nodes, where high performance computing applications are run, and node names in the form of nidXXXXXX Non-Compute Nodes (NCNs), which carry out system functions and come in three versions:  Master nodes, with names in the form of ncn-mXXX Worker nodes, with names in the form of ncn-wXXX Utility Storage nodes, with names in the form of ncn-sXXX    The HPE Cray EX system includes the following nodes:</description>
    </item>
    
    <item>
      <title>Node Management Workflows</title>
      <link>/docs-csm/en-10/operations/node_management/node_management_workflows/</link>
      <pubDate>Thu, 14 Oct 2021 05:31:05 +0000</pubDate>
      
      <guid>/docs-csm/en-10/operations/node_management/node_management_workflows/</guid>
      <description>Node Management Workflows The following workflows are intended to be high-level overviews of node management tasks. These workflows depict how services interact with each other during node management and help to provide a quicker and deeper understanding of how the system functions.
The workflows and procedures in this section include:
 Add Nodes Remove Nodes Replace Nodes Move Nodes  Add Nodes  Add a Standard Rack Node  Use Cases: Administrator permanently adds select compute nodes to expand the system.</description>
    </item>
    
    <item>
      <title>Reboot NCNs</title>
      <link>/docs-csm/en-10/operations/node_management/reboot_ncns/</link>
      <pubDate>Thu, 14 Oct 2021 05:31:05 +0000</pubDate>
      
      <guid>/docs-csm/en-10/operations/node_management/reboot_ncns/</guid>
      <description>Reboot NCNs The following is a high-level overview of the non-compute node (NCN) reboot workflow:
 Run the NCN pre-reboot checks and procedures:  Ensure ncn-m001 is not running in &amp;ldquo;LiveCD&amp;rdquo; or install mode Check the metal.no-wipe settings for all NCNs Run all platform health checks, including checks on the Border Gateway Protocol (BGP) peering sessions Validate the current boot order   Run the rolling NCN reboot procedure:  Loop through reboots on storage nodes, worker nodes, and master nodes, where each boot consists of the following workflow: - Establish console session with node to reboot - Execute a Linux graceful shutdown or power off/on sequence to the node to allow it to boot up to completion - Execute NCN/platform health checks and do not go on to reboot the next NCN until health has been ensured on the most recently rebooted NCN - Disconnect console session with the node that was rebooted   Re-run all platform health checks, including checks on BGP peering sessions  The time duration for this procedure (if health checks are being executed in between each boot, as recommended) could take between two to four hours for a system with nine management nodes.</description>
    </item>
    
    <item>
      <title>Rebuild NCNs</title>
      <link>/docs-csm/en-10/operations/node_management/rebuild_ncns/</link>
      <pubDate>Thu, 14 Oct 2021 05:31:05 +0000</pubDate>
      
      <guid>/docs-csm/en-10/operations/node_management/rebuild_ncns/</guid>
      <description>Rebuild NCNs Rebuild a master, worker, or storage non-compute node (NCN). Use this procedure in the event that a node has a hardware failure, or some other issue with the node has occurred that warrants rebuilding the node.
The following is a high-level overview of the NCN rebuild workflow:
 Prepare Node  There is a different procedure for each type of node (worker, master, and storage).   Identify Node and Update Metadata  Same procedure for all node types.</description>
    </item>
    
    <item>
      <title>Access And Update Settings For Replacement NCNs</title>
      <link>/docs-csm/en-10/operations/node_management/access_and_update_the_settings_for_replacement_ncns/</link>
      <pubDate>Thu, 14 Oct 2021 05:31:04 +0000</pubDate>
      
      <guid>/docs-csm/en-10/operations/node_management/access_and_update_the_settings_for_replacement_ncns/</guid>
      <description>Access and Update Settings for Replacement NCNs When a new NCN is added to the system as a hardware replacement, it might use the default credentials. Contact HPE Cray service to learn what these are.
Use this procedure to verify that the default BMC credentials are set correctly after a replacement NCN is installed, cabled, and powered on.
All NCN BMCs must have credentials set up for ipmitool access.
Prerequisites A new non-compute node (NCN) has been added to the system as a hardware replacement.</description>
    </item>
    
    <item>
      <title>Add A Standard Rack Node</title>
      <link>/docs-csm/en-10/operations/node_management/add_a_standard_rack_node/</link>
      <pubDate>Thu, 14 Oct 2021 05:31:04 +0000</pubDate>
      
      <guid>/docs-csm/en-10/operations/node_management/add_a_standard_rack_node/</guid>
      <description>Add a Standard Rack Node These procedures are intended for trained technicians and support personnel only. Always follow ESD precautions when handling this equipment.
  An authentication token has been retrieved.
ncn-m001# function get_token () { ADMIN_SECRET=$(kubectl get secrets admin-client-auth -ojsonpath=&#39;{.data.client-secret}&#39; | base64 -d) curl -s -d grant_type=client_credentials -d client_id=admin-client -d client_secret=$ADMIN_SECRET https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | python -c &#39;import sys, json; print json.load(sys.stdin)[&amp;quot;access_token&amp;quot;]&#39; }   The example is this procedure adds a User Access Node (UAN) or compute node to an HPE Cray standard rack system.</description>
    </item>
    
    <item>
      <title>Add TLS Certificates To BMCs</title>
      <link>/docs-csm/en-10/operations/node_management/add_tls_certificates_to_bmcs/</link>
      <pubDate>Thu, 14 Oct 2021 05:31:04 +0000</pubDate>
      
      <guid>/docs-csm/en-10/operations/node_management/add_tls_certificates_to_bmcs/</guid>
      <description>Add TLS Certificates to BMCs Use the System Configuration Service (SCSD) tool to create TLS certificates and store them in Vault secure storage. Once certificates are created, they are placed on to the target BMCs.
Prerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system.  Limitations TLS certificates can only be set for liquid-cooled BMCs. TLS certificate support for air-cooled BMCs is not supported in release 1.</description>
    </item>
    
    <item>
      <title>Change Interfaces In The Bond</title>
      <link>/docs-csm/en-10/operations/node_management/change_interfaces_in_the_bond/</link>
      <pubDate>Thu, 14 Oct 2021 05:31:04 +0000</pubDate>
      
      <guid>/docs-csm/en-10/operations/node_management/change_interfaces_in_the_bond/</guid>
      <description>Change Interfaces in the Bond Configure the interfaces for bond0 on ncn-w001 and establish an &amp;ldquo;up&amp;rdquo; bond0 on all other NCNs.
Prerequisites This procedure requires administrative privileges.
Procedure   Customize the interfaces applied by Ansible for the bond configuration by editing the nics.yml file in the NCN group variables.
ncn-w001# vi /opt/cray/crayctl/files/group_vars/ncn/nics.yml ... # Interface configuration. # Whether to use the bond or not during NCN installs. ansible_hw_bond_enabled: yes # Use Jumboframes if networking is configured as such.</description>
    </item>
    
    <item>
      <title>Change Java Security Settings</title>
      <link>/docs-csm/en-10/operations/node_management/change_java_security_settings/</link>
      <pubDate>Thu, 14 Oct 2021 05:31:04 +0000</pubDate>
      
      <guid>/docs-csm/en-10/operations/node_management/change_java_security_settings/</guid>
      <description>Change Java Security Settings If Java will not allow a connection to an Intel server via SOL or iKVM, change Java security settings to add an exception for the server&amp;rsquo;s BMC IP address.
The Intel servers ship with an insecure certificate, which causes an exception for Java when trying to connect via SOL or iKVM to these servers. The workaround is to add the server&amp;rsquo;s BMC IP address to the Exception Site List in the Java Control Panel of the machine attempting to connect to the Intel server.</description>
    </item>
    
    <item>
      <title>Change Settings For Hms Collector Polling Of Air Cooled Nodes</title>
      <link>/docs-csm/en-10/operations/node_management/change_settings_for_hms_collector_polling_of_air_cooled_nodes/</link>
      <pubDate>Thu, 14 Oct 2021 05:31:04 +0000</pubDate>
      
      <guid>/docs-csm/en-10/operations/node_management/change_settings_for_hms_collector_polling_of_air_cooled_nodes/</guid>
      <description>Change Settings for HMS Collector Polling of Air Cooled Nodes The cray-hms-hmcollector service polls all Air Cooled hardware to gather the necessary telemetry information for use by other services, such as the Cray Advanced Platform Monitoring and Control (CAPMC) service. This polling occurs every 10 seconds on a continual basis. Instabilities with the AMI Redfish implementation in the Gigabyte BMCs requires a less significant approach when gathering power and temperature telemetry data.</description>
    </item>
    
    <item>
      <title>Check And Set The `metal.no-wipe` Setting On NCNs</title>
      <link>/docs-csm/en-10/operations/node_management/check_and_set_the_metalno-wipe_setting_on_ncns/</link>
      <pubDate>Thu, 14 Oct 2021 05:31:04 +0000</pubDate>
      
      <guid>/docs-csm/en-10/operations/node_management/check_and_set_the_metalno-wipe_setting_on_ncns/</guid>
      <description>Check and Set the metal.no-wipe Setting on NCNs Configure the metal.no-wipe setting on non-compute nodes (NCNs) to preserve data on the nodes before doing an NCN reboot.
Run the ./ncnGetXnames.shscript to view the metal.no-wipe settings for each NCN. The xname and metal.no-wipe settings are also dumped out when executing the /opt/cray/platform-utils/ncnHealthChecks.sh script.
Prerequisites This procedure requires administrative privileges.
Procedure   Change to the /opt/cray/platform-utils directory on any master or worker NCN.</description>
    </item>
    
    <item>
      <title>Check The BMC Failover Mode</title>
      <link>/docs-csm/en-10/operations/node_management/check_the_bmc_failover_mode/</link>
      <pubDate>Thu, 14 Oct 2021 05:31:04 +0000</pubDate>
      
      <guid>/docs-csm/en-10/operations/node_management/check_the_bmc_failover_mode/</guid>
      <description>Check the BMC Failover Mode Gigabyte BMCs must have their failover mode disabled to prevent incorrect network assignment.
If Gigabyte BMC failover mode is not disabled, some BMCs may receive incorrect IP addresses. Specifically, a BMC may request an IP address on the wrong subnet and be unable to re-acquire a new IP address on the correct subnet. If this occurs, administrators should ensure that the impacted BMC has its failover feature disabled.</description>
    </item>
    
    <item>
      <title>Clear Space In Root File System On Worker Nodes</title>
      <link>/docs-csm/en-10/operations/node_management/clear_space_in_root_file_system_on_worker_nodes/</link>
      <pubDate>Thu, 14 Oct 2021 05:31:04 +0000</pubDate>
      
      <guid>/docs-csm/en-10/operations/node_management/clear_space_in_root_file_system_on_worker_nodes/</guid>
      <description>Clear Space in Root File System on Worker Nodes The disk space on an NCN worker node can fill up if any services are consuming a large portion of the root file system on the node. This procedure shows how to safely clear some space on worker nodes to return them to an appropriate storage threshold.
Prerequisites An NCN worker node has a full disk.
Procedure   Check to see if Docker is running.</description>
    </item>
    
  </channel>
</rss>
