<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Install on Cray System Management (CSM)</title>
    <link>/docs-csm/en-11/install/</link>
    <description>Recent content in Install on Cray System Management (CSM)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Sat, 02 Oct 2021 03:05:10 +0000</lastBuildDate><atom:link href="/docs-csm/en-11/install/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Shcd Hmn Tab/hmn Connections Rules</title>
      <link>/docs-csm/en-11/install/shcd_hmn_connections_rules/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:11 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/shcd_hmn_connections_rules/</guid>
      <description>SHCD HMN Tab/HMN Connections Rules Table of contents:  Introduction Compute Node  Dense 4 node chassis - Gigabyte or Intel chassis Single node chassis - Apollo 6500 XL675D Dual node chassis - Apollo 6500 XL645D   Chassis Management Controller (CMC) Management Node  Master Worker Storage   Application Node  Single Node Chassis  Building xnames for nodes in a single application node chassis   Dual Node Chassis  Building xnames for nodes in a dual application node chassis     Columbia Slingshot Switch PDU Cabinet Controller Cooling Door Management Switches  Introduction The HMN tab of the SHCD describes the air-cooled hardware present in the system and how these devices are connected to the Hardware Management Network (HMN).</description>
    </item>
    
    <item>
      <title>Switch PXE Boot From Onboard NIC To Pcie</title>
      <link>/docs-csm/en-11/install/switch_pxe_boot_from_onboard_nic_to_pcie/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:11 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/switch_pxe_boot_from_onboard_nic_to_pcie/</guid>
      <description>Switch PXE Boot from Onboard NIC to PCIe This page details how to migrate NCNs from using their onboard NICs for PXE booting to booting over the PCIe cards.
 Enabling UEFI PXE Mode  Mellanox  Print current UEFI and SR-IOV State Setting Expected Values High-Speed Network Obtaining Mellanox Tools   QLogic FastLinq  Kernel Modules     Disabling/Removing On-Board Connections  This applies to Newer systems (Spring 2020 or newer) where onboard NICs are still used.</description>
    </item>
    
    <item>
      <title>Troubleshooting Installation Problems</title>
      <link>/docs-csm/en-11/install/troubleshooting_installation/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:11 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/troubleshooting_installation/</guid>
      <description>Troubleshooting Installation Problems The installation of the Cray System Management (CSM) product requires knowledge of the various nodes and switches for the HPE Cray EX system. The procedures in this section should be referenced during the CSM install for additional information on system hardware, troubleshooting, and administrative tasks related to CSM.
Topics:  Reset root Password on LiveCD Reinstall LiveCD PXE Boot Troubleshooting Wipe NCN Disks for Reinstallation Restart Network Services and Interfaces on NCNs Utility Storage Node Installation Troubleshooting Ceph CSI Troubleshooting Safeguards for CSM NCN Upgrades  Details   Reset root Password on LiveCD</description>
    </item>
    
    <item>
      <title>Update NCN Firmware</title>
      <link>/docs-csm/en-11/install/update_ncn_firmware/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:11 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/update_ncn_firmware/</guid>
      <description>Update NCN Firmware Firmware and BIOS updates for the management nodes may be necessary before an install can progress.
Only non-compute nodes (NCNs) can upgrade firmware during a CSM install. Other devices, such as compute nodes or application nodes, have their upgrades managed by FAS once all CSM software and services have been installed.
FAS tracks and performs actions (upgrade, downgrade, restore, and create snapshots) on system firmware. FAS is a runtime service deployed in Kubernetes.</description>
    </item>
    
    <item>
      <title>Utility Storage Installation Troubleshooting</title>
      <link>/docs-csm/en-11/install/utility_storage_node_installation_troubleshooting/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:11 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/utility_storage_node_installation_troubleshooting/</guid>
      <description>Utility Storage Installation Troubleshooting If there is a failure in the creation of Ceph storage on the utility storage nodes for one of these scenarios, the Ceph storage might need to be reinitialized.
Topics  Scenario 1 (Shasta v1.4 only) Scenario 2 (Shasta v1.5 only)  Details Scenario 1 (Shasta 1.4 only) IMPORTANT (FOR NODE INSTALLS/REINSTALLS ONLY): If the Ceph install failed please check the following
ncn-s# ceph osd tree ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 83.</description>
    </item>
    
    <item>
      <title>Validate Management Network Cabling</title>
      <link>/docs-csm/en-11/install/validate_management_network_cabling/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:11 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/validate_management_network_cabling/</guid>
      <description>Validate Management Network Cabling This page is designed to be a guide on how all nodes in a Shasta system are wired to the management network.
The Shasta Cabling Diagram (SHCD) for this system describes how the cables connect the nodes to the management network switches and the connections between the different types of management network switches. Having SHCD data which matches how the physical system is cabled will be needed later when preparing the hmn_connections.</description>
    </item>
    
    <item>
      <title>Wipe NCN Disks For Reinstallation</title>
      <link>/docs-csm/en-11/install/wipe_ncn_disks_for_reinstallation/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:11 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/wipe_ncn_disks_for_reinstallation/</guid>
      <description>Wipe NCN Disks for Reinstallation This page will detail how disks are wiped and includes workarounds for wedged disks. Any process covered on this page will be covered by the installer.
 Everything in this section should be considered DESTRUCTIVE.
 After following these procedures an NCN can be rebooted and redeployed.
Ideally the Basic Wipe is enough, and should be tried first. All types of disk wipe can be run from Linux or an initramFS/initrd emergency shell.</description>
    </item>
    
    <item>
      <title>Create Switch Metadata Csv</title>
      <link>/docs-csm/en-11/install/create_switch_metadata_csv/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:10 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/create_switch_metadata_csv/</guid>
      <description>Create Switch Metadata CSV This page provides directions on constructing the switch_metadata.csv file.
This file is manually created to include information about all spine, leaf, CDU, and aggregation switches in the system. None of the Slingshot switches for the HSN should be included in this file.
The file should have the following format, in ascending order by Xname:
Switch Xname,Type,Brand d0w1,CDU,Dell d0w2,CDU,Dell x3000c0w38,Leaf,Dell x3000c0w36,Leaf,Dell x3000c0h33s1,Spine,Mellanox x3000c0h34s1,Spine,Mellanox The above file would lead to this pairing between component name and hostname:</description>
    </item>
    
    <item>
      <title>Deploy Management Nodes</title>
      <link>/docs-csm/en-11/install/deploy_management_nodes/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:10 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/deploy_management_nodes/</guid>
      <description>Deploy Management Nodes The following procedure deploys Linux and Kubernetes software to the management NCNs. Deployment of the nodes starts with booting the storage nodes followed by the master nodes and worker nodes together. After the operating system boots on each node, there are some configuration actions which take place. Watching the console or the console log for certain nodes can help to understand what happens and when. When the process completes for all nodes, the Ceph storage is initialized and the Kubernetes cluster is created and ready for a workload.</description>
    </item>
    
    <item>
      <title>Install Services</title>
      <link>/docs-csm/en-11/install/install_csm_services/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:10 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/install_csm_services/</guid>
      <description>Install CSM Services This procedure will install CSM applications and services into the CSM Kubernetes cluster.
 Node: Check the information in Known Issues before starting this procedure to be warned about possible problems.
 Topics:  Initialize Bootstrap Registry Create Site-Init Secret Deploy Sealed Secret Decryption Key Deploy CSM Applications and Services Setup Nexus Set NCNs to use Unbound Apply Pod Priorities Apply After Sysmgmt Manifest Workarounds Known Issues  Error: not ready: https://packages.</description>
    </item>
    
    <item>
      <title>Prepare Compute Nodes</title>
      <link>/docs-csm/en-11/install/prepare_compute_nodes/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:10 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/prepare_compute_nodes/</guid>
      <description>Prepare Compute nodes Some compute nodes types have special preparation steps, but most compute nodes are ready to be used now These nodes have an additional procedure before they can be booted.
Topics:  Configure HPE Apollo 6500 XL645d Gen10 Plus Compute Nodes  Prerequisites The time for Gigabyte compute nodes is synced with the rest of the system. See Update the Gigabyte Server BIOS Time.
Details 1. Configure HPE Apollo 6500 XL645d Gen10 Plus Compute Nodes The HPE Apollo 6500 XL645d Gen10 Plus compute node uses a NIC/shared iLO network port.</description>
    </item>
    
    <item>
      <title>Prepare Configuration Payload</title>
      <link>/docs-csm/en-11/install/prepare_configuration_payload/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:10 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/prepare_configuration_payload/</guid>
      <description>Prepare Configuration Payload The configuration payload consists of the information which must be known about the HPE Cray EX system so it can be passed to the csi (Cray Site Init) program during the CSM installation process.
Information gathered from a site survey is needed to feed into the CSM installation process, such as system name, system size, site network information for the CAN, site DNS configuration, site NTP configuration, network information for the node used to bootstrap the installation.</description>
    </item>
    
    <item>
      <title>Prepare Management Nodes</title>
      <link>/docs-csm/en-11/install/prepare_management_nodes/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:10 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/prepare_management_nodes/</guid>
      <description>Prepare Management Nodes Some preparation of the management nodes might be needed before starting an install or reinstall.
For either scenario, the BMC and BIOS firmware should be checked and may need to be updated on the node which will become the PIT node. If necessary, Set Gigabyte Node BMC to Factory Defaults.
For an install, if the system ever had any previous software installed, then storage might need to be wiped on management nodes.</description>
    </item>
    
    <item>
      <title>Prepare Site Init</title>
      <link>/docs-csm/en-11/install/prepare_site_init/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:10 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/prepare_site_init/</guid>
      <description>Prepare Site Init These procedures guide administrators through setting up the site-init directory which contains important customizations for various products. The appendix is informational only; it does not include any default install procedures.
Topics:  Background Create and Initialize Site-Init Directory Create Baseline System Customizations Generate Sealed Secrets Version Control Site-Init Files  Push to a Remote Repository   Patch cloud-init with the CA Customer-Specific Customizations  Details 1. Background The shasta-cfg directory included in CSM includes relatively static, installation-centric artifacts such as:</description>
    </item>
    
    <item>
      <title>PXE Boot Troubleshooting</title>
      <link>/docs-csm/en-11/install/pxe_boot_troubleshooting/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:10 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/pxe_boot_troubleshooting/</guid>
      <description>PXE Boot Troubleshooting This page is designed to cover various issues that arise when trying to PXE boot nodes in a Shasta system.
In order for PXE booting to work successfully, the management network switches need to be configured correctly.
Configuration required for PXE booting To successfully PXE boot nodes, the following is required.
 The IP helper-address must be configured on VLAN 1,2,4,7. This will be where the layer 3 gateway exists (spine or aggregation) The virtual-IP/VSX/MAGP IP address must be configured on VLAN 1,2,4,7.</description>
    </item>
    
    <item>
      <title>Redeploy Pit Node</title>
      <link>/docs-csm/en-11/install/redeploy_pit_node/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:10 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/redeploy_pit_node/</guid>
      <description>Redeploy PIT Node The following procedure contains information for rebooting and deploying the management node that is currently hosting the LiveCD. This assists with remote-console setup to aid in observing the reboot. At the end of this procedure, the LiveCD will no longer be active. The node it was using will join the Kubernetes cluster as the final of three master nodes forming a quorum.
Important: While the node is rebooting, it will be available only through Serial-over-LAN and local terminals.</description>
    </item>
    
    <item>
      <title>Reinstall Livecd</title>
      <link>/docs-csm/en-11/install/reinstall_livecd/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:10 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/reinstall_livecd/</guid>
      <description>Reinstall LiveCD Setup a re-install of LiveCD on a node using the previous configuration.
  Backup to the data partition:
pit# mkdir -pv /var/www/ephemeral/backup pit# pushd /var/www/ephemeral/backup pit# tar -czvf &amp;#34;dnsmasq-data-$(date &amp;#39;+%Y-%m-%d_%H-%M-%S&amp;#39;).tar.gz&amp;#34; /etc/dnsmasq.* pit# tar -czvf &amp;#34;network-data-$(date &amp;#39;+%Y-%m-%d_%H-%M-%S&amp;#39;).tar.gz&amp;#34; /etc/sysconfig/network/* pit# cp -pv /etc/hosts ./ pit# popd pit# umount -v /var/www/ephemeral   Unplug the USB device.
The USB device should now contain all the information already loaded, as well as the backups of the initialized files.</description>
    </item>
    
    <item>
      <title>Reset Root Password On Livecd</title>
      <link>/docs-csm/en-11/install/reset_root_password_on_livecd/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:10 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/reset_root_password_on_livecd/</guid>
      <description>Reset root Password on LiveCD It may become desirable to clear the password on the LiveCD.
The root password is preserved within the COW partition at cow:rw/etc/shadow. This is the modified copy of the /etc/shadow file used by the operating system.
If a site/user needs to reset/clear the password for root, they can mount their USB on another machine and remove this file from the COW partition. When next booting from the USB it will reinitialize to an empty password for root, and again at next login it will require the password to be changed.</description>
    </item>
    
    <item>
      <title>Restart Network Services And Interfaces On NCNs</title>
      <link>/docs-csm/en-11/install/restart_network_services_and_interfaces_on_ncns/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:10 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/restart_network_services_and_interfaces_on_ncns/</guid>
      <description>Restart Network Services and Interfaces on NCNs Interfaces within the network stack can be reloaded or reset to fix wedged interfaces. The NCNs have network device names set during first boot. The names vary based on the available hardware. For more information, see NCN Networking. Any process covered on this page will be covered by the installer.
The use cases for resetting services:
 Interfaces not showing up IP Addresses not applying Member/children interfaces not being included  Topics:  Restart Network Services and Interfaces))) Command Reference  Check interface status (up/down/broken) Show routing and status for all devices Print real devices ( ignore no-device ) Show the currently enabled network service (Wicked or Network Manager)    Restart Network Services There are a few daemons that make up the SUSE network stack.</description>
    </item>
    
    <item>
      <title>Safeguards For</title>
      <link>/docs-csm/en-11/install/safeguards_for_csm_ncn_upgrades/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:10 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/safeguards_for_csm_ncn_upgrades/</guid>
      <description>Safeguards for CSM This page covers safe-guards for preventing destructive behaviors on management nodes.
If you are reinstalling or upgrading you should run through these safe-guards on a by-case basis:
 Whether or not CEPH should be preserved. Whether or not the RAIDs should be protected.  Safeguard CEPH OSDs   Edit /var/www/ephemeral/configs/data.json and align the following options:
{ .. // Disables Ceph wipe: &amp;#34;wipe-ceph-osds&amp;#34;: &amp;#34;no&amp;#34; .. } { .</description>
    </item>
    
    <item>
      <title>Set Gigabyte Node BMC To Factory Defaults</title>
      <link>/docs-csm/en-11/install/set_gigabyte_node_bmc_to_factory_defaults/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:10 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/set_gigabyte_node_bmc_to_factory_defaults/</guid>
      <description>Set Gigabyte Node BMC to Factory Defaults Prerequisites Use the management scripts and text files to reset Gigabyte BMC to factory default settings. Set the BMC to the factory default settings in the following cases:
 There are problems using the ipmitool command and Redfish does not respond There are problems using the ipmitool command and Redfish is running When BIOS or BMC flash procedures fail using Redfish  Run the do_bmc_factory_default.</description>
    </item>
    
    <item>
      <title>Collecting NCN Mac Addresses</title>
      <link>/docs-csm/en-11/install/collecting_ncn_mac_addresses/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:09 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/collecting_ncn_mac_addresses/</guid>
      <description>Collecting NCN MAC Addresses This procedure will detail how to collect the NCN MAC addresses from a Shasta system. After completing this procedure, you will have the MAC addresses needed for the Bootstrap MAC, Bond0 MAC0, and Bond0 MAC1 columns in ncn_metadata.csv.
The Bootstrap MAC address will be used for identification of this node during the early part of the PXE boot process before the bonded interface can be established. The Bond0 MAC0 and Bond0 MAC1 are the MAC addresses for the physical interfaces that your node will use for the various VLANs.</description>
    </item>
    
    <item>
      <title>Configure Administrative Access</title>
      <link>/docs-csm/en-11/install/configure_administrative_access/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:09 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/configure_administrative_access/</guid>
      <description>Configure Administrative Access There are several operations which configure administrative access to different parts of the system. Ensuring that the cray CLI can be used by administrative credentials enables use of many management services via commands. The management nodes can be locked from accidental manipulation by the cray capmc and cray fas commands when the intent is to work on the entire system except the management nodes. The cray scsd command can change the SSH keys, NTP server, syslog server, and BMC/controller passwords.</description>
    </item>
    
    <item>
      <title>Configure Aruba Aggregation Switch</title>
      <link>/docs-csm/en-11/install/configure_aruba_aggregation_switch/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:09 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/configure_aruba_aggregation_switch/</guid>
      <description>Configure Aruba Aggregation Switch This page describes how Aruba aggregation switches are configured.
Management nodes and Application nodes will be plugged into aggregation switches.
Switch Models used: JL635A Aruba 8325-48Y8C
They run in a high availability pair and use VSX to provide redundancy.
Requirements:
 Three connections between the switches, two of these are used for the ISL (Inter switch link) and one used for the keepalive. The ISL uses 100GB ports and the keepalive will be a 25 GB port.</description>
    </item>
    
    <item>
      <title>Configure Aruba Cdu Switch</title>
      <link>/docs-csm/en-11/install/configure_aruba_cdu_switch/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:09 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/configure_aruba_cdu_switch/</guid>
      <description>Configure Aruba CDU Switch This page describes how Aruba CDU switches are configured.
CDU switches are located in liquid-cooled cabinets and provide connectivity to MTN (Mountain) components. CDU switches act as leaf switches in the architecture. Aruba JL720A 8360-48XT4 is the model used. They run in a high availability pair and use VSX to provide redundancy.
Requirements:
 Two uplinks from each CDU switch to the upstream switch, this is normally a spine switch.</description>
    </item>
    
    <item>
      <title>Configure Aruba Leaf Switch</title>
      <link>/docs-csm/en-11/install/configure_aruba_leaf_switch/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:09 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/configure_aruba_leaf_switch/</guid>
      <description>Configure Aruba Leaf Switch This page describes how Aruba leaf switches are configured.
Leaf switches are located in air-cooled cabinets and provide connectivity to components in those cabinets. Aruba JL762A 6300M 48G 4SFP56 is the model used.
Requirements: - Two uplinks from the switch to the upstream switch, this is can be an aggregation switch or a spine switch.
Here are example snippets from a leaf switch in the SHCD.</description>
    </item>
    
    <item>
      <title>Configure Aruba Management Network Base</title>
      <link>/docs-csm/en-11/install/configure_aruba_management_network_base/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:09 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/configure_aruba_management_network_base/</guid>
      <description>Configure Aruba Management Network Base This page provides instructions on how to setup the base network configuration of the Shasta Management network.
With the base config applied you will be able to access all Management switches and apply the remaining configuration.
Requirements  Console access to all of the switches SHCD available  Configuration Once you have console access to the switches you can begin by applying the base config. The purpose of this configuration is to have an IPv6 underlay that allows us to always be able to access the management switches.</description>
    </item>
    
    <item>
      <title>Configure Aruba Spine Switch</title>
      <link>/docs-csm/en-11/install/configure_aruba_spine_switch/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:09 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/configure_aruba_spine_switch/</guid>
      <description>Configure Aruba Spine Switch This page describes how Aruba spine switches are configured.
Depending on the size of the Shasta system the spine switches will serve different purposes. On TDS systems, the NCNs will plug directly into the spine switches, on larger systems with aggregation switches, the spine switches will provide connection between the aggregation switches.
Switch Models used JL635A Aruba 8325-48Y8C and JL636A Aruba 8325-32C
They run in a high availability pair and use VSX to provide redundancy.</description>
    </item>
    
    <item>
      <title>Configure Dell Aggregation Switch</title>
      <link>/docs-csm/en-11/install/configure_dell_aggregation_switch/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:09 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/configure_dell_aggregation_switch/</guid>
      <description>Configure Dell Aggregation Switch This page describes how Dell aggregation switches are configured.
Management nodes and Application nodes will be plugged into aggregation switches.
They run in a high availability pair and use VLT to provide redundancy.
Requirements:
 Three connections between the switches, two of these are used for the ISL (Inter switch link) and one used for the keepalive. The ISL uses 100GB ports and the keepalive will be a 25 GB port.</description>
    </item>
    
    <item>
      <title>Configure Dell Cdu Switch</title>
      <link>/docs-csm/en-11/install/configure_dell_cdu_switch/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:09 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/configure_dell_cdu_switch/</guid>
      <description>Configure Dell CDU switch This page describes how Dell CDU switches are configured.
CDU switches are located in liquid-cooled cabinets and provide connectivity to MTN (Mountain) components. CDU switches act as leaf switches in the architecture. They run in a high availability pair and use VLT to provide redundancy.
Requirements:
 Two uplinks from each CDU switch to the upstream switch, this is normally a spine switch. Three connections between the switches, two of these are used for the VLTi (VLT interconnect) and one used for the keepalive.</description>
    </item>
    
    <item>
      <title>Configure Dell Leaf Switch</title>
      <link>/docs-csm/en-11/install/configure_dell_leaf_switch/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:09 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/configure_dell_leaf_switch/</guid>
      <description>Configure Dell Leaf Switch This page describes how Dell leaf switches are configured.
Leaf switches are located in air-cooled cabinets and provide connectivity to components in those cabinets.
Requirements: - Two uplinks from the switch to the upstream switch, this is can be an aggregation switch or a spine switch.
Here are example snippets from a leaf switch in the SHCD.
   Source Source Label Info Destination Label Info Destination Description     sw-smn01 x3000u40-j49 x3105u38-j47 sw-25g01 25g-15m-LC-LC   sw-smn01 x3000u40-j50 x3105u39-j47 sw-25g02 25g-15m-LC-LC    The uplinks are port 49 and 50 on the leaf.</description>
    </item>
    
    <item>
      <title>Configure Management Network Switches</title>
      <link>/docs-csm/en-11/install/configure_management_network/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:09 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/configure_management_network/</guid>
      <description>Configure Management Network Switches HPE Cray EX systems can have network switches in many roles: spine switches, leaf switches, aggregation switches, and CDU switches. Newer systems have HPE Aruba switches, while older systems have Dell and Mellanox switches. Switch IP addresses are generated by Cray Site Init (CSI).
The configuration steps are different for these switch vendors. The switch configuration procedures for HPE Aruba will be grouped separately from the switch configuration procedures for other vendors.</description>
    </item>
    
    <item>
      <title>Configure Mellanox Spine Switch</title>
      <link>/docs-csm/en-11/install/configure_mellanox_spine_switch/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:09 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/configure_mellanox_spine_switch/</guid>
      <description>Configure Mellanox Spine Switch This page describes how Mellanox spine switches are configured.
Depending on the size of the Shasta system the spine switches will serve different purposes. On TDS systems, the NCNs will plug directly into the spine switches, on larger systems with aggregation switches, the spine switches will provide connection between the aggregation switches.
Requirements: - One connection between the switches is used for the ISL (Inter switch link).</description>
    </item>
    
    <item>
      <title>Connect To Switch Over Usb-serial Cable</title>
      <link>/docs-csm/en-11/install/connect_to_switch_over_usb_serial_cable/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:09 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/connect_to_switch_over_usb_serial_cable/</guid>
      <description>Connect to Switch over USB-Serial Cable In the event that network plumbing is lacking, down, or unconfigured for procuring devices then it is recommended to use the Serial/COM ports on the spine and leaf switches.
This guide will instruct the user on procuring MAC addresses for the NCNs metadata files with the serial console.
Mileage may vary, as some obstacles such as BAUDRATE and terminal usage vary per manufacturer.
Common Manufacturers (click any for links to support/document portals)</description>
    </item>
    
    <item>
      <title>Create Application Node Config Yaml</title>
      <link>/docs-csm/en-11/install/create_application_node_config_yaml/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:09 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/create_application_node_config_yaml/</guid>
      <description>Create Application Node Config YAML This topic provides directions on constructing the application_node_config.yaml file. This file controls how the csi config init command finds and treats application nodes discovered in the hmn_connections.json file when generating configuration files for the system.
 Requirements Background Directions  Requirements The application_node_config.yaml file can be constructed from information from one of the following sources:
 The SHCD Excel spreadsheet for your system The hmn_connections.json file generated from the system&amp;rsquo;s SHCD.</description>
    </item>
    
    <item>
      <title>Create Cabinets Yaml</title>
      <link>/docs-csm/en-11/install/create_cabinets_yaml/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:09 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/create_cabinets_yaml/</guid>
      <description>Create Cabinets YAML This page provides directions on constructing the optional cabinets.yaml file. This file lists cabinet ids for any systems with non-contiguous cabinet id numbers and controls how the csi config init command treats cabinet ids.
This file is manually created and follows this format. For each &amp;ldquo;type&amp;rdquo; of cabinet can have several fields: total_number of cabinets of this type, starting_id for this cabinet type, and a list of the ids.</description>
    </item>
    
    <item>
      <title>Create Hmn Connections Json</title>
      <link>/docs-csm/en-11/install/create_hmn_connections_json/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:09 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/create_hmn_connections_json/</guid>
      <description>Create HMN Connections JSON About this task The following procedure shows the process for generating the hmn_connections.json from the system&amp;rsquo;s SHCD Excel document. This process is typically needed when generating the hmn_connections.json file for a new system, or regenerating it when system&amp;rsquo;s SHCD is changed (specifically the HMN tab). The hms-shcd-parser tool can be used to generate the hmn_connections.json file.
The SHCD/HMN Connections Rules document explains the expected naming conventions and rules for the HMN tab of the SHCD, and the hmn_connections.</description>
    </item>
    
    <item>
      <title>Create NCN Metadata Csv</title>
      <link>/docs-csm/en-11/install/create_ncn_metadata_csv/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:09 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/create_ncn_metadata_csv/</guid>
      <description>Create NCN Metadata CSV The information in the ncn_metadata.csv file identifies each of the management nodes, assigns the function as a master, worker, or storage node, and provides the MAC address information needed to identify the BMC and the NIC which will be used to boot the node.
Some of the data in the ncn_metadata.csv can be found in the SHCD. However, the hardest data to collect is the MAC addresses for the node&amp;rsquo;s BMC, the node&amp;rsquo;s bootable network interface, and the pair of network interfaces which will become the bonded interface bond0.</description>
    </item>
    
    <item>
      <title>Affected Devices</title>
      <link>/docs-csm/en-11/install/aruba_snmp_known_issue_10_06_0010/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:08 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/aruba_snmp_known_issue_10_06_0010/</guid>
      <description>Affected devices: 8320/8325/8360 Aruba CX
Aruba defect: CR 153440
Aruba public documentation/images:  https://asp.arubanetworks.com/
 Aruba 8325 release notes 10.06.0120:
 https://www.arubanetworks.com/techdocs/AOS-CX/10.06/RN/5200-8179.pdf
 Aruba 8360 release notes 10.06.0120:
 https://www.arubanetworks.com/techdocs/AOS-CX/10.06/RN/5200-8180.pdf
 Where you may see this issue during install: During initial network discovery of Nodes, SNMP may not accurately report MAC-address from Aruba Leaf/Spine switches. This will lead you in a situation where not all connected devices are discovered as expected.</description>
    </item>
    
    <item>
      <title>Boot Livecd Virtual Iso</title>
      <link>/docs-csm/en-11/install/boot_livecd_virtual_iso/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:08 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/boot_livecd_virtual_iso/</guid>
      <description>Boot LiveCD Virtual ISO This page will walk-through booting the LiveCD .iso file directly onto a BMC.
Topics:  Requirements BMCs&#39; Virtual Mounts  HPE iLO BMCs Gigabyte BMCs   Configuring  Backing up the Overlay COW FS Restoring from an Overlay COW FS Backup    Details Requirements A Cray Pre-Install Toolkit ISO is required for this process. This ISO can be obtained from:
 The Cray Pre-Install Toolkit ISO included in a CSM release tar file.</description>
    </item>
    
    <item>
      <title>Bootstrap Pit Node From Livecd Remote Iso</title>
      <link>/docs-csm/en-11/install/bootstrap_livecd_remote_iso/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:08 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/bootstrap_livecd_remote_iso/</guid>
      <description>Bootstrap PIT Node from LiveCD Remote ISO The Pre-Install Toolkit (PIT) node needs to be bootstrapped from the LiveCD. There are two media available to bootstrap the PIT node&amp;ndash;the RemoteISO or a bootable USB device. This procedure describes using the RemoteISO. If not using the RemoteISO, see Bootstrap PIT Node from LiveCD USB
The installation process is similar to the USB based installation with adjustments to account for the lack of removable storage.</description>
    </item>
    
    <item>
      <title>Bootstrap Pit Node From Livecd Usb</title>
      <link>/docs-csm/en-11/install/bootstrap_livecd_usb/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:08 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/bootstrap_livecd_usb/</guid>
      <description>Bootstrap PIT Node from LiveCD USB The Pre-Install Toolkit (PIT) node needs to be bootstrapped from the LiveCD. There are two media available to bootstrap the PIT node&amp;ndash;the RemoteISO or a bootable USB device. This procedure describes using the USB device. If not using the USB device, see Bootstrap Pit Node from LiveCD Remote ISO.
There are 5 overall steps that provide a bootable USB with SSH enabled, capable of installing Shasta v1.</description>
    </item>
    
    <item>
      <title>Cable Management Network Servers</title>
      <link>/docs-csm/en-11/install/cable_management_network_servers/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:08 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/cable_management_network_servers/</guid>
      <description>Cable Management Network Servers This topic describes nodes in the air-cooled cabinet with diagrams and pictures showing where to find the ports on the nodes and how to cable the nodes to the management network switches.
 HPE Hardware  HPE DL385 HPE DL325 HPE Worker Node Cabling HPE Master Node Cabling HPE Storage Node Cabling HPE UAN Cabling HPE Apollo 6500 XL645D HPE Apollo 6500 XL675D   Gigabyte/Intel Hardware  Worker Node Cabling Master Node Cabling Storage Node Cabling UAN Cabling    HPE Hardware HPE DL385  The OCP Slot is noted (number 7) in the image above.</description>
    </item>
    
    <item>
      <title>Ceph Csi Troubleshooting</title>
      <link>/docs-csm/en-11/install/ceph_csi_troubleshooting/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:08 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/ceph_csi_troubleshooting/</guid>
      <description>Ceph CSI Troubleshooting If there has been a failure to initialize all Ceph CSI components on ncn-s001, then the storage node cloud-init may need to be rerun.
Topics:  Verify Ceph CSI Rerun Storage Node cloud-init  Details 1. Verify Ceph CSI Verify that the ceph-csi requirements are in place
  Log in to ncn-s00 and run this command
ncn-s001# ceph -s If it returns a connection error then assume Ceph is not installed.</description>
    </item>
    
    <item>
      <title>Clear Gigabyte Cmos</title>
      <link>/docs-csm/en-11/install/clear_gigabyte_cmos/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:08 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/clear_gigabyte_cmos/</guid>
      <description>Clear Gigabyte CMOS Because of a bug in the Gigabyte firmware, the Shasta 1.5 install may negatively impact Gigabyte motherboards when attempting to boot using bonded Mellanox network cards. The result is a board that is unusable until a CMOS clear is physically done via a jumper on the board itself.
A patched firmware release (newer than C20 BIOS) is expected to be available for a future release of Shasta. It is recommended that Gigabyte users wait for this new firmware before attempting an installation of Shasta 1.</description>
    </item>
    
    <item>
      <title>Collect Mac Addresses For NCNs</title>
      <link>/docs-csm/en-11/install/collect_mac_addresses_for_ncns/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:08 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/collect_mac_addresses_for_ncns/</guid>
      <description>Collect MAC Addresses for NCNs Now that the PIT node has been booted with the LiveCD and the management network switches have been configured, the actual MAC addresses for the management nodes can be collected. This process will include repetition of some of the steps done up to this point because csi config init will need to be run with the proper MAC addresses and some services will need to be restarted.</description>
    </item>
    
    <item>
      <title>Collecting The BMC Mac Addresses</title>
      <link>/docs-csm/en-11/install/collecting_bmc_mac_addresses/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:08 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/collecting_bmc_mac_addresses/</guid>
      <description>Collecting the BMC MAC Addresses This guide will detail how to collect BMC MAC Addresses from a Shasta system with configured switches. The BMC MAC Address is the exclusive, dedicated LAN for the onboard BMC.
If you are here with an unconfigured switch, mileage may vary.
Requirements  Configured switch with SSH access or unconfigured with COM access (serial-over-lan/DB-9) Another file to record the collected BMC information.  Procedure   Establish an SSH or Connect to Switch over USB-Serial Cable to the leaf switch.</description>
    </item>
    
    <item>
      <title>Hotfix To Workaround Known Mac-learning Issue With 8325</title>
      <link>/docs-csm/en-11/install/8325_mac_learning_hotfix/</link>
      <pubDate>Sat, 02 Oct 2021 03:05:08 +0000</pubDate>
      
      <guid>/docs-csm/en-11/install/8325_mac_learning_hotfix/</guid>
      <description>Hotfix to workaround known mac-learning issue with 8325 Issue description  Aruba CR: 90598
Affected platform: 8325
Symptom: MAC learning stops.
Scenario: Under extremely rare DMA stress conditions, anL2 learning thread may timeout and exit preventing future MAC learning.
Workaround: Reboot the switch or monitor the L2 thread and restart it with an NAE script.
Fixed in: 10.06.0130, 10.7.0010 and above.
Aruba release notes
 To fix the issue without upgrading software You can run a NAE script on the 8325 platform switches to resolve mac learning issue.</description>
    </item>
    
  </channel>
</rss>
