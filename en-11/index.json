[
{
	"uri": "/docs-csm/en-11/upgrade/1.1/",
	"title": "1.1",
	"tags": [],
	"description": "",
	"content": "1.1 Topics:\n Readme  "
},
{
	"uri": "/docs-csm/en-11/upgrade/1.0/",
	"title": "1",
	"tags": [],
	"description": "",
	"content": "1.0 Topics:\n"
},
{
	"uri": "/docs-csm/en-11/upgrade/",
	"title": "Upgrade",
	"tags": [],
	"description": "",
	"content": "Upgrade Topics:\n 1.0 1.1  "
},
{
	"uri": "/docs-csm/en-11/operations/system_configuration_service/",
	"title": "System Configuration Service",
	"tags": [],
	"description": "",
	"content": "System Configuration Service Topics:\n Configure BMC And Controller Parameters With Scsd Manage Parameters With The Scsd Service Set BMC Credentials System Configuration Service  "
},
{
	"uri": "/docs-csm/en-11/operations/compute_rolling_upgrades/",
	"title": "Compute Rolling Upgrades",
	"tags": [],
	"description": "",
	"content": "Compute Rolling Upgrades Topics:\n CRUS Workflow Compute Rolling Upgrades Troubleshoot Nodes Failing To Upgrade In A CRUS Session Troubleshoot A Failed CRUS Session Due To Bad Parameters Troubleshoot A Failed CRUS Session Due To Unmet Conditions Upgrade Compute Nodes With CRUS  "
},
{
	"uri": "/docs-csm/en-11/operations/utility_storage/",
	"title": "Utility Storage",
	"tags": [],
	"description": "",
	"content": "Utility Storage Topics:\n Add Ceph Osds Adjust Ceph Pool Quotas Ceph Daemon Memory Profiling Ceph Health States Ceph Orchestrator Usage Ceph Service Check Script Usage Ceph Storage Types Cephadm Reference Material Collect Information About The Ceph Cluster Dump Ceph Crash Data Identify Ceph Latency Issues Manage Ceph Services Restore Corrupt Nexus Shrink Ceph Osds Troubleshoot Ceph-mon Processes Stopping And Exceeding Max Restarts Troubleshoot Ceph Mds Reporting Slow Requests And Failure On Client Troubleshoot Ceph Osds Reporting Full Troubleshoot Ceph Services Not Starting Troubleshoot Failure To Get Ceph Health Troubleshoot Large Object Map Objects In Ceph Health Troubleshoot Pods Failing To Restart On Other Worker Nodes Troubleshoot Rgw Health Check Fail Troubleshoot System Clock Skew Troubleshoot A Down Osd Troubleshoot An Unresponsive S3 Endpoint Utility Storage  "
},
{
	"uri": "/docs-csm/en-11/operations/kubernetes/",
	"title": "Kubernetes",
	"tags": [],
	"description": "",
	"content": "Kubernetes Topics:\n About Kubernetes Taints And Labels About Postgres About Etcd About Kubectl Backups For Etcd-operator Clusters Cert Renewal For Kubernetes And Bare Metal Etcd Check For And Clear Etcd Cluster Alarms Check The Health And Balance Of Etcd Clusters Clear Space In An Etcd Cluster Database Configure Kubectl Credentials To Access The Kubernetes Apis Containerd Create A Manual Backup Of A Healthy Etcd Cluster Determine If Pods Are Hitting Resource Limits Disaster Recovery Postgres Increase Kafka Pod Resource Limits Increase Pod Resource Limits Kubernetes Kubernetes Networking Kubernetes Storage Pod Resource Limits Rebalance Healthy Etcd Clusters Rebuild Unhealthy Etcd Clusters Recover From Postgres Wal Event Repopulate Data In Etcd Clusters When Rebuilding Them Report The Endpoint Status For Etcd Clusters Restore Bare-metal Etcd Clusters From An S3 Snapshot Restore Postgres Restore An Etcd Cluster From A Backup Retrieve Cluster Health Information Using Kubernetes Sealed Secrets Procedures Troubleshoot Postgres Database View Postgres Information For System Databases  "
},
{
	"uri": "/docs-csm/en-11/operations/spire/",
	"title": "Spire",
	"tags": [],
	"description": "",
	"content": "Spire Topics:\n Restore Spire Postgres Without A Backup  "
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/",
	"title": "Boot Orchestration",
	"tags": [],
	"description": "",
	"content": "Boot Orchestration Topics:\n BOS Workflows Boot Issue Symptom Node Console Or Logs Indicate That The Server Response Has Timed Out Boot Issue Symptom Node HSN Interface Does Not Appear Or Shows No Link Detected Boot Orchestration Boot UANs Check The Progress Of BOS Session Operations Clean Up After A BOS-boa Job Is Completed Or CANcelled Clean Up Logs After A Boa Kubernetes Job Compute Node Boot Issue Symptom Duplicate Address Warnings And Declined DHCP Offers In Logs Compute Node Boot Issue Symptom Message About Invalid Eeprom Checksum In Node Console Or Log Compute Node Boot Issue Symptom Node Is Not Able To Download The Required Artifacts Compute Node Boot Sequence Configure The BOS Timeout When Booting Nodes Create A Session Template To Boot Compute Nodes With Cps Edit The IPXE Embedded Boot Script Healthy Compute Node Boot Process Kernel Boot Parameters Limit The Scope Of A BOS Session Limitations For Gigabyte BMC Hardware Log File Locations And Ports Used In Compute Node Boot Troubleshooting Manage A BOS Session Manage A Session Template Node Boot Root Cause Analysis Redeploy The IPXE And Tftp Services Session Templates Sessions Stage Changes Without BOS Tools For Resolving Boot Issues Troubleshoot Booting Nodes With Hardware Issues Troubleshoot Compute Node Boot Issues Related To Dynamic Host Configuration Protocol DHCP Troubleshoot Compute Node Boot Issues Related To Slow Boot Times Troubleshoot Compute Node Boot Issues Related To Trivial File Transfer Protocol Tftp Troubleshoot Compute Node Boot Issues Related To Unified Extensible Firmware Interface Uefi Troubleshoot Compute Node Boot Issues Related To The Boot Script Service Bss Troubleshoot Compute Node Boot Issues Using Kuberentes Troubleshoot Compute Node Boot Issues Using Kubernetes Troubleshoot UAN Boot Issues Upload Node Boot Information To Boot Script Service Bss View The Status Of A BOS Session  "
},
{
	"uri": "/docs-csm/en-11/operations/csm_product_management/",
	"title": "Product Management",
	"tags": [],
	"description": "",
	"content": "Product Management Topics:\n Change Passwords And Credentials Configure Keycloak Account Configure Non-compute Nodes With CFS Perform NCN Personalization Post Install Customizations Remove Artifacts From Product Installations Validate Signed Rpms  "
},
{
	"uri": "/docs-csm/en-11/operations/firmware/",
	"title": "Firmware",
	"tags": [],
	"description": "",
	"content": "Firmware Topics:\n FAS Admin Procedures FAS Cli FAS Filters FAS Recipes FAS Use Cases Update Firmware With FAS Upload Olympus BMC Recovery Firmware Into Tftp Server  "
},
{
	"uri": "/docs-csm/en-11/operations/node_management/",
	"title": "Node Management",
	"tags": [],
	"description": "",
	"content": "Node Management Topics:\n Access And Update The Settings For Replacement NCNs Add TLS Certificates To BMCs Add A Standard Rack Node Change Interfaces In The Bond Change Java Security Settings Change Settings For Hms Collector Polling Of Air Cooled Nodes Check And Set The Metalno-wipe Setting On NCNs Check The BMC Failover Mode Clear Space In Root File System On Worker Nodes Configuration Of NCN Bonding Configure NTP On NCNs Disable Nodes Dump A Non-compute Node Enable Nodes Enable Passwordless Connections To Liquid Cooled Node BMCs Find Node Type And Manufacturer Launch A Virtual KVM On Gigabyte Servers Launch A Virtual KVM On Intel Servers Move A Standard Rack Node Move A Standard Rack Node Samerack SameHSNports Node Management Node Management Workflows Reboot NCNs Rebuild NCNs Replace A Compute Blade Reset Credentials On Redfish Devices For Reinstallation Swap A Compute Blade With A Different System TLS Certificates For Redfish BMCs Troubleshoot Interfaces With Ip Address Issues Troubleshoot Issues With Redfish Endpoint Discovery Troubleshoot Loss Of Console Connections And Logs On Gigabyte Nodes Update Compute Node Mellanox HSN NIC Firmware Update The Gigabyte Server Bios Time Use The Physical KVM Verify Accuracy Of The System Clock View Bios Logs For Liquid Cooled Nodes  "
},
{
	"uri": "/docs-csm/en-11/operations/artifact_management/",
	"title": "Artifact Management",
	"tags": [],
	"description": "",
	"content": "Artifact Management Topics:\n Artifact Management Generate Temporary S3 Credentials Manage Artifacts With The Cray Cli Use S3 Libraries And Clients  "
},
{
	"uri": "/docs-csm/en-11/operations/package_repository_management/",
	"title": "Package Repository Management",
	"tags": [],
	"description": "",
	"content": "Package Repository Management Topics:\n Manage Repositories With Nexus Nexus Configuration Nexus Deployment Package Repository Management Package Repository Management With Nexus Repair Yum Repository Metadata Restrict Admin Privileges In Nexus Troubleshoot Nexus  "
},
{
	"uri": "/docs-csm/en-11/operations/system_layout_service/",
	"title": "System Layout Service",
	"tags": [],
	"description": "",
	"content": "System Layout Service Topics:\n Add UAN CAN Ip Addresses To SLS Create A Backup Of The SLS Postgres Database Dump SLS Information Load SLS Database With Dump File Restore SLS Postgres Database From Backup Restore SLS Postgres Without An Existing Backup System Layout Service SLS Update SLS With UAN Aliases  "
},
{
	"uri": "/docs-csm/en-11/operations/configuration_management/",
	"title": "Configuration Management",
	"tags": [],
	"description": "",
	"content": "Configuration Management Topics:\n Ansible Execution Environments Ansible Inventory Automatic Session Deletion With Sessionttl CFS Global Options Change The Ansible VerBOSity Logs Configuration Layers Configuration Management Configuration Management Of System Components Configuration Management With The CFS Batcher Configuration Sessions Create A CFS Configuration Create A CFS Session With Dynamic Inventory Create An Image Customization CFS Session Create And Populate A Vcs Configuration Repository Customize Configuration Values Delete CFS Sessions Enable Ansible Profiling Git Operations Manage Multiple Inventories In A Single Location Set Limits For A Configuration Session Set The Ansible-cfg For A Session Target Ansible Tasks For Image Customization Track The Status Of A Session Troubleshoot Ansible Play Failures In CFS Sessions Troubleshoot CFS Session Failing To Complete Update A CFS Configuration Update The Privacy Settings For Gitea Configuration Content Repositories Use A Custom Ansible-cfg File Use A Specific Inventory In A Configuration Session Vcs Branching Strategy Version Control Service Vcs View Configuration Session Logs Write Ansible Code For CFS  "
},
{
	"uri": "/docs-csm/en-11/operations/hardware_state_manager/",
	"title": "Hardware State Manager",
	"tags": [],
	"description": "",
	"content": "Hardware State Manager Topics:\n Add A Switch To The HSM Database Add An NCN To The HSM Database Component Group Members Component Groups And Partitions Component Memberships Component Partition Members Create A Backup Of The HSM Postgres Database HSM Roles And Subroles Hardware Management Services Hms Locking Api Hardware State Manager Hardware State Manager HSM State And Flag Fields Lock And Unlock Management Nodes Manage Component Groups Manage Component Partitions Manage Hms Locks Restore HSM Postgres From Backup Restore HSM Postgres Without A Backup  "
},
{
	"uri": "/docs-csm/en-11/operations/hmcollector/",
	"title": "Hmcollector",
	"tags": [],
	"description": "",
	"content": "Hmcollector Topics:\n Adjust Hmcollector Resource Limits Requests  "
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/",
	"title": "Security And Authentication",
	"tags": [],
	"description": "",
	"content": "Security And Authentication Topics:\n Api Authorization Access The Keycloak User Management Ui Add LDAP User Federation Authenticate An Account With The Command Line Backup And Restore Vault Clusters Certificate Types Change Ex Liquid-cooled Cabinet Global Default Password Change NCN Image Root Password And SSH Keys Change Root Passwords For Compute Nodes Change The Keycloak Admin Password Change The LDAP Server Ip Address For Existing Ldap Server Content Change The LDAP Server Ip Address For New Ldap Server Content Configure Keycloak For LDAPad Authentication Configure The Rsa Plugin In Keycloak Create Internal Groups In The Keycloak Shasta Realm Create Internal User Accounts In The Keycloak Shasta Realm Create A Service Account In Keycloak Default Keycloak Realms Accounts And Clients Delete Internal User Accounts From The Keycloak Shasta Realm Get A Long-lived Token For A Service Account Hashicorp Vault Keycloak Operations Make HTTPS Requests From Sources Outside The Management Kubernetes Cluster Manage System Passwords PKI Certificate Authority Ca PKI Services Preserve Username Capitalization For Users Exported From Keycloak Provisioning A Liquid-cooled Ex Cabinet Cec With Default Credentials Public Key Infrastructure PKI Remove Internal Groups From The Keycloak Shasta Realm Remove The Email Mapper From The LDAP User Federation Remove The LDAP User Federation From Keycloak Resync Keycloak Users To Compute Nodes Retrieve An Authentication Token Retrieve The Client Secret For Service Accounts SSH Keys System Security And Authentication Transport Layer Security For Ingress Services Troubleshoot Common Vault Cluster Issues Troubleshoot Spire Failing To Start On NCNs Update NCN Passwords Updating The Liquid-cooled Ex Cabinet Default Credentials After A Cec Password Change  "
},
{
	"uri": "/docs-csm/en-11/operations/resiliency/",
	"title": "Resiliency",
	"tags": [],
	"description": "",
	"content": "Resiliency Topics:\n NTP Resiliency Recreate Statefulset Pods On Another Node Resilience Of System Management Services Resiliency Resiliency Testing Procedure Restore System Functionality If A Kubernetes Worker Node Is Down  "
},
{
	"uri": "/docs-csm/en-11/operations/system_management_health/",
	"title": "System Management Health",
	"tags": [],
	"description": "",
	"content": "System Management Health Topics:\n Access System Management Health Services Configure Prometheus Email Alert Notifications System Management Health System Management Health Checks And Alerts Troubleshoot Prometheus Alerts  "
},
{
	"uri": "/docs-csm/en-11/operations/conman/",
	"title": "Conman",
	"tags": [],
	"description": "",
	"content": "Conman Topics:\n Access Compute Node Logs Access Console Log Data Via The System Monitoring Framework Smf Conman Disable Conman After System Software Installation Establish A Serial Connection To NCNs Log In To A Node Using Conman Manage Node Consoles Troubleshoot Conman Asking For Password On SSH Connection Troubleshoot Conman Blocking Access To A Node BMC Troubleshoot Conman Failing To Connect To A Console  "
},
{
	"uri": "/docs-csm/en-11/operations/network/dns/",
	"title": "DNS",
	"tags": [],
	"description": "",
	"content": "DNS Topics:\n DNS Enable Ncsd On UANs Manage The DNS Unbound Resolver PowerDNS Configuration Troubleshoot Common DNS Issues Troubleshoot PowerDNS  "
},
{
	"uri": "/docs-csm/en-11/operations/network/external_dns/",
	"title": "External DNS",
	"tags": [],
	"description": "",
	"content": "External DNS Topics:\n Add NCNs And UANs To External DNS External DNS External DNS Failing To Discover Services Workaround External DNS Csi Config Init Input Values Ingress Routing Troubleshoot DNS Configuration Issues Troubleshoot Systems Not Provisioned With External Ip Addresses Update The CAN-external-DNS Value Post-installation Update The System-name Site-domain Value Post-installation  "
},
{
	"uri": "/docs-csm/en-11/operations/network/customer_access_network/",
	"title": "Customer Access Network",
	"tags": [],
	"description": "",
	"content": "Customer Access Network Topics:\n CAN With Dual-spine Configuration Connect To The CAN Customer Access Network CAN Externally Exposed Services Required Labels If CAN Is Not Configured Troubleshoot CAN Issues  "
},
{
	"uri": "/docs-csm/en-11/operations/network/management_network/",
	"title": "Management Network",
	"tags": [],
	"description": "",
	"content": "Management Network Topics:\n Management Network ACL Configuration Management Network Access Port Configurations Management Network CAN Setup Management Network Flow Control Settings Management Network Switch Rename Update Management Network Firmware  "
},
{
	"uri": "/docs-csm/en-11/operations/network/dhcp/",
	"title": "DHCP",
	"tags": [],
	"description": "",
	"content": "DHCP Topics:\n DHCP Troubleshoot DHCP Issues  "
},
{
	"uri": "/docs-csm/en-11/operations/network/metallb_bgp/",
	"title": "Metallb BGP",
	"tags": [],
	"description": "",
	"content": "Metallb BGP Topics:\n Check BGP Status And Reset Sessions Metallb In BGP-mode Metallb In BGP-mode Configuration Troubleshoot BGP Not Accepting Routes From Metallb Troubleshoot Services Without An Allocated Ip Address Update BGP Neighbors  "
},
{
	"uri": "/docs-csm/en-11/operations/network/",
	"title": "Network",
	"tags": [],
	"description": "",
	"content": "Network Topics:\n Access To System Management Services Connect To The HPE Cray Ex Environment Default Ip Address Ranges Network Customer Access Network DHCP DNS External DNS Management Network Metallb BGP  "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/",
	"title": "UAS User And Admin Topics",
	"tags": [],
	"description": "",
	"content": "UAS User And Admin Topics Topics:\n Add A Volume To UAS Broker Mode UAI Management Configure End-user UAI Classes For Broker Mode Configure UAIs In UAS Configure A Broker UAI Class Configure A Default UAI Class For Legacy Mode Create UAIs From Specific Uai Images In Legacy Mode Create A UAI Create A UAI Class Create A UAI Resource Specification Create A UAI Using A Direct Administrative Command Create A UAI With Additional Ports Create And Register A Custom UAI Image Create And Use Default UAIs In Legacy Mode Customize End-user UAI Images Customize The Broker UAI Image Delete A UAI Delete A UAI Class Delete A UAI Image Registration Delete A UAI Resource Specification Delete A UAI Using An Administrative Command Delete A Volume Configuration Elements Of A UAI End User UAIs Examine A UAI Using A Direct Administrative Command Legacy Mode User-driven UAI Management List Available UAI Classes List Available UAI Images In Legacy Mode List Registered UAI Images List UAI Resource Specifications List UAIs List UAS Information List Volumes Registered In UAS List And Delete All UAIs Log In To A Broker UAI Login To A Users UAI To Troubleshoot Issues Modify A UAI Class Obtain Configuration Of A UAS Volume Register A UAI Image Reset The UAS Configuration To Original Installed Settings Resource Specifications Retrieve Resource Specification Details Retrieve UAI Image Registration Information Select And Configure Host Nodes For UAIs Special Purpose UAIs Start A Broker UAI Troubleshoot Common Mistakes When Creating A Custom End-user UAI Image Troubleshoot Duplicate Mount Paths In A UAI Troubleshoot Missing Or Incorrect UAI Images Troubleshoot Stale Brokered UAIs Troubleshoot UAI Authentication Issues Troubleshoot UAI Stuck In Containercreating Troubleshoot UAIs By Viewing Log Output Troubleshoot UAIs With Administrative Access Troubleshoot UAS Issues Troubleshoot UAS By Viewing Log Output UAI Classes UAI Host Node Selection UAI Host Nodes UAI Images UAI Management UAI Network Attachments UAI Macvlans Network Attachments UAS Limitations UAS And UAI Health Checks Update A Resource Specification Update A UAI Image Registration Update A UAS Volume User Access Service UAS View A UAI Class Volumes  "
},
{
	"uri": "/docs-csm/en-11/operations/power_management/",
	"title": "Power Management",
	"tags": [],
	"description": "",
	"content": "Power Management Topics:\n Bring Up The Slingshot Fabric Cray Advanced Platform Monitoring And Control CAPMC Ignore Nodes With CAPMC Liquid Cooled Node Card Power Management Power Off Compute And Io Cabinets Power Off The External Lustre File System Power On Compute And Io Cabinets Power On And Boot Compute Nodes And User Access Nodes Power On And Start The Management Kubernetes Cluster Power On The External Lustre File System Prepare The System For Power Off Recover From A Liquid Cooled Cabinet Epo Event Save Management Network Switch Configurations Set The Turbo Boost Limit Shut Down And Power Off Compute And User Access Nodes Shut Down And Power Off The Management Kubernetes Cluster Standard Rack Node Power Management System Power Off Procedures System Power On Procedures User Access To Compute Node Power Data Power Management  "
},
{
	"uri": "/docs-csm/en-11/operations/image_management/",
	"title": "Image Management",
	"tags": [],
	"description": "",
	"content": "Image Management Topics:\n Build A New UAN Image Using The Default Recipe Build An Image Using IMS Rest Service Convert Tgz Archives To SqUAShfs Images Create UAN Boot Images Customize An Image Root Using IMS Delete Or Recover Deleted IMS Content Image Management Image Management Workflows Upload And Register An Image Recipe  "
},
{
	"uri": "/docs-csm/en-11/operations/river_endpoint_discovery_service/",
	"title": "River Endpoint Discovery Service",
	"tags": [],
	"description": "",
	"content": "River Endpoint Discovery Service Topics:\n Clear State And Restart REDS Configure A Management Switch For REDS Initialize And Geolocate Nodes River Endpoint Discovery Service Troubleshoot Common Error Messages In REDS Logs Troubleshoot Common REDS Issues Verify Node Removal  "
},
{
	"uri": "/docs-csm/en-11/troubleshooting/known_issues/",
	"title": "Known Issues",
	"tags": [],
	"description": "",
	"content": "Known Issues Topics:\n CFS Sessions Stuck In Pending Component Power State Mismatch Craycli 403 Forbidden Errors Discovery Aruba Snmp Issue Discovery Job Not Creating Redfish Endpoints Incorrect Output For BOS Command Rerun  "
},
{
	"uri": "/docs-csm/en-11/upgrade/1.1/readme/",
	"title": "1.1.0 Upgrade Guide",
	"tags": [],
	"description": "",
	"content": "Copyright 2021 Hewlett Packard Enterprise Development LP\nCSM 1.1.0 Upgrade Guide This guide contains procedures for upgrading systems running CSM 1.0.0 to CSM 1.1.0. It is intended for system installers, system administrators, and network administrators. It assumes some familiarity with standard Linux and associated tooling.\nProcedures:\n Preparation Run Validation Checks (Pre-Upgrade) Update customizations.yaml Setup Nexus Update NCN BIOS Configuration Upgrade Services Run Validation Checks (Post-Upgrade) Verify CSM Version in Product Catalog Exit Typescript  Changes See CHANGELOG.md in the root of a CSM release distribution for a summary of changes in each CSM release.\nPreparation For convenience, these procedures make use of environment variables. This section sets the expected environment variables to appropriate values.\n  Start a typescript to capture the commands and output from this procedure.\nncn-m001# script -af csm-update.$(date +%Y-%m-%d).txt ncn-m001# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39;   Set CSM_DISTDIR to the directory of the extracted release distribution for CSM 1.1.0:\n NOTE: Use --no-same-owner and --no-same-permissions options to tar when extracting a CSM release distribution as root to ensure the extracted files are owned by root and have permissions based on the current umask value.\n If using a release distribution:\nncn-m001# tar --no-same-owner --no-same-permissions -zxvf csm-1.1.0.tar.gz ncn-m001# CSM_DISTDIR=\u0026#34;$(pwd)/csm-1.1.0\u0026#34; Else if using a hotfix distribution:\nncn-m001# CSM_HOTFIX=\u0026#34;csm-1.1.0-hotfix-0.0.1\u0026#34; ncn-m001# tar --no-same-owner --no-same-permissions -zxvf ${CSM_HOTFIX}.tar.gz ncn-m001# CSM_DISTDIR=\u0026#34;$(pwd)/${CSM_HOTFIX}\u0026#34; ncn-m001# echo $CSM_DISTDIR   Set CSM_RELEASE_VERSION to the version reported by ${CSM_DISTDIR}/lib/version.sh:\nncn-m001# CSM_RELEASE_VERSION=\u0026#34;$(${CSM_DISTDIR}/lib/version.sh --version)\u0026#34; ncn-m001# echo $CSM_RELEASE_VERSION   Download and install/upgrade the latest workaround and documentation RPMs. If this machine does not have direct internet access these RPMs will need to be externally downloaded and then copied to be installed.\nncn-m001# rpm -Uvh https://storage.googleapis.com/csm-release-public/csm-1.1/docs-csm/docs-csm-latest.noarch.rpm ncn-m001# rpm -Uvh https://storage.googleapis.com/csm-release-public/csm-1.1/csm-install-workarounds/csm-install-workarounds-latest.noarch.rpm   Set CSM_SCRIPTDIR to the scripts directory included in the docs-csm RPM for the CSM 1.1.0 upgrade:\nncn-m001# CSM_SCRIPTDIR=/usr/share/doc/csm/upgrade/1.1/scripts   Run Validation Checks (Pre-Upgrade) It is important to first verify a healthy starting state. To do this, run the CSM validation checks. If any problems are found, correct them and verify the appropriate validation checks before proceeding.\nUpdate customizations.yaml Perform these steps to update customizations.yaml:\n  Prepare work area\nIf you manage customizations.yaml in an external Git repository (as recommended), then clone a local working tree.\nncn-m001:~ # git clone \u0026lt;URL\u0026gt; /root/site-init ncn-m001:~ # cd /root/site-init If you do not have a backup of site-init then perform the following steps to create a new one using the values stored in the Kubernetes cluster.\n  Create a new site-init directory using from the CSM tarball\nncn-m001:~ # cp -r ${CSM_DISTDIR}/shasta-cfg /root/site-init ncn-m001:~ # cd /root/site-init   Extract customizations.yaml from the site-init secret\nncn-m001:~/site-init # kubectl -n loftsman get secret site-init -o jsonpath='{.data.customizations\\.yaml}' | base64 -d - \u0026gt; customizations.yaml   Extract the certificate and key used to create the sealed secrets\nncn-m001:~/site-init # mkdir certs ncn-m001:~/site-init # kubectl -n kube-system get secret sealed-secrets-key -o jsonpath='{.data.tls\\.crt}' | base64 -d - \u0026gt; certs/sealed_secrets.crt ncn-m001:~/site-init # kubectl -n kube-system get secret sealed-secrets-key -o jsonpath='{.data.tls\\.key}' | base64 -d - \u0026gt; certs/sealed_secrets.key      Note: All subsequent steps of this procedure should be performed within the /root/site-init directory created in this step.\n   Update customizations.yaml\nApply new values required for PowerDNS\nncn-m001:~/site-init # ${CSM_SCRIPTDIR}/upgrade/update-customizations.sh -i customizations.yaml   Configure DNS zone transfer and DNSSEC (optional)\nIf the DNS zone transfer and DNSSEC features are required please review the PowerDNS configuration guide and update customizations.yaml with the appropriate values.\n  Generate the new PowerDNS API key secret\n Note: This step will also generate the SealedSecrets for the DNSSEC keys if configured in the previous step.\n ncn-m001:~/site-init # ./utils/secrets-seed-customizations.sh customizations.yaml   Update the site-init secret\nncn-m001:~/site-init # kubectl delete secret -n loftsman site-init ncn-m001:~/site-init # kubectl create secret -n loftsman generic site-init --from-file=customizations.yaml   Commit changes to customizations.yaml if using an external Git repository.\nncn-m001:~/site-init # git add customizations.yaml ncn-m001:~/site-init # git commit -m 'Add required PowerDNS configuration' ncn-m001:~/site-init # git push   Setup Nexus Run lib/setup-nexus.sh to configure Nexus and upload new CSM RPM repositories, container images, and Helm charts:\nncn-m001# cd \u0026#34;$CSM_DISTDIR\u0026#34; ncn-m001# ./lib/setup-nexus.sh On success, setup-nexus.sh will output OK on stderr and exit with status code 0, e.g.:\nncn-m001# ./lib/setup-nexus.sh ... + Nexus setup complete setup-nexus.sh: OK ncn-m001# echo $? 0 In the event of an error, consult the known issues from the install documentation to resolve potential problems and then try running setup-nexus.sh again. Note that subsequent runs of setup-nexus.sh may report FAIL when uploading duplicate assets. This is ok as long as setup-nexus.sh outputs setup-nexus.sh: OK and exits with status code 0.\nUpdate NCN BIOS Configuration   Install the ilorest RPM package:\n  Internet Connected\nncn-m001# wget https://storage.googleapis.com/csm-release-public/shasta-1.5/ilorest/ilorest-latest.noarch.rpm ncn-m001# rpm -Uvh ilorest-latest.noarch.rpm   Air Gapped\nncn-m001# rpm -Uvh ${CSM_DISTDIR}/rpm/cray/csm/sle-15sp2/ilorest-*.noarch.rpm     Get the BIOS Baseline script from the 1.0 tarball.\n  Make the script accessible:\nncn-m001# mkdir -pv /mnt/livecd /mnt/rootfs /mnt/sqfs /mnt/pitdata \\ mount -L PITDATA /mnt/pitdata mount ${CSM_DISTDIR}/cray-pre-install-toolkit-*.iso /mnt/livecd/ mount /mnt/livecd/LiveOS/squashfs.img /mnt/sqfs/ mount /mnt/sqfs/LiveOS/rootfs.img /mnt/rootfs/   Run the script; this will target every BMC known in the emergency-fallback records inside of /etc/hosts:\nncn-m001# /mnt/rootfs/root/bin/bios-baseline.sh     (optionally) uninstall ilorest to conform the NCN to the rest with respect to package inventory (ilorest is installed on the LiveCD, but not on the NCNs by default).\nncn-m001# rpm -e ilorest   Upgrade Services   Run upgrade.sh to deploy upgraded CSM applications and services:\nncn-m001# ./upgrade.sh   Note: If you have not already installed the workload manager product including slurm and munge, then the cray-crus pod is expected to be in the Init state. After running upgrade.sh, you may observe there are now two copies of the cray-crus pod in the Init state. This situation is benign and should resolve itself once the workload manager product is installed.\nRun Validation Checks (Post-Upgrade)  IMPORTANT: Wait at least 15 minutes after upgrade.sh completes to let the various Kubernetes resources get initialized and started.\n Run the following validation checks to ensure that everything is still working properly after the upgrade:\n Platform health checks  Other health checks may be run as desired.\n CAUTION: The following HMS functional tests may fail because of locked components in HSM:\n test_bss_bootscript_ncn-functional_remote-functional.tavern.yaml test_smd_components_ncn-functional_remote-functional.tavern.yaml  Traceback (most recent call last): File \u0026#34;/usr/lib/python3.8/site-packages/tavern/schemas/files.py\u0026#34;, line 106, in verify_generic verifier.validate() File \u0026#34;/usr/lib/python3.8/site-packages/pykwalify/core.py\u0026#34;, line 166, in validate raise SchemaError(u\u0026#34;Schema validation failed:\\n - {error_msg}.\u0026#34;.format( pykwalify.errors.SchemaError: \u0026lt;SchemaError: error code 2: Schema validation failed: - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/0\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/5\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/6\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/7\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/8\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/9\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/10\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/11\u0026#39;. - Key \u0026#39;Locked\u0026#39; was not defined. Path: \u0026#39;/Components/12\u0026#39;.: Path: \u0026#39;/\u0026#39;\u0026gt; Failures of these tests because of locked components as shown above can be safely ignored.\n NOTE: If you plan to do any further CSM health validation, you should follow the validation procedures found in the CSM v1.1 documentation. Some of the information in the CSM v1.0 validation documentation is no longer accurate in CSM v1.1.\nVerify CSM Version in Product Catalog   Verify the CSM version has been updated in the product catalog. Verify that the following command includes version 1.1.0:\nncn-m001# kubectl get cm cray-product-catalog -n services -o jsonpath=\u0026#39;{.data.csm}\u0026#39; | yq r -j - | jq -r \u0026#39;to_entries[] | .key\u0026#39; 1.1.0 1.0.0   Confirm the import_date reflects the timestamp of the upgrade:\nncn-m001# kubectl get cm cray-product-catalog -n services -o jsonpath=\u0026#39;{.data.csm}\u0026#39; | yq r - \u0026#39;\u0026#34;1.1.0\u0026#34;.configuration.import_date\u0026#39;   Exit Typescript Remember to exit your typescript.\nncn-m001# exit It is recommended to save the typescript file for later reference.\n"
},
{
	"uri": "/docs-csm/en-11/troubleshooting/kubernetes/",
	"title": "Kubernetes",
	"tags": [],
	"description": "",
	"content": "Kubernetes Topics:\n Kubernetes Log File Locations Kubernetes Troubleshooting Information Troubleshoot Kubernetes Node Notready Troubleshoot Liveliness Readiness Probe Failures Troubleshoot Unresponsive Kubectl Commands  "
},
{
	"uri": "/docs-csm/en-11/troubleshooting/pxe_runbook/",
	"title": "PXE Booting Runbook",
	"tags": [],
	"description": "",
	"content": "PXE Booting Runbook PXE booting is a key component of a working Shasta system. There are a lot of different components involved which increases the complexity. This guide runs through the most common issues and shows the user what is needed in order to have a successful PXE boot.\n NCNs on install\n ncn-m001 on reboot or NCN boot\n2.1. Verify DHCP packets can be forwarded from the workers to the MTL network (VLAN1)\n2.2. Verify BGP\n2.3. Verify route to TFTP\n2.4. Test TFTP traffic (Aruba Only)\n2.5. Check DHCP lease is getting allocated\n2.6. Verify the DHCP traffic on the Workers\n2.7. Verify the switches are forwarding DHCP traffic.\n Computes/UANs/Application Nodes\n  1. NCNs on install  Verify the DNSMASQ configuration file matches what is configured on the switches.  Here is a DNSMASQ configuration file for the Metal network (VLAN1). As you can see, the router is 10.1.0.1. This has to match what the IP address is on the switches doing the routing for the MTL network. This is most commonly on the spines. This configuration is commonly missed on the CSI input file.     MTL dnsmasq file\n # MTL: server=/mtl/ address=/mtl/ domain=mtl,10.1.1.0,10.1.1.233,local dhcp-option=interface:bond0,option:domain-search,mtl interface=bond interface-name=pit.mtl,bond # This needs to point to the LiveCD IP address for provisioning in bare-metal environments. dhcp-option=interface:bond0,option:dns-server,10.1.1. dhcp-option=interface:bond0,option:ntp-server,10.1.1. # This must point at the router for the network; the L3/IP address for the VLAN. dhcp-option=interface:bond0,option:router,10.1.0. dhcp-range=interface:bond0,10.1.1.33,10.1.1.233,10m  Here is an example of what the Spine switch configuration should be.   Mellanox Configuration\n sw-spine-001 [standalone: master] # show run int vlan 1 interface vlan 1 interface vlan 1 ip address 10.1.0.2/16 primary interface vlan 1 ip dhcp relay instance 2 downstream interface vlan 1 magp 1 interface vlan 1 magp 1 ip virtual-router address 10.1.0. interface vlan 1 magp 1 ip virtual-router mac-address 00:00:5E:00:01: sw-spine-002 [standalone: master] # show run int vlan 1 interface vlan 1 interface vlan 1 ip address 10.1.0.3/16 primary interface vlan 1 ip dhcp relay instance 2 downstream interface vlan 1 magp 1 interface vlan 1 magp 1 ip virtual-router address 10.1.0. interface vlan 1 magp 1 ip virtual-router mac-address 00:00:5E:00:01:  Aruba Configuration\n sw-spine-001# show run int vlan 1 interface vlan vsx-sync active-gateways ip address 10.1.0.2/ active-gateway ip mac 12:01:00:00:01: active-gateway ip 10.1.0. ip mtu 9198 ip bootp-gateway 10.1.0. ip helper-address 10.92.100. exit sw-spine-002# show run int vlan 1 interface vlan vsx-sync active-gateways ip address 10.1.0.3/ active-gateway ip mac 12:01:00:00:01: active-gateway ip 10.1.0. ip mtu 9198 ip helper-address 10.92.100. exit  You should be able to ping the MTL router from ncn-m001.  2. ncn-m001 on reboot or NCN boot  Common Error messages.  2021-04-19 23:27:09 PXE-E18: Server response timeout. 2021-02-02 17:06:13 PXE-E99: Unexpected network error.  Verify the ip helper-address on VLAN 1 on the switches. This is the same configuration as above ^ \u0026ldquo;Mellanox Config\u0026rdquo; and \u0026ldquo;Aruba Config\u0026rdquo;  2.1. Verify DHCP packets can be forwarded from the workers to the MTL network (VLAN1)  If the Worker nodes cannot reach the metal network DHCP will fail. ALL WORKERS need to be able to reach the MTL network! This can normally be achieved by having a default route TEST  ncn-w001:~ # ping 10.1.0. PING 10.1.0.1 (10.1.0.1) 56(84) bytes of data. 64 bytes from 10.1.0.1: icmp_seq=1 ttl=64 time=0.361 ms 64 bytes from 10.1.0.1: icmp_seq=2 ttl=64 time=0.145 ms  If this fails you may have a misconfigured CAN or need to add a route to the MTL network.  ncn-w001:~ # ip route add 10.1.0.0/16 via 10.252.0.1 dev vlan 2.2. Verify BGP  Verify the BGP neighbors are in the established state on BOTH the switches.   Aruba BGP\n sw-spine-002# show bgp ipv4 u s VRF : default BGP Summary ----------- Local AS : 65533 BGP Router Identifier : 10.252.0.3 Peers : 4 Log Neighbor Changes : No Cfg. Hold Time : 180 Cfg. Keep Alive : 60 Confederation Id : 0 Neighbor Remote-AS MsgRcvd MsgSent Up/Down Time State AdminStatus 10.252.0.2 65533 45052 45044 02m:02w:02d Established Up 10.252.1.7 65533 78389 90090 02m:02w:02d Established Up 10.252.1.8 65533 78384 90059 02m:02w:02d Established Up 10.252.1.9 65533 78389 90108 02m:02w:02d Established Up  Mellanox BGP\n sw-spine-001 [standalone: master] # show ip bgp summary VRF name : default BGP router identifier : 10.252.0.2 local AS number : 65533 BGP table version : 39 Main routing table version: 39 IPV4 Prefixes : 18 IPV6 Prefixes : 0 L2VPN EVPN Prefixes : 0 ------------------------------------------------------------------------------------------------------------------ Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd ------------------------------------------------------------------------------------------------------------------ 10.252.1.7 4 65533 18018 20690 39 0 0 6:05:54:02 ESTABLISHED/6 10.252.1.8 4 65533 18014 20694 39 0 0 6:05:54:03 ESTABLISHED/6 10.252.1.9 4 65533 18010 20671 39 0 0 6:05:52:03 ESTABLISHED/6 2.3. Verify route to TFTP  On BOTH Aruba switches we need a single route to the TFTP server 10.92.100.60. This is needed because there are issues with Aruba ECMP hashing and TFTP traffic.  sw-spine-002# show ip route 10.92.100.60 Displaying ipv4 routes selected for forwarding \u0026#39;[x/y]\u0026#39; denotes [distance/metric] 10.92.100.60/32, vrf default, tag 0 via 10.252.1.9, [70/0], bgp  This route can be a static route or a BGP route that is pinned to a single worker. (1.4.2 patch introduces the BGP pinned route) Verify that you can ping the next hop of this route. For the example above we would ping 10.252.1.9. If this is not reachable this is your problem.  2.4. Test TFTP traffic (Aruba Only)  You can test the TFTP traffic by trying to download the ipxe.efi binary. Log into the leaf switch and try to download the iPXE binary. This requires that the leaf switch can talk to the TFTP server \u0026ldquo;10.92.100.60\u0026rdquo;  sw-leaf-001# start-shell sw-leaf-001:~$ sudo su sw-leaf-001:/home/admin# tftp 10.92.100. tftp\u0026gt; get ipxe.efi Received 1007200 bytes in 2.2 seconds tftp\u0026gt; get ipxe.efi Received 1007200 bytes in 2.2 seconds tftp\u0026gt; get ipxe.efi Received 1007200 bytes in 2.2 seconds  You can see here that the ipxe.efi binary is downloaded three times in a row. When we have seen issues with ECMP hashing this would fail intermittently.  2.5. Check DHCP lease is getting allocated  Check the KEA logs and verify that the lease is getting allocated.  kubectl logs -n services pod/$(kubectl get -n services pods | grep kea | head -n1 | cut -f 1 -d \u0026#39; \u0026#39;) -c cray-dhcp-kea 2021-04-21 00:13:05.416 INFO [kea-dhcp4.leases/24.139710796402304] DHCP4_LEASE_ALLOC [hwtype=1 02:23:28:01:30:10], cid=[00:78:39:30:30:30:63:31:73:30:62:31], tid=0x21f2433a: lease 10.104.0.23 has been allocated for 300 seconds  Here we can see that KEA is allocating a lease to 10.104.0.23. The lease MUST say DHCP4_LEASE_ALLOC. If it says DHCP4_LEASE_ADVERT, there is likely a problem. Restarting KEA will fix this issue most of the time.  2021-06-21 16:44:31.124 INFO [kea-dhcp4.leases/18.139837089017472] DHCP4_LEASE_ADVERT [hwtype=1 14:02:ec:d9:79:88], cid=[no info], tid=0xe87fad10: lease 10.252.1.16 will be advertised 2.6. Verify the DHCP traffic on the Workers  We have ran into issues on HPE servers and Aruba switches where the source address of the DHCP Offer is the Metallb address of KEA \u0026ldquo;10.92.100.222\u0026rdquo;. The source address of the DHCP Reply/Offer NEEDS to be the address of the VLAN interface on the Worker. Here is how to look at DHCP traffic on the workers.  ncn-w001:~ # tcpdump -envli bond0 port 67 or 68  You are looking for the source IP address of the DHCP Reply/Offer.   10.252.1.9.67 \u0026gt; 255.255.255.255.68: BOOTP/DHCP, Reply, length 309, hops 1, xid 0x98b0982e, Flags [Broadcast] Your-IP 10.252.1.17 Server-IP 10.92.100.60 Gateway-IP 10.252.0.1 Client-Ethernet-Address 14:02:ec:d9:79:88 file \u0026quot;ipxe.efi\u0026quot;[|bootp]  If the Source IP address of the DHCP Reply/Offer is the MetalLB IP address, the DHCP packet will never make it out of the NCN An example of this is below.  10.92.100.222.116 \u0026gt; 255.255.255.255.68: BOOTP/DHCP, Reply, length 309, hops 1, xid 0x260ea655, Flags [Broadcast] Your-IP 10.252.1.14 Server-IP 10.92.100.60 Gateway-IP 10.252.0.4 Client-Ethernet-Address 14:02:ec:d9:79:88 file \u0026quot;ipxe.efi\u0026quot;[|bootp]  If you run into this, the only solution that we have found so far is restarting KEA and making sure that it gets moved to a different worker. We believe this has something to do with conntrack.  2.7. Verify the switches are forwarding DHCP traffic.  If you made it this far and still cannot pxe boot, you may have run into the IP-Helper breaking on the switch. On all our switch vendors (Aruba, Dell, Mellanox) we have seen the IP-Helpers get stuck and stop forwarding DHCP traffic to the client.  The solutions vary from Vendor to Vendor so your mileage may vary on this. On the Arubas we have had to delete the entire VLAN configuration and re-apply it in order for the DHCP traffic to come back. This was even after a reboot. On the Dells we have had to do reboots in order to restore DHCP traffic. On the Mellanox we have had to delete the VLAN configuration and re-apply it in order for the DHCP traffic to come back. There seems to be something inherently wrong with how the IP-helper is implemented on switches, or we are doing DHCP so backwards that it breaks every switch\u0026hellip;    3. Compute Nodes/UANs/Application Nodes  The following are required for compute node PXE booting.  Verify BGP\n Verify route to TFTP Test TFTP traffic Check DHCP lease is getting allocated Verify the DHCP traffic on the Workers Verify the switches are forwarding DHCP traffic   Verify the IP-Helpers on the VLAN the computes nodes are booting over. This is typically VLAN 2 or VLAN 2xxx (MTN Computes) If the compute nodes make it past PXE and go into the PXE shell you can verify DNS and connectivity.  iPXE\u0026gt; dhcp Configuring (net0 98:03:9b:a8:60:88).................. No configuration methods succeeded (http://ipxe.org/040ee186) Configuring (net1 b4:2e:99:be:1a:37)...... ok iPXE\u0026gt; show dns net1.dhcp/dns:ipv4 = 10.92.100.225 iPXE\u0026gt; nslookup address api-gw-service-nmn.local iPXE\u0026gt; echo ${address} 10.92.100.71 "
},
{
	"uri": "/docs-csm/en-11/troubleshooting/kubernetes/troubleshoot_kubernetes_node_notready/",
	"title": "Troubleshoot Kubernetes Master Or Worker Node In Notready State",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Kubernetes Master or Worker node in NotReady state Use this procedure to check if a Kubernetes master or worker node is in a NotReady state.\nPrerequisites The kubectl get nodes command returns a NotReady state for a master or worker node.\nIdentify the node in question   Run thekubectl get nodes command to identify the node in NotReady state.\nncn-w001# kubectl get nodes NAME STATUS ROLES AGE VERSION ncn-m001 Ready master 27h v1.19.9 ncn-m002 Ready master 19h v1.19.9 ncn-m003 Ready master 18h v1.19.9 ncn-w001 NotReady \u0026lt;none\u0026gt; 36h v1.19.9 ncn-w002 Ready \u0026lt;none\u0026gt; 36h v1.19.9 ncn-w003 Ready \u0026lt;none\u0026gt; 36h v1.19.9   Recovery Steps   Ensure the node does not have an intentional NoSchedule taint.\nSee About Kubernetes Taints and Labels for more information about tainting and untainting a node.\nIf the node in question is not intentionally tainted causing the NotReady state, proceed to the next step and attempt to restart kubelet.\n  Restart the kubelet.\nRun the following command on the node in a NotReady state.\nncn-w001# systemctl restart kubelet   Try running the kubectl get nodes command and ensure the node is now in a Ready state.\n"
},
{
	"uri": "/docs-csm/en-11/troubleshooting/kubernetes/troubleshoot_liveliness_readiness_probe_failures/",
	"title": "Troubleshoot Liveliness Or Readiness Probe Failures",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Liveliness or Readiness Probe Failures Identify and troubleshoot Readiness or Liveliness probes that report services as unhealthy intermittently.\nThis is a known issue and can be classified into two categories, connection refused and client timeout. Both categories are shown in the procedure below. The commands in this procedure assume the user is logged into either a master or worker non-compute node (NCN).\nPrerequisites This procedure requires administrative privileges.\nProcedure   Troubleshoot a refused connection.\nncn-w001# kubectl get events -A | grep -i unhealthy | grep \u0026#34;connection refused\u0026#34; istio-system 5m24s Warning Unhealthy pod/istio-pilot-68477d98d-5bsmk Readiness probe failed: Get http://10.45.0.100:8080/ready: dial tcp 10.45.0.100:8080: connect: connection refused This may occur if the health check ran when the pod was being terminated. To confirm this is the case, check that the pod no longer exists. If that is true, disregard this unhealthy event.\nncn-w001# kubectl get pod/istio-pilot-68477d98d-5bsmk -n istio-system Error from server (NotFound): pods \u0026#34;istio-pilot-68477d98d-5bsmk\u0026#34; not found   Troubleshoot a client timeout.\nncn-w001# kubectl get events -A | grep -i unhealthy | grep \u0026#34;Client.Timeout|DeadlineExceeded\u0026#34; services 40m Warning Unhealthy pod/cray-bos-69f85bcd89-vdq52 Liveness probe failed: Get http://10.45.0.20:15020/app-health/cray-bos/livez: net/http: request canceled (Client.Timeout exceeded while awaiting headers) This may occur if the health check did not respond within the specified timeout. To confirm that the service is healthy, check the health using the curl command.\nncn-w001# curl -i http://10.45.0.20:15020/app-health/cray-bos/livez HTTP/1.1 200 OK Date: Tue, 07 Jul 2020 19:37:32 GMT Content-Length: 0 An HTTP Response of 2** or 3** from the curl test is considered success. For example, a response of 200 OK.\n  If there is an unhealthy event where the pod still exists and the curl test still fails, please contact support.\n"
},
{
	"uri": "/docs-csm/en-11/troubleshooting/kubernetes/troubleshoot_unresponsive_kubectl_commands/",
	"title": "Troubleshoot Unresponsive Kubectl Commands",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Unresponsive kubectl Commands Use this procedure to check if any kworkers are in an error state because of a high load. Once the error has been identified, workaround the issue by returning the high load to a normal level.\nThe kubectl command can become unresponsive because of a high load. Another symptom is that ps aux cannot return or complete because of aspects of the proc file system being locked.\nIf kubectl is non-responsive on ncn-w001, the commands can be run from any other master or worker non-compute node (NCN).\nPrerequisites The kubectl command is not responsive on a node.\nIdentify the kworker Issue   Check to see if kubectl is not responding because of a kworker issue.\n  List the process identification (pid) numbers of the kworkers in the D state.\nProcesses in the D state are blocked on I/O and are not an issue unless they remain blocked indefinitely. Use the command below to see which pids remain stuck in this state.\nncn-w001# ps aux |grep [k]worker|grep -e \u0026#34; D\u0026#34;| awk \u0026#39;{ print $2 }\u0026#39;   Show the stack for all kworkers in the D state.\nNote which kworkers clear and which ones remain stuck in this state over a period of time.\nncn-w001# for i in `ps aux | grep [k]worker | grep -e \u0026#34; D\u0026#34; |\\  awk \u0026#39;{print $2}\u0026#39;` ; do cat /proc/$i/stack; echo; done     Check to see what the load is on the worker node and gather data for any pids consuming a lot of CPU.\n  Monitor the processes and system resource usage.\nncn-w001# top top - 10:12:03 up 34 days, 17:31, 10 users, load average: 7.39, 9.16, 10.99 Tasks: 2155 total, 4 running, 2141 sleeping, 1 stopped, 9 zombie %Cpu(s): 4.3 us, 2.5 sy, 0.0 ni, 93.0 id, 0.0 wa, 0.0 hi, 0.3 si, 0.0 st MiB Mem : 257510.5+total, 69119.86+free, 89578.68+used, 98812.04+buff/cache MiB Swap: 0.000 total, 0.000 free, 0.000 used. 173468.1+avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 6105 root 20 0 193436 182772 2300 S 60.00 0.069 13485:54 lldpad 49574 root 20 0 14.299g 495212 60896 S 47.54 0.188 31582:58 kubelet 1 root 20 0 231236 19436 6572 S 38.69 0.007 16904:47 systemd 43098 root 20 0 16.148g 652640 78748 S 38.69 0.248 18721:18 containerd 20229 root 20 0 78980 14648 6448 S 35.08 0.006 15421:51 systemd 1515295 1001 20 0 16.079g 5.439g 96312 S 11.48 2.163 12480:39 java 4706 message+ 20 0 41060 5620 3724 S 8.852 0.002 3352:38 dbus-daemon 1282935 101 20 0 685476 38556 13748 S 6.557 0.015 262:09.88 patroni 81539 root 20 0 300276 161372 26036 S 5.902 0.061 4145:40 mixs 89619 root 20 0 4731796 498600 24144 S 5.902 0.189 2898:54 envoy 85600 root 20 0 2292564 123596 23248 S 4.590 0.047 2211:58 envoy ...   Generate a performance counter profile for the pids consuming a lot of CPU.\nReplace the PID value with the actual pid number.\nncn-w001# perf top -g -p PID Samples: 18 of event \u0026#39;cycles\u0026#39;, Event count (approx.): 4065227 Children Self Shared Object Symbol + 29.31% 9.77% [kernel] [k] load_balance + 19.54% 19.54% [kernel] [k] find_busiest_group + 11.17% 11.17% kubelet [.] 0x0000000000038d3c + 9.77% 9.77% [kernel] [k] select_task_rq_fair + 9.77% 9.77% [kernel] [k] cpuacct_charge ...   Verify that ps -ef completes.\nncn-w001# ps -ef     Check the /var/log/messages on the worker node to see if there are any errors.\nncn-w001# grep -i error /var/log/messages \u0026lt;nil\u0026gt;\u0026#34; 2020-07-19T07:19:34.485659+00:00 ncn-w001 containerd[43098]: time=\u0026#34;2020-07-19T07:19:34.485540765Z\u0026#34; level=info msg=\u0026#34;Exec process \\\u0026#34;9946991ef8108d21c163a04c9085fd15a60e3991b8e9d7b2250a071df9b6cbb8\\\u0026#34; exits with exit code 0 and error \u0026lt;nil\u0026gt;\u0026#34; 2020-07-19T07:19:38.468970+00:00 ncn-w001 containerd[43098]: time=\u0026#34;2020-07-19T07:19:38.468818388Z\u0026#34; level=info msg=\u0026#34;Exec process \\\u0026#34;e6fe9ccbb1127a77f8c9db84b339dafe068f9e08579962f790ebf882ee35e071\\\u0026#34; exits with exit code 0 and error \u0026lt;nil\u0026gt;\u0026#34; 2020-07-19T07:19:44.440413+00:00 ncn-w001 containerd[43098]: time=\u0026#34;2020-07-19T07:19:44.440243465Z\u0026#34; level=info msg=\u0026#34;Exec process \\\u0026#34;7a3cf826f008c37bd0fe89382561af42afe37ac4d52f37ce9312cc950248f4da\\\u0026#34; exits with exit code 0 and error \u0026lt;nil\u0026gt;\u0026#34; 2020-07-19T07:20:02.442421+00:00 ncn-w001 containerd[43098]: time=\u0026#34;2020-07-19T07:20:02.442266943Z\u0026#34; level=error msg=\u0026#34;StopPodSandbox for \\\u0026#34;d449618d075b918fd6397572c79bd758087b31788dd8bf40f4dc10bb1a013a68\\\u0026#34; failed\u0026#34; error=\u0026#34;failed to destroy network for sandbox \\\u0026#34;d449618d075b918fd6397572c79bd758087b31788dd8bf40f4dc10bb1a013a68\\\u0026#34;: Multus: Err in getting k8s network from pod: getPodNetworkAnnotation: failed to query the pod sma-monasca-agent-xkxnj in out of cluster comm: pods \\\u0026#34;sma-monasca-agent-xkxnj\\\u0026#34; not found\u0026#34; 2020-07-19T07:20:04.440834+00:00 ncn-w001 containerd[43098]: time=\u0026#34;2020-07-19T07:20:04.440742542Z\u0026#34; level=info msg=\u0026#34;Exec process \\\u0026#34;2a751ca1453d7888be88ab4010becbb0e75b7419d82e45ca63e55e4155110208\\\u0026#34; exits with exit code 0 and error \u0026lt;nil\u0026gt;\u0026#34; 2020-07-19T07:20:06.587325+00:00 ncn-w001 containerd[43098]: time=\u0026#34;2020-07-19T07:20:06.587133372Z\u0026#34; level=error msg=\u0026#34;collecting metrics for bf1d562e060ba56254f5f5ea4634ef4ae189abb462c875e322c3973b83c4c85d\u0026#34; error=\u0026#34;ttrpc: closed: unknown\u0026#34; 2020-07-19T07:20:14.450624+00:00 ncn-w001 containerd[43098]: time=\u0026#34;2020-07-19T07:20:14.450547541Z\u0026#34; level=info msg=\u0026#34;Exec process \\\u0026#34;ceb384f1897d742134e7d2c9da5a62650ed1274f0ee4c5a17fa9cac1a24b6dc4\\\u0026#34; exits with exit code 0 and error ...   Recovery Steps   Restart the kubelet.\nRun the following command on the node where kubectl in non-responsive.\nncn-w001# systemctl restart kubelet If restarting the kubelet did not resolve the issue, proceed to the next step to restart the container runtime environment.\n  Restart the container runtime environment on the node with the issue.\nThis will likely hang or fail to complete without a timeout.\nncn-w001# systemctl restart containerd   Reboot the node.\nThe node must be rebooted if the remediation of restarting kubelet and containerd did not resolve the kworker and high load average issue.\nIMPORTANT: If the node experiencing issues is ncn-w001, the ipmitool command must be run from another node that has access to the management plane. The admin will be cut off if using ncn-w001 when powering off ncn-w001-mgmt.\nReplace NCN_NAME in the commands below with the node experiencing the issue. In this example, it is ncn-w999.\nncn-w001# export USERNAME=root ncn-w001# export IPMI_PASSWORD=changeme ncn-w001# export NCN_NAME=ncn-w999 ncn-w001# ipmitool -U $USERNAME -E -I lanplus -H ${NCN_NAME}-mgmt power off; sleep 5; ncn-w001# ipmitool -U $USERNAME -E -I lanplus -H ${NCN_NAME}-mgmt power show; echo ncn-w001# ipmitool -U $USERNAME -E -I lanplus -H ${NCN_NAME}-mgmt power on; sleep 5; ncn-w001# ipmitool -U $USERNAME -E -I lanplus -H ${NCN_NAME} power show; echo   Watch the console of the node being rebooted.\nThis command will not return anything, but will show the ttyS0 console of the node. Use ~. to disconnect. The same ~. keystroke can also break an SSH session. After doing this, the connection to the SSH session may need to be reestablished.\nncn-w001# ipmitool -U $USERNAME -E -I lanplus -H NCN_NAME-mgmt sol activate   Try running a kubectl command on the node where it was previously unresponsive.\n"
},
{
	"uri": "/docs-csm/en-11/update_product_stream/",
	"title": "Update Product Stream",
	"tags": [],
	"description": "",
	"content": "Update CSM Product Stream The software included in the CSM product stream is released in more than one way. The initial product release may be augmented with patches, late-breaking workarounds and documentation updates, or hotfixes after the release.\nThe CSM documentation is included within the CSM product release tarball inside the docs-csm RPM. After it has been installed, the documentation will be available at /usr/share/doc/csm as installed by the docs-csm RPM.\nTopics:  Download and Extract CSM Product Release Apply Patch to CSM Release Check for Latest Workarounds and Documentation Updates Check for and Apply Workarounds Check for Field Notices about Hotfixes  The topics in this chapter need to be done as part of an ordered procedure so are shown here with numbered topics.\nDetails Download and Extract CSM Product Release About this task Role System installer\nObjective Acquire a CSM software release tarball for installation on the HPE Cray EX supercomputer.\nLimitations None.\nProcedure   Download the CSM software release tarball for the HPE Cray EX system to a Linux system.\nlinux# export ENDPOINT=URL_SERVER_Hosting_tarball linux# export CSM_RELEASE=csm-x.y.z linux# wget ${ENDPOINT}/${CSM_RELEASE}.tar.gz   Extract the source release distribution.\nIf doing a first time install, this can be done on a Linux system, but for an upgrade, it could be done on one of the NCNs, such as ncn-m001.\nlinux# tar -xzvf ${CSM_RELEASE}.tar.gz   Before using this software release, check for any patches available for it. If patches are available, see Apply Patch to CSM Release.\n  Apply Patch to CSM Release About this task Role System installer\nObjective Apply a CSM update patch to the release tarball. This ensures that the latest CSM product artifacts are installed on the HPE Cray EX supercomputer.\nLimitations None.\nProcedure   Verify that the Git version is at least 2.16.5 on the Linux system which will apply the patch.\nThe patch process is known to work with Git \u0026gt;= 2.16.5. Older versions of Git may not correctly apply the binary patch.\nlinux# git version git version 2.26.2 If the Git version is less than 2.16.15, update Git to at least that version.\n  Download the compressed CSM software package patch csm-x.y.z-x.z.a.patch.gz for the HPE Cray EX system.\nlinux# export ENDPOINT=URL_SERVER_Hosting_tarball linux# export CSM_RELEASE=csm-x.y.z linux# export PATCH_RELEASE=x.z.a linux# wget ${ENDPOINT}/${CSM_RELEASE}-${PATCH_RELEASE}.patch.gz The following steps should be run from the node to which the original $CSM_RELEASE release was downloaded and extracted.\n  Uncompress the patch.\nlinux# gunzip -v ${CSM_RELEASE}-${PATCH_RELEASE}.patch.gz   Apply the patch.\nlinux# git apply -p2 --whitespace=nowarn \\  --directory=${CSM_RELEASE} \\  ${CSM_RELEASE}-${PATCH_RELEASE}.patch   Set a variable to reflect the new version.\nlinux# export NEW_CSM_RELEASE=\u0026#34;$(./${CSM_RELEASE/lib/version.sh)\u0026#34;   Update the name of the CSM release distribution directory.\nlinux# mv -v $CSM_RELEASE $NEW_CSM_RELEASE   Create a tarball from the patched release distribution.\nlinux# tar -cvzf ${NEW_CSM_RELEASE}.tar.gz \u0026#34;${NEW_CSM_RELEASE}/\u0026#34;   This tarball can now be used in place of the original CSM software release tarball.\nCheck for Latest Workarounds and Documentation Updates About this task Role System installer\nObjective Acquire the late-breaking CSM workarounds and documentation update RPMs. These fixes were not available until after the software release. The software installation and upgrade processes have several breakpoints where you check and apply workarounds before or after a critical procedure.\nThis command will report the version of your installed documentation.\nncn# rpm -q docs-csm Limitations None.\nProcedure   Check the version of the currently installed CSM documentation.\nncn# rpm -q docs-csm   Download and upgrade the latest workaround and documentation RPMs.\nlinux# rpm -Uvh --force https://storage.googleapis.com/csm-release-public/shasta-1.5/docs-csm/docs-csm-latest.noarch.rpm linux# rpm -Uvh --force https://storage.googleapis.com/csm-release-public/shasta-1.5/csm-install-workarounds/csm-install-workarounds-latest.noarch.rpm If this machine does not have direct Internet access these RPMs will need to be externally downloaded and then copied to the system. This example copies them to ncn-m001.\nlinux# wget https://storage.googleapis.com/csm-release-public/shasta-1.5/docs-csm/docs-csm-latest.noarch.rpm linux# wget https://storage.googleapis.com/csm-release-public/shasta-1.5/csm-install-workarounds/csm-install-workarounds-latest.noarch.rpm linux# scp -p docs-csm-*rpm csm-install-workarounds-*rpm ncn-m001:/root linux# ssh ncn-m001 ncn-m001# rpm -Uvh --force docs-csm-latest.noarch.rpm ncn-m001# rpm -Uvh --force csm-install-workarounds-latest.noarch.rpm   Check the version of the newly installed documentation.\nncn# rpm -q docs-csm   Check for and Apply Workarounds About this task Role System installer\nObjective The software installation and upgrade processes have several breakpoints where you check and apply workarounds before or after a critical procedure. Check to see if workarounds need to be applied at a particular point of the install process. If there are, apply those workarounds.\nLimitations None.\nPrerequisites  The latest workaround RPM is installed. The name of the workaround breakpoint (e.g. before-configuration-payload or after-sysmgmt-manifest) is known.  Procedure   Change to the directory containing the workarounds to be applied at this breakpoint.\nlinux# pushd /opt/cray/csm/workarounds/\u0026lt;put-actual-breakpoint-name-here\u0026gt;   List all subdirectories of this directory.\nlinux# find -maxdepth 1 -type d ! -name . | cut -c3- If there is nothing listed, there are no workarounds to be applied at this breakpoint, and you can skip the next step.\n  For each subdirectory which is listed, apply the workaround described within it.\nPerform the following steps for each subdirectory which was listed in the previous step.\n  Change directory into the subdirectory.\nlinux# pushd \u0026lt;put-subdirectory-name-here\u0026gt;   View the README.md file in this directory, and carefully follow its instructions.\n  Return to the main directory for workarounds for this breakpoint.\nlinux# popd     The procedure is complete. Return to your original directory.\nlinux# popd   Check for Field Notices about Hotfixes About this task Role System installer\nObjective Collect all available Field Notices about Hotfixes which should be applied to this CSM software release.\nLimitations None.\nProcedure Check with HPE Cray service for any Field Notices about Hotfixes which should be applied to this CSM software release.\n"
},
{
	"uri": "/docs-csm/en-11/troubleshooting/known_issues/discovery_aruba_snmp_issue/",
	"title": "Air Cooled Hardware Is Not Getting Properly Discovered With Aruba Leaf Switches.",
	"tags": [],
	"description": "",
	"content": "Air cooled hardware is not getting properly discovered with Aruba leaf switches. Symptoms:  The System has Aruba leaf switches. Air cooled hardware is reported to not be present under State Components and Inventory Redfish Endpoints in Hardware State Manager by the hsm_discovery_verify.sh script. BMCs have IP addresses given out by DHCP, but in DNS their xname hostname does not resolve.  Procedure to determine if you affected by this known issue:   Determine the name of the last HSM discovery job that ran.\nncn# HMS_DISCOVERY_POD=$(kubectl -n services get pods -l app=hms-discovery | tail -n 1 | awk \u0026#39;{ print $1 }\u0026#39;) ncn# echo $HMS_DISCOVERY_POD hms-discovery-1624314420-r8c49   Look at the logs of the HMS discovery job to find the MAC addresses associated with instances of the MAC address in HSM not found in any switch! error messages. The following command will parse the logs are report these MAC addresses.\n Each of the following MAC address does not contain a ComponentID in Hardware State Manager in the Ethernet interfaces table, which can be viewed with: cray hsm inventory ethernetInterfaces list.\n ncn# UNKNOWN_MACS=$(kubectl -n services logs $HMS_DISCOVERY_POD hms-discovery | jq \u0026#39;select(.msg == \u0026#34;MAC address in HSM not found in any switch!\u0026#34;).unknownComponent.ID\u0026#39; -r -c) ncn# echo \u0026#34;$UNKNOWN_MACS\u0026#34; b42e99dff361 9440c9376780 b42e99bdd255 b42e99dfecf1 b42e99dfebc1 b42e99dfec49   Look at the logs of the HMS discovery job to find the MAC address associated with instances of the Found MAC address in switch. log messages. The following command will parse the logs are report these MAC addresses.\nncn# FOUND_IN_SWITCH_MACS=$(kubectl -n services logs $HMS_DISCOVERY_POD hms-discovery | jq \u0026#39;select(.msg == \u0026#34;Found MAC address in switch.\u0026#34;).macWithoutPunctuation\u0026#39; -r) ncn# echo \u0026#34;$FOUND_IN_SWITCH_MACS\u0026#34; b42e99bdd255   Perform a diff between the 2 sets of collected MAC addresses to see if the Aruba leaf switches in the system are affected by a known SNMP issues with Aruba switches.\nncn# diff -y \u0026lt;(echo \u0026#34;$UNKNOWN_MACS\u0026#34; | sort -u) \u0026lt;(echo \u0026#34;$FOUND_IN_SWITCH_MACS\u0026#34; | sort -u) 9440c9376780 \u0026lt; b42e99bdd255 b42e99bdd255 b42e99dfebc1 \u0026lt; b42e99dfec49 \u0026lt; b42e99dfecf1 \u0026lt; b42e99dff361 \u0026lt; If there are any MAC addresses on the left column that are not on the right column, then it is likely the leaf switches in the system are being affected by the SNMP issue. Apply the workaround described in the following procedure to the Aruba leaf switches in the system.\nIf all of the MAC addresses on the left column are present in the right column, then you are not affected by this known issue.\n  "
},
{
	"uri": "/docs-csm/en-11/troubleshooting/known_issues/incorrect_output_for_bos_command_rerun/",
	"title": "BOS/boa Incorrect Command Is Output To Rerun A Failed Operation.",
	"tags": [],
	"description": "",
	"content": "BOS/BOA Incorrect command is output to rerun a failed operation. When the Boot Orchestration Agent (BOA), an agent of the Boot Orchestration Service (BOS), encounters a failure, it issues a command to rerun the operation for any nodes that experienced the failure. However, the syntax of this command is faulty.\nThe faulty command includes squiggly braces around a comma separated list of quoted nodes. These squiggly braces, single quotes, and the spaces separating the individual nodes all need to be removed. Then, this reformatted command can be run.\nExample of a faulty command in the BOA log:\nERROR - cray.boa.agent - You can attempt to boot these nodes by issuing the command: cray bos v1 session create --template-uuid shasta-1.4-csm-bare-bones-image --operation boot --limit {'x3000c0s25b3n0', 'x3000c0s23b4n0', 'x3000c0s20b4n0', 'x3000c0s37b2n0', 'x1000c0s7b0n0', 'x1000c3s3b0n0', 'x1000c1s1b1n1', 'x1000c0s7b1n0', 'x1000c2s3b1n1', 'x3000c0s20b3n0', 'x3000c0s25b1n0', 'x3000c0s20b1n0', 'x3000c0s20b2n0', 'x3000c0s25b2n0', 'x3000c0s23b3n0', 'x3000c0s25b4n0', 'x1000c1s1b1n0', 'x3000c0s37b1n0', 'x3000c0s37b4n0', 'x1000c2s3b1n0', 'x3000c0s23b2n0', 'x3000c0s23b1n0', 'x1000c0s7b1n1', 'x3000c0s37b3n0', 'x1000c0s7b0n1'} Example of the correct command:\ncray bos v1 session create --template-uuid shasta-1.4-csm-bare-bones-image --operation boot --limit x3000c0s25b3n0,x3000c0s23b4n0,x3000c0s20b4n0,x3000c0s37b2n0,x1000c0s7b0n0,x1000c3s3b0n0,x1000c1s1b1n1,x1000c0s7b1n0,x1000c2s3b1n1,x3000c0s20b3n0,x3000c0s25b1n0,x3000c0s20b1n0,x3000c0s20b2n0,x3000c0s25b2n0,x3000c0s23b3n0,x3000c0s25b4n0,x1000c1s1b1n0,x3000c0s37b1n0,x3000c0s37b4n0,x1000c2s3b1n0,x3000c0s23b2n0,x3000c0s23b1n0,x1000c0s7b1n1,x3000c0s37b3n0,x1000c0s7b0n1 Cut and paste the faulty command and assign to an environment variable.\n# CMD=\u0026quot;cray bos v1 session create --template-uuid shasta-1.4-csm-bare-bones-image --operation boot --limit {'x3000c0s25b3n0', 'x3000c0s23b4n0', 'x3000c0s20b4n0', 'x3000c0s37b2n0', 'x1000c0s7b0n0', 'x1000c3s3b0n0', 'x1000c1s1b1n1', 'x1000c0s7b1n0', 'x1000c2s3b1n1', 'x3000c0s20\\ b3n0', 'x3000c0s25b1n0', 'x3000c0s20b1n0', 'x3000c0s20b2n0', 'x3000c0s25b2n0', 'x3000c0s23b3n0', 'x3000c0s25b4n0', 'x1000c1s1b1n0', 'x3000c0s37b1n0', 'x3000c0s37b4n0', 'x1000c2s3b1n0', 'x3000c0s23b2n0', 'x3000c0s23b1n0', 'x1000c0s7b1n1', 'x3000c0s37b3n0', 'x1000c0s7b0n\\ 1'}\u0026quot; Then, paste this command to get the corrected output.\n# echo $CMD |sed s/,\\ /,/g|sed s/{//g|sed s/}//g|sed s/\\'//g "
},
{
	"uri": "/docs-csm/en-11/troubleshooting/known_issues/discovery_job_not_creating_redfish_endpoints/",
	"title": "Hms Discovery Job Not Creating Redfishendpoints In Hardware State Manager",
	"tags": [],
	"description": "",
	"content": "HMS Discovery job not creating RedfishEndpoints in Hardware State Manager There is a known issue with the HMS Discovery cronjob when a BMC does not respond by its IP address for some reason the discovery job will not create a RedfishEndpoint for the BMC in Hardware State Manager (HSM). However, it does update the BMC MAC address in HSM with its xname. The discovery job only creates a new RedfishEndpoints when it encounters an unknown MAC address without a xname associated with it.\nThis troubleshooting procedure is only applicable for Air Cooled NodeBMCs and RouterBMCs.\nPrerequisites  The Cray CLI has been initialized. Only applicable to an Air Cooled NodeBMC or RouterBMC.  Symptoms  The MAC address for the BMC in HSM has an IP and component id. The BMC is pingable. There is no RedfishEndpoint for the BMC in HSM.  Check for symptoms   Setup an environment variable with to store the xname of the BMC.\n This should be either the xname for a NodeBMC (xXcCsSbB) or RouterBMC (xXcCrRbB).\n ncn# export BMC=x3000c0s18b0   Check to see if HSM if the component ID for a BMC has a MAC address and IP associated with it.\nncn# cray hsm inventory ethernetInterfaces list --component-id $BMC [[results]] ID = \u0026#34;54802852b706\u0026#34; Description = \u0026#34; MACAddress = \u0026#34;54:80:28:52:b7:06\u0026#34; LastUpdate = \u0026#34;2021-06-15T14:30:21.195015Z\u0026#34; ComponentID = \u0026#34;x3000c0s18b0\u0026#34; Type = \u0026#34;NodeBMC\u0026#34; [[results.IPAddresses]] IPAddress = \u0026#34;10.254.1.27\u0026#34; [[results]] ID = \u0026#34;54802852b707\u0026#34; Description = \u0026#34;Configuration of this Manager Network Interface\u0026#34; MACAddress = \u0026#34;54:80:28:52:b7:07\u0026#34; LastUpdate = \u0026#34;2021-06-15T14:37:52.078528Z\u0026#34; ComponentID = \u0026#34;x3000c0s18b0\u0026#34; Type = \u0026#34;NodeBMC\u0026#34; IPAddresses = [] Now set an environment variable to store the MAC address of the BMC that has an IP address:\n Make sure to use the normalized MAC address from the ID field.\n ncn# export BMC_MAC=54802852b706   Verify that the IP address associated with the MAC address is pingable.\nncn# ping $BMC PING x3000c0s18b0 (10.254.1.27) 56(84) bytes of data. 64 bytes from x3000c0s18b0 (10.254.1.27): icmp_seq=1 ttl=255 time=0.342 ms 64 bytes from x3000c0s18b0 (10.254.1.27): icmp_seq=2 ttl=255 time=0.152 ms 64 bytes from x3000c0s18b0 (10.254.1.27): icmp_seq=3 ttl=255 time=0.205 ms 64 bytes from x3000c0s18b0 (10.254.1.27): icmp_seq=4 ttl=255 time=0.291 ms ^C --- x3000c0s18b0 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3067ms rtt min/avg/max/mdev = 0.152/0.247/0.342/0.075 ms   Verify that No Redfish endpoint for the NodeBMC or RouterBMC is present in HSM.\nncn# cray hsm inventory redfishEndpoints describe $BMC Usage: cray hsm inventory redfishEndpoints describe [OPTIONS] XNAME Try \u0026#39;cray hsm inventory redfishEndpoints describe --help\u0026#39; for help. Error: Missing argument \u0026#39;XNAME\u0026#39;.   If the BMC has a MAC Address with a component ID and does not have a RedfishEndpoint in HSM, then proceed to the next section.\n  Solution   Delete the MAC address associated with the BMC from HSM.\nncn# cray hsm inventory ethernetInterfaces delete $BMC_MAC After a few minutes the MAC address and IP address should get added back into HSM:\nncn# cray hsm inventory ethernetInterfaces describe $BMC_MAC ID = \u0026#34;54802852b706\u0026#34; Description = \u0026#34; MACAddress = \u0026#34;54:80:28:52:b7:06\u0026#34; LastUpdate = \u0026#34;2021-06-28T18:15:21.50797Z\u0026#34; ComponentID = \u0026#34; Type = \u0026#34; [[IPAddresses]] IPAddress = \u0026#34;10.254.1.27\u0026#34;   Wait for the hms-discovery cronjob to run again and run to completion after the MAC was deleted.\nncn# kubectl -n services get pods -l app=hms-discovery NAME READY STATUS RESTARTS AGE hms-discovery-1624901400-wsfxv 0/2 Completed 0 28m hms-discovery-1624901580-xpsj7 0/2 Completed 0 25m hms-discovery-1624901760-tbw6t 0/2 Completed 0 22m hms-discovery-1624901940-rxwjk 0/2 Completed 0 19m hms-discovery-1624902120-4njrx 0/2 Completed 0 16m hms-discovery-1624902300-jcgd8 0/2 Completed 0 13m hms-discovery-1624902480-468sx 0/2 Completed 0 10m hms-discovery-1624902660-gdkmh 0/2 Completed 0 7m52s hms-discovery-1624902840-nlzw2 0/2 Completed 0 4m50s hms-discovery-1624903020-qk6ww 0/2 Completed 0 109s   Verify that the MAC address has a component ID associated with it.\nncn# cray hsm inventory ethernetInterfaces describe $BMC_MAC ID = \u0026#34;54802852b706\u0026#34; Description = \u0026#34; MACAddress = \u0026#34;54:80:28:52:b7:06\u0026#34; LastUpdate = \u0026#34;2021-06-28T18:18:15.960235Z\u0026#34; ComponentID = \u0026#34;x3000c0s18b0\u0026#34; Type = \u0026#34;NodeBMC\u0026#34; [[IPAddresses]] IPAddress = \u0026#34;10.254.1.27\u0026#34;   Verify that that a RedfishEndpoint now exists for the BMC.\n The BMC when first added to HSM may not be DiscoverOK right away. It may take up 5 minutes for BMC hostname to start resolving in DNS. The HMS Discovery cronjob should automatically trigger a discovery for any RedfishEndpoints that are not in the DiscoveryOk or DiscoveryStated states, such as HTTPsGETFailed.\n ncn# cray hsm inventory redfishEndpoints describe $BMC ID = \u0026#34;x3000c0s18b0\u0026#34; Type = \u0026#34;NodeBMC\u0026#34; Hostname = \u0026#34;x3000c0s18b0\u0026#34; Domain = \u0026#34; FQDN = \u0026#34;x3000c0s18b0\u0026#34; Enabled = true UUID = \u0026#34;9a856688-e286-54ff-989f-1f8475430231\u0026#34; User = \u0026#34;root\u0026#34; Password = \u0026#34; MACAddr = \u0026#34;54802852b706\u0026#34; RediscoverOnUpdate = true [DiscoveryInfo] LastDiscoveryAttempt = \u0026#34;2021-06-28T18:26:05.902976Z\u0026#34; LastDiscoveryStatus = \u0026#34;DiscoverOK\u0026#34; RedfishVersion = \u0026#34;1.6.0\u0026#34;   "
},
{
	"uri": "/docs-csm/en-11/troubleshooting/kubernetes/kubernetes_log_file_locations/",
	"title": "Kubernetes Log File Locations",
	"tags": [],
	"description": "",
	"content": "Kubernetes Log File Locations Locations of various K8s log types on the system.\n   Log Type Component Purpose Location     Kubernetes Master API server Responsible for serving the API kubectl -n kube-system logs -l component=kube-apiserver   Scheduler Responsible for making scheduling decisions kubectl -n kube-system logs -l component=kube-scheduler    Controller Manages replication controllers kubectl -n kube-system logs -l component=kube-controller-manager    Kubernetes Worker Kubelet Responsible for running containers on the node journalctl -xeu kubelet   Kube proxy Responsible for service load balancing kubectl -n kube-system logs -l k8s-app=kube-proxy     "
},
{
	"uri": "/docs-csm/en-11/troubleshooting/kubernetes/kubernetes_troubleshooting_information/",
	"title": "Kubernetes Troubleshooting Information",
	"tags": [],
	"description": "",
	"content": "Kubernetes Troubleshooting Information Commands for performing basic Kubernetes cluster troubleshooting.\nAccess Pod Logs Use one of the following commands to retrieve pod-related logs:\nncn# kubectl logs POD_NAME ncn# kubectl logs POD_NAME -c CONTAINER_NAME If the pods keeps crashing, open a log for the previous instance using the following command:\nncn# kubectl logs -p POD_NAME Describe a Node Use the following command to retrieve information about a node\u0026rsquo;s condition, such as OutOfDisk, MemoryPressure, DiskPressure, etc.\nncn# kubectl describe node NODE_NAME Describe a Pod Use the following command to retrieve information that can help debug pod-related errors.\nncn# kubectl describe pod POD_NAME Use the following command to list all of the containers in a pod, as shown in the following example:\nncn# kubectl describe pod/cray-tftp-6f85767d76-b28gc -n default Open a Shell on a Pod Use the following command to connect to a pod:\nncn# kubectl exec -it POD_NAME -c CONTAINER_NAME /bin/sh Run a single Command on a Pod Use the following command to execute a command inside a pod:\nncn# kubectl exec POD_NAME ls / Connect to a Running Container Use the following command to connect to a currently running container:\nncn# kubectl attach POD_NAME -i Scale a Deployment Use the deployment command to scale a deployment up or down, as shown in the following examples:\nncn# kubectl scale deployment APPLICATION_NAME --replicas=0 ncn# kubectl scale deployment APPLICATION_NAME --replicas=3 Remove a Deployment with the Manifest and Reapply the Deployment Use the following command to remove components of the deployment\u0026rsquo;s manifest, such as services, network policies, and more:\nncn# kubectl delete –f APPLICATION_NAME.yaml Use the following command to reapply the deployment:\nncn# kubectl apply –f APPLICATION_NAME.yaml Delete a Pod Pods can be configured to restart after getting deleted. Use the following command to delete a pod:\nncn# kubectl delete pod POD_NAME CAUTION: It is recommended to be careful while deleting deployments or pods, as doing so can have an affect on other pods.\n"
},
{
	"uri": "/docs-csm/en-11/troubleshooting/known_issues/cfs_sessions_stuck_in_pending/",
	"title": "CFS Sessions Are Stuck In Pending State",
	"tags": [],
	"description": "",
	"content": "CFS Sessions are Stuck in Pending State In rare cases it is possible that a CFS session can be stuck in a pending state. Sessions should only enter the pending state briefly, for no more than a few seconds while the corresponding Kubernetes job is being scheduled. If any sessions are in this state for more than a minute, they can safely be deleted. If the sessions were created automatically and retires are enabled, the sessions should be recreated automatically.\nPending sessions can be found with the following command:\ncray cfs sessions list --status pending Stuck sessions that were created by the cfs-batcher can block further sessions from being scheduled against the same components. In the event that sessions are not being scheduled against some components, check the list of pending sessions to see if any are stuck and targeting the same component.\n"
},
{
	"uri": "/docs-csm/en-11/troubleshooting/known_issues/craycli_403_forbidden_errors/",
	"title": "Cray Cli 403 Forbidden Errors",
	"tags": [],
	"description": "",
	"content": "Cray CLI 403 Forbidden Errors There is a known issue where the keycloak configuration obtained from LDAP is incomplete causing the keycloak-users-localize job to fail to complete. This, in turn, causes 403 Forbidden errors when trying to use the craycli. This can also cause a keycloak test to fail during CSM health validation.\nFix To recover from this situation, the following can be done.\n Log into the Keycloak admin console. See Access the Keycloak User Management UI Delete the shasta-user-federation-ldap entry from the “User Federation” page. Wait three minutes for the configuration to resync. Check to see if the keycloak-users-localize job has completed. ncn# kubectl -n services wait --for=condition=complete --timeout=10s job/`kubectl -n services get jobs | grep users-localize | awk \u0026#39;{print $1}\u0026#39;`  If the above command returns output containing condition met then the issue is resolved and you can skip the rest of the steps. If the above command returns output containing error: timed out waiting for the condition then check the logs of the keycloak-users-localize pod. ncn# kubectl -n services logs `kubectl -n services get pods | grep users-localize | awk \u0026#39;{print $1}\u0026#39;` keycloak-localize  If you see an error showing that there is a duplicate group, complete the next step. Go to the Groups page in the Keycloak admin console and delete the groups. Wait three minutes for the configuration to resync. Check again to make sure the job has now completed. ncn# kubectl -n services wait --for=condition=complete --timeout=10s job/`kubectl -n services get jobs | grep users-localize | awk \u0026#39;{print $1}\u0026#39;` You should see output containing condition met\n  "
},
{
	"uri": "/docs-csm/en-11/troubleshooting/interpreting_hms_health_check_results/",
	"title": "Interpreting Hms Health Check Results",
	"tags": [],
	"description": "",
	"content": "Interpreting HMS Health Check Results Table of contents:  Introduction HMS Smoke Tests HMS Functional Tests Additional Troubleshooting  Introduction This document describes how to interpret the results of the HMS health check scripts and techniques for troubleshooting when failures occur.\nHMS Smoke Tests The HMS smoke tests consist of bash scripts that check the status of HMS service pods and jobs in Kubernetes and verify HTTP status codes returned by the HMS service APIs. Additionally, there is one test called smd_discovery_status_test_ncn-smoke.sh which verifies that the system hardware has been discovered successfully. The hms_run_ct_smoke_tests_ncn-resources.sh wrapper script checks for executable files in the HMS smoke test directory on the NCN and runs all tests found in succession.\nncn# /opt/cray/tests/ncn-resources/hms/hms-test/hms_run_ct_smoke_tests_ncn-resources.sh searching for HMS CT smoke tests... found 11 HMS CT smoke tests... running HMS CT smoke tests... A summary of the test results is printed at the bottom of the output.\nHMS smoke tests ran with 2/11 failures exiting with status code: 1 The tests print the commands being executed while running. They also print the command output and status code if failures occur in order to help with debugging.\nThe following is an example of a pod status failure:\nrunning '/opt/cray/tests/ncn-smoke/hms/hms-reds/reds_smoke_test_ncn-smoke.sh'... Running reds_smoke_test... (11:40:33) Running '/opt/cray/tests/ncn-resources/hms/hms-test/hms_check_pod_status_ncn-resources_remote-resources.sh cray-reds'... services cray-reds-867c65879d-cr4mg 1/2 CrashLoopBackOff 266 24h Pod status: CrashLoopBackOff ERROR: '/opt/cray/tests/ncn-resources/hms/hms-test/hms_check_pod_status_ncn-resources_remote-resources.sh cray-reds' failed with error code: 1 FAIL: reds_smoke_test ran with failures cleaning up... '/opt/cray/tests/ncn-smoke/hms/hms-reds/reds_smoke_test_ncn-smoke.sh' exited with status code: 1 The following is an example of an API call failure:\nrunning '/opt/cray/tests/ncn-smoke/hms/hms-capmc/capmc_smoke_test_ncn-smoke.sh'... Running capmc_smoke_test... (11:40:27) Running '/opt/cray/tests/ncn-resources/hms/hms-test/hms_check_pod_status_ncn-resources_remote-resources.sh cray-capmc'... (11:40:27) Running 'kubectl get secrets admin-client-auth -o jsonpath='{.data.client-secret}''... (11:40:27) Running 'curl -k -i -s -S -d grant_type=client_credentials -d client_id=admin-client -d client_secret=${CLIENT_SECRET} https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token'... (11:40:28) Testing 'curl -k -i -s -S -o /tmp/capmc_smoke_test_out-${DATETIME}.${RAND}.curl${NUM}.tmp -X POST -d '{}' -H \u0026quot;Authorization: Bearer ${TOKEN}\u0026quot; https://api-gw-service-nmn.local/apis/capmc/capmc/v1/get_power_cap_capabilities'... HTTP/2 503 ERROR: 'curl -k -i -s -S -o /tmp/capmc_smoke_test_out-${DATETIME}.${RAND}.curl${NUM}.tmp -X POST -d '{}' -H \u0026quot;Authorization: Bearer ${TOKEN}\u0026quot; https://api-gw-service-nmn.local/apis/capmc/capmc/v1/get_power_cap_capabilities' did not return \u0026quot;200\u0026quot; or \u0026quot;204\u0026quot; status code as expected MAIN_ERRORS=1 FAIL: capmc_smoke_test ran with failures cleaning up... '/opt/cray/tests/ncn-smoke/hms/hms-capmc/capmc_smoke_test_ncn-smoke.sh' exited with status code: 1 HMS Functional Tests The HMS functional tests consist of Tavern-based API tests for HMS services that are written in yaml and execute within hms-pytest containers on the NCNs that are spun up using podman. The functional tests are more rigorous than the smoke tests and verify the behavior of HMS service APIs in greater detail. The hms_run_ct_functional_tests_ncn-resources.sh wrapper script checks for executable files in the HMS functional test directory on the NCN and runs all tests found in succession.\nncn# /opt/cray/tests/ncn-resources/hms/hms-test/hms_run_ct_functional_tests_ncn-resources.sh searching for HMS CT functional tests... found 4 HMS CT functional tests... running HMS CT functional tests... A summary of the test results is printed at the bottom of the output.\nHMS functional tests ran with 1/4 failures exiting with status code: 1 The tests print the commands being executed while running. They also print the command output and status code if failures occur in order to help with debugging.\nThe following is an example of an hms-pytest container spin-up failure, which may occur if the hms-pytest image is unavailable or missing from the local image registry on the NCN:\n(20:06:04) Running '/usr/bin/hms-pytest --tavern-global-cfg=/opt/cray/tests/ncn-functional/hms/hms-bss/common.yaml /opt/cray/tests/ncn-functional/hms/hms-bss'... Trying to pull registry.local/cray/hms-pytest:1.1.1... manifest unknown: manifest unknown Error: unable to pull registry.local/cray/hms-pytest:1.1.1: Error initializing source docker://registry.local/cray/hms-pytest:1.1.1: Error reading manifest 1.1.1 in registry.local/cray/hms-pytest: manifest unknown: manifest unknown FAIL: bss_tavern_api_test ran with failures cleaning up... A summary of the test suites executed and their results is printed for each HMS service tested. Period \u0026lsquo;.\u0026rsquo; characters represent test cases that passed and letter \u0026lsquo;F\u0026rsquo; characters represent test cases that failed within each test suite.\nThe following is an example of a pytest summary table for Tavern test suites executed against a service:\n============================= test session starts ============================== platform linux -- Python 3.8.5, pytest-6.1.2, py-1.10.0, pluggy-0.13.1 rootdir: /opt/cray/tests/ncn-functional/hms/hms-smd, configfile: pytest.ini plugins: tap-3.2, tavern-1.12.2 collected 38 items test_smd_component_endpoints_ncn-functional_remote-functional.tavern.yaml . [ 2%] ...... [ 18%] test_smd_components_ncn-functional_remote-functional.tavern.yaml F.F.... [ 36%] [ 36%] test_smd_discovery_status_ncn-functional_remote-functional.tavern.yaml . [ 39%] [ 39%] test_smd_groups_ncn-functional_remote-functional.tavern.yaml . [ 42%] test_smd_hardware_ncn-functional_remote-functional.tavern.yaml ....F [ 55%] test_smd_memberships_ncn-functional_remote-functional.tavern.yaml . [ 57%] test_smd_partitions_ncn-functional_remote-functional.tavern.yaml . [ 60%] test_smd_redfish_endpoints_ncn-functional_remote-functional.tavern.yaml . [ 63%] [ 63%] test_smd_service_endpoints_ncn-functional_remote-functional.tavern.yaml . [ 65%] ...F........ [ 97%] test_smd_state_change_notifications_ncn-functional_remote-functional.tavern.yaml . [100%] When API test failures occur, output from Tavern is printed by pytest indicating the following:\n The Source test stage that was executing when the failure occurred which is a portion of the source code for the failed test case. The Formatted stage that was executing when the failure occurred which is a portion of the source code for the failed test case with its variables filled in with the values that were set at the time of the failure. This includes the request header, method, url, and other options of the failed test case which is useful for attempting to reproduce the failure using the curl command. The specific Errors encountered when processing the API response that caused the failure. This is the first place to look when debugging API test failures.  The following is an example Source test stage:\nSource test stage (line 179): - name: Ensure the boot script service can provide the bootscript for a given node request: url: \u0026quot;{base_url}/bss/boot/v1/bootscript?nid={nid}\u0026quot; method: GET headers: Authorization: \u0026quot;Bearer {access_token}\u0026quot; verify: !bool \u0026quot;{verify}\u0026quot; response: status_code: 200 The following is an example Formatted stage:\nFormatted stage: name: Ensure the boot script service can provide the bootscript for a given node request: headers: Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJXcXFhelBLNnNVSnpUV250bThmYWh3cGVLOGRjeTB4SjFpSmRWRGZaLV8wIn0.eyJqdGkiOiI4ZGZkNTQ0YS1jNTY5LTQyNmUtYThiYy02NDg4MzgxOWUyOTAiLCJleHAiOjE2NDQ0MzczODEsIm5iZiI6MCwiaWF0IjoxNjEyOTAxMzgxLCJpc3MiOiJodHRwczovL2FwaS1ndy1zZXJ2aWNlLW5tbi5sb2NhbC9rZXljbG9hay9yZWFsbXMvc2hhc3RhIiwiYXVkIjpbImdhdGVrZWVwZXIiLCJzaGFzdGEiLCJhY2NvdW50Il0sInN1YiI6IjJjNDFiYjgwLTM2NGEtNGNkOS1hMGZkLTQyYzQ5ODRmMTM2ZSIsInR5cCI6IkJlYXJlciIsImF6cCI6ImFkbWluLWNsaWVudCIsImF1dGhfdGltZSI6MCwic2Vzc2lvbl9zdGF0ZSI6IjEzZGQ1YmY2LWQxNGMtNDcwZC05ZWI0LTQ5MDFmYzc3YWYwOSIsImFjciI6IjEiLCJyZWFsbV9hY2Nlc3MiOnsicm9sZXMiOlsib2ZmbGluZV9hY2Nlc3MiLCJ1bWFfYXV0aG9yaXphdGlvbiJdfSwicmVzb3VyY2VfYWNjZXNzIjp7InNoYXN0YSI6eyJyb2xlcyI6WyJhZG1pbiJdfSwiYWNjb3VudCI6eyJyb2xlcyI6WyJtYW5hZ2UtYWNjb3VudCIsIm1hbmFnZS1hY2NvdW50LWxpbmtzIiwidmlldy1wcm9maWxlIl19fSwic2NvcGUiOiJwcm9maWxlIGVtYWlsIiwiY2xpZW50SG9zdCI6IjEwLjMyLjAuMSIsImNsaWVudElkIjoiYWRtaW4tY2xpZW50IiwiZW1haWxfdmVyaWZpZWQiOmZhbHNlLCJwcmVmZXJyZWRfdXNlcm5hbWUiOiJzZXJ2aWNlLWFjY291bnQtYWRtaW4tY2xpZW50IiwiY2xpZW50QWRkcmVzcyI6IjEwLjMyLjAuMSJ9.c37RtSOzcM_-6poyPecS7HP_t1hnqURkgjRcXTQj2S0IEZAkMyfeOVYRFr1gDlAFP-BnRPkf_X2_B7d63j9gI15M1gksYcv8PP0bZFX3PAOaBu-hHGfIw2pDsNsJEA-L72Pb9nmcaPR1CnnVijwRFV-jAmGBJ_vv612mjR5nbI_YJUHDkgdzDfWbpWKQzuCxiJ8USxPD-ASqx_pLecUzcihorb6PNngMaeisc2TqLTV8YRhSZYeL3cssEcXyTxRBe3zjPDawlPArjY2FUkEdzbtl-Tq3D2Ulii44esOf4_ooGmUsOc9vrvYvM_JNPAVamv0-0709PRwwNjFl9nEd5g method: GET url: 'https://api-gw-service-nmn.local/apis/bss/boot/v1/bootscript?nid=None' verify: !bool 'False' response: status_code: 200 The following is an example Errors section:\nErrors: E tavern.util.exceptions.TestFailError: Test 'Ensure the boot script service can provide the bootscript for a given node' failed: - Status code was 400, expected 200: {\u0026quot;type\u0026quot;: \u0026quot;about:blank\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;Bad Request\u0026quot;, \u0026quot;detail\u0026quot;: \u0026quot;Need a mac=, name=, or nid= parameter\u0026quot;, \u0026quot;status\u0026quot;: 400} Additional Troubleshooting This section provides guidance for handling specific HMS Health Check failures that may occur.\nsmd_discovery_status_test_ncn-smoke.sh This test verifies that the system hardware has been discovered successfully.\nThe following is an example of a failed test execution:\nRunning smd_discovery_status_test... (22:19:34) Running 'kubectl get secrets admin-client-auth -o jsonpath='{.data.client-secret}''... (22:19:34) Running 'curl -k -i -s -S -d grant_type=client_credentials -d client_id=admin-client -d client_secret=4c591ddc-b770-41c8-a4de-465ec034c7cf https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token'... (22:19:35) Testing 'curl -s -k -H \u0026quot;Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJEdDZ3ZGNMSzNvT196LWFKaGNkYzBMTkpNSVY5cXRiX25GSXhlRUxCaWFRIn0.eyJqdGkiOiI2NGI0OWE0Zi03YzFiLTRkMjQtYmI2Zi1lYzRhYjczYTI0MDIiLCJleHAiOjE2NTk2NTE1NzQsIm5iZiI6MCwiaWF0IjoxNjI4MTE1NTc0LCJpc3MiOiJodHRwczovL2FwaS1ndy1zZXJ2aWNlLW5tbi5sb2NhbC9rZXljbG9hay9yZWFsbXMvc2hhc3RhIiwiYXVkIjpbImdhdGVrZWVwZXIiLCJzaGFzdGEiLCJhY2NvdW50Il0sInN1YiI6IjdhNGM3YWI5LTMyY2EtNGE5Ny04NGJiLWIzNjc3NmUyZTUwZSIsInR5cCI6IkJlYXJlciIsImF6cCI6ImFkbWluLWNsaWVudCIsImF1dGhfdGltZSI6MCwic2Vzc2lvbl9zdGF0ZSI6ImZjOTdiMzVlLWVjMmUtNGZmYy05NjEzLTg2MDZhY2RiODUxMyIsImFjciI6IjEiLCJyZWFsbV9hY2Nlc3MiOnsicm9sZXMiOlsib2ZmbGluZV9hY2Nlc3MiLCJ1bWFfYXV0aG9yaXphdGlvbiJdfSwicmVzb3VyY2VfYWNjZXNzIjp7InNoYXN0YSI6eyJyb2xlcyI6WyJhZG1pbiJdfSwiYWNjb3VudCI6eyJyb2xlcyI6WyJtYW5hZ2UtYWNjb3VudCIsIm1hbmFnZS1hY2NvdW50LWxpbmtzIiwidmlldy1wcm9maWxlIl19fSwic2NvcGUiOiJlbWFpbCBwcm9maWxlIiwiZW1haWxfdmVyaWZpZWQiOmZhbHNlLCJjbGllbnRIb3N0IjoiMTAuNDYuMC4wIiwiY2xpZW50SWQiOiJhZG1pbi1jbGllbnQiLCJwcmVmZXJyZWRfdXNlcm5hbWUiOiJzZXJ2aWNlLWFjY291bnQtYWRtaW4tY2xpZW50IiwiY2xpZW50QWRkcmVzcyI6IjEwLjQ2LjAuMCJ9.Moecw0ygc_G5whueojAwT3V6Tqtyp7pmqJlvMS5fMz0NxLhb6-3FYK60N5XIqK4RgXeP8TY004hxMyfel9ZqHwI1e5jC8ZHx0y4N41-1t3dgPZnmxvKiaIE14WfovYFDJGU3xcugZcFAnpylVFIABrPQG4Sk66MVaiOubsqW-i855Z0GZurSJOJMAl6LceJ_ek6OWhVlEsEh3S2phCmUA4C-lxRNviDcXhHThPZ0ruOb9bhtQV5uVD7BviIA_VBBN1BSrNJIfIyps5ZwpYr0KwbnntwbYap8zf56UC5MVz0kOyGk1n6qMlVNKn2W0tB4oTJynBdGgehNIqv93rXZyA\u0026quot; https://api-gw-service-nmn.local/apis/smd/hsm/v1/Inventory/RedfishEndpoints'... (22:19:35) Processing response with: 'jq '.RedfishEndpoints[] | { ID: .ID, LastDiscoveryStatus: .DiscoveryInfo.LastDiscoveryStatus}' -c | sort -V | jq -c'... (19:06:02) Verifying endpoint discovery statuses... {\u0026quot;ID\u0026quot;:\u0026quot;x3000c0s1b0\u0026quot;,\u0026quot;LastDiscoveryStatus\u0026quot;:\u0026quot;HTTPsGetFailed\u0026quot;} {\u0026quot;ID\u0026quot;:\u0026quot;x3000c0s9b0\u0026quot;,\u0026quot;LastDiscoveryStatus\u0026quot;:\u0026quot;ChildVerificationFailed\u0026quot;} {\u0026quot;ID\u0026quot;:\u0026quot;x3000c0s19b999\u0026quot;,\u0026quot;LastDiscoveryStatus\u0026quot;:\u0026quot;HTTPsGetFailed\u0026quot;} {\u0026quot;ID\u0026quot;:\u0026quot;x3000c0s27b0\u0026quot;,\u0026quot;LastDiscoveryStatus\u0026quot;:\u0026quot;ChildVerificationFailed\u0026quot;} FAIL: smd_discovery_status_test found 4 endpoints that failed discovery, maximum allowable is 1 '/opt/cray/tests/ncn-smoke/hms/hms-smd/smd_discovery_status_test_ncn-smoke.sh' exited with status code: 1 The expected state of LastDiscoveryStatus is DiscoverOK for all endpoints with the exception of the BMC for ncn-m001 which is not normally connected to the site network and expected to be HTTPsGetFailed. If the test fails because of two or more endpoints not having been discovered successfully, the following additional steps can be taken to determine the cause of the failure:\nHTTPsGetFailed  Check to see if the failed xname resolves using the nslookup command. If not, then the problem may be a DNS issue.  ncn# nslookup \u0026lt;xname\u0026gt; Check to see if the failed xname responds to the ping command. If not, then the problem may be a network or hardware issue.  ncn# ping -c 1 \u0026lt;xname\u0026gt; Check to see if the failed xname responds to a Redfish query. If not, then the problem may be a credentials issue. Use the password set in the REDS sealed secret when creating site init.  ncn# curl -s -k -u root:\u0026lt;password\u0026gt; https://\u0026lt;xname\u0026gt;/redfish/v1/Managers | jq ChildVerificationFailed Check the SMD logs to determine the cause of the bad Redfish path encountered during discovery.\n# get the SMD pod names ncn # kubectl -n services get pods -l app.kubernetes.io/name=cray-smd NAME READY STATUS RESTARTS AGE cray-smd-5b9d574756-9b2lj 2/2 Running 0 24d cray-smd-5b9d574756-bnztf 2/2 Running 0 24d cray-smd-5b9d574756-hhc5p 2/2 Running 0 24d # get the logs from each of the SMD pods ncn# kubectl -n services logs \u0026lt;cray-smd-pod1\u0026gt; cray-smd \u0026gt; smd_pod1_logs ncn# kubectl -n services logs \u0026lt;cray-smd-pod2\u0026gt; cray-smd \u0026gt; smd_pod2_logs ncn# kubectl -n services logs \u0026lt;cray-smd-pod3\u0026gt; cray-smd \u0026gt; smd_pod3_logs DiscoveryStarted The endpoint is in the process of being inventoried by Hardware State Manager (HSM). Wait for the current discovery operation to end which should result in a new LastDiscoveryStatus state being set for the endpoint.\nUse the following command to check the current discovery status of the endpoint:\nncn# cray hsm inventory redfishEndpoints describe \u0026lt;xname\u0026gt; "
},
{
	"uri": "/docs-csm/en-11/troubleshooting/known_issues/component_power_state_mismatch/",
	"title": "Sat/HSM/CAPMC Component Power State Mismatch",
	"tags": [],
	"description": "",
	"content": "SAT/HSM/CAPMC Component Power State Mismatch Because of various hardware or communication issues, the node state reported by SAT and HSM (Hardware State Manager) may become out of sync with the actual hardware state reported by CAPMC or via redfish. In most cases this will be noticed when trying to power on or off nodes with BOS/BOA and will present as SAT or HSM reporting nodes are \u0026lsquo;On\u0026rsquo; while CAPMC reports them as \u0026lsquo;Off\u0026rsquo; (or vice versa).\nPossible Causes Possible reasons the power state got out of sync include but are not limited to:\n A known issue with Gigabyte nodes where power Redfish events can get sent out of order when rebooting nodes. Network issues preventing the flow of Redfish events (telemetry will also be affected) Issues with the cray-hms-hmcollector pod  Fix In most cases, once the underlying cause has been corrected, this should correct itself when the node boots OS, starts heartbeating, and goes to the \u0026lsquo;Ready\u0026rsquo; state. If not, the power state for the affected nodes can be re-synced by kicking off HSM re-discovery of those nodes' BMCs.\ncray hsm inventory discover create --xnames \u0026lt;list_of_BMCs\u0026gt; e.g. cray hsm inventory discover create --xnames x3000c0s0b0,x3000c0s1b0 The power state will be re-synced after all of the BMCs listed have a \u0026lsquo;LastDiscoveryStatus\u0026rsquo; of \u0026lsquo;DiscoverOK\u0026rsquo;.\n\u0026gt; cray hsm inventory redfishEndpoints describe x3000c0s0b0 { \u0026quot;ID\u0026quot;: \u0026quot;x3000c0s0b0\u0026quot;, \u0026quot;Type\u0026quot;: \u0026quot;NodeBMC\u0026quot;, \u0026quot;Hostname\u0026quot;: \u0026quot;, \u0026quot;Domain\u0026quot;: \u0026quot;, \u0026quot;FQDN\u0026quot;: \u0026quot;x3000c0s0b0\u0026quot;, \u0026quot;Enabled\u0026quot;: true, \u0026quot;UUID\u0026quot;: \u0026quot;808cde6e-debf-0010-e603-b42e993b708c\u0026quot;, \u0026quot;User\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;Password\u0026quot;: \u0026quot;, \u0026quot;RediscoverOnUpdate\u0026quot;: true, \u0026quot;DiscoveryInfo\u0026quot;: { \u0026quot;LastDiscoveryAttempt\u0026quot;: \u0026quot;2021-08-12T16:00:56.937774Z\u0026quot;, \u0026quot;LastDiscoveryStatus\u0026quot;: \u0026quot;DiscoverOK\u0026quot;, \u0026quot;RedfishVersion\u0026quot;: \u0026quot;1.7.0\u0026quot; } } Any power operations done manually with CAPMC that alter the nodes' power state may also cause the power state to re-sync.\n"
},
{
	"uri": "/docs-csm/en-11/operations/utility_storage/troubleshoot_a_down_osd/",
	"title": "Troubleshoot A Down Osd",
	"tags": [],
	"description": "",
	"content": "Troubleshoot a Down OSD Identify down OSDs and manually bring them back up.\nTroubleshoot the Ceph health detail reporting down OSDs. Ensuring that OSDs are operational and data is balanced across them will help remove the likelihood of hotspots being created.\nPrerequisites This procedure requires admin privileges.\nProcedure   Identify the down OSDs.\nncn-m/s(001/2/3)# ceph osd tree down ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 62.87558 root default -7 20.95853 host ncn-s002 1 ssd 3.49309 osd.1 down 1.00000 1.00000 3 ssd 3.49309 osd.3 down 1.00000 1.00000 7 ssd 3.49309 osd.7 down 1.00000 1.00000 10 ssd 3.49309 osd.10 down 1.00000 1.00000 13 ssd 3.49309 osd.13 down 1.00000 1.00000 16 ssd 3.49309 osd.16 down 1.00000 1.00000 Option 1\n Restart the OSD utilizing ceph orch  ncn-m/s00(1/2/3)# ceph orch daemon restart osd.\u0026lt;number\u0026gt; Option 2\n  Check the logs for the OSD that is down.\nUse the OSD number for the down OSD returned in the command above.\nncn-m/s(001/2/3)# ceph osd find OSD_ID   Manually restart the OSD.\nThis step must be done on the node with the reported down OSD.\nncn-s# ceph orch daemon restart osd.\u0026lt;number\u0026gt;   Troubleshooting: If the service is not restarted with ceph orch, restart it using Manage Ceph Services\n  Verify the OSDs are running again.\n# ceph osd tree down ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 62.87558 root default -7 20.95853 host ncn-s002 1 ssd 3.49309 osd.1 up 1.00000 1.00000 3 ssd 3.49309 osd.3 up 1.00000 1.00000 7 ssd 3.49309 osd.7 up 1.00000 1.00000 10 ssd 3.49309 osd.10 up 1.00000 1.00000 13 ssd 3.49309 osd.13 up 1.00000 1.00000 16 ssd 3.49309 osd.16 up 1.00000 1.00000   If the OSD dies again, check dmesg for drive failures.\n"
},
{
	"uri": "/docs-csm/en-11/operations/utility_storage/troubleshoot_an_unresponsive_s3_endpoint/",
	"title": "Troubleshoot An Unresponsive Rados-gateway (radosgw) S3 Endpoint",
	"tags": [],
	"description": "",
	"content": "Troubleshoot an Unresponsive Rados-Gateway (radosgw) S3 Endpoint Issue 1: Rados-Gateway/s3 endpoint is not accessible ncn# response=$(curl --write-out \u0026#39;%{http_code}\u0026#39; --silent --output /dev/null http://rgw-vip)|echo \u0026#34;Curl Response Code: $response\u0026#34; Curl Response Code: 200 Expected Responses: 2xx, 3xx\nProcedure:   Check the individual endpoints.\nncn# num_storage_nodes=$(craysys metadata get num_storage_nodes);for node_num in $(seq 1 \u0026#34;$num_storage_nodes\u0026#34;); do nodename=$(printf \u0026#34;ncn-s%03d\u0026#34; \u0026#34;$node_num\u0026#34;); response=$(curl --write-out \u0026#39;%{http_code}\u0026#39; --silent --output /dev/null http://$nodename:8080); echo \u0026#34;Curl Response Code for ncn-s00$endpoint: $response\u0026#34;; done Curl Response Code for ncn-s003: 200 Curl Response Code for ncn-s003: 200 Curl Response Code for ncn-s003: 200 NOTE: If an error occurs with the above script, then echo $num_storage_nodes. If it is not an integer that matches the known configuration of the number of Utility Storage nodes, then you can run cloud-init init to refresh your cloud-init cache. Alternatively you can manually set that number if you know your number of Utility Storage nodes.\n  Check the HAProxy endpoint\nncn# response=$(curl --write-out \u0026#39;%{http_code}\u0026#39; --silent --output /dev/null http://rgw-vip)|echo \u0026#34;Curl Response Code: $response\u0026#34; Curl Response Code: 200   Verify HAProxy and KeepAlived Status\nKeepAlived:\n Check KeepAlived on each node running ceph-radosgw. By default this will be all Utility Storage nodes, but may differ based on your configuration.  ncn-s# systemctl is-active keepalived.service active  Check for the KeepAlived instance hosting the VIP (Virtual IP). This command will have to be run on each node until you find the expected output.  ncn-s# journalctl -u keepalived.service --no-pager |grep -i gratuitous Aug 25 19:33:12 ncn-s001 Keepalived_vrrp[12439]: Registering gratuitous ARP shared channel Aug 25 19:43:08 ncn-s001 Keepalived_vrrp[12439]: Sending gratuitous ARP on vlan002 for 10.252.1.3 Aug 25 19:43:08 ncn-s001 Keepalived_vrrp[12439]: (VI_0) Sending/queueing gratuitous ARPs on vlan002 for 10.252.1.3 HAProxy:\nncn-s# systemctl is-active haproxy.service active   Issue 2 Ceph reports HEALTH_OK but s3 operations not functioning Restart Ceph OSDs to help make the rgw.local:8080 endpoint responsive.\nCeph has an issue where it appears healthy but the rgw.local:8080 endpoint is unresponsive.\nThis issue occurs when ceph -s is run and produces a very high reads per second output:\nio: client: 103 TiB/s rd, 725 KiB/s wr, 2 op/s rd, 44 op/s wr The rgw.local endpoint needs to be responsive in order to interact directly with the Simple Storage Service (S3) RESTful API.\nPrerequisites This procedure requires admin privileges.\nProcedure   View the OSD status.\nncn-m001# ceph osd tree ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 20.95312 root default -7 6.98438 host ncn-s001 0 ssd 3.49219 osd.0 up 1.00000 1.00000 3 ssd 3.49219 osd.3 up 1.00000 1.00000 -3 6.98438 host ncn-s002 2 ssd 3.49219 osd.2 up 1.00000 1.00000 5 ssd 3.49219 osd.5 up 1.00000 1.00000 -5 6.98438 host ncn-s003 1 ssd 3.49219 osd.1 up 1.00000 1.00000 4 ssd 3.49219 osd.4 up 1.00000 1.00000   Log in to each node and restart the OSDs.\nThe OSD number in the example below should be replaced with the number of the OSD being restarted.\nncn-m001# ceph orch restart osd.3 Wait for Ceph health to return to OK before moving between nodes.\n  "
},
{
	"uri": "/docs-csm/en-11/troubleshooting/",
	"title": "Troubleshooting Information",
	"tags": [],
	"description": "",
	"content": "CSM Troubleshooting Information This document provides troubleshooting information for services and functionality provided by CSM.\nTopics  Known Issues  Hardware Discovery initrd.img.xz not found SAT/HSM/CAPMC Component Power State Mismatch BOS/BOA Incorrect command is output to rerun a failed operation. CFS Sessions are Stuck in a Pending State   Kubernetes troubleshooting  General Kubernetes Commands for Troubleshooting Kubernetes Log File Locations Troubleshoot Liveliness or Readiness Probe Failures Troubleshoot Unresponsive kubectl Commands Troubleshoot Kubernetes Node NotReady    Known Issues Listing of known issues and procedures to workaround them in this CSM release.\nHardware Discovery Known issues related to hardware discovery in a system.\n Air cooled hardware is not getting properly discovered with Aruba leaf switches HMS Discovery job not creating RedfishEndpoints in Hardware State Manager  initrd.img.xz Not Found This is a problem that is fixed in CSM 1.0+, but if your system was upgraded from CSM 0.9.x you may run into this. Below is the full error seen when attempting to boot:\nLoading Linux ... Loading initial ramdisk ... error: file `/boot/grub2/../initrd.img.xz' not found. Press any key to continue... [ 2.528752] Kernel panic - not syncing: VFS: Unable to mount root fs on unknown-block(0,0) [ 2.537264] CPU: 0 PID: 1 Comm: swapper/0 Not tainted 5.3.18-24.64-default #1 SLE15-SP2 [ 2.545499] Hardware name: Cray Inc. R272-Z30-00/MZ32-AR0-00, BIOS C27 05/12/2021 [ 2.553196] Call Trace: [ 2.555716] dump_stack+0x66/0x8b [ 2.559127] panic+0xfe/0x2d7 [ 2.562184] mount_block_root+0x27d/0x2e1 [ 2.566306] ? set_debug_rodata+0x11/0x11 [ 2.570431] prepare_namespace+0x130/0x166 [ 2.574645] kernel_init_freeable+0x23f/0x26b [ 2.579125] ? rest_init+0xb0/0xb0 [ 2.582623] kernel_init+0xa/0x110 [ 2.586127] ret_from_fork+0x22/0x40 [ 2.590747] Kernel Offset: 0x0 from 0xffffffff81000000 (relocation range: 0xffffffff80000000-0xffffffffbfffffff) [ 2.690969] ---[ end Kernel panic - not syncing: VFS: Unable to mount root fs on unknown-block(0,0) ]--- Fix Follow these steps on any NCN to fix the issue:\n  Run the CASMINST-2689.sh script from the CASMINST-2689 workaround at the livecd-post-reboot breakpoint.\nFollow the usual workaround instructions with the following exceptions:\n Use the latest Shasta 1.4 install workaround RPM, not the Shasta 1.5 install workaround RPM For the livecd-post-reboot breakpoint, ignore any workarounds other than CASMINST-2689 Do not follow the workaround README.md instructions \u0026ndash; only run the CASMINST-2689.sh script in the CASMINST-2689 subdirectory    Run these commands:\nncn# for i in $(grep -oP \u0026#39;ncn-\\w\\d+\u0026#39; /etc/hosts | sort -u | tr -t \u0026#39;\\n\u0026#39; \u0026#39; \u0026#39;); do scp -r csm-install-workarounds/workarounds/livecd-post-reboot/CASMINST-2689/ $i:/opt/cray/csm/workarounds/livecd-post-reboot/ done ncn# pdsh -b -S -w $(grep -oP \u0026#39;ncn-\\w\\d+\u0026#39; /etc/hosts | sort -u | tr -t \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39;) \u0026#39;/opt/cray/csm/workarounds/livecd-post-reboot/CASMINST-2689/CASMINST-2689.sh\u0026#39;   Remove the Shasta 1.4 install workaround RPM from the NCN.\nncn# rpm -e csm-install-workarounds   Validate Running the script again will produce this output:\nExamining /metal/boot/boot/kernel...kernel is OK. Examining /metal/boot/boot/initrd.img.xz...initrd.img.xz is OK. Examining /metal/boot/boot/kernel...kernel is OK. Examining /metal/boot/boot/initrd.img.xz...initrd.img.xz is OK. Examining /metal/boot/boot/kernel...kernel is OK. Examining /metal/boot/boot/initrd.img.xz...initrd.img.xz is OK. Examining /metal/boot/boot/kernel...kernel is OK. Examining /metal/boot/boot/initrd.img.xz...initrd.img.xz is OK. Examining /metal/boot/boot/kernel...kernel is OK. Examining /metal/boot/boot/initrd.img.xz...initrd.img.xz is OK. Examining /metal/boot/boot/kernel...kernel is OK. Examining /metal/boot/boot/initrd.img.xz...initrd.img.xz is OK. Examining /metal/boot/boot/kernel...kernel is OK. Examining /metal/boot/boot/initrd.img.xz...initrd.img.xz is OK. Examining /metal/boot/boot/kernel...kernel is OK. Examining /metal/boot/boot/initrd.img.xz...initrd.img.xz is OK. "
},
{
	"uri": "/docs-csm/en-11/operations/utility_storage/utility_storage/",
	"title": "Utility Storage",
	"tags": [],
	"description": "",
	"content": "Utility Storage Utility storage is designed to support Kubernetes and the System Management Services (SMS) it orchestrates. Utility storage is a cost-effective solution for storing the large amounts of telemetry and log data collected.\nCeph is the utility storage platform that is used to enable pods to store persistent data. It is deployed to provide block, object, and file storage to the management services running on Kubernetes, as well as for telemetry data coming from the compute nodes.\nIMPORTANT NOTES\n  When running commands for Ceph health you must run those from either ncn-m or ncn-s001/2/3. Unless otherwise specified to run on the host in question, you can run the commands on either masters or ncn-s00(1/2/3) Those are the only servers with the credentials. The document will specify when to run a command from a node other than those   Table of Contents  Cephadm Reference Material Collect Information about the Ceph Cluster Ceph Storage Types Manage Ceph Services Adjust Ceph Pool Quotas Add Ceph OSDs Shrink Ceph OSDs Ceph Health States Dump Ceph Crash Data Identify Ceph Latency Issues Restore Nexus Data After Data Corruption Troubleshoot Failure to Get Ceph Health Troubleshoot a Down OSD Troubleshoot Ceph OSDs Reporting Full Troubleshoot System Clock Skew Troubleshoot an Unresponsive S3 Endpoint Troubleshoot Ceph-Mon Processes Stopping and Exceeding Max Restarts Troubleshoot Pods Failing to Restart on Other Worker Nodes Troubleshoot Large Object Map Objects in Ceph Health Troubleshoot Failure of RGW Health Check Troubleshoot Ceph MDS reporting slow requests Ceph Service Check Script Usage  "
},
{
	"uri": "/docs-csm/en-11/operations/validate_csm_health/",
	"title": "Validate Health",
	"tags": [],
	"description": "",
	"content": "Validate CSM Health Anytime after the installation of the CSM services, the health of the management nodes and all CSM services can be validated.\nThe following are examples of when to run health checks:\n After CSM install.sh completes Before and after NCN reboots After the system is brought back up Any time there is unexpected behavior observed In order to provide relevant information to create support tickets  The areas should be tested in the order they are listed on this page. Errors in an earlier check may cause errors in later checks because of dependencies.\nTopics:  Validate CSM Health  Topics: 1. Platform Health Checks  1.1 ncnHealthChecks 1.2 ncnPostgresHealthChecks 1.3 BGP Peering Status and Reset  1.3.1 Mellanox Switch 1.3.2 Aruba Switch   1.4 Verify that KEA has active DHCP leases 1.5 Verify ability to resolve external DNS 1.6 Verify Spire Agent is Running on Kubernetes NCNs 1.7 Verify the Vault Cluster is Healthy 1.8 Automated Goss Testing  1.8.1 Known Test Issues   1.9 OPTIONAL Check of System Management Monitoring Tools   2. Hardware Management Services Health Checks  2.1 HMS Test Execution 2.2 Hardware State Manager Discovery Validation  2.2.1 Interpreting results 2.2.2 Known Issues     3 Software Management Services Health Checks  3.1 SMS Test Execution 3.2 Interpreting cmsdev Results   4. Booting CSM Barebones Image  4.1 Locate CSM Barebones Image in IMS 4.2 Create a BOS Session Template for the CSM Barebones Image 4.3 Find an available compute node 4.4 Reboot the node using a BOS session template 4.5 Connect to the node\u0026rsquo;s console and watch the boot   5. UAS / UAI Tests  5.1 Validate the Basic UAS Installation 5.2 Validate UAI Creation 5.3 UAS/UAI Troubleshooting  5.3.1 Authorization Issues 5.3.2 UAS Cannot Access Keycloak 5.3.3 UAI Images not in Registry 5.3.4 Missing Volumes and other Container Startup Issues        1. Platform Health Checks Scripts do not verify results. Script output includes analysis needed to determine pass/fail for each check. All health checks are expected to pass.\nHealth Check scripts can be run:\n After CSM install.sh has been run (not before) Before and after one of the NCNs reboots After the system or a single node goes down unexpectedly After the system is gracefully shut down and brought up Any time there is unexpected behavior on the system to get a baseline of data for CSM services and components In order to provide relevant information to support tickets that are being opened after CSM install.sh has been run  Available Platform Health Checks:\n ncnHealthChecks ncnPostgresHealthChecks BGP Peering Status and Reset KEA / DHCP External DNS Spire Agent Vault Cluster Automated Goss Testing  1.1 ncnHealthChecks Health Check scripts can be found and run on any worker or master node (not on PIT node), from any directory.\nncn# /opt/cray/platform-utils/ncnHealthChecks.sh The ncnHealthChecks script reports the following health information:\n Kubernetes status for master and worker NCNs Ceph health status Health of etcd clusters Number of pods on each worker node for each etcd cluster Alarms set for any of the Etcd clusters Health of Etcd cluster\u0026rsquo;s database List of automated etcd backups for the Boot Orchestration Service (BOS), Boot Script Service (BSS), Compute Rolling Upgrade Service (CRUS), and Domain Name Service (DNS), and Firmware Action Service (FAS) clusters NCN node uptimes NCN master and worker node resource consumption NCN node xnames and metal.no-wipe status NCN worker node pod counts Pods yet to reach the running state  Execute the ncnHealthChecks script and analyze the output of each individual check.\nIMPORTANT: When the PIT node is booted, the NCN node metal.no-wipe status is not available and is correctly reported as \u0026lsquo;unavailable\u0026rsquo;. Once ncn-m001 has been booted, the NCN metal.no-wipe status is expected to be reported as metal.no-wipe=1.\nIMPORTANT: Only when ncn-m001 has been booted, if the output of the ncnHealthChecks.sh script shows that there are nodes that do not have the metal.no-wipe=1 status, then do the following:\nncn# csi handoff bss-update-param --set metal.no-wipe=1 --limit \u0026lt;SERVER_XNAME\u0026gt; IMPORTANT: If the output of pod statuses indicates that there are pods in the Evicted state, it may be due to the /root file system being filled up on the Kubernetes node in question. Kubernetes will begin evicting pods once the root file system space is at 85% until it is back under 80%. This may commonly happen on ncn-m001 as it is a location that install and doc files may be downloaded to. It may be necessary to clean up space in the /root directory if this is the root cause of pod evictions. The following commands can be used to determine if analysis of files under /root is needed to free-up space.\nncn# df -h /root Filesystem Size Used Avail Use% Mounted on LiveOS_rootfs 280G 245G 35G 88% / ncn# du -h -s /root/ 225G /root/ ncn# du -ah -B 1024M /root | sort -n -r | head -n 10 Note: The cray-crus- pod is expected to be in the Init state until slurm and munge are installed. In particular, this will be the case if executing this as part of the validation after completing the Install CSM Services. If in doubt, validate the CRUS service using the CMS Validation Tool. If the CRUS check passes using that tool, do not worry about the cray-crus- pod state.\nAdditionally, hmn-discovery and unbound manager cronjob pods may be in a \u0026lsquo;NotReady\u0026rsquo; state. This is expected as these pods are periodically started and transition to the completed state.\n1.2 ncnPostgresHealthChecks Postgres Health Check scripts can be found and run on any worker or master node (not on PIT node), from any directory. The ncnPostgresHealthChecks script reports the following postgres health information:\n The status of each postgresql resource The number of cluster members The node which is the Leader The state of the each cluster member Replication Lag for any cluster member Kubernetes postgres pod status  Execute ncnPostgresHealthChecks script and analyze the output of each individual check.\nncn# /opt/cray/platform-utils/ncnPostgresHealthChecks.sh   Check the STATUS of the postgresql resources which are managed by the operator:\nNAMESPACE NAME TEAM VERSION PODS VOLUME CPU-REQUEST MEMORY-REQUEST AGE STATUS services cray-sls-postgres cray-sls 11 3 1Gi 12d Running If any postgresql resources remains in a STATUS other than Running (such as SyncFailed), refer to Troubleshoot Postgres Database.\n  For a particular Postgres cluster, the expected output is similar to the following:\n--- patronictl, version 1.6.5, list for services leader pod cray-sls-postgres-0 --- + Cluster: cray-sls-postgres (6938772644984361037) ---+----+-----------+ | Member | Host | Role | State | TL | Lag in MB | +---------------------+------------+--------+---------+----+-----------+ | cray-sls-postgres-0 | 10.47.0.35 | Leader | running | 1 | | | cray-sls-postgres-1 | 10.36.0.33 | | running | 1 | 0 | | cray-sls-postgres-2 | 10.44.0.42 | | running | 1 | 0 | +---------------------+------------+--------+---------+----+-----------+ The points below will cover the data in the table above for Member, Role, State, and Lag in MB columns.\nFor each Postgres cluster:\n  Verify there are three cluster members (with the exception of sma-postgres-cluster where there should be only two cluster members). If the number of cluster members is not correct, refer to Troubleshoot Postgres Database.\n  Verify there is one cluster member with the Leader Role. If there is no Leader, refer to Troubleshoot Postgres Database.\n  Verify the State of each cluster member is \u0026lsquo;running\u0026rsquo;. If any cluster members are found to be in a non \u0026lsquo;running\u0026rsquo; state (such as \u0026lsquo;start failed\u0026rsquo;), refer to Troubleshoot Postgres Database.\n  Verify there is no large or growing lag. If any cluster members are found to have lag or lag is \u0026lsquo;unknown\u0026rsquo;, refer to Troubleshoot Postgres Database.\n   If all the above four checks indicate postgres clusters are healthy, the log output for the postgres pods can be ignored. If possible health issues exist, re-check the health by re-running the ncnPostgresHealthChecks script in 15 minutes. If health issues persist, then review the log output and consult Troubleshoot Postgres Database. During NCN reboots, temporary errors related to re-election are common but should resolve upon the re-check.    Check that all Kubernetes Postgres pods have a STATUS of Running.\nncn# kubectl get pods -A -o wide -l application=spilo NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES services cray-sls-postgres-0 3/3 Running 3 6d 10.38.0.102 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; services cray-sls-postgres-1 3/3 Running 3 5d20h 10.42.0.89 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; services cray-sls-postgres-2 3/3 Running 0 5d20h 10.36.0.31 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; If any Postgres pods have a STATUS other then Running, gather more information from the pod and refer to Troubleshoot Postgres Database.\nncn# kubectl describe pod \u0026lt;pod name\u0026gt; -n \u0026lt;pod namespace\u0026gt; ncn# kubectl logs \u0026lt;pod name\u0026gt; -n \u0026lt;pod namespace\u0026gt; -c \u0026lt;pod container name\u0026gt;   1.3 BGP Peering Status and Reset Verify that Border Gateway Protocol (BGP) peering sessions are established for each worker node on the system.\nCheck the Border Gateway Protocol (BGP) status on the Aruba or Mellanox switches. Verify that all sessions are in an Established state. If the state of any session in the table is Idle, reset the BGP sessions.\nOn an NCN, determine the IP addresses of switches:\nncn-m001# kubectl get cm config -n metallb-system -o yaml | head -12 Expected output looks similar to the following:\napiVersion: v1 data: config: | peers: - peer-address: 10.252.0.2 peer-asn: 65533 my-asn: 65533 - peer-address: 10.252.0.3 peer-asn: 65533 my-asn: 65533 address-pools: - name: customer-access Using the first peer-address (10.252.0.2 here), log in using ssh as the administrator to the first switch and note in the returned output if a Mellanox or Aruba switch is indicated.\nncn-m001# ssh admin@10.252.0.2  On a Mellanox switch, Mellanox Onyx Switch Management or Mellanox Switch may be displayed after logging in to the switch with ssh. In this case, proceed to the Mellanox steps. On an Aruba switch, Please register your products now at: https://asp.arubanetworks.com may be displayed after logging in to the switch with ssh. In this case, proceed to the Aruba steps.  1.3.1 Mellanox Switch   Enable:\nsw-spine-001# enable   Verify BGP is enabled:\nsw-spine-001# show protocols | include bgp Expected output looks similar to the following:\nbgp: enabled   Check peering status:\nsw-spine-001# show ip bgp summary Expected output looks similar to the following:\nVRF name : default BGP router identifier : 10.252.0.2 local AS number : 65533 BGP table version : 3 Main routing table version: 3 IPV4 Prefixes : 59 IPV6 Prefixes : 0 L2VPN EVPN Prefixes : 0 ------------------------------------------------------------------------------------------------------------------ Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd ------------------------------------------------------------------------------------------------------------------ 10.252.1.10 4 65533 2945 3365 3 0 0 1:00:21:33 ESTABLISHED/20 10.252.1.11 4 65533 2942 3356 3 0 0 1:00:20:49 ESTABLISHED/19 10.252.1.12 4 65533 2945 3363 3 0 0 1:00:21:33 ESTABLISHED/20   If one or more BGP session is reported in an Idle state, reset BGP to re-establish the sessions:\nsw-spine-001# clear ip bgp all   It may take several minutes for all sessions to become Established. Wait a minute or so, and then verify that all sessions now are all reported as Established. If some sessions remain in an Idle state, re-run the clear ip bgp all command and check again.\n  If after several tries one or more BGP session remains Idle, see Check BGP Status and Reset Sessions.\n    Repeat the above Mellanox procedure using the second peer-address (10.252.0.3 here).\n  1.3.2 Aruba Switch On an Aruba switch, the prompt may include sw-spine or sw-agg.\n  Check BGP peering status.\nsw-agg01# show bgp ipv4 unicast summary Expected output looks similar to the following:\nVRF : default BGP Summary ----------- Local AS : 65533 BGP Router Identifier : 10.252.0.4 Peers : 7 Log Neighbor Changes : No Cfg. Hold Time : 180 Cfg. Keep Alive : 60 Confederation Id : 0 Neighbor Remote-AS MsgRcvd MsgSent Up/Down Time State AdminStatus 10.252.0.5 65533 19579 19588 20h:40m:30s Established Up 10.252.1.7 65533 34137 39074 20h:41m:53s Established Up 10.252.1.8 65533 34134 39036 20h:36m:44s Established Up 10.252.1.9 65533 34104 39072 00m:01w:04d Established Up 10.252.1.10 65533 34105 39029 00m:01w:04d Established Up 10.252.1.11 65533 34099 39042 00m:01w:04d Established Up 10.252.1.12 65533 34101 39012 00m:01w:04d Established Up   If one or more BGP session is reported in a Idle state, reset BGP to re-establish the sessions:\nsw-agg01# clear bgp *   It may take several minutes for all sessions to become Established. Wait a minute or so, and then verify that all sessions now are reported as Established. If some sessions remain in an Idle state, re-run the clear bgp * command and check again.\n  If after several tries one or more BGP session remains Idle, see Check BGP Status and Reset Sessions\n    Repeat the above Aruba procedure using the second peer-address (10.252.0.5 in this example).\n  1.4 Verify that KEA has active DHCP leases Verify that KEA has active DHCP leases. Right after an fresh install of CSM, it is important to verify that KEA is currently handing out DHCP leases on the system. The following commands can be run on any of the master nodes or worker nodes.\nGet an API Token:\nncn# export TOKEN=$(curl -s -S -d grant_type=client_credentials \\  -d client_id=admin-client \\  -d client_secret=`kubectl get secrets admin-client-auth \\  -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\  https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39;) Retrieve all the leases currently in KEA:\nncn# curl -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{ \u0026#34;command\u0026#34;: \u0026#34;lease4-get-all\u0026#34;, \u0026#34;service\u0026#34;: [ \u0026#34;dhcp4\u0026#34; ] }\u0026#39; https://api-gw-service-nmn.local/apis/dhcp-kea | jq If there is an non-zero amount of DHCP leases for air-cooled hardware returned, that is a good indication that KEA is working.\n1.5 Verify ability to resolve external DNS If unbound is configured to resolve outside hostnames, then the following check should be performed. If unbond is not configured to resolve outside hostnames, then this check may be skipped.\nRun the following on one of the master or worker nodes (not the PIT node):\nncn# nslookup cray.com ; echo \u0026#34;Exit code is $?\u0026#34; Expected output looks similar to the following:\nServer: 10.92.100.225 Address: 10.92.100.225#53 Non-authoritative answer: Name: cray.com Address: 52.36.131.229 Exit code is 0 Verify that the command has exit code 0, reports no errors, and resolves the address.\n1.6 Verify Spire Agent is Running on Kubernetes NCNs Execute the following command on all Kubernetes NCNs (i.e. all worker nodes and master nodes, excluding the PIT):\nncn# goss -g /opt/cray/tests/install/ncn/tests/goss-spire-agent-service-running.yaml validate Known failures and how to recover:\n  K8S Test: Verify spire-agent is enabled and running\n  The spire-agent service may fail to start on Kubernetes NCNs, logging errors (via journalctl) similar to \u0026ldquo;join token does not exist or has already been used\u0026rdquo; or the last logs containing multiple lines of \u0026ldquo;systemd[1]: spire-agent.service: Start request repeated too quickly.\u0026rdquo;. Deleting the request-ncn-join-token daemonset pod running on the node may clear the issue. Even though the spire-agent systemctl service on the Kubernetes node should eventually restart cleanly, the user may have to log in to the impacted nodes and restart the service. The following recovery procedure can be run from any Kubernetes node in the cluster.\n Set NODE to the NCN which is experiencing the issue. In this example, ncn-w002. ncn# export NODE=ncn-w002  Define the following function ncn# function renewncnjoin() { for pod in $(kubectl get pods -n spire |grep request-ncn-join-token | awk \u0026#39;{print $1}\u0026#39;); do if kubectl describe -n spire pods $pod | grep -q \u0026#34;Node:.*$1\u0026#34;; then echo \u0026#34;Restarting $podrunning on $1\u0026#34;; kubectl delete -n spire pod \u0026#34;$pod\u0026#34;; fi done }  Run the function as follows: ncn# renewncnjoin $NODE     The spire-agent service may also fail if an NCN was powered off for too long and its tokens expired. If this happens, delete /root/spire/agent_svid.der, /root/spire/bundle.der, and /root/spire/data/svid.key off the NCN before deleting the request-ncn-join-token daemonset pod.\n    1.7 Verify the Vault Cluster is Healthy Execute the following commands on ncn-m002:\nncn-m002# goss -g /opt/cray/tests/install/ncn/tests/goss-k8s-vault-cluster-health.yaml validate Check the output to verify no failures are reported:\nCount: 2, Failed: 0, Skipped: 0 1.8 Automated Goss Testing There are multiple Goss test suites available that cover a variety of sub-systems.\nRun the NCN health checks against the three different types of nodes with the following commands:\nIMPORTANT: These tests should only be run while booted into the PIT node. Do not run these as part of upgrade testing. This includes the Kubernetes check in the next block.\npit# /opt/cray/tests/install/ncn/automated/ncn-healthcheck-master pit# /opt/cray/tests/install/ncn/automated/ncn-healthcheck-worker pit# /opt/cray/tests/install/ncn/automated/ncn-healthcheck-storage And the Kubernetes test suite via:\npit# /opt/cray/tests/install/ncn/automated/ncn-kubernetes-checks 1.8.1 Known Test Issues  These tests can only reliably be executed from the PIT node. Should be addressed in a future release. K8S Test: Kubernetes Query BSS Cloud-init for ca-certs  May fail immediately after platform install. Should pass after the TrustedCerts Operator has updated BSS (Global cloud-init meta) with CA certificates.   K8S Test: Kubernetes Velero No Failed Backups  Because of a known issue with Velero, a backup may be attempted immediately upon the deployment of a backup schedule (for example, vault). It may be necessary to use the velero command to delete backups from a Kubernetes node to clear this situation.    1.9 Optional Check of System Management Monitoring Tools If all designated prerequisites are met, the availability of system management health services may optionally be validated by accessing the URLs listed in Access System Management Health Services. It is very important to check the Prerequisites section of this document.\nIf one or more of the the URLs listed in the procedure are inaccessible, it does not necessarily mean that system is not healthy. It may simply mean that not all of the prerequisites have been met to allow access to the system management health tools via URL.\nInformation to assist with troubleshooting some of the components mentioned in the prerequisites can be accessed here:\n Troubleshoot CAN Issues Troubleshoot DNS Configuration Issues Check BGP Status and Reset Sessions Troubleshoot BGP not Accepting Routes from MetalLB Troubleshoot Services without an Allocated IP Address  2. Hardware Management Services Health Checks Execute the HMS smoke and functional tests after the CSM install to confirm that the Hardware Management Services are running and operational.\n2.1 HMS Test Execution These tests should be executed as root on at least one worker NCN and one master NCN (but not ncn-m001 if it is still the PIT node).\nRun the HMS smoke tests.\nncn# /opt/cray/tests/ncn-resources/hms/hms-test/hms_run_ct_smoke_tests_ncn-resources.sh Examine the output. If one or more failures occur, investigate the cause of each failure. See the interpreting_hms_health_check_results documentation for more information.\nOtherwise, run the HMS functional tests.\nncn# /opt/cray/tests/ncn-resources/hms/hms-test/hms_run_ct_functional_tests_ncn-resources.sh Examine the output. If one or more failures occur, investigate the cause of each failure. See the interpreting_hms_health_check_results documentation for more information.\n2.2 Hardware State Manager Discovery Validation  NOTE: The Cray CLI must be configured in order to complete this task. See Configure the Cray Command Line Interface for details on how to do this.\n By this point in the installation process, the Hardware State Manager (HSM) should have done its discovery of the system.\nThe foundational information for this discovery is from the System Layout Service (SLS). Thus, a comparison needs to be done to see that what is specified in SLS (focusing on BMC components and Redfish endpoints) are present in HSM.\nExecute the hsm_discovery_verify.sh script on a Kubernetes master or worker NCN:\nncn# /opt/cray/csm/scripts/hms_verification/hsm_discovery_verify.sh The output will ideally appear as follows, if there are mismatches these will be displayed in the appropriate section of the output. Refer to 2.2.1 Interpreting results and 2.2.2 Known Issues below to troubleshoot any mismatched BMCs.\nncn# /opt/cray/csm/scripts/hms_verification/hsm_discovery_verify.sh Fetching SLS Components... Fetching HSM Components... Fetching HSM Redfish endpoints... =============== BMCs in SLS not in HSM components =============== ALL OK =============== BMCs in SLS not in HSM Redfish Endpoints =============== ALL OK 2.2.1 Interpreting results Both sections BMCs in SLS not in HSM components and BMCs in SLS not in HSM Redfish Endpoints have the same format for mismatches between SLS and HSM. Each row starts with the xname of the BMC. If the BMC does not have an associated MgmtSwitchConnector in SLS, then # No mgmt port association will be displayed alongside the BMC xname.\n MgmtSwitchConnectors in SLS are used to represent the switch port on a leaf switch that an the BMC of an air cooled device is connected to.\n =============== BMCs in SLS not in HSM components =============== x3000c0s1b0 # No mgmt port association For each of the BMCs that show up in either of mismatch lists use the following notes to determine if the issue with the BMC can be safely ignored, or if there is a legitimate issue with the BMC.\n  The node BMC of \u0026lsquo;ncn-m001\u0026rsquo; will not typically be present in HSM component data, as it is typically connected to the site network instead of the HMN network.\n The following can be used to determine the friendly name of the Node that the NodeBMC controls:\nncn# cray sls search hardware list --parent \u0026lt;NODE_BMC_XNAME\u0026gt; --format json | \\  jq \u0026#39;.[] | { Xname: .Xname, Aliases: .ExtraProperties.Aliases }\u0026#39; -c  Example mismatch for the BMC of ncn-m001:\n=============== BMCs in SLS not in HSM components =============== x3000c0s1b0 # No mgmt port association   The node BMCs for HPE Apollo XL645D nodes may report as a mismatch depedning on the state of the system when the hsm_discovery_verify.sh script is ran. If the system is currently going through the process of installation, then this is an expected mistmatch as the Preapre Compute Nodes procedure required to configure the BMC of the HPE Apollo 6500 XL645D node may not have been completed yet.\n For more information refer to Configure HPE Apollo 6500 XL645d Gen10 Plus Compute Nodes for additional required configuration for this type of BMC.\n Example mistmatch for the BMC of a HPE Apollo XL654D:\n=============== BMCs in SLS not in HSM components =============== x3000c0s30b1 =============== BMCs in SLS not in HSM Redfish Endpoints =============== x3000c0s30b1   Chassis Management Controllers (CMC) may show up as not being present in HSM. CMCs for Intel server blades can be ignored. Gigabyte server blade CMCs not found in HSM is not normal and should be investigated. If a Gigabyte CMC is expected to not be connected to the HMN network, then it can be ignored.\n CMCs have xnames in the form of xXc0sSb999, where X is the cabinet and S is the rack U of the compute node chassis.\n Example mismatch for a CMC an Intel server blade:\n=============== BMCs in SLS not in HSM components =============== x3000c0s10b999 # No mgmt port association =============== BMCs in SLS not in HSM Redfish Endpoints =============== x3000c0s10b999 # No mgmt port association   HPE PDUs are not supported at this time and will likely show up as not being found in HSM. They can be ignored.\n Cabinet PDU Controllers have xnames in the form of xXmM, where X is the cabinet and M is the ordinal of the Cabinet PDU Controller.\n Example mismatch for HPE PDU:\n=============== BMCs in SLS not in HSM components =============== x3000m0 =============== BMCs in SLS not in HSM Redfish Endpoints =============== x3000m0   BMCs having no association with a management switch port will be annotated as such, and should be investigated. Exceptions to this are in Mountain or Hill configurations where Mountain BMCs will show this condition on SLS/HSM mismatches, which is normal.\n  In Hill configurations SLS assumes BMCs in chassis 1 and 3 are fully populated (32 Node BMCs), and in Mountain configurations SLS assumes all BMCs are fully populated (128 Node BMCs). Any non-populated BMCs will have no HSM data and will show up in the mismatch list.\n  If it was determined that the mismatch can not be ignored, then proceed onto the the 2.2.2 Known Issues below to troubleshoot any mismatched BMCs.\n2.2.2 Known Issues Known issues that may prevent hardware from getting discovered by Hardware State Manager:\n Air cooled hardware is not getting properly discovered with Aruba leaf switches HMS Discovery job not creating RedfishEndpoints in Hardware State Manager  3 Software Management Services Health Checks The Software Management Services health checks are run using /usr/local/bin/cmsdev.\n The tool logs to /opt/cray/tests/cmsdev.log The -q (quiet) and -v (verbose) flags can be used to decrease or increase the amount of information sent to the screen.  The same amount of data is written to the log file in either case.     SMS Test Execution Interpreting cmsdev Results  3.1 SMS Test Execution The following test can be run on any Kubernetes node (any master or worker node, but not the PIT node).\nncn# /usr/local/bin/cmsdev test -q bos cfs conman crus ims ipxe 3.2 Interpreting cmsdev Results If all checks passed:\n The return code will be 0 The final line of output will begin with SUCCESS For example: ncn# /usr/local/bin/cmsdev test -q bos cfs conman crus ims ipxe ... SUCCESS: All 6 service tests passed: bos, cfs, conman, crus, ims, ipxe ncn# echo $? 0   If one or more checks failed:\n The return code will be non-0 The final line of output will begin with FAILURE and will list which checks failed For example: ncn# /usr/local/bin/cmsdev test -q bos cfs conman crus ims ipxe ... FAILURE: 2 service tests FAILED (conman, ims), 4 passed (bos, cfs, crus, ipxe) ncn# echo $? 1   Additional test execution details can be found in /opt/cray/tests/cmsdev.log on the node where the test was run.\n4. Booting CSM Barebones Image Included with the Cray System Management (CSM) release is a pre-built node image that can be used to validate that core CSM services are available and responding as expected. The CSM barebones image contains only the minimal set of RPMs and configuration required to boot an image and is not suitable for production usage. To run production work loads, it is suggested that an image from the Cray OS (COS) product, or similar, be used.\n NOTES\n The CSM Barebones image included with the release will not successfully complete beyond the dracut stage of the boot process. However, if the dracut stage is reached, the boot can be considered successful and shows that the necessary CSM services needed to boot a node are up and available.  This inability to boot the barebones image fully will be resolved in future releases of the CSM product.   In addition to the CSM Barebones image, the release also includes an IMS Recipe that can be used to build the CSM Barebones image. However, the CSM Barebones recipe currently requires RPMs that are not installed with the CSM product. The CSM Barebones recipe can be built after the Cray OS (COS) product stream is also installed on to the system.  In future releases of the CSM product, work will be undertaken to resolve these dependency issues.   This procedure can be followed on any NCN or the PIT node. The Cray CLI must be configured on the node where this procedure is being performed. See Configure the Cray Command Line Interface for details on how to do this.    Locate CSM Barebones Image in IMS Create a BOS Session Template for the CSM Barebones Image Find an available compute node Reboot the node using a BOS session template Watch Boot on Console  4.1 Locate CSM Barebones Image in IMS Locate the CSM Barebones image and note the etag and path fields in the output.\nncn# cray ims images list --format json | jq \u0026#39;.[] | select(.name | contains(\u0026#34;barebones\u0026#34;))\u0026#39; Expected output is similar to the following:\n{ \u0026#34;created\u0026#34;: \u0026#34;2021-01-14T03:15:55.146962+00:00\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;293b1e9c-2bc4-4225-b235-147d1d611eef\u0026#34;, \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;6d04c3a4546888ee740d7149eaecea68\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/293b1e9c-2bc4-4225-b235-147d1d611eef/manifest.json\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;cray-shasta-csm-sles15sp1-barebones.x86_64-shasta-1.4\u0026#34; } 4.2 Create a BOS Session Template for the CSM Barebones Image The session template below can be copied and used as the basis for the BOS Session Template. As noted below, make sure the S3 path for the manifest matches the S3 path shown in the Image Management Service (IMS).\n  Create sessiontemplate.json\nncn# vi sessiontemplate.json The session template should contain the following:\n{ \u0026#34;boot_sets\u0026#34;: { \u0026#34;compute\u0026#34;: { \u0026#34;boot_ordinal\u0026#34;: 2, \u0026#34;etag\u0026#34;: \u0026#34;etag_value_from_cray_ims_command\u0026#34;, \u0026#34;kernel_parameters\u0026#34;: \u0026#34;console=ttyS0,115200 bad_page=panic crashkernel=340M hugepagelist=2m-2g intel_iommu=off intel_pstate=disable iommu=pt ip=dhcp numa_interleave_omit=headless numa_zonelist_order=node oops=panic pageblock_order=14 pcie_ports=native printk.synchronous=y rd.neednet=1 rd.retry=10 rd.shell turbo_boost_limit=999 spire_join_token=${SPIRE_JOIN_TOKEN}\u0026#34;, \u0026#34;network\u0026#34;: \u0026#34;nmn\u0026#34;, \u0026#34;node_roles_groups\u0026#34;: [ \u0026#34;Compute\u0026#34; ], \u0026#34;path\u0026#34;: \u0026#34;path_value_from_cray_ims_command\u0026#34;, \u0026#34;rootfs_provider\u0026#34;: \u0026#34;cpss3\u0026#34;, \u0026#34;rootfs_provider_passthrough\u0026#34;: \u0026#34;dvs:api-gw-service-nmn.local:300:nmn0\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; } }, \u0026#34;cfs\u0026#34;: { \u0026#34;configuration\u0026#34;: \u0026#34;cos-integ-config-1.4.0\u0026#34; }, \u0026#34;enable_cfs\u0026#34;: false, \u0026#34;name\u0026#34;: \u0026#34;shasta-1.4-csm-bare-bones-image\u0026#34; } NOTE: Be sure to replace the values of the etag and path fields with the ones you noted earlier in the cray ims images list command.\n  Create the BOS session template using the following file as input:\nncn# cray bos sessiontemplate create --file sessiontemplate.json --name shasta-1.4-csm-bare-bones-image The expected output is:\n/sessionTemplate/shasta-1.4-csm-bare-bones-image   4.3 Find an available compute node ncn# cray hsm state components list --role Compute --enabled true Example output:\n[[Components]] ID = \u0026quot;x3000c0s17b1n0\u0026quot; Type = \u0026quot;Node\u0026quot; State = \u0026quot;On\u0026quot; Flag = \u0026quot;OK\u0026quot; Enabled = true Role = \u0026quot;Compute\u0026quot; NID = 1 NetType = \u0026quot;Sling\u0026quot; Arch = \u0026quot;X86\u0026quot; Class = \u0026quot;River\u0026quot; [[Components]] ID = \u0026quot;x3000c0s17b2n0\u0026quot; Type = \u0026quot;Node\u0026quot; State = \u0026quot;On\u0026quot; Flag = \u0026quot;OK\u0026quot; Enabled = true Role = \u0026quot;Compute\u0026quot; NID = 2 NetType = \u0026quot;Sling\u0026quot; Arch = \u0026quot;X86\u0026quot; Class = \u0026quot;River\u0026quot;  If it is noticed that compute nodes are missing from Hardware State Manager, refer to 2.2.2 Known Issues to troubleshoot any Node BMCs that have not been discovered.\n Choose a node from those listed and set XNAME to its ID. In this example, x3000c0s17b2n0:\nncn# export XNAME=x3000c0s17b2n0 4.4 Reboot the node using a BOS session template Create a BOS session to reboot the chosen node using the BOS session template that was created:\nncn# cray bos session create --template-uuid shasta-1.4-csm-bare-bones-image --operation reboot --limit $XNAME Expected output looks similar to the following:\nlimit = \u0026quot;x3000c0s17b2n0\u0026quot; operation = \u0026quot;reboot\u0026quot; templateUuid = \u0026quot;shasta-1.4-csm-bare-bones-image\u0026quot; [[links]] href = \u0026quot;/v1/session/8f2fc013-7817-4fe2-8e6f-c2136a5e3bd1\u0026quot; jobId = \u0026quot;boa-8f2fc013-7817-4fe2-8e6f-c2136a5e3bd1\u0026quot; rel = \u0026quot;session\u0026quot; type = \u0026quot;GET\u0026quot; [[links]] href = \u0026quot;/v1/session/8f2fc013-7817-4fe2-8e6f-c2136a5e3bd1/status\u0026quot; rel = \u0026quot;status\u0026quot; type = \u0026quot;GET\u0026quot; 4.5 Connect to the node\u0026rsquo;s console and watch the boot See Manage Node Consoles for information on how to connect to the node\u0026rsquo;s console (and for instructions on how to close it later).\nThe boot may take up to 10 or 15 minutes. The image being booted does not support a complete boot, so the node will not boot fully into an operating system. This test is merely to verify that the CSM services needed to boot a node are available and working properly.\nThis boot test is considered successful if the boot reaches the dracut stage. You know this has happened if the console output has something similar to the following somewhere within the final 20 lines of its output:\n[ 7.876909] dracut: FATAL: Don't know how to handle 'root=craycps-s3:s3://boot-images/e3ba09d7-e3c2-4b80-9d86-0ee2c48c2214/rootfs:c77c0097bb6d488a5d1e4a2503969ac0-27:dvs:api-gw-service-nmn.local:300:nmn0' [ 7.898169] dracut: Refusing to continue NOTE: As long as the preceding text is found near the end of the console output, the test is considered successful. It is normal (and not indicative of a test failure) to see something similar to the following at the very end of the console output:\n Starting Dracut Emergency Shell... [ 11.591948] device-mapper: uevent: version 1.0.3 [ 11.596657] device-mapper: ioctl: 4.40.0-ioctl (2019-01-18) initialised: dm-devel@redhat.com Warning: dracut: FATAL: Don't know how to handle Press Enter for maintenance (or press Control-D to continue): After the node has reached this point, close the console session. The test is complete.\n5. UAS / UAI Tests The procedures below use the CLI as an authorized user and run on two separate node types. The first part runs on the LiveCD node, while the second part runs on a non-LiveCD Kubernetes master or worker node. When using the CLI on either node, the CLI configuration needs to be initialized and the user running the procedure needs to be authorized.\nThe following procedures run on separate nodes of the system. They are, therefore, separated into separate sub-sections.\n Validate Basic UAS Installation Validate UAI Creation UAS/UAI Troubleshooting  Authorization Issues UAS Cannot Access Keycloak UAI Images not in Registry Missing Volumes and Other Container Startup Issues    5.1 Validate the Basic UAS Installation This section can be run on any NCN or the PIT node.\n  Initialize the Cray CLI on the node where you are running this section. See Configure the Cray Command Line Interface for details on how to do this.\n  Basic UAS installation is validated using the following: 1.\nncn# cray uas mgr-info list Expected output looks similar to the following:\nservice_name = \u0026quot;cray-uas-mgr\u0026quot; version = \u0026quot;1.11.5\u0026quot; In this example output, it shows that UAS is installed and running the 1.11.5 version. 1.\nncn# cray uas list Expected output looks similar to the following:\nresults = [] This example output shows that there are no currently running UAIs. It is possible, if someone else has been using the UAS, that there could be UAIs in the list. That is acceptable too from a validation standpoint.\n  Verify that the pre-made UAI images are registered with UAS\nncn# cray uas images list Expected output looks similar to the following:\ndefault_image = \u0026quot;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026quot; image_list = [ \u0026quot;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026quot;,] This example output shows that the pre-made end-user UAI image (cray/cray-uai-sles15sp1:latest) is registered with UAS. This does not necessarily mean this image is installed in the container image registry, but it is configured for use. If other UAI images have been created and registered, they may also show up here, which is acceptable.\n  5.2 Validate UAI Creation  IMPORTANT: If you are upgrading CSM and your site does not use UAIs, skip UAS and UAI validation. If you do use UAIs, there are products that configure UAS like Cray Analytics and Cray Programming Environment. These must be working correctly with UAIs and should be validated and corrected (the procedures for this are beyond the scope of this document) prior to validating UAS and UAI. Failures in UAI creation that result from incorrect or incomplete installation of these products will generally take the form of UAIs stuck in \u0026lsquo;waiting\u0026rsquo; state trying to set up volume mounts. See the UAI Troubleshooting section for more information.\n This procedure must run on a master or worker node (not the PIT node and not ncn-w001) on the system. (It is also possible to do from an external host, but the procedure for that is not covered here).\n  Initialize the Cray CLI on the node where you are running this section. See Configure the Cray Command Line Interface for details on how to do this.\n  Verify that a UAI can be created:\nncn# cray uas create --publickey ~/.ssh/id_rsa.pub Expected output looks similar to the following:\nuai_connect_string = \u0026quot;ssh vers@10.16.234.10\u0026quot; uai_host = \u0026quot;ncn-w001\u0026quot; uai_img = \u0026quot;registry.local/cray/cray-uai-sles15sp1:latest\u0026quot; uai_ip = \u0026quot;10.16.234.10\u0026quot; uai_msg = \u0026quot; uai_name = \u0026quot;uai-vers-a00fb46b\u0026quot; uai_status = \u0026quot;Pending\u0026quot; username = \u0026quot;vers\u0026quot; [uai_portmap] This has created the UAI and the UAI is currently in the process of initializing and running.\n  Set UAINAME to the value of the uai_name field in the previous command output (uai-vers-a00fb46b in our example):\nncn# export UAINAME=uai-vers-a00fb46b   Check the current status of the UAI:\nncn# cray uas list Expected output looks similar to the following:\n[[results]] uai_age = \u0026quot;0m\u0026quot; uai_connect_string = \u0026quot;ssh vers@10.16.234.10\u0026quot; uai_host = \u0026quot;ncn-w001\u0026quot; uai_img = \u0026quot;registry.local/cray/cray-uai-sles15sp1:latest\u0026quot; uai_ip = \u0026quot;10.16.234.10\u0026quot; uai_msg = \u0026quot; uai_name = \u0026quot;uai-vers-a00fb46b\u0026quot; uai_status = \u0026quot;Running: Ready\u0026quot; username = \u0026quot;vers\u0026quot; If the uai_status field is Running: Ready, proceed to the next step. Otherwise, wait and repeat this command until that is the case. It normally should not take more than a minute or two.\n  The UAI is ready for use. Log into it with the command in the uai_connect_string field in the previous command output:\nncn# ssh vers@10.16.234.10 vers@uai-vers-a00fb46b-6889b666db-4dfvn:~\u0026gt;   Run a command on the UAI:\nvers@uai-vers-a00fb46b-6889b666db-4dfvn:~\u0026gt; ps -afe Expected output looks similar to the following:\nUID PID PPID C STIME TTY TIME CMD root 1 0 0 18:51 ? 00:00:00 /bin/bash /usr/bin/uai-ssh.sh munge 36 1 0 18:51 ? 00:00:00 /usr/sbin/munged root 54 1 0 18:51 ? 00:00:00 su vers -c /usr/sbin/sshd -e -f /etc/uas/ssh/sshd_config -D vers 55 54 0 18:51 ? 00:00:00 /usr/sbin/sshd -e -f /etc/uas/ssh/sshd_config -D vers 62 55 0 18:51 ? 00:00:00 sshd: vers [priv] vers 67 62 0 18:51 ? 00:00:00 sshd: vers@pts/0 vers 68 67 0 18:51 pts/0 00:00:00 -bash vers 120 68 0 18:52 pts/0 00:00:00 ps -afe   Log out from the UAI\nvers@uai-vers-a00fb46b-6889b666db-4dfvn:~\u0026gt; exit ncn#   Clean up the UAI.\nncn# cray uas delete --uai-list $UAINAME Expected output looks similar to the following:\nresults = [ \u0026quot;Successfully deleted uai-vers-a00fb46b\u0026quot;,]   If the commands ran with similar results, then the basic functionality of the UAS and UAI is working.\n5.3 UAS/UAI Troubleshooting The following subsections include common failure modes seen with UAS / UAI operations and how to resolve them.\n5.3.1 Authorization Issues An error will be returned when running CLI commands if the user is not logged in as a valid Keycloak user or is accidentally using the CRAY_CREDENTIALS environment variable. This variable is set regardless of the user credentials being used.\nFor example:\nncn# cray uas list The symptom of this problem is output similar to the following:\nUsage: cray uas list [OPTIONS] Try 'cray uas list --help' for help. Error: Bad Request: Token not valid for UAS. Attributes missing: ['gidNumber', 'loginShell', 'homeDirectory', 'uidNumber', 'name'] Fix this by logging in as a real user (someone with actual Linux credentials) and making sure that CRAY_CREDENTIALS is unset.\n5.3.2 UAS Cannot Access Keycloak When running CLI commands, a Keycloak error may be returned.\nFor example:\nncn# cray uas list The symptom of this problem is output similar to the following:\nUsage: cray uas list [OPTIONS] Try 'cray uas list --help' for help. Error: Internal Server Error: An error was encountered while accessing Keycloak If the wrong hostname was used to reach the API gateway, re-run the CLI initialization steps above and try again to check that. There may also be a problem with the Istio service mesh inside of the system. Troubleshooting this is beyond the scope of this section, but there may be useful information in the UAS pod logs in Kubernetes. There are generally two UAS pods, so the user may need to look at logs from both to find the specific failure. The logs tend to have a very large number of GET events listed as part of the liveness checking.\nThe following shows an example of looking at UAS logs effectively (this example shows only one UAS manager, normally there would be two):\n  Determine the pod name of the uas-mgr pod\nncn# kubectl get po -n services | grep \u0026#34;^cray-uas-mgr\u0026#34; | grep -v etcd Expected output looks similar to:\ncray-uas-mgr-6bbd584ccb-zg8vx 2/2 Running 0 12d   Set PODNAME to the name of the manager pod whose logs are being viewed.\nncn# export PODNAME=cray-uas-mgr-6bbd584ccb-zg8vx   View its last 25 log entries of the cray-uas-mgr container in that pod, excluding GET events:\nncn# kubectl logs -n services $PODNAME cray-uas-mgr | grep -v \u0026#39;GET \u0026#39; | tail -25 Example output:\n2021-02-08 15:32:41,211 - uas_mgr - INFO - getting deployment uai-vers-87a0ff6e in namespace user 2021-02-08 15:32:41,225 - uas_mgr - INFO - creating deployment uai-vers-87a0ff6e in namespace user 2021-02-08 15:32:41,241 - uas_mgr - INFO - creating the UAI service uai-vers-87a0ff6e-ssh 2021-02-08 15:32:41,241 - uas_mgr - INFO - getting service uai-vers-87a0ff6e-ssh in namespace user 2021-02-08 15:32:41,252 - uas_mgr - INFO - creating service uai-vers-87a0ff6e-ssh in namespace user 2021-02-08 15:32:41,267 - uas_mgr - INFO - getting pod info uai-vers-87a0ff6e 2021-02-08 15:32:41,360 - uas_mgr - INFO - No start time provided from pod 2021-02-08 15:32:41,361 - uas_mgr - INFO - getting service info for uai-vers-87a0ff6e-ssh in namespace user 127.0.0.1 - - [08/Feb/2021 15:32:41] \u0026quot;POST /v1/uas?imagename=registry.local%2Fcray%2Fno-image-registered%3Alatest HTTP/1.1\u0026quot; 200 - 2021-02-08 15:32:54,455 - uas_auth - INFO - UasAuth lookup complete for user vers 2021-02-08 15:32:54,455 - uas_mgr - INFO - UAS request for: vers 2021-02-08 15:32:54,455 - uas_mgr - INFO - listing deployments matching: host None, labels uas=managed,user=vers 2021-02-08 15:32:54,484 - uas_mgr - INFO - getting pod info uai-vers-87a0ff6e 2021-02-08 15:32:54,596 - uas_mgr - INFO - getting service info for uai-vers-87a0ff6e-ssh in namespace user 2021-02-08 15:40:25,053 - uas_auth - INFO - UasAuth lookup complete for user vers 2021-02-08 15:40:25,054 - uas_mgr - INFO - UAS request for: vers 2021-02-08 15:40:25,054 - uas_mgr - INFO - listing deployments matching: host None, labels uas=managed,user=vers 2021-02-08 15:40:25,085 - uas_mgr - INFO - getting pod info uai-vers-87a0ff6e 2021-02-08 15:40:25,212 - uas_mgr - INFO - getting service info for uai-vers-87a0ff6e-ssh in namespace user 2021-02-08 15:40:51,210 - uas_auth - INFO - UasAuth lookup complete for user vers 2021-02-08 15:40:51,210 - uas_mgr - INFO - UAS request for: vers 2021-02-08 15:40:51,210 - uas_mgr - INFO - listing deployments matching: host None, labels uas=managed,user=vers 2021-02-08 15:40:51,261 - uas_mgr - INFO - deleting service uai-vers-87a0ff6e-ssh in namespace user 2021-02-08 15:40:51,291 - uas_mgr - INFO - delete deployment uai-vers-87a0ff6e in namespace user 127.0.0.1 - - [08/Feb/2021 15:40:51] \u0026quot;DELETE /v1/uas?uai_list=uai-vers-87a0ff6e HTTP/1.1\u0026quot; 200 -   5.3.3 UAI Images not in Registry When listing or describing a UAI, an error in the uai_msg field may be returned. For example:\nncn# cray uas list There may be something similar to the following output:\n[[results]] uai_age = \u0026quot;0m\u0026quot; uai_connect_string = \u0026quot;ssh vers@10.103.13.172\u0026quot; uai_host = \u0026quot;ncn-w001\u0026quot; uai_img = \u0026quot;registry.local/cray/cray-uai-sles15sp1:latest\u0026quot; uai_ip = \u0026quot;10.103.13.172\u0026quot; uai_msg = \u0026quot;ErrImagePull\u0026quot; uai_name = \u0026quot;uai-vers-87a0ff6e\u0026quot; uai_status = \u0026quot;Waiting\u0026quot; username = \u0026quot;vers\u0026quot; This means the pre-made end-user UAI image is not in the local registry (or whatever registry it is being pulled from; see the uai_img value for details). To correct this, locate and push/import the image to the registry.\n5.3.4 Missing Volumes and other Container Startup Issues Various packages install volumes in the UAS configuration. All of those volumes must also have the underlying resources available, sometimes on the host node where the UAI is running sometimes from with Kubernetes. If a UAI gets stuck with a ContainerCreating uai_msg field for an extended time, this is a likely cause. UAIs run in the user Kubernetes namespace, and are pods that can be examined using kubectl describe.\n  Locate the pod.\nncn# kubectl get po -n user | grep \u0026lt;uai-name\u0026gt;   Investigate the problem using the pod name from the previous step.\nncn# kubectl describe pod -n user \u0026lt;pod-name\u0026gt; If volumes are missing they will show up in the Events: section of the output. Other problems may show up there as well. The names of the missing volumes or other issues should indicate what needs to be fixed to make the UAI run.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/utility_storage/troubleshoot_rgw_health_check_fail/",
	"title": "Troubleshoot If Rgw Health Check Fails",
	"tags": [],
	"description": "",
	"content": "Troubleshoot if RGW Health Check Fails Use this procedure to determine why the rgw health check failed and what needs to be fixed.\nProcedure In the goss test output, look at the value of x in Expected \\\u0026lt; int \\\u0026gt;: x (possible values are 1, 2, 3, 4, 5). Based on the value, navigate to the corresponding numbered item below for troubleshooting this issue.\nOptional: Manually run the rgw health check script to see descriptive output.\nncn-m001# GOSS_BASE=/opt/cray/tests/install/ncn /opt/cray/tests/install/ncn/scripts/rgw_health_check.sh   A value of 1 is returned if unable to connect to rgw-vip. This happens if any of the following three commands fail.\nncn-m001# curl -i -s -S -k https://rgw-vip.nmn ncn-m001# curl -i -s -S http://rgw-vip.nmn ncn-m001# curl -i -s -S http://rgw-vip.hmn Log into a storage node and look at the version and status of Ceph.\nncn-s# ceph --version ncn-s# ceph -s   A value of 2 is returned if a storage node is not able to be reached. In this case, run the rgw_health_check.sh as stated in the optional step above. Find which storage nodes are not able to be reached, and run the following checks on those nodes.\n  Check if HAProxy is running on the node.\nncn-s# systemctl status haproxy If HAProxy is not running, restart it and check the status again.\nncn-s# systemctl restart haproxy ncn-s# systemctl status haproxy   Check if keepalived is running on the node.\nncn-s# systemctl status keepalived.service If keepalived is not running, restart it and check the status again.\nncn-s# systemctl restart keepalived.service ncn-s# systemctl status keepalived.service   Check if the ceph-rgw daemon is running.\nncn-s# ceph -s | grep rgw If the ceph-rgw daemon is not running on 3 storage nodes, restart the daemon and watch it come up within a few seconds.\nncn-s# ceph orch ps | grep rgw #use this to wach the daemon start ncn-s# ceph orch daemon restart \u0026lt;name\u0026gt;     A value of 3 is returned if a craysys command fails. This implies \u0026lsquo;cloud-init\u0026rsquo; is not healthy. Run the command below to determine the health.\nncn-s# cloud-init query -a If the command above fails, reinitialize \u0026lsquo;cloud-init\u0026rsquo; using the following command.\nncn-s# cloud-init init   If a value of 4 or 5 is returned, then rgw-vip and the storage nodes are reachable. The error occurred when attempting to create a bucket, upload an object to a bucket, or download an object from a bucket. This implies Ceph may be unhealthy. Check Ceph status with the following command.\nncn-s# ceph -s If Ceph reports any status other than \u0026ldquo;HEALTH_OK\u0026rdquo;, refer to Utility Storage for general Ceph troubleshooting.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/utility_storage/troubleshoot_large_object_map_objects_in_ceph_health/",
	"title": "Troubleshoot Large Object Map Objects In Ceph Health",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Large Object Map Objects in Ceph Health Troubleshoot an issue where Ceph reports a HEALTH_WARN of 1 large omap objects. Adjust the omap object key threshold or number of placement groups (PG) to resolve this issue.\nPrerequisites Ceph health is reporting a HEALTH_WARN for large Object Map (omap) objects.\nncn-m001# ceph -s cluster: id: 464f8ee0-667d-49ac-a82b-43ba8d377f81 health: HEALTH_WARN 1 large omap objects clock skew detected on mon.ncn-m002, mon.ncn-m003 services: mon: 3 daemons, quorum ncn-s001,ncn-s002,ncn-s003 (age 20h) mgr: ncn-s003(active, since 9d), standbys: ncn-s001, ncn-s002 mds: cephfs:1 {0=ncn-s002=up:active} 2 up:standby osd: 18 osds: 18 up (since 20h), 18 in (since 9d) rgw: 3 daemons active (ncn-s001.rgw0, ncn-s002.rgw0, ncn-s003.rgw0) data: pools: 11 pools, 600 pgs objects: 1.82M objects, 2.6 TiB usage: 6.1 TiB used, 57 TiB / 63 TiB avail pgs: 600 active+clean io: client: 162 KiB/s rd, 9.0 MiB/s wr, 2 op/s rd, 890 op/s wr Procedure   Adjust the number of omap objects.\nUse one of the options below to resolve the issue:\n  Use the ceph config command.\nIn the example below, the omap object key threshold is set to 350000, but it can be set to a higher number if desired.\nncn-m001# ceph config set client.osd osd_deep_scrub_large_omap_object_key_threshold 350000   Increase the number of PGs for the Ceph pool.\n  Get the current threshold and PG numbers.\nncn-m001# ceph osd pool autoscale-status POOL SIZE TARGET SIZE RATE RAW CAPACITY RATIO TARGET RATIO BIAS PG_NUM NEW PG_NUM AUTOSCALE cephfs_data 477.8M 3.0 64368G 0.0000 1.0 4 on cephfs_metadata 781.9M 3.0 64368G 0.0000 4.0 16 on .rgw.root 384.0k 3.0 64368G 0.0000 1.0 4 on default.rgw.buckets.data 4509G 3.0 64368G 0.2102 0.2000 1.0 128 on default.rgw.control 0 3.0 64368G 0.0000 1.0 4 on default.rgw.buckets.index 1199M 3.0 64368G 0.0001 0.1800 1.0 128 on default.rgw.meta 4898k 3.0 64368G 0.0000 1.0 4 on default.rgw.log 0 3.0 64368G 0.0000 1.0 4 on kube 307.0G 3.0 64368G 0.0143 0.1000 1.0 48 on smf 1414G 2.0 64368G 0.0440 0.3000 1.0 256 on default.rgw.buckets.non-ec 0 3.0 64368G 0.0000 1.0 4 on   Adjust the target_size_ratio value to increase the PGs for the pool.\nThis number should be increased a tenth or smaller at a time. Check the autoscale-status between each adjustment. When there is a change to the New PG NUM, stop adjusting the number.\nIn the example below, the target_size_ratio is set to 0.2.\nncn-m001# ceph osd pool set POOL_NAME target_size_ratio 0.2   Check to see if the change is taking effect.\nncn-m001# ceph osd pool autoscale-status   Watch the status of the Ceph health.\nVerify the recovery traffic is taking place on the keys. The -w option can be used to watch the cluster.\nncn-m001# ceph -s       "
},
{
	"uri": "/docs-csm/en-11/operations/utility_storage/troubleshoot_pods_failing_to_restart_on_other_worker_nodes/",
	"title": "Troubleshoot Pods Failing To Restart On Other Worker Nodes",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Pods Failing to Restart on Other Worker Nodes Troubleshoot an issue where pods cannot restart on another worker node because of the \u0026ldquo;Volume is already exclusively attached to one node and can\u0026rsquo;t be attached to another\u0026rdquo; error. Kubernetes does not currently support \u0026ldquo;readwritemany\u0026rdquo; access mode for Rados Block Device (RBD) devices, which causes an issue where devices fail to unmap correctly.\nThe issue occurs when unmounting the mounts tied to the RBD devices, which causes the rbd-task (watcher) to not stop for the RBD device.\nWARNING: If this process is followed and there are mount points that cannot be unmounted without using the force option, then a process may still be writing to them. If mount points are forcefully unmounted, there is a high probability of data loss or corruption.\nPrerequisites This procedure requires administrative privileges.\nProcedure   Force delete the pod.\nThis may not be successful, but it is important to try before proceeding.\nncn-w001# kubectl delete pod -n NAMESPACE POD_NAME --force --grace-period=0   Log in to a manager node and proceed if the previous step did not fix the issue.\n  Describe the pod experiencing issues.\nThe returned Persistent Volume Claim (PVC) information will be needed in future steps.\nncn-m001# kubectl -n services describe pod POD_ID ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 23s default-scheduler Successfully assigned services/cray-ims-6578bf7874-twwp7 to ncn-w002 Warning FailedAttachVolume 23s attachdetach-controller Multi-Attach error for volume \u0026#34;**pvc-6ac68e32-de91-4e21-ac9f-c743b3ecb776**\u0026#34; Volume is already exclusively attached to one node and can\u0026#39;t be attached to another In this example, pvc-6ac68e32-de91-4e21-ac9f-c743b3ecb776 is the PVC information required for the next step.\n  Retrieve the Ceph volume.\nncn-m001# kubectl describe -n NAMESPACE pv PVC_NAME Name: pvc-6ac68e32-de91-4e21-ac9f-c743b3ecb776 Labels: \u0026lt;none\u0026gt; Annotations: pv.kubernetes.io/provisioned-by: ceph.com/rbd rbdProvisionerIdentity: ceph.com/rbd Finalizers: [kubernetes.io/pv-protection] StorageClass: ceph-rbd-external Status: Bound Claim: services/cray-ims-data-claim Reclaim Policy: Delete Access Modes: RWO VolumeMode: Filesystem Capacity: 10Gi Node Affinity: \u0026lt;none\u0026gt; Message: Source: Type: RBD (a Rados Block Device mount on the host that shares a pod\u0026#39;s lifetime) CephMonitors: [10.252.0.10 10.252.0.11 10.252.0.12] RBDImage: kubernetes-dynamic-pvc-3ce9ec37-846b-11ea-acae-86f521872f4c \u0026lt;\u0026lt;-- Ceph image name FSType: RBDPool: kube \u0026lt;\u0026lt;-- Ceph pool RadosUser: kube Keyring: /etc/ceph/keyring SecretRef: \u0026amp;SecretReference{Name:ceph-rbd-kube,Namespace:ceph-rbd,} ReadOnly: false Events: \u0026lt;none\u0026gt;   Find the worker node that has the RBD locked.\n  Find the RBD status.\nTake a note of the returned IP address.\nncn-m001# rbd status CEPH_POOL_NAME/CEPH_IMAGE_NAME For example:\nncn-m001# rbd status kube/kubernetes-dynamic-pvc-3ce9ec37-846b-11ea-acae-86f521872f4c Watchers: watcher=**10.252.0.4**:0/3520479722 client.689192 cookie=18446462598732840976   Use the returned IP address to get the host name attached to it.\nTake note of the returned host name.\nncn-m001# grep IP_ADDRESS /etc/hosts 10.252.0.4 ncn-w001.local ncn-w001 ncn-w001-nmn.local x3000c0s7b0n0 ncn-w001-nmn sms01-nmn.local sms04-nmn sms.local sms-nmn sms-nmn.local mgmt-plane-cmn mgmt-plane-cmn.local mgmt-plane-nmn.local bis.local bis time-nmn time-nmn.local #-label-10.252.0.4     SSH to the host name returned in the previous step.\nncn-m001# ssh HOST_NAME   Unmap the device.\n  Find the RBD number.\nUse the CEPH_IMAGE_NAME value returned in step 4.\nncn-m001# rbd showmapped|grep CEPH_IMAGE_NAME 16 kube kubernetes-dynamic-pvc-3ce9ec37-846b-11ea-acae-86f521872f4c - /dev/**rbd16** Take note of the returned RBD number, which will be used in the next step.\n  Verify it is not in use by an unstopped container.\nncn-m001# mount|grep RBD_NUMBER If no mount points are returned, proceed to the next step. If mount points are returned, run the following command:\nncn-m001# unmount MOUNT_POINT Troubleshooting: If that still does not succeed, use the unmount -f option.\nWARNING: If mount points are forcefully unmounted, there is a chance for data loss or corruption.\n  Unmap the device.\nncn-m001# rbd unmap -o force /dev/RBD_NUMBER     Check the status of the pod.\nncn-m001# kubectl get pod -n NAMESPACE POD_NAME Troubleshooting: If the pod status has not changes, try deleting the pod to restart it.\nncn-m001# kubectl delete pod -n NAMESPACE POD_NAME   "
},
{
	"uri": "/docs-csm/en-11/operations/utility_storage/troubleshoot_system_clock_skew/",
	"title": "Troubleshoot System Clock Skew",
	"tags": [],
	"description": "",
	"content": "Troubleshoot System Clock Skew Resynchronize system clocks after Ceph reports a clock skew.\nSystems use chronyd to synchronize their system clocks. If systems are not able to communicate, then the clocks can drift, causing clock skew. Another reason for this issue would be an individual manually changing the clocks or a task that may change the clocks and require a series of steps (time adjustments) to resynchronize.\nMajor time jumps where the clock is set back in time will require a full restart of all Ceph services.\nClock skew can cause issues with Kubernetes operations, etcd, node responsiveness, and more.\nPrerequisites This procedure requires admin privileges.\nProcedure   Verify that the system is impact by clock skew.\nCeph provides block storage and requires a clock skew of less than 0.05 seconds to report back healthy.\nncn-m001# ceph -s cluster: id: b6d509e6-772e-4785-a421-e4a138b1780c health: HEALTH_WARN clock skew detected on mon.ncn-m002, mon.ncn-m003 services: mon: 3 daemons, quorum ncn-s001,ncn-s002,ncn-s003 (age 20h) mgr: ncn-s003(active, since 9d), standbys: ncn-s001, ncn-s002 mds: cephfs:1 {0=ncn-s002=up:active} 2 up:standby osd: 18 osds: 18 up (since 20h), 18 in (since 9d) rgw: 3 daemons active (ncn-s001.rgw0, ncn-s002.rgw0, ncn-s003.rgw0) data: pools: 10 pools, 224 pgs objects: 19.41k objects, 59 GiB usage: 167 GiB used, 274 GiB / 441 GiB avail pgs: 224 active+clean io: client: 919 KiB/s wr, 0 op/s rd, 16 op/s wr IMPORTANT: If you see this message in the ceph logs unable to obtain rotating service keys; retrying it will also indicate clock skew. You may have to utilize xzgrep skew *.xz to see the skew if you logs have rolled over.\n  View the Ceph health details.\n  View the Ceph logs.\nIf looking back to earlier logs, use the xzgrep command for the ceph.log or the ceph-mon*.log. There are cases where the MGR and OSD logs are not in the ceph-mon logs. This indicates that the skew was very drastic and sudden, thus causing the ceph-mon process to panic and not log the issue.\nncn-m001# grep skew /var/log/ceph/*.log   View the system time.\nncn-w001# ansible ceph_all -m shell -a date     Sync the clocks to fix the issue.\nncn-w001# systemctl restart chronyd.service Wait a bit after running the command and the Ceph alert will clear. Restart the Ceph mon service on that node if the alert does not clear.\n  Check Ceph health to verify the clock skew issue is resolved.\nIt may take up to 15 minutes for this warning to resolve.\nncn-m001# ceph -s cluster: id: 5f3b4031-d6c0-4118-94c0-bffd90b534eb health: HEALTH_OK services: mon: 3 daemons, quorum ncn-s001,ncn-s002,ncn-s003 (age 20h) mgr: ncn-s003(active, since 9d), standbys: ncn-s001, ncn-s002 mds: cephfs:1 {0=ncn-s002=up:active} 2 up:standby osd: 18 osds: 18 up (since 20h), 18 in (since 9d) rgw: 3 daemons active (ncn-s001.rgw0, ncn-s002.rgw0, ncn-s003.rgw0) data: pools: 11 pools, 240 pgs objects: 3.12k objects, 11 GiB usage: 45 GiB used, 39 GiB / 84 GiB avail pgs: 240 active+clean If clocks are in sync and Ceph is still reporting skew, refer to Manage Ceph Services on restarting services.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/utility_storage/troubleshoot_ceph_osds_reporting_full/",
	"title": "Troubleshoot Ceph Osds Reporting Full",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Ceph OSDs Reporting Full Use this procedure to examine the Ceph cluster and troubleshoot issues where Ceph runs out of space and the Kubernetes cluster cannot write data. The OSDs need to be reweighed to move data from the drive and get it back under the warning threshold.\nWhen a single OSD for a pool fills up, the pool will go into read-only mode to protect the data. This can occur if the data distribution is unbalanced or if more storage nodes are needed.\nReturn the Ceph cluster to a healthy state after it reports a full OSD.\nPrerequisites The commands in this procedure need to be run on a ceph-mon node.\nProcedure   View the status of the Ceph cluster.\nncn-m001# ceph -s cluster: id: 64e553c3-e7d9-4636-81a4-56f26c1b20e1 health: HEALTH_ERR 1 full osd(s) 13 pool(s) full services: mon: 3 daemons, quorum ncn-s001,ncn-s002,ncn-s003 (age 20h) mgr: ncn-s003(active, since 9d), standbys: ncn-s001, ncn-s002 mds: cephfs:1 {0=ncn-s002=up:active} 2 up:standby osd: 18 osds: 18 up (since 20h), 18 in (since 9d) rgw: 3 daemons active (ncn-s001.rgw0, ncn-s002.rgw0, ncn-s003.rgw0) data: pools: 13 pools, 288 pgs objects: 2.98M objects, 11 TiB usage: 32 TiB used, 24 TiB / 56 TiB avail pgs: 288 active+clean io: client: 379 KiB/s rd, 2.2 KiB/s wr, 13 op/s rd, 1 op/s wr   View the Ceph health detail.\nThe OSD_NEARFULL list can have multiple results. Take a note of the returned results to compare with the output of the ceph osd df output.\nncn-m001# ceph health detail HEALTH_ERR 1 nearfull osd(s); 13 pool(s) nearfull; Degraded data redundancy (low space): 3 pgs backfill_toofull OSD_NEARFULL 1 nearfull osd(s) osd.9 is near full \u0026lt;\u0026lt;-- Note this value   View the storage utilization of the cluster and pools.\nncn-m001# ceph df RAW STORAGE: CLASS SIZE AVAIL USED RAW USED %RAW USED ssd 56 TiB 24 TiB 32 TiB 32 TiB 57.15 TOTAL 56 TiB 24 TiB 32 TiB 32 TiB 57.15 POOLS: POOL ID STORED OBJECTS USED %USED MAX AVAIL cephfs_data 1 39 MiB 387 121 MiB 0.10 39 GiB cephfs_metadata 2 257 MiB 123 770 MiB 0.64 39 GiB .rgw.root 3 3.7 KiB 8 400 KiB 0 39 GiB defaults.rgw.buckets.data 4 0 B 0 0 B 0 39 GiB default.rgw.control 5 0 B 8 0 B 0 39 GiB defaults.rgw.buckets.index 6 0 B 0 0 B 0 39 GiB default.rgw.meta 7 22 KiB 114 4.4 MiB 0 39 GiB default.rgw.log 8 0 B 207 0 B 0 39 GiB kube 9 220 GiB 61.88k 661 GiB 84.93 39 GiB smf 10 10 TiB 2.86M 31 TiB 99.63 39 GiB default.rgw.buckets.index 11 5.9 MiB 14 5.9 MiB 0 39 GiB default.rgw.buckets.data 12 145 GiB 48.11k 436 GiB 78.81 39 GiB default.rgw.buckets.non-ec 13 305 KiB 34 1.9 MiB 0 39 GiB   View the utilization of the OSDs to see if data is not balanced across them.\nIn the example below, the OSD.9 value is showing that it is 95.17 percent full.\nncn-m001# ceph osd df ID CLASS WEIGHT REWEIGHT SIZE RAW USE DATA OMAP META AVAIL %USE VAR PGS STATUS 1 ssd 3.49219 1.00000 3.5 TiB 2.1 TiB 2.1 TiB 6.3 MiB 3.9 GiB 1.4 TiB 60.81 1.06 57 up 4 ssd 3.49219 1.00000 3.5 TiB 2.0 TiB 2.0 TiB 133 KiB 3.7 GiB 1.5 TiB 57.58 1.01 56 up 5 ssd 3.49219 1.00000 3.5 TiB 2.1 TiB 2.1 TiB 195 KiB 3.5 GiB 1.4 TiB 61.33 1.07 49 up 6 ssd 3.49219 1.00000 3.5 TiB 1.2 TiB 1.2 TiB 321 KiB 2.4 GiB 2.3 TiB 33.90 0.59 40 up 7 ssd 3.49219 1.00000 3.5 TiB 1.5 TiB 1.5 TiB 1012 KiB 2.9 GiB 2.0 TiB 43.03 0.75 39 up 8 ssd 3.49219 1.00000 3.5 TiB 1.7 TiB 1.7 TiB 194 KiB 4.0 GiB 1.8 TiB 47.96 0.84 47 up 0 ssd 3.49219 1.00000 3.5 TiB 2.8 TiB 2.8 TiB 485 KiB 5.2 GiB 696 GiB 80.53 1.41 75 up **9 ssd 3.49219 1.00000 3.5 TiB 3.3 TiB 3.3 TiB 642 KiB 6.1 GiB 173 GiB 95.17 1.67 67 up** 10 ssd 3.49219 1.00000 3.5 TiB 1.7 TiB 1.7 TiB 6.7 MiB 3.1 GiB 1.8 TiB 47.74 0.84 68 up 11 ssd 3.49219 1.00000 3.5 TiB 2.8 TiB 2.8 TiB 1.1 MiB 5.4 GiB 675 GiB 81.14 1.42 78 up 2 ssd 3.49219 1.00000 3.5 TiB 2.2 TiB 2.2 TiB 27 KiB 4.0 GiB 1.3 TiB 62.14 1.09 40 up 3 ssd 3.49219 1.00000 3.5 TiB 2.3 TiB 2.3 TiB 445 KiB 4.4 GiB 1.2 TiB 65.90 1.15 55 up 12 ssd 3.49219 1.00000 3.5 TiB 541 GiB 540 GiB 1006 KiB 1.3 GiB 3.0 TiB 15.14 0.27 48 up 13 ssd 3.49219 1.00000 3.5 TiB 2.6 TiB 2.6 TiB 176 KiB 4.9 GiB 895 GiB 74.96 1.31 56 up 14 ssd 3.49219 1.00000 3.5 TiB 1.8 TiB 1.8 TiB 6.4 MiB 3.3 GiB 1.7 TiB 52.03 0.91 48 up 15 ssd 3.49219 1.00000 3.5 TiB 1.2 TiB 1.2 TiB 179 KiB 2.5 GiB 2.3 TiB 34.44 0.60 41 up   Use the ceph osd reweight command on the OSD to move data from the drive and get it back under the warning threshold of 85 percent.\nThis command tells Ceph that the drive can now only hold 80 percent of the usable space (crush weight).\nncn-m001# ceph osd reweight osd.9 0.80   Confirm the reweight command made the change.\nIn this example, the new reweight is .79999 and the use is now at 80 percent.\nncn-m001# ceph osd df ID CLASS WEIGHT REWEIGHT SIZE RAW USE DATA OMAP META AVAIL %USE VAR PGS STATUS 1 ssd 3.49219 1.00000 3.5 TiB 2.1 TiB 2.1 TiB 7.1 MiB 4.7 GiB 1.4 TiB 60.91 1.07 57 up 4 ssd 3.49219 1.00000 3.5 TiB 2.0 TiB 2.0 TiB 137 KiB 3.7 GiB 1.5 TiB 57.65 1.01 56 up 5 ssd 3.49219 1.00000 3.5 TiB 2.1 TiB 2.1 TiB 207 KiB 4.0 GiB 1.3 TiB 61.42 1.07 49 up 6 ssd 3.49219 1.00000 3.5 TiB 1.2 TiB 1.2 TiB 293 KiB 2.4 GiB 2.3 TiB 33.94 0.59 40 up 7 ssd 3.49219 1.00000 3.5 TiB 1.5 TiB 1.5 TiB 1012 KiB 2.9 GiB 2.0 TiB 43.08 0.75 39 up 8 ssd 3.49219 1.00000 3.5 TiB 1.7 TiB 1.7 TiB 198 KiB 3.1 GiB 1.8 TiB 48.00 0.84 47 up 0 ssd 3.49219 1.00000 3.5 TiB 3.0 TiB 3.0 TiB 497 KiB 5.5 GiB 522 GiB 85.40 1.49 80 up **9 ssd 3.49219 0.79999 3.5 TiB 2.8 TiB 2.8 TiB 650 KiB 6.1 GiB 687 GiB 80.80 1.41 51 up** 10 ssd 3.49219 1.00000 3.5 TiB 2.0 TiB 2.0 TiB 7.2 MiB 3.6 GiB 1.5 TiB 57.35 1.00 75 up 11 ssd 3.49219 1.00000 3.5 TiB 2.8 TiB 2.8 TiB 1.1 MiB 5.3 GiB 664 GiB 81.43 1.42 82 up 2 ssd 3.49219 1.00000 3.5 TiB 2.2 TiB 2.2 TiB 31 KiB 4.0 GiB 1.3 TiB 62.22 1.09 40 up 3 ssd 3.49219 1.00000 3.5 TiB 2.3 TiB 2.3 TiB 457 KiB 4.2 GiB 1.2 TiB 65.98 1.15 55 up 12 ssd 3.49219 1.00000 3.5 TiB 542 GiB 541 GiB 990 KiB 1.3 GiB 3.0 TiB 15.16 0.27 48 up 13 ssd 3.49219 1.00000 3.5 TiB 2.6 TiB 2.6 TiB 196 KiB 4.9 GiB 892 GiB 75.05 1.31 56 up 14 ssd 3.49219 1.00000 3.5 TiB 1.8 TiB 1.8 TiB 7.1 MiB 3.3 GiB 1.7 TiB 52.10 0.91 48 up 15 ssd 3.49219 1.00000 3.5 TiB 1.2 TiB 1.2 TiB 171 KiB 2.7 GiB 2.3 TiB 34.48 0.60 41 up TOTAL 56 TiB 32 TiB 32 TiB 27 MiB 62 GiB 24 TiB 57.19 MIN/MAX VAR: 0.27/1.49 STDDEV: 18.51   Monitor the Ceph cluster during recovery.\nncn-m001# ceph -s   "
},
{
	"uri": "/docs-csm/en-11/operations/utility_storage/troubleshoot_ceph_services_not_starting/",
	"title": "Troubleshoot Ceph Services Not Starting After A Server Crash",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Ceph services not starting after a server crash Issue There is a known issue where the Ceph container images will not start after a power failure or server component failure that causes the server to crash and not boot back up\nThere will be a message like this in the journalctl logs for the ceph services on the machine that crashed\nceph daemons will not start due to: Error: readlink /var/lib/containers/storage/overlay/l/CXMD7IEI4LUKBJKX5BPVGZLY3Y: no such file or directory\nWhen the issue materializes, then it is highly likely the Ceph container images have been corrupted.\nFix   Remove the corrupted images\nfor i in $(podman images|grep -v REPO|awk {\u0026#39;print $1\u0026#34;:\u0026#34;$2\u0026#39;}); do podman image rm $i; done   Reload the images\n/srv/cray/scripts/common/pre-load-images.sh   Validate services are starting\nncn-s00(1/2/3)# ceph orch ps # ceph orch ps NAME HOST STATUS REFRESHED AGE VERSION IMAGE NAME IMAGE ID CONTAINER ID alertmanager.ncn-s001 ncn-s001 running (95m) 2m ago 97m 0.20.0 registry.local/prometheus/alertmanager:v0.20.0 0881eb8f169f a3fbad5afe50 crash.ncn-s001 ncn-s001 running (97m) 2m ago 97m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c ddc724e9a18e crash.ncn-s002 ncn-s002 running (97m) 2m ago 97m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 3925895be42d crash.ncn-s003 ncn-s003 running (97m) 2m ago 97m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c b9eb9f3582f7 grafana.ncn-s001 ncn-s001 running (97m) 2m ago 97m 6.6.2 registry.local/ceph/ceph-grafana:6.6.2 a0dce381714a 269fd70c881f mds.cephfs.ncn-s001.dkpjnt ncn-s001 running (95m) 2m ago 95m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 600c4a5513e5 mds.cephfs.ncn-s002.nyirpe ncn-s002 running (95m) 2m ago 95m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 6c9295a5a795 mds.cephfs.ncn-s003.gqxuoc ncn-s003 running (95m) 2m ago 95m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c c92990c970f4 mgr.ncn-s001.lhjrhi ncn-s001 running (98m) 2m ago 98m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c e85dbd963f0d mgr.ncn-s002.hvqjgu ncn-s002 running (96m) 2m ago 96m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c a9ba72dfde66 mgr.ncn-s003.zqoych ncn-s003 running (97m) 2m ago 97m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c a33f6f1a265c mon.ncn-s001 ncn-s001 running (98m) 2m ago 99m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 53245f1e60b7 mon.ncn-s002 ncn-s002 running (97m) 2m ago 97m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c cdbda41fc32e mon.ncn-s003 ncn-s003 running (97m) 2m ago 97m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 24578b34f6cd node-exporter.ncn-s001 ncn-s001 running (97m) 2m ago 97m 0.18.1 registry.local/prometheus/node-exporter:v0.18.1 e5a616e4b9cf 79617e2d92ed node-exporter.ncn-s002 ncn-s002 running (97m) 2m ago 97m 0.18.1 registry.local/prometheus/node-exporter:v0.18.1 e5a616e4b9cf d5a93a7ab603 node-exporter.ncn-s003 ncn-s003 running (96m) 2m ago 96m 0.18.1 registry.local/prometheus/node-exporter:v0.18.1 e5a616e4b9cf 8ba07c965a83 osd.0 ncn-s003 running (96m) 2m ago 96m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 9dd55acc0475 osd.1 ncn-s001 running (96m) 2m ago 96m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 08548417e7ea osd.10 ncn-s001 running (96m) 2m ago 96m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 5d3f372c2164 osd.11 ncn-s002 running (96m) 2m ago 96m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c c3697f42ee78 osd.12 ncn-s003 running (96m) 2m ago 96m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 3671a6897993 osd.13 ncn-s001 running (96m) 2m ago 96m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 35bc02ccd8a6 osd.14 ncn-s002 running (96m) 2m ago 96m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c a777b16e6e8b osd.15 ncn-s003 running (96m) 2m ago 96m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c b725bd38b753 osd.16 ncn-s001 running (96m) 2m ago 96m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c fa4e211a6632 osd.17 ncn-s002 running (96m) 2m ago 96m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c ae5cd8b169cc osd.2 ncn-s002 running (96m) 2m ago 96m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c abbf4563210b osd.3 ncn-s003 running (96m) 2m ago 96m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 765115ca70e8 osd.4 ncn-s001 running (96m) 2m ago 96m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c ef4186a535df osd.5 ncn-s002 running (96m) 2m ago 96m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c d4c96c856f2a osd.6 ncn-s003 running (96m) 2m ago 96m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c ff7c0c2a8b66 osd.7 ncn-s001 running (96m) 2m ago 96m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c c18c09dd115f osd.8 ncn-s002 running (96m) 2m ago 96m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 5ea54dfa7cbe osd.9 ncn-s003 running (96m) 2m ago 96m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 0bd8f2e7cbe6 prometheus.ncn-s001 ncn-s001 running (95m) 2m ago 97m 2.18.1 docker.io/prom/prometheus:v2.18.1 de242295e225 43c3411ae2cb rgw.site1.zone1.ncn-s001.hjmgem ncn-s001 running (94m) 2m ago 94m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 0f23173da9b0 rgw.site1.zone1.ncn-s002.eccwzc ncn-s002 running (94m) 2m ago 94m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c a71878b4847b rgw.site1.zone1.ncn-s003.lsmzng ncn-s003 running (94m) 2m ago 94m 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 0a5f56e8fc98 At this point the processes starting/running on the node that crashed, this may take a few minutes\nIf after 5 mins the service are still reporting down then fail-over the ceph mgr daemon and recheck the daemons\nceph mgr fail $(ceph mgr dump | jq -r .active_name)   "
},
{
	"uri": "/docs-csm/en-11/operations/utility_storage/troubleshoot_ceph-mon_processes_stopping_and_exceeding_max_restarts/",
	"title": "Troubleshoot Ceph-mon Processes Stopping And Exceeding Max Restarts",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Ceph-Mon Processes Stopping and Exceeding Max Restarts Troubleshoot an issue where all of the ceph-mon processes stop and exceed their maximum amount of attempts at restarting. This bug corrupts the health of the Ceph cluster.\nReturn the Ceph cluster to a healthy state by resolving issues with ceph-mon processes.\nPrerequisites This procedure requires admin privileges.\nProcedure See Collect Information about the Ceph Cluster for more information on how to interpret the output of the Ceph commands used in this procedure.\n  Log on to the manager nodes via ssh.\nThe commands in the next step will need to be run on each manager node.\n  Verify the ceph-mon process is running as expected.\n  Check to see if the ceph-mon process is running on all of the manager nodes.\nThis command needs to be run on each manager node to determine where the issues are occurring. Make a note of which nodes do not have the ceph-mon process running.\nncn-m001# ps -ef |grep ceph-mon root 24465 24175 0 10:04 pts/0 00:00:00 grep ceph-mon ceph 33480 1 0 Jan15 ? 00:11:36 /usr/bin/ceph-mon -f --cluster ceph --id ncn-m001 --setuser ceph --setgroup ceph \u0026lt;\u0026lt;-- If missing, it is not running   Restart the ceph-mon process on any node where it was not running.\nThis is expected to crash again, but this is a good way to verify there is an issue.\nncn-s00(1/2/3)# systemctl daemon-reload ncn-s00(1/2/3)# ceph orch daemon restart mon.\u0026lt;hostname\u0026gt;     Check the health of the Ceph cluster on one of the manager nodes.\nThis command will report a HEALTH_WARN status. There will be a message below this warning indicating that a ceph-mon node or multiple ceph-mon nodes are out of quorum.\nncn-s00(1/2/3)# ceph -s To watch nodes that drop out of quorum, run the following command:\nncn-s00(1/2/3)# ceph -ws Once it is clear that the ceph-mon processes keep crashing across all of the manager nodes, proceed to the next step. If only a single ceph-mon process on a manager node is having issues, then a different issue is occurring.\n  Restart the ceph-mds services on all manager nodes.\nncn-s00(1/2/3)# ceph orch daemon restart mds.cephfs.\u0026lt;container id\u0026gt;   Restart the ceph-mon process on all manager nodes.\nncn-m001# systemctl daemon-reload ncn-s00(1/2/3)# ceph orch daemon restart mon.\u0026lt;hostname\u0026gt;   Monitor the cluster to ensure the ceph-mon processes are running on all manager nodes.\nThe health status should return to reporting as HEALTH_OK. Monitor the health of the cluster over the next 30 minutes to ensure the debugging was successful.\nncn-s00(1/2/3)# ceph -s   "
},
{
	"uri": "/docs-csm/en-11/operations/utility_storage/troubleshoot_failure_to_get_ceph_health/",
	"title": "Troubleshoot Failure To Get Ceph Health",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Failure to Get Ceph Health Inspect Ceph commands that are failing by looking into the Ceph monitor logs (ceph-mon). For example, the monitoring logs can help determine any issues causing the ceph -s command to hang.\nTroubleshoot Ceph commands failing to run and determine how to make them operational again. These commands need to be operational to obtain critical information about the Ceph cluster on the system.\nPrerequisites This procedure requires admin privileges.\nProcedure   Verify the node being used is running ceph-mon.\n  Verify ceph-mon processes are running on the first three NCN storage nodes.\n See Manage_Ceph_Service for more information.  If more than three storage nodes exist, check the output of ceph orch ps for more information\n  Check ceph-mon logs to see if the cluster is out of quorum.\nVerify the issue is resolved by rerunning the Ceph command that failed.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/utility_storage/troubleshoot_ceph_mds_reporting_slow_requests_and_failure_on_client/",
	"title": "Troubleshooting Ceph Mds Slow Ops",
	"tags": [],
	"description": "",
	"content": "Troubleshooting Ceph MDS slow ops Before doing any steps on this page, please make sure you looked at Identify_Ceph_Latency_Issues\nIMPORTANT: This will be a mix of commands that need to be run on the host(s) running the MDS daemon(s) and other commands that can be run from any of the ceph-mon nodes.\nNOTICE: These steps are based off upstream documentation. This can be viewed here. https://docs.ceph.com/en/octopus/cephfs/troubleshooting/.\n Please ensure you are on the correct version of documentation for the cluster you are running.\n If you are here, then we are going to assume the following:\n You followed Identify_Ceph_Latency_Issues and that page guided you here. You have previously encountered this issue and are using this page as a reference for commands.  Procedure:\n  Identify the active MDS\nncn-s00(1/2/3):~ # ceph fs status -f json-pretty|jq -r \u0026#39;.mdsmap[]|select(.state==\u0026#34;active\u0026#34;)|.name\u0026#39; cephfs.ncn-s003.ihwkop   ssh to the host running the active MDS\n  Enter into a cephadm shell\nncn-s003:~ # cephadm shell Inferring fsid 7350865a-0b21-11ec-b9fa-fa163e06c459 Inferring config /var/lib/ceph/7350865a-0b21-11ec-b9fa-fa163e06c459/mon.ncn-s003/config Using recent ceph image arti.dev.cray.com/third-party-docker-stable-local/ceph/ ceph@sha256:70536e31b29a4241999ec4fd13d93e5860a5ffdc5467911e57e6bf04dfe68337 [ceph: root@ncn-s003 /]# NOTE: You may see messages like this \u0026ldquo;WARNING: The same type, major and minor should not be used for multiple devices.\u0026rdquo; They can be ignored. There is an upstream bug to address this.\n  Dump in flight ops\n[ceph: root@ncn-s003 /]# ceph daemon mds.cephfs.ncn-s003.ihwkop dump_ops_in_flight { \u0026#34;ops\u0026#34;: [], \u0026#34;num_ops\u0026#34;: 0 } NOTE: The example above is about how to run the command. Recreating the exact scenario to provide a full example is not easily done. This will be updated when the information is available.\n  General Steps from upstream:\n Identify the stuck commands and examine why they are stuck.  Usually the last \u0026ldquo;event\u0026rdquo; will have been an attempt to gather locks, or sending the operation off to the MDS log. If it is waiting on the OSDs, fix them. If operations are stuck on a specific inode, you probably have a client holding caps which prevent others from using it, either because the client is trying to flush out dirty data or because you have encountered a bug in CephFS' distributed file lock code (the file \u0026ldquo;capabilities\u0026rdquo; [\u0026ldquo;caps\u0026rdquo;] system).  If it is a result of a bug in the capabilities code, restarting the MDS is likely to resolve the problem.   If there are no slow requests reported on the MDS, and it is not reporting that clients are misbehaving, either the client has a problem or its requests are not reaching the MDS.    "
},
{
	"uri": "/docs-csm/en-11/operations/utility_storage/dump_ceph_crash_data/",
	"title": "Dump Ceph Crash Data",
	"tags": [],
	"description": "",
	"content": "Dump Ceph Crash Data Ceph includes an option to dump crash data. Retrieve this data to get more information on a Ceph cluster that has crashed.\nPrerequisites Ceph is reporting the cluster [WRN] overall HEALTH_WARN 1 daemons have recently crashed error in the output of the ceph -s or ceph health detail commands.\nProcedure   Get the Ceph crash listing and the corresponding IDs.\nncn-m001# ceph crash ls ID ENTITY NEW 2021-02-02_13:45:18.543633Z_a31173f7-44c8-45b1-a253-80efa25b45f1 mon.ncn-s003 *   Get information about the crash to include in a support ticket.\nReplace the CRASH_ID value with the ID returned in the previous step.\nncn-m001# ceph crash info CRASH_ID { \u0026#34;crash_id\u0026#34;: \u0026#34;2021-02-02_13:45:18.543633Z_a31173f7-44c8-45b1-a253-80efa25b45f1\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2021-02-02 13:45:18.543633Z\u0026#34;, \u0026#34;process_name\u0026#34;: \u0026#34;ceph-mon\u0026#34;, \u0026#34;entity_name\u0026#34;: \u0026#34;mon.ncn-s003\u0026#34;, \u0026#34;ceph_version\u0026#34;: \u0026#34;14.2.11-394-g9cbbc473c0\u0026#34;, \u0026#34;utsname_hostname\u0026#34;: \u0026#34;ncn-s003\u0026#34;, \u0026#34;utsname_sysname\u0026#34;: \u0026#34;Linux\u0026#34;, \u0026#34;utsname_release\u0026#34;: \u0026#34;5.3.18-24.46-default\u0026#34;, \u0026#34;utsname_version\u0026#34;: \u0026#34;#1 SMP Tue Jan 5 16:11:50 UTC 2021 (4ff469b)\u0026#34;, \u0026#34;utsname_machine\u0026#34;: \u0026#34;x86_64\u0026#34;, \u0026#34;os_name\u0026#34;: \u0026#34;SLE_HPC\u0026#34;, \u0026#34;os_id\u0026#34;: \u0026#34;15.2\u0026#34;, \u0026#34;os_version_id\u0026#34;: \u0026#34;15.2\u0026#34;, \u0026#34;os_version\u0026#34;: \u0026#34;15-SP2\u0026#34;, \u0026#34;assert_condition\u0026#34;: \u0026#34;session_map.sessions.empty()\u0026#34;, \u0026#34;assert_func\u0026#34;: \u0026#34;virtual Monitor::~Monitor()\u0026#34;, \u0026#34;assert_file\u0026#34;: \u0026#34;/home/abuild/rpmbuild/BUILD/ceph-14.2.11-394-g9cbbc473c0/src/mon/Monitor.cc\u0026#34;, \u0026#34;assert_line\u0026#34;: 267, \u0026#34;assert_thread_name\u0026#34;: \u0026#34;ceph-mon\u0026#34;, \u0026#34;assert_msg\u0026#34;: \u0026#34;/home/abuild/rpmbuild/BUILD/ceph-14.2.11-394-g9cbbc473c0/src/mon/Monitor.cc: In function \u0026#39;virtual Monitor::~Monitor()\u0026#39; thread 7f6f68869480 time 2021-02-02 13:45:18.538782\\n/home/abuild/rpmbuild/BUILD/ceph-14.2.11-394-g9cbbc473c0/src/mon/Monitor.cc: 267: FAILED ceph_assert(session_map.sessions.empty())\\n\u0026#34;, \u0026#34;backtrace\u0026#34;: [ \u0026#34;(()+0x132d0) [0x7f6f5e9322d0]\u0026#34;, \u0026#34;(gsignal()+0x110) [0x7f6f5da6e520]\u0026#34;, \u0026#34;(abort()+0x151) [0x7f6f5da6fb01]\u0026#34;, \u0026#34;(ceph::__ceph_assert_fail(char const*, char const*, int, char const*)+0x1a3) [0x7f6f5fac99f7]\u0026#34;, \u0026#34;(ceph::__ceph_assertf_fail(char const*, char const*, int, char const*, char const*, ...)+0) [0x7f6f5fac9b81]\u0026#34;, \u0026#34;(Monitor::~Monitor()+0x962) [0x5602667940f2]\u0026#34;, \u0026#34;(Monitor::~Monitor()+0x9) [0x560266794169]\u0026#34;, \u0026#34;(main()+0x289b) [0x5602667231cb]\u0026#34;, \u0026#34;(__libc_start_main()+0xea) [0x7f6f5da5934a]\u0026#34;, \u0026#34;(_start()+0x2a) [0x5602667526da]\u0026#34; ] }   Archive the crash data for further triage.\nThis command can be used to archive all crash data, or just the data for a specific Ceph entity.\nncn-m001# ceph crash archive ALL/CRASH_ID   "
},
{
	"uri": "/docs-csm/en-11/operations/utility_storage/identify_ceph_latency_issues/",
	"title": "Identify Ceph Latency Issues",
	"tags": [],
	"description": "",
	"content": "Identify Ceph Latency Issues Examine the output of the ceph -s command to get context for potential issues causing latency.\nTroubleshoot the underlying causes for the ceph -s command reporting slow PGs.\nPrerequisites This procedure requires admin privileges.\nProcedure   View the status of Ceph.\nncn-m001# ceph -s cluster: id: 73084634-9534-434f-a28b-1d6f39cf1d3d health: HEALTH_WARN 1 filesystem is degraded 1 MDSs report slow metadata IOs Reduced data availability: 15 pgs inactive, 15 pgs peering 46 slow ops, oldest one blocked for 1395 sec, daemons [osd,2,osd,5,mon,ceph-1,mon,ceph-2,mon,ceph-3] have slow ops. services: mon: 3 daemons, quorum ceph-3,ceph-1,ceph-2 (age 38m) mgr: ceph-2(active, since 30m), standbys: ceph-3, ceph-1 mds: cephfs:1/1 {0=ceph-2=up:replay} 2 up:standby osd: 15 osds: 15 up (since 93s), 15 in (since 23m); 10 remapped pgs rgw: 1 daemon active (ceph-1.rgw0) data: pools: 9 pools, 192 pgs objects: 15.93k objects, 55 GiB usage: 175 GiB used, 27 TiB / 27 TiB avail pgs: 7.812% pgs not active 425/47793 objects misplaced (0.889%) 174 active+clean 8 peering 7 remapped+peering 2 active+remapped+backfill_wait 1 active+remapped+backfilling io: client: 7.0 KiB/s wr, 0 op/s rd, 1 op/s wr recovery: 12 MiB/s, 3 objects/s The output can provide a lot of context to potential issues causing latency. In the example output above, the following troubleshooting information can be observed:\n health - Shows latency and what daemons/OSDs are associated with it. mds - MDS is functional, but it is in replay because of the slow ops. osd - All OSDs are up and in. This could be related to a network issue or a single system issue if both OSDs are on the same box. client - Shows the amount of IO/Throughput that clients using Ceph are performing. If health is not set to HEALTH_OK and traffic is passing through, then Ceph is functioning and re-balancing data because of typical hardware/network issues. recovery - Shows recovery traffic as the system ensures all the copies of data are available to ensure data redundancy.    Fixes Based on the output from ceph -s (using our example above) we can correlate some information to help determine our bottleneck.\n  When reporting slow ops for OSDs, then it is good to find out if those OSDs are on the same node or different nodes.\n If on the same node, then you will want to look at networking or other hardware related issues on that node. If the osds are on different nodes, then networking issues should be investigated  As an initial step, restart the OSDs, if the slow ops go away and do not return, then we can investigate the logs for possible software bugs or memory issues. If the slow ops come right back, then there is an issue with replication between the 2 OSDs which tends to be network related.      When reporting slow ops for MONs, then it is typically an issue with the process.\n The most common cause here is either an abrupt clock skew or a hung mon/mgr process.  The recommended remediation is to do a rolling restart of the ceph MON and MGR daemons.      When reporting slow ops for MDS, then it could be due to a couple of different reasons.\n If listed in addition to OSDs, then the root cause for this will typically be the OSDs and the process above should be used followed by restarting the MDS daemons If it is only listing MDS, then restart the MDS daemons. If the problem persists, then the logs will have to be investigated for the root cause. See Troubleshoot_Ceph_MDS_reporting_slow_requests_and_failure_on_client for additional steps to help identify MDS slow ops    "
},
{
	"uri": "/docs-csm/en-11/operations/utility_storage/manage_ceph_services/",
	"title": "Manage Ceph Services",
	"tags": [],
	"description": "",
	"content": "Manage Ceph Services The following commands are required to start, stop, or restart Ceph services. Restarting Ceph services is helpful for troubleshoot issues with the utility storage platform.\nList Ceph services ncn-s00(1/2/3)# ceph orch ps NAME HOST STATUS REFRESHED AGE VERSION IMAGE NAME IMAGE ID CONTAINER ID mds.cephfs.ncn-s001.zwptsg ncn-s001 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c bb08bcb2f034 mds.cephfs.ncn-s002.qyvoyv ncn-s002 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 32c3ff10be42 mds.cephfs.ncn-s003.vvsuvy ncn-s003 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c e172b979c747 mgr.ncn-s001 ncn-s001 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c ad887936d37f mgr.ncn-s002 ncn-s002 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 80902ae9010c mgr.ncn-s003 ncn-s003 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 28700bb4053e mon.ncn-s001 ncn-s001 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c af8d64f9df8a mon.ncn-s002 ncn-s002 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 698557732bf4 mon.ncn-s003 ncn-s003 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 27421ddd81bd osd.0 ncn-s003 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 97f4f922edc7 osd.1 ncn-s002 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 69d11a7ddecb osd.10 ncn-s002 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c aa054d15d4ab osd.11 ncn-s001 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c b3814f3348ed osd.2 ncn-s001 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 998c334e41c6 osd.3 ncn-s002 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c da2daa780fd0 osd.4 ncn-s003 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c b831003fdf32 osd.5 ncn-s001 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 6850b49c9bc1 osd.6 ncn-s003 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 6aeb8b274212 osd.7 ncn-s002 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 4eb2f577daba osd.8 ncn-s001 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c f0386c093874 osd.9 ncn-s003 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 50d7066f66a6 rgw.site1.zone1.ncn-s001.xtjggh ncn-s001 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 6a90ba6415e6 rgw.site1.zone1.ncn-s002.divvfs ncn-s002 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 825c5b5f33c5 rgw.site1.zone1.ncn-s003.spojqa ncn-s003 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c f95116a16e41 Ceph Monitor Service (ceph-mon) IMPORTANT: All of the below ceph orch commands should be run from ncn-s001/2/3 or ncn-m001/2/3.\nStart the ceph-mon service:\n# Please note the the mon process can have a container id appended to the end of the host name. So please make sure to use the output from above to ensure you have the correct name. ncn-s00(1/2/3)# ceph orch daemon start mon.\u0026lt;hostname\u0026gt; Stop the ceph-mon service:\nncn-s00(1/2/3)# ceph orch daemon stop mon.\u0026lt;hostname\u0026gt; Restart the ceph-mon service:\nncn-s00(1/2/3)# ceph orch daemon restart mon.\u0026lt;hostname\u0026gt; Ceph OSD Service (ceph-osd) Start the ceph-osd service:\nncn-s00(1/2/3)# ceph orch daemon start osd.\u0026lt;number\u0026gt; Stop the ceph-osd service:\nnncn-s00(1/2/3)# ceph orch daemon stop osd.\u0026lt;number\u0026gt; Restart the ceph-osd service:\nncn-s00(1/2/3)# ceph orch daemon restart osd.\u0026lt;number\u0026gt; Ceph Manager Service (ceph-mgr) Start the ceph-mgr service:\nncn-s00(1/2/3)# ceph orch daemon start mgr.\u0026lt;hostname\u0026gt; Stop the ceph-mgr service:\nncn-s00(1/2/3)# ceph orch daemon stop mgr.\u0026lt;hostname\u0026gt; Restart the ceph-mgr service:\nncn-s00(1/2/3)# ceph orch daemon restart mgr.\u0026lt;hostname\u0026gt; Ceph MDS Service (cephfs) Start the ceph-mgr service:\nncn-s00(1/2/3)# ceph orch daemon start mds.cephfs.\u0026lt;container id from ceph orch ps output\u0026gt; Stop the ceph-mgr service:\nncn-s00(1/2/3)# ceph orch daemon stop mds.cephfs.\u0026lt;container id from ceph orch ps output\u0026gt; Restart the ceph-mgr service:\nncn-s00(1/2/3)# ceph orch daemon restart mds.cephfs.\u0026lt;container id from ceph orch ps output\u0026gt; Ceph Rados-Gateway Service (ceph-radosgw) Start the rados-gateway:\nncn-s00(1/2/3)# ceph orch daemon start rgw.site1.zone1.\u0026lt;container id from ceph orch ls\u0026gt; Stop the rados-gateway:\nncn-s00(1/2/3)# ceph orch daemon stop rgw.site1.zone1.\u0026lt;container id from ceph orch ls\u0026gt; Restart the rados-gateway:\nncn-s00(1/2/3)# ceph orch daemon restart rgw.site1.zone1.\u0026lt;container id from ceph orch ls\u0026gt; Ceph Service restart using CEPHADM Important: This command needs to run from the host you are going to start or stop services on.\n Get the service system_unit  ncn-s# cephadm ls { \u0026#34;style\u0026#34;: \u0026#34;cephadm:v1\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;mgr.ncn-s001\u0026#34;, \u0026#34;fsid\u0026#34;: \u0026#34;01a0d9d2-ea7f-43dc-af25-acdfa5242a48\u0026#34;, \u0026#34;systemd_unit\u0026#34;: \u0026#34;ceph-01a0d9d2-ea7f-43dc-af25-acdfa5242a48@mgr.ncn-s001\u0026#34;, \u0026#34;enabled\u0026#34;: false, \u0026#34;state\u0026#34;: \u0026#34;running\u0026#34;, \u0026#34;container_id\u0026#34;: \u0026#34;ad887936d37fd87999b140c366ef288443faf03e869219bdd282b5825be14d6e\u0026#34;, \u0026#34;container_image_name\u0026#34;: \u0026#34;registry.local/ceph/ceph:v15.2.8\u0026#34;, \u0026#34;container_image_id\u0026#34;: \u0026#34;5553b0cb212ca2aa220d33ba39d9c602c8412ce6c5febc57ef9cdc9c5844b185\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;15.2.8\u0026#34;, \u0026#34;started\u0026#34;: \u0026#34;2021-06-17T22:41:45.132838Z\u0026#34;, \u0026#34;created\u0026#34;: \u0026#34;2021-06-17T22:17:51.063202Z\u0026#34;, \u0026#34;deployed\u0026#34;: \u0026#34;2021-06-17T22:16:03.898845Z\u0026#34;, \u0026#34;configured\u0026#34;: \u0026#34;2021-06-17T22:41:07.807004Z\u0026#34; },  restart the service  ncn-s# systemctl restart ceph-01a0d9d2-ea7f-43dc-af25-acdfa5242a48@mgr.ncn-s001 Ceph Manager Modules Location: Ceph manager modules can be enabled or disabled from any ceph-mon nodes.\nEnable Ceph manager modules:\nncn-m001# ceph mgr MODULE_NAME enable MODULE Disable Ceph manager modules:\nncn-m001# ceph mgr MODULE_NAME disable MODULE Scale Ceph services We can use the ability to deploy/scale/reconfigure/redeploy Ceph processes down and back up to restart the services.\nIMPORTANT: When scaling the Ceph manager daemon (mgr.hostname.), you need to keep in mind that you need to have a running manager daemon as it is what is controlling the orchestration processes.\nIMPORTANT: You cannot scale osd.all-available-devices, this is the process to auto-discover available OSDs.\nIMPORTANT: You cannot scale the crash service; this is the equivalent of a Kubernetes daemon set and runs on all nodes to collect crash data.\nFor our example we are going to show scaling the mgr service down and back up.\nYou will need two SSH sessions. One to do the work from and another that is running watch ceph -s so you can monitor the progress.\n  List your services\nncn-s001:~ # ceph orch ls NAME RUNNING REFRESHED AGE PLACEMENT IMAGE NAME IMAGE ID crash 6/6 9s ago 4d * registry.local/ceph/ceph:v15.2.8 5553b0cb212c mds.cephfs 3/3 9s ago 4d ncn-s001;ncn-s002;ncn-s003;count:3 registry.local/ceph/ceph:v15.2.8 5553b0cb212c mgr 3/3 9s ago 4d ncn-s001;ncn-s002;ncn-s003;count:3 registry.local/ceph/ceph:v15.2.8 5553b0cb212c mon 3/3 9s ago 4d ncn-s001;ncn-s002;ncn-s003;count:3 registry.local/ceph/ceph:v15.2.8 5553b0cb212c osd.all-available-devices 6/6 9s ago 4d * registry.local/ceph/ceph:v15.2.8 5553b0cb212c rgw.site1.zone1 3/3 9s ago 4d ncn-s001;ncn-s002;ncn-s003;ncn-s004;ncn-s005;ncn-s006;count:3 registry.local/ceph/ceph:v15.2.8 5553b0cb212c   Optionally you can limit the results. ceph orch [\u0026lt;service_type\u0026gt;] [\u0026lt;service_name\u0026gt;] [\u0026ndash;export] [plain|json|json-pretty|yaml] [\u0026ndash;refresh]\nncn-s001:~ # ceph orch ls mgr NAME RUNNING REFRESHED AGE PLACEMENT IMAGE NAME IMAGE ID mgr 3/3 17s ago 4d ncn-s001;ncn-s002;ncn-s003;count:3 registry.local/ceph/ceph:v15.2.8 5553b0cb212c  From this we can get our placement of the services.      Choose the service you want to scale. (reminder the example will use the MGR service)\n If you are scaling mds or mgr daemons then you will want to make sure you are failing over the active mgr/mds daemon so you always have one running.  # To get the active MDS ncn-s# ceph fs status -f json-pretty|jq -r \u0026#39;.mdsmap[]|select(.state==\u0026#34;active\u0026#34;)|.name\u0026#39; cephfs.ncn-s001.juehkw \u0026lt;-- current active MDS. note this will change when you fail it over so keep this command handy # To get the active MGR ncn-s# ceph mgr dump | jq -r .active_name ncn-s002.fumzfm \u0026lt;-- current active MGR. note this will change when you fail it over so keep this command handy   Now we have our service, the current placement policy, and if applicable the active MGR/MDS daemon\n  Scale the service\nceph orch apply --placement=\u0026#34;1 \u0026lt;host where the active mgr is running\u0026gt;\u0026#34; example: ncn-s001:~ # ceph orch apply mgr --placement=\u0026#34;1 ncn-s002\u0026#34; Scheduled mgr update...   Watch your SSH session that is showing the Ceph status (ceph -s)\nncn-s001:~ # ceph -s cluster: id: 11d5d552-cfac-11eb-ab69-fa163ec012bf health: HEALTH_OK services: mon: 3 daemons, quorum ncn-s001,ncn-s002,ncn-s003 (age 70s) mgr: ncn-s002.fumzfm(active, since 14s) mds: cephfs:1 {0=cephfs.ncn-s001.juehkw=up:active} 1 up:standby-replay 1 up:standby osd: 6 osds: 6 up (since 4d), 6 in (since 4d) rgw: 2 daemons active (site1.zone1.ncn-s005.hzfbkd, site1.zone1.ncn-s006.vjuwkf) task status: data: pools: 7 pools, 193 pgs objects: 256 objects, 7.5 MiB usage: 9.1 GiB used, 261 GiB / 270 GiB avail pgs: 193 active+clean io: client: 663 B/s rd, 1 op/s rd, 0 op/s wr Note that our mgr service is now showing 1 active on the node we chose.\n  Scale the service back up to 3 mgrs.\nncn-s001:~ # ceph orch apply mgr --placement=\u0026#34;3 ncn-s001 ncn-s002 ncn-s003\u0026#34; Scheduled mgr update...   When you see your Ceph status output shows you have 3 running mgr daemons then you can scale down the last daemon back down and up.\n IF it is the MDS or MGR daemons then REMEMBER we have to fail over the active daemon  ncn-s001:~ # ceph mgr fail ncn-s002.fumzfm # This was our active MGR. in your Ceph status output you should see the ACTIVE ceph mgr process change.\nmgr: ncn-s003.wtvbtz(active, since 2m), standbys: ncn-s001.cgbxdw   Finally scale your service back to its original deployment size\nncn-s001:~ # ceph orch apply mgr --placement=\u0026#34;3 ncn-s001 ncn-s002 ncn-s003\u0026#34; Scheduled mgr update...   Monitor the Ceph status to make sure all your daemons come back online.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/utility_storage/restore_corrupt_nexus/",
	"title": "Restore Nexus Data After Data Corruption",
	"tags": [],
	"description": "",
	"content": "Restore Nexus Data After Data Corruption In rare cases, if a Ceph upgrade is not completed successfully and has issues, the eventual Ceph health can end up with a damaged mds (cephfs) daemon. Ceph reports this as follows running the ceph -s command:\nncn-s002:~ # ceph -s cluster: id: 7ed70f4c-852e-494a-b9e7-5f722af6d6e7 health: HEALTH_ERR 1 filesystem is degraded 1 filesystem is offline 1 mds daemon damaged When Ceph is in this state, Nexus will likely not operate properly and can be recovered using the following procedure.\nProcedure The commands in this procedure should be run from a master NCN (unless otherwise noted).\n  Determine where the originally installed Nexus helm chart and manifest exists.\nDuring normal installs, the initial artifacts (docker images and helm charts) are available on ncn-m001 in /mnt/pitdata after it has been rebooted out of PIT mode. The administrator may need to re-mount this volume (first step below):\n  Re-mount the volume\nncn-m001# mount -vL PITDATA /mnt/pitdata   Find the Nexus helm chart to use.\nncn-m001# ls /mnt/pitdata/csm-0.9.4/helm/*nexus* /mnt/pitdata/csm-0.9.4/helm/cray-nexus-0.6.0.tgz \u0026lt;-- this is the helm chart to use   Find the Nexus manifest to use.\nncn-m001# ls /mnt/pitdata/csm-0.9.4/manifests/nexus.yaml /mnt/pitdata/csm-0.9.4/manifests/nexus.yaml \u0026lt;-- this is the manifest to use  NOTE: Do not proceed with further steps until these two files are located, as they are necessary to re-install the helm chart after deletion.\n     Uninstall the Nexus helm chart.\nncn-m001# helm uninstall -n nexus cray-nexus   Delete the backing Nexus PVC.\nncn-m001# kubectl -n nexus delete pvc nexus-data   Re-install the cray-nexus helm chart (using the chart and manifest determined in step 1).\nncn-m001# loftsman ship --manifest-path /mnt/pitdata/csm-0.9.4/manifests/nexus.yaml --charts-path /mnt/pitdata/csm-0.9.4/helm/cray-nexus-0.6.0.tgz   Re-populate Nexus with 0.9.x artifacts.\nDepending on where initial PIT data was determined in step 1, now locate the Nexus setup script:\nncn-m001# ls /mnt/pitdata/csm-0.9.4/lib/setup-nexus.sh /mnt/pitdata/csm-0.9.4/lib/setup-nexus.sh Re-populate the 0.9.x artifacts by running the setup-nexus.sh script:\nncn-m001# /mnt/pitdata/csm-0.9.4/lib/setup-nexus.sh   Re-populate Nexus with 1.0 artifacts. This step is also necessary if the mds/cephfs corruption occurred when upgrading from 0.9.x to 1.0. Nexus must be populated with both versions of the artifacts in order to support both old/new docker images during upgrade.\nLocate the Nexus setup script, this is typically in /root/csm-1.0.* on ncn-m001:\nncn-m001# ls /root/csm-1.0.0-beta.50/lib/setup-nexus.sh /root/csm-1.0.0-beta.50/lib/setup-nexus.sh Re-populate the 1.0.x artifacts by running the setup-nexus.sh script:\nncn-m001# /root/csm-1.0.0-beta.50/lib/setup-nexus.sh   Re-populate Nexus with any add-on products that had been installed on this system.\nIf any additional products were installed on this system after initial install, locate the install documentation for those products and re-run the appropriate portion of their install script(s) to load those artifacts back into Nexus. For example, if SLES repositories have been added to Nexus prior to this content restore, refer to the install documentation contained in that distribution\u0026rsquo;s tar file for instructions on how to again load that content into Nexus.\n  Scale up any deployments/statefulsets that may have been scaled down during upgrade (if applicable). These commands should be run from ncn-s001.\nSource the k8s-scale-utils.sh script in order to define the scale_up_cephfs_clients function:\nncn-m001# source /usr/share/doc/csm/upgrade/1.0/scripts/ceph/lib/k8s-scale-utils.sh Execute the scale_up_cephfs_clients function in order to scale up any cephfs clients that may still be scaled down:\nncn-m001# scale_up_cephfs_clients   "
},
{
	"uri": "/docs-csm/en-11/operations/utility_storage/shrink_ceph_osds/",
	"title": "Shrink Ceph Osds",
	"tags": [],
	"description": "",
	"content": "Shrink Ceph OSDs This procedure describes how to remove an OSD(s) from a Ceph cluster. Once the OSD is removed, the cluster is also rebalanced to account for the changes. Use this procedure to reduce the size of a cluster or to replace hardware.\nPrerequisites This procedure requires administrative privileges.\nProcedure   Log in as root on the first master node (ncn-m001).\n  Monitor the progress of the OSDs that have been added.\nncn-m001# ceph -s   View the status of each OSD and see where they reside.\nncn-m001# ceph osd tree   Reweigh the OSD(s) being removed to rebalance the cluster.\nThe first two substeps below can be skipped if there is a down drive and OSD.\n  Change the weight of the OSD being removed to 0.\nThe OSD_ID value should be replaced with the ID of the OSD being removed. For example, if the ID is osd.1, the OSD_ID value would be 1 in the command below.\nncn-m001# ceph osd reweight osd.OSD_ID 0   Change the weight in the CRUSH map to 0.\nncn-m001# ceph osd crush reweight osd.OSD_ID 0   Prevent the removed OSD from getting marked up.\nncn-m001# ceph osd set noup     Remove the OSD after the reweighing work is complete.\n  Take down the OSD being removed.\nncn-m001# ceph osd down osd.OSD_ID   Destroy the OSD.\nncn-m001# ceph osd destroy osd.OSD_ID   Remove the OSD authentication key.\nncn-m001# ceph auth rm osd.OSD_ID   Remove the OSD.\nncn-m001# ceph osd rm osd.OSD_ID   Remove the OSD from the CRUSH map.\nncn-m001# ceph osd crush rm osd.OSD_ID   Remove references to the OSDs on the storage node(s) they were located on.\nThe following commands must be run on the storage node(s) that held the OSDs being removed.\nncn-s001# umount /var/lib/ceph/osd/ceph-OSD_ID ncn-s001# rm -rf /var/lib/ceph/osd/ceph-OSD_ID     Clear the flags that were set earlier in the procedure.\nncn-m001# ceph osd unset noup   Monitor the cluster until the rebalancing is complete.\nncn-m001# ceph -s   "
},
{
	"uri": "/docs-csm/en-11/operations/utility_storage/ceph_orchestrator_usage/",
	"title": "Ceph Orchestrator General Usage And Tips",
	"tags": [],
	"description": "",
	"content": "Ceph Orchestrator General Usage and Tips Description \u0026ldquo;Is a module that provides a command line interface (CLI) to orchestrator modules (ceph-mgr modules which interface with external orchestration services).\u0026rdquo; - source (https://docs.ceph.com/en/latest/mgr/orchestrator/)\nThis provides a nice centralized interface for the management of the ceph cluster. This includes:\n Single command upgrades, assuming all images are in place. Reduces the need to be on the physical server to address a large number of ceph service restarts or configuration changes Better integration with the Ceph Dashboard (Coming soon) Ability to write custom orchestration modules  Troubleshooting Ceph Orchestrator Watching cephadm Log Messages This is useful when making changes via the orchestrator like add/remove/scale services or upgrades.\nncn-s# ceph -w cephadm With Debug\nncn-s# ceph config set mgr mgr/cephadm/log_to_cluster_level debug ncn-s# ceph -W cephadm --watch-debug Note: For use with orchestration tasks, this can be typically run from a node running the ceph mon process (typically ncn-s00(1/2/3)). There may be cases where you are running a cephadm locally on a host and it will be more efficient to tail /var/log/ceph/cephadm.log.\nUsage Examples This section will provide some in-depth usage with examples of the more commonly used ceph orch subcommands.\nList Service Deployments\nncn-s# ceph orch ls NAME RUNNING REFRESHED AGE PLACEMENT IMAGE NAME IMAGE ID alertmanager 1/1 6m ago 4h count:1 registry.local/prometheus/alertmanager:v0.20.0 0881eb8f169f crash 3/3 6m ago 4h * registry.local/ceph/ceph:v15.2.8 5553b0cb212c grafana 1/1 6m ago 4h count:1 registry.local/ceph/ceph-grafana:6.6.2 a0dce381714a mds.cephfs 3/3 6m ago 4h ncn-s001;ncn-s002;ncn-s003;count:3 registry.local/ceph/ceph:v15.2.8 5553b0cb212c mgr 3/3 6m ago 4h ncn-s001;ncn-s002;ncn-s003;count:3 registry.local/ceph/ceph:v15.2.8 5553b0cb212c mon 3/3 6m ago 4h ncn-s001;ncn-s002;ncn-s003;count:3 registry.local/ceph/ceph:v15.2.8 5553b0cb212c node-exporter 3/3 6m ago 4h * registry.local/prometheus/node-exporter:v0.18.1 e5a616e4b9cf osd.all-available-devices 9/9 6m ago 4h * registry.local/ceph/ceph:v15.2.8 5553b0cb212c prometheus 1/1 6m ago 4h count:1 docker.io/prom/prometheus:v2.18.1 de242295e225 rgw.site1.zone1 3/3 6m ago 4h ncn-s001;ncn-s002;ncn-s003;count:3 registry.local/ceph/ceph:v15.2.8 5553b0cb212c FILTERS: You can apply filters by adding --service_type \u0026lt;service type\u0026gt; or --service_name \u0026lt;service name\u0026gt;.\nReference Key\n PLACEMENT - Represents a service deployed on all nodes. Otherwise the listed placement is where it is expected to be deployed. NAME - The deployment name. this is a generalized name to reference the deployment. This is being noted as additional subcommands the name is more specific to the actual deployed daemon.  List Deployed Daemons\nncn-s# ceph orch ps NAME HOST STATUS REFRESHED AGE VERSION IMAGE NAME IMAGE ID CONTAINER ID alertmanager.ncn-s001 ncn-s001 running (5h) 5m ago 5h 0.20.0 registry.local/prometheus/alertmanager:v0.20.0 0881eb8f169f 0e6a24469465 crash.ncn-s001 ncn-s001 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c b6a582ed7573 crash.ncn-s002 ncn-s002 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 3778e29099eb crash.ncn-s003 ncn-s003 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c fe085e310cbd grafana.ncn-s001 ncn-s001 running (5h) 5m ago 5h 6.6.2 registry.local/ceph/ceph-grafana:6.6.2 a0dce381714a 2fabb486928c mds.cephfs.ncn-s001.qrxkih ncn-s001 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 03a3a1ce682e mds.cephfs.ncn-s002.qhferv ncn-s002 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 56dca5cca407 mds.cephfs.ncn-s003.ihwkop ncn-s003 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 38ab6a6c8bc6 mgr.ncn-s001.vkfdue ncn-s001 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 456587705eab mgr.ncn-s002.wjaxkl ncn-s002 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 48222c38dd7e mgr.ncn-s003.inwpij ncn-s003 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 76ff8e485504 mon.ncn-s001 ncn-s001 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c bcca26f69191 mon.ncn-s002 ncn-s002 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 43c8472465b2 mon.ncn-s003 ncn-s003 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 7aa1b1f19a00 node-exporter.ncn-s001 ncn-s001 running (5h) 5m ago 5h 0.18.1 registry.local/prometheus/node-exporter:v0.18.1 e5a616e4b9cf 0be431766c8e node-exporter.ncn-s002 ncn-s002 running (5h) 5m ago 5h 0.18.1 registry.local/prometheus/node-exporter:v0.18.1 e5a616e4b9cf 6ae81d01d963 node-exporter.ncn-s003 ncn-s003 running (5h) 5m ago 5h 0.18.1 registry.local/prometheus/node-exporter:v0.18.1 e5a616e4b9cf 330dc09d0845 osd.0 ncn-s002 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c a8c7314b484b osd.1 ncn-s001 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 8f9941887053 osd.2 ncn-s003 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 49cf2c532efb osd.3 ncn-s001 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 69e89cf18216 osd.4 ncn-s002 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 72d7f51a3690 osd.5 ncn-s003 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 76d598c40824 osd.6 ncn-s001 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c d2372e45c8eb osd.7 ncn-s002 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 5bd22f1d4cad osd.8 ncn-s003 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 7c5282f2e107 prometheus.ncn-s001 ncn-s001 running (5h) 5m ago 5h 2.18.1 docker.io/prom/prometheus:v2.18.1 de242295e225 bf941a1306e9 rgw.site1.zone1.ncn-s001.qegfux ncn-s001 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c e833fc05acfe rgw.site1.zone1.ncn-s002.wqrzoa ncn-s002 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 83a131a7022c rgw.site1.zone1.ncn-s003.tzkxya ncn-s003 running (5h) 5m ago 5h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c c67d75adc620 FILTERS: You can apply filters by adding any or all of [\u0026ndash;hostname  \u0026ndash;service_name \u0026lt;service_name\u0026gt; \u0026ndash;daemon_type \u0026lt;daemon_type\u0026gt; \u0026ndash;daemon_id \u0026lt;daemon_id\u0026gt;]\nCeph daemon start|stop|restart|reconfig\nNOTE: The service name is from ceph orch ps NOT ceph orch ls.\nncn-s# ceph orch daemon restart alertmanager.ncn-s001 Scheduled to restart alertmanager.ncn-s001 on host \u0026#39;ncn-s001\u0026#39; Additional Task: You should monitor the restart using the ceph orch ps command and the time associated with the STATUS should be reset and show \u0026ldquo;running (time since started).\u0026rdquo;\nDeploy or Scale services\nNOTE: The service name is from ceph orch ls NOT ceph orch ps.\nncn-s# ceph orch apply alertmanager --placement=\u0026#34;2 ncn-s001 ncn-s002\u0026#34; Scheduled alertmanager update... Reference Key\n PLACEMENT - This will show the nodes and the count. if you only specify --placement=\u0026quot;2\u0026quot; then it will automatically pick where to put it.  IMPORTANT: When working with the placement, you can have several combinations. For example you can specify a placement of 1 but list a sub set of your nodes. This is a good way to contain the process to those nodes.\nIMPORTANT: This is not available for any deployments with a PLACEMENT of *\nList Hosts Known to Ceph Orchestrator\nncn-s# ceph orch host ls HOST ADDR LABELS STATUS ncn-s001 ncn-s001 ncn-s002 ncn-s002 ncn-s003 ncn-s003 List Drives on Hosts Known to Ceph Orchestrator\nncn-s# ceph orch device ls Hostname Path Type Serial Size Health Ident Fault Available ncn-s001 /dev/vdb hdd fb794832-f402-4f4f-a 107G Unknown N/A N/A No ncn-s001 /dev/vdc hdd 9bdef369-6bac-40ca-a 107G Unknown N/A N/A No ncn-s001 /dev/vdd hdd 3cda8ba2-ccaf-4515-b 107G Unknown N/A N/A No ncn-s002 /dev/vdb hdd 775639a6-092e-4f3a-9 107G Unknown N/A N/A No ncn-s002 /dev/vdc hdd 261e8a40-2349-484e-8 107G Unknown N/A N/A No ncn-s002 /dev/vdd hdd 8f01f9c6-2c6c-449c-a 107G Unknown N/A N/A No ncn-s003 /dev/vdb hdd 46467f02-1d11-44b2-b 107G Unknown N/A N/A No ncn-s003 /dev/vdc hdd 4797e919-667e-4376-b 107G Unknown N/A N/A No ncn-s003 /dev/vdd hdd 3b2c090d-37a0-403b-a 107G Unknown N/A N/A No IMPORTANT: If you use the --wide, it will give the reasons a drive is not Available.. This DOES NOT mean something is wrong. If Ceph already has the drive provisioned, then you may see similar reasons\nGeneral Use Update the size or placement for a service or apply a large yaml spec\nncn-s# ceph orch apply [mon|mgr|rbd-mirror|crash|alertmanager|grafana|node-exporter|prometheus] [\u0026lt;placement\u0026gt;] [--dry-run] [plain|json|json-pretty|yaml] [--unmanaged] Scale an iSCSI service\nncn-s# ceph orch apply iscsi \u0026lt;pool\u0026gt; \u0026lt;api_user\u0026gt; \u0026lt;api_password\u0026gt; [\u0026lt;trusted_ip_list\u0026gt;][\u0026lt;placement\u0026gt;] [--dry-run] [plain|json|json-pretty|yaml] [--unmanaged] Update the number of MDS instances for the given fs_name\nncn-s# ceph orch apply mds \u0026lt;fs_name\u0026gt; [\u0026lt;placement\u0026gt;] [--dry-run] [--unmanaged] [plain|json|json-pretty|yaml] Scale an NFS service\nncn-s# ceph orch apply nfs \u0026lt;svc_id\u0026gt; \u0026lt;pool\u0026gt; [\u0026lt;namespace\u0026gt;] [\u0026lt;placement\u0026gt;] [--dry-run] [plain|json|json-pretty|yaml] [--unmanaged] Create OSD daemon(s) using a drive group spec\nncn-s# ceph orch apply osd [--all-available-devices] [--dry-run] [--unmanaged] [plain|json|json-pretty|yaml] Update the number of RGW instances for the given zone\nncn-s# ceph orch apply rgw \u0026lt;realm_name\u0026gt; \u0026lt;zone_name\u0026gt; [\u0026lt;subcluster\u0026gt;] [\u0026lt;port:int\u0026gt;] [--ssl] [\u0026lt;placement\u0026gt;] [--dry-run] [plain|json|json-pretty|yaml] [--unmanaged] Cancels ongoing operations\nncn-s# ceph orch cancel Add daemon(s)\nncn-s# ceph orch daemon add [mon|mgr|rbd-mirror|crash|alertmanager|grafana|node-exporter|prometheus] [\u0026lt;placement\u0026gt;] Start iscsi daemon(s)\nncn-s# ceph orch daemon add iscsi \u0026lt;pool\u0026gt; \u0026lt;api_user\u0026gt; \u0026lt;api_password\u0026gt; [\u0026lt;trusted_ip_list\u0026gt;] [\u0026lt;placement\u0026gt;] Start MDS daemon(s)\nncn-s# ceph orch daemon add mds \u0026lt;fs_name\u0026gt; [\u0026lt;placement\u0026gt;] Start NFS daemon(s)\nncn-s# ceph orch daemon add nfs \u0026lt;svc_id\u0026gt; \u0026lt;pool\u0026gt; [\u0026lt;namespace\u0026gt;] [\u0026lt;placement\u0026gt;] Create an OSD service. Either \u0026ndash;svc_arg=host:drives\nncn-s# ceph orch daemon add osd [\u0026lt;svc_arg\u0026gt;] Start RGW daemon(s)\nncn-s# ceph orch daemon add rgw \u0026lt;realm_name\u0026gt; \u0026lt;zone_name\u0026gt; [\u0026lt;subcluster\u0026gt;] [\u0026lt;port:int\u0026gt;] [--ssl] [\u0026lt;placement\u0026gt;] Redeploy a daemon (with a specific image)\nncn-s# ceph orch daemon redeploy \u0026lt;name\u0026gt; [\u0026lt;image\u0026gt;] Remove specific daemon(s)\nncn-s# ceph orch daemon rm \u0026lt;names\u0026gt;... [--force] Start, stop, restart, (redeploy,) or reconfig a specific daemon\nncn-s# ceph orch daemon start|stop|restart|reconfig \u0026lt;name\u0026gt; List devices on a host\nncn-s# ceph orch device ls [\u0026lt;hostname\u0026gt;...] [plain|json|json-pretty|yaml] [--refresh] [--wide] Zap (erase!) a device so it can be re-used\nncn-s# ceph orch device zap \u0026lt;hostname\u0026gt; \u0026lt;path\u0026gt; [--force] Add a host\nncn-s# ceph orch host add \u0026lt;hostname\u0026gt; [\u0026lt;addr\u0026gt;] [\u0026lt;labels\u0026gt;...] Add a host label\nncn-s# ceph orch host label add \u0026lt;hostname\u0026gt; \u0026lt;label\u0026gt; Remove a host label\nncn-s# ceph orch host label rm \u0026lt;hostname\u0026gt; \u0026lt;label\u0026gt; List hosts\nncn-s# ceph orch host ls [plain|json|json-pretty|yaml] Check if the specified host can be safely stopped without reducing availability\nncn-s# ceph orch host ok-to-stop \u0026lt;hostname\u0026gt; Remove a host\nncn-s# cephorch host rm \u0026lt;hostname\u0026gt; Update a host address\nncn-s# cephorch host set-addr \u0026lt;hostname\u0026gt; \u0026lt;addr\u0026gt; List services known to orchestrator\nncn-s# cephorch ls [\u0026lt;service_type\u0026gt;] [\u0026lt;service_name\u0026gt;] [--export] [plain|json|json-pretty|yaml] [--refresh] Remove OSD services\nncn-s# cephorch osd rm \u0026lt;svc_id\u0026gt;... [--replace] [--force] Status of OSD removal operation\nncn-s# cephorch osd rm status [plain|json|json-pretty|yaml] Remove OSD services\nncn-s# cephorch osd rm stop \u0026lt;svc_id\u0026gt;... Pause orchestrator background work\nncn-s# cephorch pause List daemons known to orchestrator\nncn-s# cephorch ps [\u0026lt;hostname\u0026gt;] [\u0026lt;service_name\u0026gt;] [\u0026lt;daemon_type\u0026gt;] [\u0026lt;daemon_id\u0026gt;] [plain|json|json-pretty|yaml] [--refresh] Resume orchestrator background work (if paused)\nncn-s# cephorch resume Remove a service\nncn-s# cephorch rm \u0026lt;service_name\u0026gt; [--force] Select orchestrator module backend\nncn-s# cephorch set backend \u0026lt;module_name\u0026gt; Start, stop, restart, redeploy, or reconfig an entire service (i.e. all daemons)\nncn-s# cephorch start|stop|restart|redeploy|reconfig \u0026lt;service_name\u0026gt; Report configured backend and its status\nncn-s# cephorch status [plain|json|json-pretty|yaml] Check service versions vs available and target containers\nncn-s# cephorch upgrade check [\u0026lt;image\u0026gt;] [\u0026lt;ceph_version\u0026gt;] Pause an in-progress upgrade\nncn-s# cephorch upgrade pause Resume paused upgrade\nncn-s# cephorch upgrade resume Initiate upgrade\nncn-s# cephorch upgrade start [\u0026lt;image\u0026gt;] [\u0026lt;ceph_version\u0026gt;] Check service versions vs available and target containers\nncn-s# cephorch upgrade status Stop an in-progress upgrade\nncn-s# cephorch upgrade stop "
},
{
	"uri": "/docs-csm/en-11/operations/utility_storage/ceph_service_check_script_usage/",
	"title": "Ceph Service Check Script Usage Page",
	"tags": [],
	"description": "",
	"content": "Ceph Service Check Script Usage page Description: This is a new Ceph service script that will check the status of ceph and then verify that status against the individual Ceph storage nodes.\nLocation: /opt/cray/tests/install/ncn/scripts/ceph-service-status.sh\nUsage: usage: ceph-service-status.sh # runs a simple ceph health check ceph-service-status.sh -n \u0026lt;node\u0026gt; -s \u0026lt;service\u0026gt; # checks a single service on a single node ceph-service-status.sh -n \u0026lt;node\u0026gt; -a true # checks all Ceph services on a node ceph-service-status.sh -A true # checks all Ceph services on all nodes in a rolling fashion ceph-service-status.sh -s \u0026lt;service name\u0026gt; # will find the where the service is running and report its status Important: By default this output of this command will not be verbose. This is to accommodate goss testing. For manual runs, please use the -v true flag.\nImportant: If you encounter this message parse error: Invalid numeric literal at line 1, column 5 that is indicating that the cached ssh keys in known_hosts are no longer valid. The simple fix is \u0026gt; ~/.ssh/known_hosts and re-run the script. It will update the keys.\nExamples Simple Ceph Health Check # /opt/cray/tests/install/ncn/scripts/ceph-service-status.sh -v true FSID: c84ecf41-c535-4588-96c3-f6892bbd81ce FSID_STR: ceph-c84ecf41-c535-4588-96c3-f6892bbd81ce Ceph is reporting a status of HEALTH_OK Updating ssh keys.. Tests run: 1 Tests Passed: 1 Service Check for a Single Service on a Single Node # /opt/cray/tests/install/ncn/scripts/ceph-service-status.sh -n ncn-s001 -v true -s mon.ncn-s001 FSID: c84ecf41-c535-4588-96c3-f6892bbd81ce FSID_STR: ceph-c84ecf41-c535-4588-96c3-f6892bbd81ce Ceph is reporting a status of HEALTH_OK Updating ssh keys.. HOST: ncn-s001####################### Service mon.ncn-s001 on ncn-s001 has been restarted and up for 9280 seconds mon.ncn-s001\u0026#39;s status is: running Service unit name: ceph-c84ecf41-c535-4588-96c3-f6892bbd81ce-mon.ncn-s001 Status: running Tests run: 2 Tests Passed: 2 Service Check for All Services on a Single Node # /opt/cray/tests/install/ncn/scripts/ceph-service-status.sh -n ncn-s001 -a true -v true FSID: c84ecf41-c535-4588-96c3-f6892bbd81ce FSID_STR: ceph-c84ecf41-c535-4588-96c3-f6892bbd81ce Ceph is reporting a status of HEALTH_OK Updating ssh keys.. HOST: ncn-s001####################### Service mds.cephfs.ncn-s001.rmisfx on ncn-s001 has been restarted and up for 9206 seconds mds.cephfs.ncn-s001.rmisfx\u0026#39;s status is: running Service unit name: ceph-c84ecf41-c535-4588-96c3-f6892bbd81ce-mds.cephfs.ncn-s001.rmisfx Status: running Service mgr.ncn-s001 on ncn-s001 has been restarted and up for 9201 seconds mgr.ncn-s001\u0026#39;s status is: running Service unit name: ceph-c84ecf41-c535-4588-96c3-f6892bbd81ce-mgr.ncn-s001 Status: running Service mon.ncn-s001 on ncn-s001 has been restarted and up for 9228 seconds mon.ncn-s001\u0026#39;s status is: running Service unit name: ceph-c84ecf41-c535-4588-96c3-f6892bbd81ce-mon.ncn-s001 Status: running Service node-exporter.ncn-s001 on ncn-s001 has been restarted and up for 1231 seconds node-exporter.ncn-s001\u0026#39;s status is: running Service unit name: ceph-c84ecf41-c535-4588-96c3-f6892bbd81ce-node-exporter.ncn-s001 Status: running Service on ncn-s001 is reporting up for 9209 seconds osd.0\u0026#39;s status is reporting up: 1 in: 1 Service unit name: ceph-c84ecf41-c535-4588-96c3-f6892bbd81ce-osd.0 Status: running Service on ncn-s001 is reporting up for 9200 seconds osd.11\u0026#39;s status is reporting up: 1 in: 1 Service unit name: ceph-c84ecf41-c535-4588-96c3-f6892bbd81ce-osd.11 Status: running Service on ncn-s001 is reporting up for 9208 seconds osd.14\u0026#39;s status is reporting up: 1 in: 1 Service unit name: ceph-c84ecf41-c535-4588-96c3-f6892bbd81ce-osd.14 Status: running Service on ncn-s001 is reporting up for 9206 seconds osd.17\u0026#39;s status is reporting up: 1 in: 1 Service unit name: ceph-c84ecf41-c535-4588-96c3-f6892bbd81ce-osd.17 Status: running Service on ncn-s001 is reporting up for 9213 seconds osd.5\u0026#39;s status is reporting up: 1 in: 1 Service unit name: ceph-c84ecf41-c535-4588-96c3-f6892bbd81ce-osd.5 Status: running Service on ncn-s001 is reporting up for 9207 seconds osd.8\u0026#39;s status is reporting up: 1 in: 1 Service unit name: ceph-c84ecf41-c535-4588-96c3-f6892bbd81ce-osd.8 Status: running Service rgw.site1.zone1.ncn-s001.kvxhwi on ncn-s001 has been restarted and up for 9210 seconds rgw.site1.zone1.ncn-s001.kvxhwi\u0026#39;s status is: running Service unit name: ceph-c84ecf41-c535-4588-96c3-f6892bbd81ce-rgw.site1.zone1.ncn-s001.kvxhwi Status: running Tests run: 12 Tests Passed: 12 Service Check for a Service Type # /opt/cray/tests/install/ncn/scripts/ceph-service-status.sh -v true -s mon FSID: c84ecf41-c535-4588-96c3-f6892bbd81ce FSID_STR: ceph-c84ecf41-c535-4588-96c3-f6892bbd81ce Ceph is reporting a status of HEALTH_OK Updating ssh keys.. HOST: ncn-s001####################### Service mon on ncn-s001 has been restarted and up for 9547 seconds mon\u0026#39;s status is: running Service unit name: ceph-c84ecf41-c535-4588-96c3-f6892bbd81ce-mon.ncn-s001 Status: running HOST: ncn-s002####################### Service mon on ncn-s002 has been restarted and up for 5643 seconds mon\u0026#39;s status is: running Service unit name: ceph-c84ecf41-c535-4588-96c3-f6892bbd81ce-mon.ncn-s002 Status: running HOST: ncn-s003####################### Service mon on ncn-s003 has been restarted and up for 2588 seconds mon\u0026#39;s status is: running Service unit name: ceph-c84ecf41-c535-4588-96c3-f6892bbd81ce-mon.ncn-s003 Status: running Tests run: 4 Tests Passed: 4 Service Check for All Services and All Nodes # /opt/cray/tests/install/ncn/scripts/ceph-service-status.sh -v true -A true Note: The output is similar to the above output, but all services on all nodes. We are not showing this to keep the document usable.\nIMPORTANT: You can always run this script without the verbose flag and echo the return code echo $?. rc = 0 clean check, rc = 1 or greater then there was an issue and re-run with the -v true flag.\n"
},
{
	"uri": "/docs-csm/en-11/operations/utility_storage/ceph_storage_types/",
	"title": "Ceph Storage Types",
	"tags": [],
	"description": "",
	"content": "Ceph Storage Types As a reference, the following ceph and rbd commands are run from a master node or ncn-s001/2/3. Certain commands will work on different systems. For example, the rbd command can be used on the worker nodes if specifying the proper key.\nCeph Block (rbd) List block devices in a specific pool:\nncn-m001# rbd -p POOL_NAME ls -l NAME SIZE PARENT FMT PROT LOCK kube_vol 4 GiB 2 Create a block device:\nncn-m001# rbd create -p POOL_NAME VOLUME_NAME -size SIZE Remove a block device:\nncn-m001# rbd -p POOL_NAME remove VOLUME_NAME Show mapped devices:\nncn-m001# rbd showmapped id pool namespace image snap device 0 test test_vol - /dev/rbd0 1 kube kube_vol - /dev/rbd1 2 smf smf_vol - /dev/rbd2 Ceph MDS (File) Display CephFS shares with their pool information:\nncn-m001# ceph fs ls name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ] Show the status of all CephFS components:\nncn-m001# ceph fs status cephfs - 0 clients \u0026lt;\u0026lt;-- Containers or hosts attached to cephfs are represented here ====== +------+--------+-----------+---------------+-------+-------+ | Rank | State | MDS | Activity | dns | inos | +------+--------+-----------+---------------+-------+-------+ | 0 | active | ceph-2 | Reqs: 0 /s | 10 | 13 | \u0026lt;\u0026lt;-- Active server +------+--------+-----------+---------------+-------+-------+ +-----------------+----------+-------+-------+ | Pool | type | used | avail | +-----------------+----------+-------+-------+ | cephfs_metadata | metadata | 1536k | 13.1G | | cephfs_data | data | 0 | 13.1G | \u0026lt;\u0026lt;-- Where files get stored +-----------------+----------+-------+-------+ +-------------+ | Standby MDS | +-------------+ | ceph-1 | | ceph-3 | +-------------+ MDS version: ceph version 14.2.0-300-gacd2f2b9e1 (acd2f2b9e196222b0350b3b59af9981f91706c7f) nautilus (stable) Ceph RadosGW (object/s3) List the services to learn more about the radosgw service. The following command lists more than just the radosgw service, so ensure the correct sections are used.\nncn-m001# ceph service dump { \u0026#34;epoch\u0026#34;: 2, \u0026#34;modified\u0026#34;: \u0026#34;2019-08-11 04:37:31.464120\u0026#34;, \u0026#34;services\u0026#34;: { \u0026#34;rgw\u0026#34;: { \u0026lt;\u0026lt;-- Note this section \u0026#34;daemons\u0026#34;: { \u0026#34;summary\u0026#34;: \u0026#34;, \u0026#34;\u0026lt;hostname redacted\u0026gt;.rgw0\u0026#34;: { \u0026#34;start_epoch\u0026#34;: 2, \u0026#34;start_stamp\u0026#34;: \u0026#34;2019-08-11 04:37:31.454975\u0026#34;, \u0026#34;gid\u0026#34;: 24609, \u0026#34;addr\u0026#34;: \u0026#34;10.2.0.1:0/3889467377\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;arch\u0026#34;: \u0026#34;x86_64\u0026#34;, \u0026#34;ceph_release\u0026#34;: \u0026#34;nautilus\u0026#34;, \u0026#34;ceph_version\u0026#34;: \u0026#34;ceph version 14.2.0-300-gacd2f2b9e1 (acd2f2b9e196222b0350b3b59af9981f91706c7f) nautilus (stable)\u0026#34;, \u0026#34;ceph_version_short\u0026#34;: \u0026#34;14.2.0-300-gacd2f2b9e1\u0026#34;, \u0026#34;cpu\u0026#34;: \u0026#34;Intel(R) Xeon(R) Platinum 8176 CPU @ 2.10GHz\u0026#34;, \u0026#34;distro\u0026#34;: \u0026#34;sles\u0026#34;, \u0026#34;distro_description\u0026#34;: \u0026#34;SUSE Linux Enterprise Server 15\u0026#34;, \u0026#34;distro_version\u0026#34;: \u0026#34;15\u0026#34;, \u0026#34;frontend_config#0\u0026#34;: \u0026#34;beast endpoint=\u0026lt;ip address redacted\u0026gt;:8080\u0026#34;, \u0026#34;frontend_type#0\u0026#34;: \u0026#34;beast\u0026#34;, \u0026#34;hostname\u0026#34;: \u0026#34;\u0026lt;hostname redacted\u0026gt;\u0026#34;, \u0026#34;kernel_description\u0026#34;: \u0026#34;#1 SMP Thu Jul 11 11:24:28 UTC 2019 (bf2abc2)\u0026#34;, \u0026#34;kernel_version\u0026#34;: \u0026#34;4.12.14-150.27-default\u0026#34;, \u0026#34;mem_swap_kb\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;mem_total_kb\u0026#34;: \u0026#34;196736052\u0026#34;, \u0026#34;num_handles\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;os\u0026#34;: \u0026#34;Linux\u0026#34;, \u0026#34;pid\u0026#34;: \u0026#34;48512\u0026#34;, \u0026#34;zone_id\u0026#34;: \u0026#34;f9b1f6cc-3396-4161-b694-f2d5019b80c6\u0026#34;, \u0026#34;zone_name\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;zonegroup_id\u0026#34;: \u0026#34;cea2e773-7e4e-4673-b6fd-91adb76e25f5\u0026#34;, \u0026#34;zonegroup_name\u0026#34;: \u0026#34;default\u0026#34; } }, The radosgw-admin user command is be used to edit and view user information. The following command is an example of how to get information about a specific user.\nncn-m001# radosgw-admin user info --uid TEST_USER { \u0026#34;user_id\u0026#34;: \u0026#34;test_user\u0026#34;, \u0026#34;display_name\u0026#34;: \u0026#34;test_user\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;, \u0026#34;suspended\u0026#34;: 0, \u0026#34;max_buckets\u0026#34;: 1000, \u0026#34;subusers\u0026#34;: [], \u0026lt;\u0026lt;-- Any users created and maintained by this user \u0026#34;keys\u0026#34;: [ { \u0026#34;user\u0026#34;: \u0026#34;test_user\u0026#34;, \u0026#34;access_key\u0026#34;: \u0026#34;QEA6PG8VDSJ41JR4C6GZ\u0026#34;, \u0026lt;\u0026lt;-- Random key unique to this user and system \u0026#34;secret_key\u0026#34;: \u0026#34;SzNCqWwZ7XlGZ1tdtuVdhLTno48ugthx5YwCF6E8\u0026#34; \u0026lt;\u0026lt;-- Random key unique to this user and system } ], \u0026#34;swift_keys\u0026#34;: [], \u0026#34;caps\u0026#34;: [], \u0026#34;op_mask\u0026#34;: \u0026#34;read, write, delete\u0026#34;, \u0026#34;default_placement\u0026#34;: \u0026#34;, \u0026#34;default_storage_class\u0026#34;: \u0026#34;, \u0026#34;placement_tags\u0026#34;: [], \u0026#34;bucket_quota\u0026#34;: { \u0026#34;enabled\u0026#34;: false, \u0026#34;check_on_raw\u0026#34;: false, \u0026#34;max_size\u0026#34;: -1, \u0026#34;max_size_kb\u0026#34;: 0, \u0026#34;max_objects\u0026#34;: -1 }, \u0026#34;user_quota\u0026#34;: { \u0026#34;enabled\u0026#34;: false, \u0026#34;check_on_raw\u0026#34;: false, \u0026#34;max_size\u0026#34;: -1, \u0026#34;max_size_kb\u0026#34;: 0, \u0026#34;max_objects\u0026#34;: -1 }, \u0026#34;temp_url_keys\u0026#34;: [], \u0026#34;type\u0026#34;: \u0026#34;rgw\u0026#34;, \u0026#34;mfa_ids\u0026#34;: [] } The radosgw-admin bucket command is used to remove or view buckets.\nTo list the buckets:\nncn-m001# radosgw-admin bucket list To remove a specific bucket:\nncn-m001# radosgw-admin bucket rm --bucket-name BUCKET_NAME "
},
{
	"uri": "/docs-csm/en-11/operations/utility_storage/cephadm_reference_material/",
	"title": "Cephadm Reference Material",
	"tags": [],
	"description": "",
	"content": "Cephadm Reference Material cephadm is a new function introduced in Ceph Octopus 15. It allows for an easier method to install and manage Ceph nodes.\nThe following sections include common examples:\nInvoke Shells to Run Traditional Ceph Commands On ncn-s001/2/3:\nncn-s00[123]# cephadm shell # creates a container with access to run ceph commands the traditional way Optionally, execute the following command:\nncn-s00[123]# cephadm shell -- ceph -s Ceph-Volume There are multiple ways to do Ceph device operations now.\nUse cephadm ncn-s# cephadm ceph-volume Use cephadm shell Optionally, this can be done by invoking a cephadm shell by appending a ceph command to the cephadm command.\nncn-s# cephadm shell -- ceph-volume Use ceph orch Optionally, the following command will allow users to specify a single node name to just list that nodes drives.\nncn-s00[123]# ceph orch device ls ncn-s00[123]# ceph orch device ls \u0026lt;node name\u0026gt; "
},
{
	"uri": "/docs-csm/en-11/operations/utility_storage/collect_information_about_the_ceph_cluster/",
	"title": "Collect Information About The Ceph Cluster",
	"tags": [],
	"description": "",
	"content": "Collect Information about the Ceph Cluster These general commands for Ceph are helpful for obtaining information pertinent to troubleshooting issues.\nAs a reference, the Ceph commands below are run from a ceph-mon node. Certain commands will work on different systems. For example, the rbd command can be used on the worker nodes if specifying the proper key.\nCeph Log and File Locations  Ceph configurations are located under /etc/ceph/ceph.conf Ceph data structure and bootstrap is located under /var/lib/ceph// Ceph logs are now accessible by a couple of different methods  Utilizing cephadm ls to retrieve the systemd_unit on the node for the process, then utilize journalctl to dump the logs ceph log last [\u0026lt;num:int\u0026gt;] [debug|info|sec|warn|error] [*|cluster|audit|cephadm]  Note that that this will dump general cluster logs   cephadm logs [-h] [--fsid FSID] --name \u0026lt;systemd_unit\u0026gt;    Check the Status of Ceph Print the status of the Ceph cluster with the following command:\nncn-m001# ceph -s cluster: id: 5f3b4031-d6c0-4118-94c0-bffd90b534eb health: HEALTH_OK \u0026lt;\u0026lt;-- WARN/ERROR/CRITICAL are other states services: mon: 3 daemons, quorum ncn-s001,ncn-s002,ncn-s003 (age 20h) \u0026lt;\u0026lt;-- Should have quorum and ideally an odd number of mon nodes mgr: ncn-s003(active, since 9d), standbys: ncn-s001, ncn-s002 \u0026lt;\u0026lt;-- The watchdog for the cluster mds: cephfs:1 {0=ncn-s002=up:active} 2 up:standby \u0026lt;\u0026lt;-- Filesystem service osd: 18 osds: 18 up (since 20h), 18 in (since 9d) \u0026lt;\u0026lt;-- Data devices: 1 OSD = 1 hard drive designated for Ceph rgw: 3 daemons active (ncn-s001.rgw0, ncn-s002.rgw0, ncn-s003.rgw0) \u0026lt;\u0026lt;-- Object storage data: \u0026lt;\u0026lt;-- Health stats related to the above services pools: 11 pools, 220 pgs objects: 825.66k objects, 942 GiB usage: 2.0 TiB used, 61 TiB / 63 TiB avail pgs: 220 active+clean io: client: 2.5 KiB/s rd, 13 MiB/s wr, 2 op/s rd, 1.27k op/s wr The -w option can be used to watch the cluster.\nPrint the OSD tree Check the OSD status, weight, and location.\nncn-m001# ceph osd tree ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 62.85938 root default -7 20.95312 host ncn-s001 1 ssd 3.49219 osd.1 up 1.00000 1.00000 5 ssd 3.49219 osd.5 up 1.00000 1.00000 6 ssd 3.49219 osd.6 up 1.00000 1.00000 7 ssd 3.49219 osd.7 up 1.00000 1.00000 8 ssd 3.49219 osd.8 up 1.00000 1.00000 9 ssd 3.49219 osd.9 up 1.00000 1.00000 -5 20.95312 host ncn-s002 2 ssd 3.49219 osd.2 up 1.00000 1.00000 4 ssd 3.49219 osd.4 up 1.00000 1.00000 10 ssd 3.49219 osd.10 up 1.00000 1.00000 11 ssd 3.49219 osd.11 up 1.00000 1.00000 12 ssd 3.49219 osd.12 up 1.00000 1.00000 13 ssd 3.49219 osd.13 up 1.00000 1.00000 -3 20.95312 host ncn-s003 0 ssd 3.49219 osd.0 up 1.00000 1.00000 3 ssd 3.49219 osd.3 up 1.00000 1.00000 14 ssd 3.49219 osd.14 up 1.00000 1.00000 15 ssd 3.49219 osd.15 up 1.00000 1.00000 16 ssd 3.49219 osd.16 up 1.00000 1.00000 17 ssd 3.49219 osd.17 up 1.00000 1.00000 Storage Utilization The following command shows the storage utilization of the cluster and pools:\nncn-m001# ceph df RAW STORAGE: CLASS SIZE AVAIL USED RAW USED %RAW USED kube 27 GiB 15 GiB 9.2 GiB 12 GiB 45.25 smf 57 GiB 30 GiB 24 GiB 27 GiB 48.09 TOTAL 84 GiB 44 GiB 34 GiB 40 GiB 47.18 POOLS: POOL ID STORED OBJECTS USED %USED MAX AVAIL cephfs_data 1 0 B 0 0 B 0 13 GiB cephfs_metadata 2 2.2 KiB 22 1.5 MiB 0 13 GiB .rgw.root 3 1.2 KiB 4 768 KiB 0 13 GiB defaults.rgw.buckets.data 4 0 B 0 0 B 0 13 GiB default.rgw.control 5 0 B 8 0 B 0 13 GiB defaults.rgw.buckets.index 6 0 B 0 0 B 0 13 GiB default.rgw.meta 7 0 B 0 0 B 0 13 GiB default.rgw.log 8 0 B 175 0 B 0 13 GiB kube 10 3.1 GiB 799 9.2 GiB 40.64 4.5 GiB smf 11 8.1 GiB 2.11k 24 GiB 47.71 8.9 GiB Show OSD Usage Show the utilization of the OSDs with the following command. This is very helpful to see if the data is not balanced across OSDs, which can create hotspots.\nncn-m001# ceph osd df ID CLASS WEIGHT REWEIGHT SIZE RAW USE DATA OMAP META AVAIL %USE VAR PGS STATUS 1 kube 0.00879 1.00000 9 GiB 4.1 GiB 3.1 GiB 0 B 1 GiB 4.9 GiB 45.25 0.96 99 up 4 smf 0.01859 1.00000 19 GiB 9.1 GiB 8.1 GiB 0 B 1 GiB 9.9 GiB 48.09 1.02 141 up 0 kube 0.00879 1.00000 9 GiB 4.1 GiB 3.1 GiB 0 B 1 GiB 4.9 GiB 45.25 0.96 93 up 3 smf 0.01859 1.00000 19 GiB 9.1 GiB 8.1 GiB 0 B 1 GiB 9.9 GiB 48.09 1.02 147 up 2 kube 0.00879 1.00000 9 GiB 4.1 GiB 3.1 GiB 0 B 1 GiB 4.9 GiB 45.25 0.96 100 up 5 smf 0.01859 1.00000 19 GiB 9.1 GiB 8.1 GiB 0 B 1 GiB 9.9 GiB 48.09 1.02 140 up TOTAL 84 GiB 40 GiB 34 GiB 0 B 6 GiB 44 GiB 47.18 MIN/MAX VAR: 0.96/1.02 STDDEV: 1.51 Check the Status of a Single OSD Use the following command to obtain information about a single OSD using the OSD number. For example, osd.0 would be an OSD number.\nncn-m001# ceph osd find OSD.ID { \u0026#34;osd\u0026#34;: 1, \u0026#34;addrs\u0026#34;: { \u0026#34;addrvec\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;addr\u0026#34;: \u0026#34;10.248.2.127:6800\u0026#34;, \u0026#34;nonce\u0026#34;: 4966 } ] }, \u0026#34;osd_fsid\u0026#34;: \u0026#34;9d41f723-e86f-4b98-b1e7-12c0f5f15546\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;ceph-1\u0026#34;, \u0026#34;crush_location\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;ceph-1\u0026#34;, \u0026#34;root\u0026#34;: \u0026#34;default\u0026#34; } } List Storage Pools List the storage pools with the following commands:\nncn-m001# ceph osd lspools 1 cephfs_data 2 cephfs_metadata 3 .rgw.root 4 defaults.rgw.buckets.data 5 default.rgw.control 6 defaults.rgw.buckets.index 7 default.rgw.meta 8 default.rgw.log 10 kube 11 smf "
},
{
	"uri": "/docs-csm/en-11/operations/utility_storage/add_ceph_osds/",
	"title": "Add Ceph Osds",
	"tags": [],
	"description": "",
	"content": "Add Ceph OSDs IMPORTANT: This document is addressing how to add an OSD when the OSD auto-discovery fails to add in new drives.\nCheck to ensure you have OSD auto-discovery enabled.\nncn-s00(1/2/3)# ceph orch ls osd NAME RUNNING REFRESHED AGE PLACEMENT IMAGE NAME IMAGE ID osd.all-available-devices 9/9 4m ago 3d * registry.local/ceph/ceph:v15.2.8 5553b0cb212c  NOTE: Ceph version 15.2.x and newer will utilize the ceph orchestrator to add any available drives on the storage nodes to the OSD pool. The process below is in the event that the orchestrator did not add the available drives into the cluster\n Prerequisites This procedure requires administrative privileges and will require at least two windows.\nProcedure   In the first window, log in as root on the first master node (ncn-m001).\nncn-w001# ssh ncn-m001   Watch the status of the cluster to monitor the progress of the drives being added.\nThe following example shows only six OSDs in use.\nncn-m001# watch -n 10 ceph -s cluster: id: 5b359a58-e6f7-4f0c-98b8-f528f620896a health: HEALTH_OK services: mon: 3 daemons, quorum ncn-s001,ncn-s002,ncn-s003 (age 20h) mgr: ncn-s003(active, since 9d), standbys: ncn-s001, ncn-s002 mds: cephfs:1 {0=ncn-s002=up:active} 2 up:standby osd: 18 osds: 18 up (since 20h), 18 in (since 9d) rgw: 3 daemons active (ncn-s001.rgw0, ncn-s002.rgw0, ncn-s003.rgw0) data: pools: 11 pools, 220 pgs objects: 826.23k objects, 944 GiB usage: 2.0 TiB used, 61 TiB / 63 TiB avail pgs: 220 active+clean io: client: 1.7 KiB/s rd, 12 MiB/s wr, 1 op/s rd, 1.32k op/s wr   In the second window, log into ncn-s00(1/2/3) or an ncn-m node and fail over the mgr process.\n There is an issue where orchestration tasks can get hung up and the failover will clear that up.  ncn-w001# ceph mgr fail $(ceph mgr dump | jq -r .active_name)   In the second window, list your available drives on the node(s) where the OSDs are missing\n# Our example is utilizing ncn-s001 so make sure you are on the correct host for your situation ncn-s001# ceph orch device ls ceph orch device ls ncn-s001 Hostname Path Type Serial Size Health Ident Fault Available ncn-s001 /dev/sdb hdd f94bd091-cc25-476b-9 48.3G Unknown N/A N/A No  Note that our drive in question is reporting available. The following steps are going to erase that drive so PLEASE make sure you know that drive is not being used.\n ncn-s001:~ # podman ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 596d1c235da8 registry.local/ceph/ceph:v15.2.8 -n client.rgw.sit... Less than a second ago Up Less than a second ago ceph-11d5d552-cfac-11eb-ab69-fa163ec012bf-rgw.site1.zone1.ncn-s001.oztynu eecfac35fe7c registry.local/ceph/ceph:v15.2.8 -n mon.ncn-s001 -... 2 seconds ago Up 2 seconds ago ceph-11d5d552-cfac-11eb-ab69-fa163ec012bf-mon.ncn-s001 3140f5062945 registry.local/ceph/ceph:v15.2.8 -n mgr.ncn-s001.b... 17 seconds ago Up 17 seconds ago ceph-11d5d552-cfac-11eb-ab69-fa163ec012bf-mgr.ncn-s001.bfdept 3d25564047e1 registry.local/ceph/ceph:v15.2.8 -n mds.cephfs.ncn... 3 days ago Up 3 days ago ceph-11d5d552-cfac-11eb-ab69-fa163ec012bf-mds.cephfs.ncn-s001.juehkw 4ebd6db27d08 registry.local/ceph/ceph:v15.2.8 -n osd.2 -f --set... 4 days ago Up 4 days ago ceph-11d5d552-cfac-11eb-ab69-fa163ec012bf-osd.2 96c6e11677f0 registry.local/ceph/ceph:v15.2.8 -n client.crash.n... 4 days ago Up 4 days ago ceph-11d5d552-cfac-11eb-ab69-fa163ec012bf-crash.ncn-s001   If you find an Running OSD container then we should assume that the drive is being used or might have critical data on it. If you know this to 100% not be the case (example a rebuild), then you can proceed.\nRepeat this step for all drives on the storage node(s) that have unused storage which should be added to Ceph.\nncn-s001# ceph orch device zap ncn-s001 /dev/sdb (optional --force) Proceed to the next step after all of the OSDs have been added to the storage nodes.\n  In the first window, check how many OSDs are available.\nThe following example shows 18 OSDs in use.\ncluster: id: 5b359a58-e6f7-4f0c-98b8-f528f620896a health: HEALTH_ERR Degraded data redundancy (low space): 3 pgs backfill_toofull services: mon: 3 daemons, quorum ncn-s001,ncn-s002,ncn-s003 (age 20h) mgr: ncn-s003(active, since 9d), standbys: ncn-s001, ncn-s002 mds: cephfs:1 {0=ncn-s002=up:active} 2 up:standby {0=ncn-m002=up:active} 2 up:standby osd: 18 osds: 18 up (since 20h), 18 in (since 9d) rgw: 3 daemons active (ncn-s001.rgw0, ncn-s002.rgw0, ncn-s003.rgw0) data: pools: 11 pools, 204 pgs objects: 70.98k objects, 241 GiB usage: 547 GiB used, 62 TiB / 63 TiB avail pgs: 39582/212949 objects misplaced (18.588%) 163 active+clean 36 active+remapped+backfill_wait 3 active+remapped+backfill_wait+backfill_toofull 2 active+remapped+backfilling io: client: 20 MiB/s wr, 0 op/s rd, 807 op/s wr recovery: 559 MiB/s, 187 objects/s ncn-s001:~ # ceph orch ps --daemon_type osd ncn-s001 NAME HOST STATUS REFRESHED AGE VERSION IMAGE NAME IMAGE ID CONTAINER ID osd.2 ncn-s001 running (4d) 20s ago 4d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 4ebd6db27d08   Run the ceph-pool-quotas.yml playbook to reset the pool quotas.\nThis step is only necessary when the cluster capacity has increased.\nncn-w001# ansible-playbook /opt/cray/crayctl/ansible_framework/main/ceph-pool-quotas.yml   "
},
{
	"uri": "/docs-csm/en-11/operations/utility_storage/adjust_ceph_pool_quotas/",
	"title": "Adjust Ceph Pool Quotas",
	"tags": [],
	"description": "",
	"content": "Adjust Ceph Pool Quotas Ceph pools are used for storing data. Use this procedure to set the Ceph pool quotas to determine the wanted number of bytes per pool. The smf Ceph pool now has replication factor of two.\nResolve Ceph health issues caused by a pool reaching its quota.\nPrerequisites This procedure requires administrative privileges.\nLimitations Currently, only smf includes a quota.\nProcedure   Log in as root on ncn-m001.\n  Determine the available space.\nIn the following example, the 3.5 TiB is 33 percent of the 21 TiB total. Ceph keeps three copies of data, so a 3.5 TiB quota is actually provisioning 7.0 TiB of storage, which is 33 percent of 21 TiB.\nncn-m001# ceph df detail RAW STORAGE: CLASS SIZE AVAIL USED RAW USED %RAW USED ssd 21 TiB 21 TiB 122 GiB 134 GiB 0.62 TOTAL **21 TiB** 21 TiB 122 GiB 134 GiB 0.62 POOLS: POOL ID STORED OBJECTS USED %USED MAX AVAIL QUOTA OBJECTS QUOTA BYTES DIRTY USED COMPR UNDER COMPR cephfs_data 1 1.0 MiB 74 6.2 MiB 0 6.6 TiB N/A N/A 74 0 B 0 B cephfs_metadata 2 22 MiB 28 68 MiB 0 6.6 TiB N/A N/A 28 0 B 0 B .rgw.root 3 3.5 KiB 8 384 KiB 0 6.6 TiB N/A N/A 8 0 B 0 B default.rgw.buckets.data 4 22 GiB 22.57k 65 GiB 0.32 6.6 TiB N/A N/A 22.57k 0 B 0 B default.rgw.control 5 0 B 8 0 B 0 6.6 TiB N/A N/A 8 0 B 0 B default.rgw.buckets.index 6 197 KiB 13 197 KiB 0 6.6 TiB N/A N/A 13 0 B 0 B default.rgw.meta 7 19 KiB 107 4.2 MiB 0 6.6 TiB N/A N/A 107 0 B 0 B default.rgw.log 8 0 B 207 0 B 0 6.6 TiB N/A N/A 207 0 B 0 B kube 9 15 GiB 6.48k 28 GiB 0.14 6.6 TiB N/A N/A 6.48k 17 GiB 33 GiB smf 10 19 TiB 7.88k 28 GiB 0.14 9.9 TiB N/A **3.5 TiB** 7.88k 9.4 GiB 19 GiB default.rgw.buckets.non-ec 11 0 B 0 0 B 0 9.9 TiB N/A N/A 0 0 B 0 B   Determine the maximum quota percentage.\n6TiB must be left for Kubernetes, Ceph RGW, and other services. To calculate the quota percentage, use the following equation:\n(TOTAL_SIZE-6)/TOTAL_SIZE Using the example output in step 2, the following would be the quota percentage:\n(21-6)/21 = .71   Edit the quota percentage as wanted.\nDo not exceed the percentage determined in the previous step.\nncn-s001# vim /etc/ansible/ceph-rgw-users/ceph-pool-quotas.yml ceph_pool_quotas: - pool_name: smf percent_of_total: .71 \u0026lt;-- Change this to desired percentage replication_factor: 2.0   Run the ceph-pool-quotas.yml playbook from ncn-s001.\nncn-s001# ansible-playbook /etc/ansible/ceph-rgw-users/ceph-pool-quotas.yml   View the quota/pool usage.\nLook at the USED and QUOTA BYTES columns to view usage and the new quota setting.\nncn-m001# ceph df detail RAW STORAGE: CLASS SIZE AVAIL USED RAW USED %RAW USED ssd 21 TiB 21 TiB 122 GiB 134 GiB 0.62 TOTAL **21 TiB** 21 TiB 122 GiB 134 GiB 0.62 POOLS: POOL ID STORED OBJECTS USED %USED MAX AVAIL QUOTA OBJECTS QUOTA BYTES DIRTY USED COMPR UNDER COMPR cephfs_data 1 1.0 MiB 74 6.2 MiB 0 6.6 TiB N/A N/A 74 0 B 0 B cephfs_metadata 2 22 MiB 28 68 MiB 0 6.6 TiB N/A N/A 28 0 B 0 B .rgw.root 3 3.5 KiB 8 384 KiB 0 6.6 TiB N/A N/A 8 0 B 0 B default.rgw.buckets.data 4 22 GiB 22.57k 65 GiB 0.32 6.6 TiB N/A N/A 22.57k 0 B 0 B default.rgw.control 5 0 B 8 0 B 0 6.6 TiB N/A N/A 8 0 B 0 B default.rgw.buckets.index 6 197 KiB 13 197 KiB 0 6.6 TiB N/A N/A 13 0 B 0 B default.rgw.meta 7 19 KiB 107 4.2 MiB 0 6.6 TiB N/A N/A 107 0 B 0 B default.rgw.log 8 0 B 207 0 B 0 6.6 TiB N/A N/A 207 0 B 0 B kube 9 15 GiB 6.48k 28 GiB 0.14 6.6 TiB N/A N/A 6.48k 17 GiB 33 GiB smf 10 19 TiB 7.88k 28 GiB 0.14 9.9 TiB N/A **7.4 TiB** 7.88k 9.4 GiB 19 GiB default.rgw.buckets.non-ec 11 0 B 0 0 B 0 9.9 TiB N/A N/A 0 0 B 0 B   "
},
{
	"uri": "/docs-csm/en-11/operations/utility_storage/ceph_daemon_memory_profiling/",
	"title": "Ceph Daemon Memory Profiling",
	"tags": [],
	"description": "",
	"content": "Ceph Daemon Memory Profiling Use Case: This page is meant as an instructional guide to provide information back to HPECray to assist in tuning and troubleshooting exercises.\nProcedure:\nNOTE: For this example we are going to use a ceph-mon process on ncn-s001\n  Identify the process and location of the daemon to profile.\nncn-s00(1/2/3)# ceph orch ps --daemon_type mon NAME HOST STATUS REFRESHED AGE VERSION IMAGE NAME IMAGE ID CONTAINER ID mon.ncn-s001 ncn-s001 running (1h) 60s ago 1h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c bcca26f69191 mon.ncn-s002 ncn-s002 running (1h) 61s ago 1h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 43c8472465b2 mon.ncn-s003 ncn-s003 running (1h) 61s ago 1h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 7aa1b1f19a00   ssh to the node where you process is running if it is different from your current node.\n  Start the profiler.\nncn-s001:~ # ceph tell mon.ncn-s001 heap start_profiler mon.ncn-s001 started profiler   Dump stats. This does NOT require the profiler to be running.\nncn-s001:~ # ceph tell mon.ncn-s001 heap stats mon.ncn-s001 tcmalloc heap stats:------------------------------------------------ MALLOC: 972461744 ( 927.4 MiB) Bytes in use by application MALLOC: + 0 ( 0.0 MiB) Bytes in page heap freelist MALLOC: + 8804424 ( 8.4 MiB) Bytes in central cache freelist MALLOC: + 3706880 ( 3.5 MiB) Bytes in transfer cache freelist MALLOC: + 25649416 ( 24.5 MiB) Bytes in thread cache freelists MALLOC: + 5636096 ( 5.4 MiB) Bytes in malloc metadata MALLOC: ------------ MALLOC: = 1016258560 ( 969.2 MiB) Actual memory used (physical + swap) MALLOC: + 189841408 ( 181.0 MiB) Bytes released to OS (aka unmapped) MALLOC: ------------ MALLOC: = 1206099968 ( 1150.2 MiB) Virtual address space used MALLOC: MALLOC: 14833 Spans in use MALLOC: 25 Thread heaps in use MALLOC: 8192 Tcmalloc page size ------------------------------------------------ Call ReleaseFreeMemory() to release freelist memory to the OS (via madvise()). Bytes released to the OS take up virtual address space but no physical memory.   Dump heap. This requires the profiler to be running.\n# ceph tell mon.ncn-s001 heap dump mon.ncn-s001 dumping heap profile now. ------------------------------------------------ MALLOC: 976849264 ( 931.6 MiB) Bytes in use by application MALLOC: + 0 ( 0.0 MiB) Bytes in page heap freelist MALLOC: + 8819048 ( 8.4 MiB) Bytes in central cache freelist MALLOC: + 3617280 ( 3.4 MiB) Bytes in transfer cache freelist MALLOC: + 25531176 ( 24.3 MiB) Bytes in thread cache freelists MALLOC: + 5636096 ( 5.4 MiB) Bytes in malloc metadata MALLOC: ------------ MALLOC: = 1020452864 ( 973.2 MiB) Actual memory used (physical + swap) MALLOC: + 185647104 ( 177.0 MiB) Bytes released to OS (aka unmapped) MALLOC: ------------ MALLOC: = 1206099968 ( 1150.2 MiB) Virtual address space used MALLOC: MALLOC: 14834 Spans in use MALLOC: 25 Thread heaps in use MALLOC: 8192 Tcmalloc page size ------------------------------------------------ Call ReleaseFreeMemory() to release freelist memory to the OS (via madvise()). Bytes released to the OS take up virtual address space but no physical memory.   Release memory.\nncn-s001:~ # ceph tell mon.ncn-s001 heap release mon.ncn-s001 releasing free RAM back to system.   Stop the profiler\nncn-s001:~ # ceph tell mon.ncn-s001 heap stop_profiler mon.ncn-s001 stopped profiler   "
},
{
	"uri": "/docs-csm/en-11/operations/utility_storage/ceph_health_states/",
	"title": "Ceph Health States",
	"tags": [],
	"description": "",
	"content": "Ceph Health States Ceph reports several different health states depending on the condition of a cluster. These health states can provide a lot of information about the current functionality of the Ceph cluster, what troubleshooting steps needs to be taken, and if a support ticket needs to be filed.\nThe health of a Ceph cluster can be viewed with the following command:\nncn-m001# ceph -s cluster: id: 5f3b4031-d6c0-4118-94c0-bffd90b534eb health: HEALTH_OK \u0026lt;\u0026lt;-- Health state ... The following is an overview of potential health states:\n  HEALTH_OK\nThe cluster is operating as expected with no issues.\n  HEALTH_WARN\nThere is a WARNING condition on the cluster. There are lots of potential causes, but this warning does not mean any functionality is lost. For example, this health state could occur if a pool is at its quota. This health state does not mean that the cluster is not servicing data.\nMost HEALTH_WARN states resolve on their own as they pertain to functionality that tends to self correct.\n  HEALTH_ERROR\nThere is an ERROR condition on the cluster. This is typical for a configuration issue or if there is a component that is having issues completing its functions. This does not mean that the cluster is not servicing data. The HEALTH_ERROR state is primarily for individual components experiencing issues.\nMost HEALTH_ERROR states may not be covered by troubleshooting documentation and should result in a ticket to customer support for guidance.\n  HEALTH_CRITICAL\nThere is a CRITICAL condition on the cluster. This means that cluster functions have stopped or have gone into read-only mode to protect data. When this is present, the cluster is not servicing data properly, or even at all in order to protect data integrity. This will be dependent on the configuration and the reason behind the CRITICAL health state.\nAll HEALTH_CRITICAL states should result in an immediate ticket to customer support for guidance on returning the cluster back to service.\n  For a list of possible states, refer to https://docs.ceph.com/docs/master/rados/operations/health-checks/.\n"
},
{
	"uri": "/docs-csm/en-11/operations/system_management_health/configure_prometheus_email_alert_notifications/",
	"title": "Configure Prometheus Email Alert Notifications",
	"tags": [],
	"description": "",
	"content": "Configure Prometheus Email Alert Notifications Configure an email alert notification for all Prometheus Postgres replication alerts: PostgresReplicationLagSMA, PostgresReplicationServices, PostgresqlFollowerReplicationLagSMA and PostgresqlFollowerReplicationLagServices.\nProcedure   Save the current alert notification configuration in case a rollback is needed.\nncn-w001# kubectl get secret -n sysmgmt-health \\ alertmanager-cray-sysmgmt-health-promet-alertmanager -ojsonpath=\u0026#39;{.data.alertmanager.yaml}\u0026#39; \\ | base64 --decode \u0026gt; /tmp/alertmanager-default.yaml   Create a secret and an alert configuration that will be used to add email notifications for the alerts.\nCreate these files on ncn-w001.\ncat \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; \u0026gt; /tmp/alertmanager-secret.yaml apiVersion: v1 data: alertmanager.yaml: ALERTMANAGER_CONFIG kind: Secret metadata: labels: app: prometheus-operator-alertmanager chart: prometheus-operator-8.15.4 heritage: Tiller release: cray-sysmgmt-health name: alertmanager-cray-sysmgmt-health-promet-alertmanager namespace: sysmgmt-health type: Opaque EOF In the following example file, the Gmail SMTP server is used in this example to relay the notification to receiver-email@yourcompany.com. Update the fields under email_configs: accordingly before running the following command.\ncat \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; \u0026gt; /tmp/alertmanager-new.yaml global: resolve_timeout: 5m route: group_by: - job group_interval: 5m group_wait: 30s receiver: \u0026#34;null\u0026#34; repeat_interval: 12h routes: - match: alertname: Watchdog receiver: \u0026#34;null\u0026#34; - match: alertname: PostgresqlReplicationLagSMA receiver: email-alert - match: alertname: PostgresqlReplicationLagServices receiver: email-alert - match: alertname: PostgresqlFollowerReplicationLagSMA receiver: email-alert - match: alertname: PostgresqlFollowerReplicationLagServices receiver: email-alert receivers: - name: \u0026#34;null\u0026#34; - name: email-alert email_configs: - to: receiver-email@yourcompany.com from: sender-email@gmail.com # Your smtp server address smarthost: smtp.gmail.com:587 auth_username: sender-email@gmail.com auth_identity: sender-email@gmail.com auth_password: xxxxxxxxxxxxxxxx EOF   Replace the alert notification configuration based on the files created in the previous step.\nncn-w001# sed \u0026#34;s/ALERTMANAGER_CONFIG/$(cat /tmp/alertmanager-new.yaml \\ | base64 -w0)/g\u0026#34; /tmp/alertmanager-secret.yaml \\ | kubectl replace --force -f -   Validate the configuration changes.\nncn-w001# kubectl exec alertmanager-cray-sysmgmt-health-promet-alertmanager-0 \\ -n sysmgmt-health -c alertmanager -- cat /etc/alertmanager/config/alertmanager.yaml   Check the logs for any errors if the configuration does not look accurate.\nncn-w001# kubectl logs -f -n sysmgmt-health \\ pod/alertmanager-cray-sysmgmt-health-promet-alertmanager-0 alertmanager   An email notification will be sent once either of the alerts set in this procedure is FIRING in Prometheus. See https://prometheus.SYSTEM-NAME.SITE-DOMAIN/alerts for more information.\nIf an alert is received, refer to Troubleshoot Postgres Database section for more information about recovering replication.\n"
},
{
	"uri": "/docs-csm/en-11/operations/system_management_health/system_management_health/",
	"title": "System Management Health",
	"tags": [],
	"description": "",
	"content": "System Management Health The primary goal of the System Management Health service is to enable system administrators to assess the health of their system. Operators need to quickly and efficiently troubleshoot system issues as they occur and be confident that a lack of issues indicates the system is operating normally. This service currently runs as a Helm chart on the system\u0026rsquo;s management Kubernetes cluster and monitors the health status of core system components, triggering alerts as potential issues are observed. It uses Prometheus to aggregate metrics from etcd, Kubernetes, Istio, and Ceph, all of which include support for the Prometheus API. The System Management Health service relies on the following tools:\n Prometheus is the standard cloud-native metrics and monitoring tool, which includes Alertmanager, a tool that handles alert duplication, silences, and notifications The Prometheus operator provides custom resource definitions (CRDs) that make it easy to operate Prometheus and Alertmanager instances, scrape metrics from service endpoints, and trigger alerts Grafana supports pulling data from Prometheus, and dashboards for system components are readily available from the open source community The stable/prometheus-operator Helm chart integrates the Prometheus operator, Prometheus, Alertmanager, Grafana, node exporters (daemon set), and kube-state-metrics to provide a monitoring solution for Kubernetes clusters Istio supports service mesh tracing and observability using Jaeger and Kiali, respectively  The System Management Health service is intended to complement the System Monitoring Application (SMA) Framework, but the two are currently not integrated. The System Management Health metrics are not available using the Telemetry API. This service scrapes metrics from system components like Ceph, Kubernetes, and the hosts using node exporter, kube-state-metrics, and cadvisor. The design is flexible and supports:\n Filtering metrics such that only those necessary to determine system health are aggregated to the top level; all metrics are currently aggregated—no filtering is implemented Independent retention and persistence settings based on needs for specific services; the current default configuration retains metrics for ten days at the top level and four hours at intermediate levels Component-specific tooling for more detailed visibility:  Grafana dashboards for Kubernetes Grafana dashboards, Kiali, and Jaeger for Istio Grafana dashboards for Ceph    "
},
{
	"uri": "/docs-csm/en-11/operations/system_management_health/system_management_health_checks_and_alerts/",
	"title": "System Management Health Checks And Alerts",
	"tags": [],
	"description": "",
	"content": "System Management Health Checks and Alerts A health check corresponds to a Prometheus query against metrics aggregated to the Prometheus instance. Core platform components like Kubernetes and Istio collect service-related metrics by default, which enables the System Management Health service to implement generic service health checks without custom instrumentation. Health checks are intended to be coarse-grained and comprehensive, as opposed to fine-grained and exhaustive. Health checks related to infrastructure adhere to the Utilization Saturation Errors (USE) method whereas services follow the Rate Errors Duration (RED) method.\nPrometheus alerting rules periodically evaluate health checks and trigger alerts to Alertmanager, which manages silencing, inhibition, aggregation, and sending out notifications. Alertmanager supports a number of notification options, but the most relevant ones are listed below:\n Email - Sends notification emails periodically regarding alerts Slack - Publishes notifications to a Slack channel Web hook- Send an HTTP request to a configurable URL (requires custom integration)  Similar to Prometheus metrics, alerts use labels to identify a particular dimensional instantiation, and the Alertmanager dashboard enables operators to preemptively silence alerts based on them.\nCheck Active Alerts from NCNs Prometheus includes the /api/v1/alerts endpoint, which returns a JSON object containing the active alerts. From a non-compute node (NCN), can connect to sysmgmt-health/cray-sysmgmt-health-promet-prometheus directly and bypass service authentication and authorization.\nObtain the cluster IP address:\nncn-w001# kubectl -n sysmgmt-health get svc cray-sysmgmt-health-promet-prometheus NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE cray-sysmgmt-health-promet-prometheus ClusterIP 10.16.201.80 \u0026lt;none\u0026gt; 9090/TCP 2d6h Get active alerts, which includes KubeletTooManyPods if it is going off:\nncn-w001# curl -s http://CLUSTER-IP:PORT/api/v1/alerts | jq . | grep -B 10 -A 20 KubeletTooManyPods { \u0026#34;status\u0026#34;: \u0026#34;success\u0026#34;, \u0026#34;data\u0026#34;: { \u0026#34;alerts\u0026#34;: [ { \u0026#34;labels\u0026#34;: { \u0026#34;alertname\u0026#34;: \u0026#34;KubeletTooManyPods\u0026#34;, \u0026#34;endpoint\u0026#34;: \u0026#34;https-metrics\u0026#34;, \u0026#34;instance\u0026#34;: \u0026#34;10.252.1.6:10250\u0026#34;, \u0026#34;job\u0026#34;: \u0026#34;kubelet\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;kube-system\u0026#34;, \u0026#34;node\u0026#34;: \u0026#34;ncn-w003\u0026#34;, \u0026#34;prometheus\u0026#34;: \u0026#34;kube-monitoring/cray-prometheus-operator-prometheus\u0026#34;, \u0026#34;prometheus_replica\u0026#34;: \u0026#34;prometheus-cray-prometheus-operator-prometheus-0\u0026#34;, \u0026#34;service\u0026#34;: \u0026#34;cray-prometheus-operator-kubelet\u0026#34;, \u0026#34;severity\u0026#34;: \u0026#34;warning\u0026#34; }, \u0026#34;annotations\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;Kubelet 10.252.1.6:10250 is running 107 Pods, close to the limit of 110.\u0026#34;, \u0026#34;runbook_url\u0026#34;: \u0026#34;https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubelettoomanypods\u0026#34; }, \u0026#34;state\u0026#34;: \u0026#34;firing\u0026#34;, \u0026#34;activeAt\u0026#34;: \u0026#34;2020-01-11T18:13:35.086499854Z\u0026#34;, \u0026#34;value\u0026#34;: 107 }, { \u0026#34;labels\u0026#34;: { In the example above, the alert actually indicates it is getting close to the limit, but the value included in the alert is the actual number of pods on ncn-w003.\nTroubleshooting: If an alert titled KubeCronJobRunning is encountered, this could be an indication that a Kubernetes cronjob is misbehaving. The Labels section under the firing alert will indicate the name of the cronjob that is taking longer than expected to complete. Refer to the \u0026ldquo;CHECK CRON JOBS\u0026rdquo; header in the Power On and Start the Management Kubernetes Cluster procedure for instructions on how to troubleshoot the cronjob, as well as how to restart (export and reapply) the cronjob.\n"
},
{
	"uri": "/docs-csm/en-11/operations/system_management_health/troubleshoot_prometheus_alerts/",
	"title": "Troubleshoot Prometheus Alerts",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Prometheus Alerts General Prometheus Alert Troubleshooting Topics\n PostgresqlFollowerReplicationLagSMA PostgresqlHighRollbackRate PostgresqlInactiveReplicationSlot PostgresqlNotEnoughConnections CPUThrottlingHigh  PostgresqlFollowerReplicationLagSMA Alerts for PostgresqlFollowerReplicationLagSMA on sma-postgres-cluster pods with slot_name=\u0026ldquo;permanent_physical_1\u0026rdquo; can be ignored. This slot_name is disabled and will be removed in a future release.\nPostgresqlHighRollbackRate Alerts for PostgresqlHighRollbackRate on spire-postgres pods can be ignored. This is caused by an idle session that requires a timeout. This will be fixed in a future release.\nPostgresqlInactiveReplicationSlot Alerts for PostgresqlInactiveReplicationSlot on sma-postgres-cluster pods with slot_name=\u0026ldquo;permanent_physical_1\u0026rdquo; can be ignored. This slot_name is disabled and will be removed in a future release.\nPostgresqlNotEnoughConnections Alerts for PostgresqlNotEnoughConnections for datname=\u0026ldquo;foo\u0026rdquo; and datname=\u0026ldquo;bar\u0026rdquo; can be ignored. These databases are not used and will be removed in a future release.\nCPUThrottlingHigh Alerts for CPUThrottlingHigh on gatekeeper-audit can be ignored. This pod is not utilized in this release.\nAlerts for CPUThrottlingHigh on CFS services such as cfs-batcher and cfs-trust can be ignored. Because CFS is idle most of the time these services have low CPU requests, and it is normal for CFS service resource usage to spike when it is in use.\n"
},
{
	"uri": "/docs-csm/en-11/operations/system_management_health/access_system_management_health_services/",
	"title": "Access System Management Health Services",
	"tags": [],
	"description": "",
	"content": "Access System Management Health Services All System Management Health services are exposed outside the cluster through the Keycloak gatekeeper and Istio\u0026rsquo;s ingress gateway to enforce the authentication and authorization policies. The URLs to access these services are available on any system with CAN, BGP, MetalLB, and external DNS properly configured.\nThe {{shasta_domain}} value in the examples below is an Ansible variable defined as follows and is expected to be the systems' FQDN from the CAN.\nncn-m001# kubectl get secret site-init -n loftsman -o jsonpath='{.data.customizations\\.yaml}' \\ | base64 -d | grep \u0026quot;external:\u0026quot; external: SHASTA_EXTERNAL_DOMAIN This procedure enables administrators to set up the service and access its components via the Grafana and Kiali applications.\nPrerequisites  Access to the System Management Health web UIs is through Istio\u0026rsquo;s ingress gateway and requires clients (browsers) to set the appropriate HTTP Host header to route traffic to the desired service. This procedure requires administrative privileges on the workstation running the user\u0026rsquo;s web browser. The Customer Access Network (CAN), Border Gateway Protocol (BGP), MetalLB, and external DNS are properly configured.  Procedure   Access any System Management Health service with the provided links.\nWhen accessing the URLs listed below, it will be necessary to accept one or more browser security warnings in order to proceed to the login screen and navigate through the application after successfully logging in. The details of the security warning will indicate that a self-signed certificate/unknown issuer is being used for the site. Support for incorporation of certificates from Trusted Certificate Authorities is planned for a future release.\n  https://prometheus.{{shasta_domain}}/\nCentral Prometheus instance scrapes metrics from Kubernetes, Ceph, and the hosts (part of prometheus-operator Helm chart).\nPrometheus generates alerts based on metrics and reports them to the alertmanager. The \u0026lsquo;Alerts\u0026rsquo; link at the top of the page will show all of the inactive, pending, and firing alerts on the system. Clicking on any of the alerts will expand them, enabling users to use the \u0026lsquo;Labels\u0026rsquo; data to discern the details of the alert. The details will also show the state of the alert, how long it has been active, and the value for the alert.\nFor more information regarding the use of the Prometheus interface, see https://prometheus.io/docs/prometheus/latest/getting_started/.\n  https://alertmanager.{{shasta_domain}}/\nCentral Alertmanager instance that manages prometheus alerts.\nThe alertmanager manages the alerts it receives and generates notifications to users or applications. For more information about alert-manager, refer to the following documentation: https://prometheus.io/docs/prometheus/latest/getting_started/.\n  https://grafana.{{shasta_domain}}/\nCentral Grafana instance that includes numerous dashboards for visualizing metrics from prometheus and prometheus-istio.\nFor more information about Grafana\u0026rsquo;s features and dashboard creation, please see the online documentation here: https://grafana.com/docs/grafana/latest/.\nFor a description of the Grafana Panel: https://grafana.com/docs/grafana/latest/features/panels/panels/.\nFor a description of the Grafana Dashboard: https://grafana.com/docs/grafana/latest/features/dashboard/dashboards/.\n  https://kiali-istio.{{shasta_domain}}/\nKiali provides real-time introspection into the Istio service mesh using metrics from prometheus-istio and traces from jaeger-istio.\nFor more information about the features of this interface, refer to the following documentation: https://kiali.io/documentation/.\n  https://jaeger-istio.{{shasta_domain}}/\nJaeger provides distributed tracing of requests across micro-services based on headers automatically injected by Envoy.\nFor more information regarding the jaeger-istio front end/UI configuration, refer to the online documentation (https://www.jaegertracing.io/). Click on the \u0026lsquo;Docs\u0026rsquo; section to get more information around the Jaeger Frontend/UI.\n  Additional components are also exposed, though only for convenience. Do not rely on these components to always be available:\n  https://prometheus-istio.{{shasta_domain}}/\nPrometheus instance that collects Istio metrics (included as part of istio Helm chart).\nFor more information regarding the use of the Prometheus interface, see https://prometheus.io/docs/alerting/overview/.\n    "
},
{
	"uri": "/docs-csm/en-11/operations/system_layout_service/restore_sls_postgres_database_from_backup/",
	"title": "Restore SLS Postgres Database From Backup",
	"tags": [],
	"description": "",
	"content": "Restore SLS Postgres Database from Backup This procedure can be used to restore the SLS Postgres database from a previously taken backup. This can be a manual backup created by the Create a Backup of the SLS Postgres Database procedure, or an automatic backup created by the cray-sls-postgresql-db-backup Kubernetes cronjob.\nPrerequisites   Healthy Postgres Cluster.\n Use patronictl list on the SLS Postgres cluster to determine the current state of the cluster, and a healthy cluster will look similar to the following:\nncn# kubectl exec cray-sls-postgres-0 -n services -c postgres -it -- patronictl list + Cluster: cray-sls-postgres (6975238790569058381) ---+----+-----------+ | Member | Host | Role | State | TL | Lag in MB | +---------------------+------------+--------+---------+----+-----------+ | cray-sls-postgres-0 | 10.44.0.40 | Leader | running | 1 | | | cray-sls-postgres-1 | 10.36.0.37 | | running | 1 | 0 | | cray-sls-postgres-2 | 10.42.0.42 | | running | 1 | 0 | +---------------------+------------+--------+---------+----+-----------+    Previously taken backup of the SLS Postgres cluster either a manual or automatic backup.\n Check for any available automatic SLS Postgres backups:\nncn# cray artifacts list postgres-backup --format json | jq -r \u0026#39;.artifacts[].Key | select(contains(\u0026#34;sls\u0026#34;))\u0026#39; cray-sls-postgres-2021-07-11T23:10:08.manifest cray-sls-postgres-2021-07-11T23:10:08.psql    Procedure   Retrieve a previously taken SLS Postgres backup. This can be either a previously taken manual SLS backup or an automatic Postgres backup in the postgres-backup S3 bucket.\n  From a previous manual backup:\n  Copy over the folder or tarball containing the Postgres back up to be restored. If it is a tarball extract it.\n  Set the environment variable POSTGRES_SQL_FILE to point toward the .psql file in the backup folder:\nncn# export POSTGRES_SQL_FILE=/root/cray-sls-postgres-backup_2021-07-07_16-39-44/cray-sls-postgres-backup_2021-07-07_16-39-44.psql   Set the environment variable POSTGRES_SECRET_MANIFEST to point toward the .manifest file in the backup folder:\nncn# export POSTGRES_SECRET_MANIFEST=/root/cray-sls-postgres-backup_2021-07-07_16-39-44/cray-sls-postgres-backup_2021-07-07_16-39-44.manifest     From a previous automatic Postgres backup:\n  Check for available backups:\nncn# cray artifacts list postgres-backup --format json | jq -r \u0026#39;.artifacts[].Key | select(contains(\u0026#34;sls\u0026#34;))\u0026#39; cray-sls-postgres-2021-07-11T23:10:08.manifest cray-sls-postgres-2021-07-11T23:10:08.psql Then set the following environment variables for the name of the files in the backup:\nncn# export POSTGRES_SECRET_MANIFEST_NAME=cray-sls-postgres-2021-07-11T23:10:08.manifest ncn# export POSTGRES_SQL_FILE_NAME=cray-sls-postgres-2021-07-11T23:10:08.psql   Download the .psql file for the postgres backup:\nncn# cray artifacts get postgres-backup \u0026#34;$POSTGRES_SQL_FILE_NAME\u0026#34; \u0026#34;$POSTGRES_SQL_FILE_NAME\u0026#34;   Download the .manifest file for the SLS backup:\nncn# cray artifacts get postgres-backup \u0026#34;$POSTGRES_SECRET_MANIFEST_NAME\u0026#34; \u0026#34;$POSTGRES_SECRET_MANIFEST_NAME\u0026#34;   Setup environment variables pointing to the full path of the .psql and .manifest files:\nncn# export POSTGRES_SQL_FILE=$(realpath \u0026#34;$POSTGRES_SQL_FILE_NAME\u0026#34;) ncn# export POSTGRES_SECRET_MANIFEST=$(realpath \u0026#34;$POSTGRES_SECRET_MANIFEST_NAME\u0026#34;)       Verify the POSTGRES_SQL_FILE and POSTGRES_SECRET_MANIFEST environment variables are set correctly:\nncn# echo \u0026#34;$POSTGRES_SQL_FILE\u0026#34; /root/cray-sls-postgres-backup_2021-07-07_16-39-44/cray-sls-postgres-backup_2021-07-07_16-39-44.psql ncn# echo \u0026#34;$POSTGRES_SECRET_MANIFEST\u0026#34; /root/cray-sls-postgres-backup_2021-07-07_16-39-44/cray-sls-postgres-backup_2021-07-07_16-39-44.manifest   Re-run the SLS loader job:\nncn# kubectl -n services get job cray-sls-init-load -o json | jq \u0026#39;del(.spec.selector)\u0026#39; | jq \u0026#39;del(.spec.template.metadata.labels.\u0026#34;controller-uid\u0026#34;)\u0026#39; | kubectl replace --force -f - Wait for the job to complete:\nncn# kubectl wait -n services job cray-sls-init-load --for=condition=complete --timeout=5m   Determine leader of the Postgres cluster:\nncn# export POSTGRES_LEADER=$(kubectl exec cray-sls-postgres-0 -n services -c postgres -t -- patronictl list -f json | jq -r \u0026#39;.[] | select(.Role == \u0026#34;Leader\u0026#34;).Member\u0026#39;) Check the environment variable to see the current leader of the Postgres cluster:\nncn# echo $POSTGRES_LEADER cray-sls-postgres-0   Determine the database schema version of the currently running SLS database and verify that it matches the database schema version from the Postgres backup:\nDatabase schema of the currently running SLS Postgres instance.\nncn# kubectl exec $POSTGRES_LEADER -n services -c postgres -it -- bash -c \u0026#34;psql -U slsuser -d sls -c \u0026#39;SELECT * FROM schema_migrations\u0026#39;\u0026#34; version | dirty ---------+------- 3 | f (1 row)  The output above shows the database schema is at version 3.\n Database schema version from the Postgres backup:\nncn# cat \u0026#34;$POSTGRES_SQL_FILE\u0026#34; | grep \u0026#34;COPY public.schema_migrations\u0026#34; -A 2 COPY public.schema_migrations (version, dirty) FROM stdin; 3 f \\.  The output above shows the database schema is at version 3.\n If the database schema versions match, proceed to the next step. Otherwise, the Postgres backup taken is not applicable to the currently running instance of SLS.\nWARNING: If the database schema versions do not match the version of the SLS deployed will need to be either upgraded/downgraded to a version with a compatible database schema version. Ideally to the same version of SLS that was used to create the Postgres backup.\n  Restore the database from the backup using the restore_sls_postgres_from_backup.sh script. This script requires the POSTGRES_SQL_FILE and POSTGRES_SECRET_MANIFEST environment variables to be set.\n THIS WILL DELETE AND REPLACE THE CURRENT CONTENTS OF THE SLS DATABASE\n ncn# /usr/share/doc/csm/operations/system_layout_service/scripts/restore_sls_postgres_from_backup.sh   Verify the health of the SLS Postgres cluster by running the ncnPostgresHealthChecks.sh script. Follow the ncnPostgresHealthChecks topic in Validate CSM Health document.\n  Verify that the service is functional:\nncn# cray sls version list Counter = 5 LastUpdated = \u0026#34;2021-04-05T22:51:36.575276Z\u0026#34; Get the number of hardware objects stored in SLS:\nncn# cray sls hardware list --format json | jq .[].Xname | wc -l 37 Get the name of networks stored in SLS:\n If the system does not have liquid cooled hardware, the HMN_MTN and NMN_MTN networks may not be present.\n ncn# cray sls networks list --format json | jq -r .[].Name HMN_MTN HMN_RVR NMNLB NMN NMN_MTN NMN_RVR CAN HMN HMNLB HSN MTL   "
},
{
	"uri": "/docs-csm/en-11/operations/system_layout_service/restore_sls_postgres_without_an_existing_backup/",
	"title": "Restore SLS Postgres Without An Existing Backup",
	"tags": [],
	"description": "",
	"content": "Restore SLS Postgres without an Existing Backup This procedure is intended to repopulate SLS in the event when no Postgres backup exists.\nPrerequisite  Healthy SLS Service.  Verify all 3 SLS replicas are up and running:\nncn# kubectl -n services get pods -l cluster-name=cray-sls-postgres NAME READY STATUS RESTARTS AGE cray-sls-postgres-0 3/3 Running 0 18d cray-sls-postgres-1 3/3 Running 0 18d cray-sls-postgres-2 3/3 Running 0 18d    Procedure   Retrieve the initial sls_input_file.json that was used to initially install the system with from sls S3 bucket.\nncn# cray artifacts get sls sls_input_file.json sls_input_file.json   Perform a SLS load state operation to replace the contents of SLS with the data from the sls_input_file.json file.\nGet an API Token:\nncn# export TOKEN=$(curl -s -S -d grant_type=client_credentials \\  -d client_id=admin-client \\  -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\  https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39;) Perform the load state operation:\nncn# curl -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X POST -F sls_dump=@sls_input_file.json \\  https://api-gw-service-nmn.local/apis/sls/v1/loadstate   Any previously made customizations made to SLS will need to be applied again. This includes any SLS API operations that modified the state of SLS.\n  The HSN network in SLS will be missing HSN subnet data, this data will need to be repopulated again using the \u0026ldquo;Set up DNS for HSN IP addresses\u0026rdquo; procedure in the Slingshot Operations Guide.\n  Any hardware that was added or moved in the system using one of the following procedures will need to be performed again.\n Add a Standard Rack Node Move a Standard Rack Node Same Rack/Same HSN Ports Move a Standard Rack Node      "
},
{
	"uri": "/docs-csm/en-11/operations/system_layout_service/system_layout_service_sls/",
	"title": "System Layout Service (SLS)",
	"tags": [],
	"description": "",
	"content": "System Layout Service (SLS) The System Layout Service (SLS) holds information about the system design, such as the physical locations of network hardware, compute nodes, and cabinets. It also stores information about the network, such as which port on which switch should be connected to each compute node.\nSLS stores a generalized abstraction of the system that other services can access. The Hardware State Manager (HSM) keeps track of information for hardware state or identifiers. SLS does not need to change as hardware within the system is replaced.\nInteraction with SLS is required if the system setup changes. For example, if system cabling is altered, or if the system is expanded or reduced. SLS does not interact with the hardware. Interaction with SLS should occur only during system installation, expansion, and contraction.\nSLS is responsible for the following:\n Providing an HTTP API to access site information Storing a list of all hardware Storing a list of all network links Storing a list of all power links  "
},
{
	"uri": "/docs-csm/en-11/operations/system_layout_service/update_sls_with_uan_aliases/",
	"title": "Update SLS With UAN Aliases",
	"tags": [],
	"description": "",
	"content": "Update SLS with UAN Aliases This guide shows the process for manually adding an alias to a UAN in SLS and ensuring that the node is being monitored by conman for console logs.\nPrerequisites  SLS is up and running and has been populated with data. Access to the API gateway api-gw-service (legacy: api-gw-service-nmn.local)  Procedure   Authenticate with Keycloak to obtain an API token:\nexport TOKEN=$(curl -k -s -S -d grant_type=client_credentials \\ -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; \\ | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token \\ | jq -r \u0026#39;.access_token\u0026#39;)   Find the xname of the UAN by searching through all Application nodes until found.\ncurl -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; \\ \u0026#34;https://api-gw-service-nmn.local/apis/sls/v1/search/hardware?extra_properties.Role=Application\u0026#34; \\ | jq This will return an array of application nodes currently known in SLS:\nncn-w001# curl -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; \\ \u0026#34;https://api-gw-service-nmn.local/apis/sls/v1/search/hardware?extra_properties.Role=Application\u0026#34; \\ | jq [ { \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0s19b0\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0s19b0n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;LastUpdated\u0026#34;: 1606332877, \u0026#34;LastUpdatedTime\u0026#34;: \u0026#34;2020-11-25 19:34:37.183293 +0000 +0000\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;Role\u0026#34;: \u0026#34;Application\u0026#34;, \u0026#34;SubRole\u0026#34;: \u0026#34;UAN\u0026#34; } } ]   Update the UAN object in SLS by adding Aliases array with the UAN\u0026rsquo;s hostname.\nReplace UAN_XNAME in the URL and JSON object with the UAN\u0026rsquo;s xname. Replace UAN_PARENT_XNAME in the JSON object with the UAN\u0026rsquo;s parent xname. Replace UAN_ALIAS in the Aliases array with the UAN\u0026rsquo;s hostname. The LastUpdated and LastUpdatedTime fields are not required to be in the PUT payload.\ncurl -X PUT -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; \\ \u0026#34;https://api-gw-service-nmn.local/apis/sls/v1/hardware/\u0026lt;UAN_XNAME\u0026gt;\u0026#34; -d \u0026#39; { \u0026#34;Parent\u0026#34;: \u0026#34;UAN_PARENT_XNAME\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;UAN_XNAME\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;Role\u0026#34;: \u0026#34;Application\u0026#34;, \u0026#34;SubRole\u0026#34;: \u0026#34;UAN\u0026#34;, \u0026#34;Aliases\u0026#34;: [\u0026#34;UAN_ALIAS\u0026#34;] } }\u0026#39; Using the response from the previous step, we can build the following command. The hostname for this uan is uan01, and this is reflected in the added Aliases field in the UAN\u0026rsquo;s ExtraProperties.\ncurl -X PUT -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; \\ \u0026#34;https://api-gw-service-nmn.local/apis/sls/v1/hardware/x3000c0s19b0n0\u0026#34; -d \u0026#39; { \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0s19b0\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0s19b0n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;Role\u0026#34;: \u0026#34;Application\u0026#34;, \u0026#34;SubRole\u0026#34;: \u0026#34;UAN\u0026#34;, \u0026#34;Aliases\u0026#34;: [\u0026#34;uan01\u0026#34;] } }\u0026#39; Example response:\nncn-w001# curl -X PUT -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; \\ \u0026gt;\u0026#34;https://api-gw-service-nmn.local/apis/sls/v1/hardware/x3000c0s19b0n0\u0026#34; -d \u0026#39; \u0026gt; { \u0026gt; \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0s19b0\u0026#34;, \u0026gt; \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0s19b0n0\u0026#34;, \u0026gt; \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026gt; \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026gt; \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026gt; \u0026#34;ExtraProperties\u0026#34;: { \u0026gt; \u0026#34;Role\u0026#34;: \u0026#34;Application\u0026#34;, \u0026gt; \u0026#34;SubRole\u0026#34;: \u0026#34;UAN\u0026#34;, \u0026gt; \u0026#34;Aliases\u0026#34;: [\u0026#34;uan01\u0026#34;] \u0026gt; } \u0026gt; }\u0026#39; {\u0026#34;Parent\u0026#34;:\u0026#34;x3000c0s19b0\u0026#34;,\u0026#34;Xname\u0026#34;:\u0026#34;x3000c0s19b0n0\u0026#34;,\u0026#34;Type\u0026#34;:\u0026#34;comptype_node\u0026#34;,\u0026#34;Class\u0026#34;:\u0026#34;River\u0026#34;,\u0026#34;TypeString\u0026#34;:\u0026#34;Node\u0026#34;,\u0026#34;LastUpdated\u0026#34;:1606332877,\u0026#34;LastUpdatedTime\u0026#34;:\u0026#34;2020-11-25 19:34:37.183293 +0000 +0000\u0026#34;,\u0026#34;ExtraProperties\u0026#34;:{\u0026#34;Aliases\u0026#34;:[\u0026#34;uan01\u0026#34;],\u0026#34;Role\u0026#34;:\u0026#34;Application\u0026#34;,\u0026#34;SubRole\u0026#34;:\u0026#34;UAN\u0026#34;}} After a few minutes the -mgmt name should begin resolving. Communication with the BMC should be available via the alias uan01-mgmt.\n  Confirm that the BMC for the UAN is up and running at the aliased address.\nncn-w001# ping -c 4 uan01-mgmt PING uan01-mgmt (10.254.2.53) 56(84) bytes of data. 64 bytes from x3000c0s19b0 (10.254.2.53): icmp_seq=1 ttl=255 time=0.170 ms 64 bytes from x3000c0s19b0 (10.254.2.53): icmp_seq=2 ttl=255 time=0.228 ms 64 bytes from x3000c0s19b0 (10.254.2.53): icmp_seq=3 ttl=255 time=0.311 ms 64 bytes from x3000c0s19b0 (10.254.2.53): icmp_seq=4 ttl=255 time=0.240 ms --- uan01-mgmt ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3061ms rtt min/avg/max/mdev = 0.170/0.237/0.311/0.051 ms When this node boots, the DHCP request of its -nmn interface will cause the uan01 to be created and resolved.\n  Confirm that the UAN is being monitored by the console services. Follow the procedure in Manage Node Consoles.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/system_layout_service/add_uan_can_ip_addresses_to_sls/",
	"title": "Add UAN CAN Ip Addresses To SLS",
	"tags": [],
	"description": "",
	"content": "Add UAN CAN IP Addresses to SLS Add the Customer Access Network (CAN) IP addresses for User Access Nodes (UANs) to the IP address reservations in the System Layout Service (SLS). Adding these IP addresses will propagate the data needed for the Domain Name Service (DNS).\nFor more information on CAN IP addresses, refer to the Customer Access Network (CAN).\nPrerequisites This procedure requires administrative privileges.\nProcedure   Retrieve the SLS data for the CAN.\nncn-m001# export TOKEN=$(curl -s -k -S -d grant_type=client_credentials \\ -d client_id=admin-client -d client_secret=`kubectl get secrets admin-client-auth \\ -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token \\ | jq -r \u0026#39;.access_token\u0026#39;) ncn-m001# curl -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; \\ https://api-gw-service-nmn.local/apis/sls/v1/networks/CAN|jq \u0026gt; CAN.json ncn-m001# cp CAN.json CAN.json.bak   Edit the CAN.json file and add the desired UAN CAN IP addresses in the ExtraProperties.Subnets section.\nThis subsection is located under the CAN Bootstrap DHCP Subnet section. The IP address reservations array needs to be added in the following JSON format:\n{ \u0026#34;Comment\u0026#34;: \u0026#34;x3000c0s23b0n0\u0026#34;, \u0026#34;IPAddress\u0026#34;: \u0026#34;10.103.2.20\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;uan01\u0026#34; }, If multiple alias are required, the JSON format would be:\n{ \u0026#34;Aliases\u0026#34;: [ \u0026#34;uan01-can\u0026#34;, \u0026#34;uan01-slurm\u0026#34; ], \u0026#34;Comment\u0026#34;: \u0026#34;x3000c0s23b0n0\u0026#34;, \u0026#34;IPAddress\u0026#34;: \u0026#34;10.103.2.20\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;uan01\u0026#34; }, IMPORTANT: There must be an alias or name defined in a format that matches the hostname of the UAN. This is required by the CFS play uan_interfaces that configures the CAN interface on UANs. If the CAN is not being configured for a particular UAN, then this requirement is not needed.\n  Upload the updated CAN.json file to SLS.\nncn-m001# curl -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; --header \\ \u0026#34;Content-Type: application/json\u0026#34; --request PUT --data @CAN.json \\ https://api-gw-service-nmn.local/apis/sls/v1/networks/CAN   Verify that DNS records were created.\nIt will take about five minutes before any records will show up.\nFor example:\nncn-m001:~ # nslookup uan01.can Server:\t10.92.100.225 Address:\t10.92.100.225#53 Name:\tuan01.can Address: 10.103.2.24 ncn-m001:~ # nslookup uan01-can.can Server:\t10.92.100.225 Address:\t10.92.100.225#53 Name:\tuan01-can.can Address: 10.103.2.24 As stated above, the UAN play uan_interfaces will attempt to nslookup the hostname of the node with with \u0026ldquo;.can\u0026rdquo; appended. Make sure this alias resolves if the CAN is going to be configured on that particular UAN. In certain upgrade scenarios, the expected alias may not have been added by default.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/system_layout_service/create_a_backup_of_the_sls_postgres_database/",
	"title": "Create A Backup Of The SLS Postgres Database",
	"tags": [],
	"description": "",
	"content": "Create a Backup of the SLS Postgres Database Perform a manual backup of the contents of the SLS Postgres database. This backup can be used to restore the contents of the SLS Postgres database at a later point in time using the Restoring SLS Postgres cluster from backup procedure.\nPrerequisites  Healthy SLS Postgres Cluster.  Use patronictl list on the SLS Postgres cluster to determine the current state of the cluster, and a healthy cluster will look similar to the following:\nncn# kubectl exec cray-sls-postgres-0 -n services -c postgres -it -- patronictl list + Cluster: cray-sls-postgres (6975238790569058381) ---+----+-----------+ | Member | Host | Role | State | TL | Lag in MB | +---------------------+------------+--------+---------+----+-----------+ | cray-sls-postgres-0 | 10.44.0.40 | Leader | running | 1 | | | cray-sls-postgres-1 | 10.36.0.37 | | running | 1 | 0 | | cray-sls-postgres-2 | 10.42.0.42 | | running | 1 | 0 | +---------------------+------------+--------+---------+----+-----------+   Healthy SLS Service.  Verify all 3 SLS replicas are up and running:\nncn# kubectl -n services get pods -l cluster-name=cray-sls-postgres NAME READY STATUS RESTARTS AGE cray-sls-postgres-0 3/3 Running 0 18d cray-sls-postgres-1 3/3 Running 0 18d cray-sls-postgres-2 3/3 Running 0 18d    Procedure   Create a directory to store the SLS backup files in:\nncn# BACKUP_LOCATION=\u0026#34;/root\u0026#34; ncn# export BACKUP_NAME=\u0026#34;cray-sls-postgres-backup_`date \u0026#39;+%Y-%m-%d_%H-%M-%S\u0026#39;`\u0026#34; ncn# export BACKUP_FOLDER=\u0026#34;${BACKUP_LOCATION}/${BACKUP_NAME}\u0026#34; ncn# mkdir -p \u0026#34;$BACKUP_FOLDER\u0026#34; The SLS backup will be located at the following directory:\nncn# echo $BACKUP_FOLDER /root/cray-sls-postgres-backup_2021-07-07_16-39-44   Run the backup_sls_postgres.sh script to take a backup of the SLS Postgres:\nncn# /usr/share/doc/csm/operations/system_layout_service/scripts/backup_sls_postgres.sh ~/cray-sls-postgres-backup_2021-07-07_16-39-44 ~ SLS postgres backup file will land in /root/cray-sls-postgres-backup_2021-07-07_16-39-44 Determining the postgres leader... The SLS postgres leader is cray-sls-postgres-0 Using pg_dumpall to dump the contents of the SLS database... PSQL dump is available at /root/cray-sls-postgres-backup_2021-07-07_16-39-44/cray-sls-postgres-backup_2021-07-07_16-39-44.psql Saving Kubernetes secret service-account.cray-sls-postgres.credentials Saving Kubernetes secret slsuser.cray-sls-postgres.credentials Saving Kubernetes secret postgres.cray-sls-postgres.credentials Saving Kubernetes secret standby.cray-sls-postgres.credentials Removing extra fields from service-account.cray-sls-postgres.credentials.yaml Removing extra fields from slsuser.cray-sls-postgres.credentials.yaml Removing extra fields from postgres.cray-sls-postgres.credentials.yaml Removing extra fields from standby.cray-sls-postgres.credentials.yaml Adding Kubernetes secret service-account.cray-sls-postgres.credentials to secret manifest Adding Kubernetes secret slsuser.cray-sls-postgres.credentials to secret manifest Adding Kubernetes secret postgres.cray-sls-postgres.credentials to secret manifest Adding Kubernetes secret standby.cray-sls-postgres.credentials to secret manifest Secret manifest is located at /root/cray-sls-postgres-backup_2021-07-07_16-39-44/cray-sls-postgres-backup_2021-07-07_16-39-44.manifest Performing SLS dumpstate... SLS dumpstate is available at /root/cray-sls-postgres-backup_2021-07-07_16-39-44/sls_dump.json SLS Postgres backup is available at: /root/cray-sls-postgres-backup_2021-07-07_16-39-44   Copy the backup folder off of the cluster, and store it in a secure location.\nThe BACKUP_FOLDER environment variable is the name of the folder to backup.\nncn# echo $BACKUP_FOLDER /root/cray-sls-postgres-backup_2021-07-07_16-39-44 Optionally, create a tarball of the Postgres backup files:\nncn# cd $BACKUP_FOLDER \u0026amp;\u0026amp; cd .. ncn# tar -czvf $BACKUP_NAME.tar.gz $BACKUP_NAME   "
},
{
	"uri": "/docs-csm/en-11/operations/system_layout_service/dump_sls_information/",
	"title": "Dump SLS Information",
	"tags": [],
	"description": "",
	"content": "Dump SLS Information Perform a dump of the System Layout Service (SLS) database and an encrypted dump of the credentials stored in Vault.\nThis procedure will create three files in the current directory (private_key.pem, public_key.pem, sls_dump.json). These files should be kept in a safe and secure place as the private key can decrypt the encrypted passwords stored in the SLS dump file.\nThis procedure preserves the information stored in SLS when backing up or reinstalling the system.\nPrerequisites This procedure requires administrative privileges.\nProcedure   Use the get_token function to retrieve a token to validate requests to the API gateway.\nfunction get_token () { ADMIN_SECRET=$(kubectl get secrets admin-client-auth -ojsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d) curl -s -d grant_type=client_credentials -d client_id=admin-client -d client_secret=$ADMIN_SECRET \\  https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | python -c \u0026#39;import sys, json; print json.load(sys.stdin)[\u0026#34;access_token\u0026#34;]\u0026#39; }   Generate a private and public key pair.\nExecute the following commands to generate a private and public key to use for the dump.\nncn-m001# openssl genpkey -out private_key.pem -algorithm RSA -pkeyopt rsa_keygen_bits:2048 ncn-w001# openssl rsa -in private_key.pem -outform PEM -pubout -out public_key.pem The above commands will create two files the private key private_key.pem file and the public key public_key.pem file.\nMake sure to use a new private and public key pair for each dump operation, and do not reuse an existing private and public key pair. The private key should be treated securely because it will be required to decrypt the SLS dump file when the dump is loaded back into SLS. Once the private key is used to load state back into SLS, it should be considered insecure.\n  Perform the SLS dump.\nThe SLS dump will be stored in the sls_dump.json file. The sls_dump.json and private_key.pem files are required to perform the SLS load state operation.\nncn-m001# curl -X POST \\ https://api-gw-service-nmn.local/apis/sls/v1/dumpstate \\ -H \u0026#34;Authorization: Bearer $(get_token)\u0026#34; \\ -F public_key=@public_key.pem \u0026gt; sls_dump.json   "
},
{
	"uri": "/docs-csm/en-11/operations/system_layout_service/load_sls_database_with_dump_file/",
	"title": "Load SLS Database With Dump File",
	"tags": [],
	"description": "",
	"content": "Load SLS Database with Dump File Load the contents of the SLS dump file to restore SLS to the state of the system at the time of the dump. This will upload and overwrite the current SLS database with the contents of the SLS dump file, and update Vault with the encrypted credentials.\nUse this procedure to restore SLS data after a system re-install.\nPrerequisites The System Layout Service (SLS) database has been dumped. See Dump SLS Information for more information.\nProcedure   Use the get_token function to retrieve a token to validate requests to the API gateway.\nfunction get_token () { ADMIN_SECRET=$(kubectl get secrets admin-client-auth -ojsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d) curl -s -d grant_type=client_credentials -d client_id=admin-client -d client_secret=$ADMIN_SECRET \\  https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | python -c \u0026#39;import sys, json; print json.load(sys.stdin)[\u0026#34;access_token\u0026#34;]\u0026#39; }   Load the dump file into SLS.\nThis will upload and overwrite the current SLS database with the contents of the posted file, as well as update the Vault with the encrypted credentials. The private key that was used to generate the SLS dump file is required.\nncn-m001# curl -X POST \\ https://api-gw-service-nmn.local/apis/sls/v1/loadstate \\ -H \u0026#34;Authorization: Bearer $(get_token)\u0026#34; \\ -F sls_dump=@sls_dump.json \\ -F private_key=@private_key.pem After performing the load state operation, the private key should be considered insecure and should no longer be used.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/system_configuration_service/system_configuration_service/",
	"title": "System Configuration Service",
	"tags": [],
	"description": "",
	"content": "System Configuration Service The System Configuration Service (SCSD) allows administrators to set various BMC and controller parameters. These parameters are typically set during discovery, but this tool enables parameters to be set before or after discovery. The operations to change these parameters are available in the Cray CLI under the scsd command.\nThe following are the parameters that most commonly must be set:\n  SSH keys\nIMPORTANT: If the scsd tool is used to update the SSHConsoleKey value outside of ConMan, it will disrupt the ConMan connection to the console and collection of console logs. Refer to ConMan for more information.\n  NTP server\n  Syslog server\n  BMC/Controller passwords\n  The scsd tool includes a REST API to facilitate operations to set parameters. It will contact the Hardware State Manager (HSM) to verify that targets are correct and in a valid hardware state, unless the \u0026ldquo;Force\u0026rdquo; flag is specified. Once it has a list of targets, scsd will perform the needed Redfish operations in parallel using TRS. Any credentials needed will be retrieved from Vault.\nIn all POST operation payloads, there is an optional \u0026ldquo;Force\u0026rdquo; parameter. If this parameter is present and set to \u0026ldquo;true\u0026rdquo;, then HSM will not be contacted and the Redfish operations will be attempted without verifying they are present or in a good state. If the \u0026ldquo;Force\u0026rdquo; option is not present, or is present but set to \u0026ldquo;false\u0026rdquo;, HSM will be used.\nThe specified targets can be BMCs, controller xnames, or HSM group IDs. If BMCs and controllers are grouped in HSM, this service becomes much easier to use because single targets can be used rather than long lists.\nTo view the current build version of the scsd service:\nncn-m001# cray scsd version list Version = \u0026quot;v1.2.3\u0026quot; "
},
{
	"uri": "/docs-csm/en-11/operations/system_configuration_service/configure_bmc_and_controller_parameters_with_scsd/",
	"title": "Configure BMC And Controller Parameters With Scsd",
	"tags": [],
	"description": "",
	"content": "Configure BMC and Controller Parameters with SCSD The System Configuration Service (SCSD) allows administrators to set various BMC and controller parameters for components in liquid-cooled cabinets. These parameters are typically set during discovery, but this tool enables parameters to be set before or after discovery. The operations to change these parameters are available in the cray CLI under the scsd command.\nThe parameters which can be set are:\n  SSH key\n  NTP server\n  Syslog server\n  BMC/Controller passwords\n  SSH console key\nIMPORTANT: If the scsd tool is used to update the SSHConsoleKey value outside of ConMan, it will disrupt the ConMan connection to the console and collection of console logs. See ConMan for more information about remote consoles and collecting console logs.\n  However, this procedure only describes how to change the SSH key to enable passwordless SSH for troubleshooting of power down and power up logs on the node BMCs.\nSee Manage Parameters with the scsd Service for more information about these topics for changing the other parameters.\n Retrieve Current Information from Targets Retrieve Information from a Single target Set Parameters for Targets Set Parameters for a Single BMC or Controller Set Redfish Credentials for Multiple Targets Set Redfish Credentials for a Single Target  The NTP server and syslog server for BMCs in the liquid-cooled cabinet are typically set by MEDS.\nDetails   Save the public SSH key for the root user.\nncn# export SCSD_SSH_KEY=$(cat /root/.ssh/id_rsa.pub | sed \u0026#39;s/[[:space:]]*$//\u0026#39;)   Generate a System Configuration Service configuration via the scsd tool. The admin must be authenticated to the Cray CLI before proceeding.\nncn# cat \u0026gt; scsd_cfg.json \u0026lt;\u0026lt;DATA { \u0026#34;Force\u0026#34;:false, \u0026#34;Targets\u0026#34;: $(cray hsm inventory redfishEndpoints list --format=json | jq \u0026#39;[.RedfishEndpoints[] | .ID]\u0026#39; | sed \u0026#39;s/^/ /\u0026#39;), \u0026#34;Params\u0026#34;:{ \u0026#34;SSHKey\u0026#34;:\u0026#34;$(echo $SCSD_SSH_KEY)\u0026#34; } } DATA   Inspect the generated scsd_cfg.json file.\nEnsure the following are true before running the command below:\n The xname list looks valid/appropriate The SSHKey settings match the desired public key  ncn# cray scsd bmc loadcfg create scsd_cfg.json Check the output to verify all hardware has been set with the correct keys. Passwordless SSH to the root user should now function as expected.\n  Test access to a node controller in the liquid-cooled cabinet.\nSSH into the node controller for the host xname. For example, if the host xname is x1000c1s0b0n0, the node controller xname would be x1000c1s0b0.\nIf the node controller is not powered up, this SSH attempt will fail.\nncn-w001# ssh x1000c1s0b0 x1000c1s0b0:\u0026gt; Notice that the command prompt includes the hostname for this node controller\n  The logs from power actions for node 0 and node 1 on this node controller are in /var/log.\nx1000c1s0b0:\u0026gt; cd /var/log x1000c1s0b0:\u0026gt; ls -l powerfault_* -rw-r--r-- 1 root root 306 May 10 15:32 powerfault_dn.Node0 -rw-r--r-- 1 root root 306 May 10 15:32 powerfault_dn.Node1 -rw-r--r-- 1 root root 5781 May 10 15:36 powerfault_up.Node0 -rw-r--r-- 1 root root 5781 May 10 15:36 powerfault_up.Node1   "
},
{
	"uri": "/docs-csm/en-11/operations/system_configuration_service/manage_parameters_with_the_scsd_service/",
	"title": "Manage Parameters With The Scsd Service",
	"tags": [],
	"description": "",
	"content": "Manage Parameters with the scsd Service The System Configuration Service commands below enable administrators to set various BMC and controller parameters. These parameters are controlled with the scsd command in the Cray CLI.\nRetrieve Current Information from Targets Get the network protocol parameters (NTP/syslog server, SSH keys) and boot order for the targets in the payload. All fields are only applicable to Liquid Cooled controllers. Attempts to set them for Air Cooled BMCs will be ignored, and retrieving them for Air Cooled BMCs will return empty strings.\nCommand line options can be used to set parameters as desired. For example, if only the NTP server info is to be set, only the \u0026ldquo;NTPServer\u0026rdquo; value has to be present.\nThe following is an example payload file that was used to generate the output in the command below:\n{ \u0026quot;Force\u0026quot;: true, \u0026quot;Targets\u0026quot;: [ \u0026quot;x0c0s0b0\u0026quot;, \u0026quot;x0c0s1b0\u0026quot; ], \u0026quot;Params\u0026quot;: [ \u0026quot;NTPServerInfo\u0026quot;, \u0026quot;SyslogServerInfo\u0026quot;, \u0026quot;SSHKey\u0026quot;, \u0026quot;SSHConsoleKey\u0026quot;, \u0026quot;BootOrder\u0026quot; ] } To retrieve information from the targets:\nncn-m001# cray scsd bmc dumpcfg create PAYLOAD_FILE --format json { \u0026quot;Targets\u0026quot;: [ { \u0026quot;StatusCode\u0026quot;: 200, \u0026quot;StatusMsg\u0026quot;: \u0026quot;OK\u0026quot;, \u0026quot;Xname\u0026quot;: \u0026quot;x0c0s0b0\u0026quot;, \u0026quot;Params\u0026quot;: { \u0026quot;NTPServerInfo\u0026quot;: { \u0026quot;NTPServers\u0026quot;: \u0026quot;sms-ncn-w001\u0026quot;, \u0026quot;Port\u0026quot;: 123, \u0026quot;ProtocolEnabled\u0026quot;: true }, \u0026quot;SyslogServerInfo\u0026quot;: { \u0026quot;SyslogServers\u0026quot;: \u0026quot;sms-ncn-w001\u0026quot;, \u0026quot;Port\u0026quot;:514, \u0026quot;ProtocolEnabled\u0026quot;: true }, \u0026quot;SSHKey\u0026quot;: \u0026quot;xxxxyyyyzzzz\u0026quot;, \u0026quot;SSHConsoleKey\u0026quot;: \u0026quot;aaaabbbbcccc\u0026quot;, \u0026quot;BootOrder\u0026quot;: [\u0026quot;Boot0\u0026quot;,Boot1\u0026quot;,Boot2\u0026quot;,Boot3\u0026quot;] } }, { \u0026quot;StatusCode\u0026quot;: 200, \u0026quot;StatusMsg\u0026quot;: \u0026quot;OK\u0026quot;, \u0026quot;Xname\u0026quot;: \u0026quot;x0c0s0b0\u0026quot;, \u0026quot;Params\u0026quot;: { \u0026quot;NTPServerInfo\u0026quot;: { \u0026quot;NTPServers\u0026quot;: \u0026quot;sms-ncn-w001\u0026quot;, \u0026quot;Port\u0026quot;: 123, \u0026quot;ProtocolEnabled\u0026quot;: true }, \u0026quot;SyslogServerInfo\u0026quot;: { \u0026quot;SyslogServers\u0026quot;: \u0026quot;sms-ncn-w001\u0026quot;, \u0026quot;Port\u0026quot;:514, \u0026quot;ProtocolEnabled\u0026quot;: true }, \u0026quot;SSHKey\u0026quot;: \u0026quot;xxxxyyyyzzzz\u0026quot;, \u0026quot;SSHConsoleKey\u0026quot;: \u0026quot;aaaabbbbcccc\u0026quot;, \u0026quot;BootOrder\u0026quot;: [\u0026quot;Boot0\u0026quot;,Boot1\u0026quot;,Boot2\u0026quot;,Boot3\u0026quot;] } } ] } Retrieve Information from a Single Target Retrieve NTP server information, syslog information, or the SSH key from a single target.\nncn-m001# cray scsd bmc cfg describe XNAME --format json { \u0026quot;Force\u0026quot;: false, \u0026quot;Params\u0026quot;: { \u0026quot;SSHConsoleKey\u0026quot;: \u0026quot;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQCg1yUfF5zPwOWjp3B/4LtuEGdbwo23L8BkQwhBz/4lVX2K2viYhGBAGwzqWMe1OQjSz3cUiSE/A6Kr7tKwB77j4U51XLbTRy5tcqzhIVFb7kFgdSqyUxfv+5s0aNLkwpI00w2TVVSp7xy8t+CLHwgdC7RXtWHOmI35NdUc8y8monn+q4mQV0ms29h1/tpHETocPQjCMwIsOtvUyS91XAn72Va1Xe8uTaAO+SqTZMYVTOxfLeLTg6QLox8PBXpVz422E4bcKZOYT68s1DxL5Rtz7HB6iKtXOvLaJSe8S5AUEe1G4eojQ/NEHcNobZkO00wSIzce2TwZV1il7410yGle1njnWLZBSpYmfH8d2joX434IEdESTwgdrYBEBAtOe7yvXu+2Qiux4AFaQwI0Aiif2Q5FndgqUiN6pD1IkVkInBYGFR5La8ZdZAgUdptvIZNJE67D3aGj0cseJFMHY4hfLEK34xne5yvL3OqpyjSS/0oPd1kLk4BgA8npGroLP+bP2GH6fMe7Wu9Sk/UUoM1W6N7127xVlvIogKxTG27zes8LSw7R/vOpVnWqJ2/BVIblTkMV45lCBQXaj4xG8ju8Zofh23BMusTthu8Q+T48k6H17g2dVlYTuUN+/i1KnSMPI2+dbOyV+X/maW+TBS8zK1pV5VTptg0UZgaZim+WIQ== \u0026quot;, \u0026quot;SyslogServerInfo\u0026quot;: { \u0026quot;ProtocolEnabled\u0026quot;: true, \u0026quot;SyslogServers\u0026quot;: [ \u0026quot;rsyslog_agg_service_hmn.local\u0026quot; ], \u0026quot;Port\u0026quot;: 514, \u0026quot;Transport\u0026quot;: \u0026quot;udp\u0026quot; }, \u0026quot;NTPServerInfo\u0026quot;: { \u0026quot;NTPServers\u0026quot;: [ \u0026quot;10.254.0.4\u0026quot; ], \u0026quot;ProtocolEnabled\u0026quot;: true, \u0026quot;Port\u0026quot;: 123 }, \u0026quot;SSHKey\u0026quot;: \u0026quot;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQCg1yUfF5zPwOWjp3B/4LtuEGdbwo23L8BkQwhBz/4lVX2K2viYhGBAGwzqWMe1OQjSz3cUiSE/A6Kr7tKwB77j4U51XLbTRy5tcqzhIVFb7kFgdSqyUxfv+5s0aNLkwpI00w2TVVSp7xy8t+CLHwgdC7RXtWHOmI35NdUc8y8monn+q4mQV0ms29h1/tpHETocPQjCMwIsOtvUyS91XAn72Va1Xe8uTaAO+SqTZMYVTOxfLeLTg6QLox8PBXpVz422E4bcKZOYT68s1DxL5Rtz7HB6iKtXOvLaJSe8S5AUEe1G4eojQ/NEHcNobZkO00wSIzce2TwZV1il7410yGle1njnWLZBSpYmfH8d2joX434IEdESTwgdrYBEBAtOe7yvXu+2Qiux4AFaQwI0Aiif2Q5FndgqUiN6pD1IkVkInBYGFR5La8ZdZAgUdptvIZNJE67D3aGj0cseJFMHY4hfLEK34xne5yvL3OqpyjSS/0oPd1kLk4BgA8npGroLP+bP2GH6fMe7Wu9Sk/UUoM1W6N7127xVlvIogKxTG27zes8LSw7R/vOpVnWqJ2/BVIblTkMV45lCBQXaj4xG8ju8Zofh23BMusTthu8Q+T48k6H17g2dVlYTuUN+/i1KnSMPI2+dbOyV+X/maW+TBS8zK1pV5VTptg0UZgaZim+WIQ== \u0026quot; } } Individual parameters can be specified in the command line with the --param option. Multiple parameters can be specified by using a comma separated list with the --params option. This makes it easier to find information for certain parameters. For example, to only view the NTP server information, the following option can be used:\nncn-m001# cray scsd bmc cfg describe --param NTPServerInfo \\ XNAME --format json { \u0026quot;Force\u0026quot;: false, \u0026quot;Params\u0026quot;: { \u0026quot;NTPServerInfo\u0026quot;: { \u0026quot;NTPServers\u0026quot;: [ \u0026quot;10.254.0.4\u0026quot; ], \u0026quot;ProtocolEnabled\u0026quot;: true, \u0026quot;Port\u0026quot;: 123 } } Set Parameters for Targets Set syslog, NTP server information, or SSH key for a set of targets.\nThe following is an example payload file that was used to generate the output in the command below:\n{ \u0026quot;Force\u0026quot;: false, \u0026quot;Targets\u0026quot;: [ \u0026quot;x0c0s0b0\u0026quot;, \u0026quot;x0c0s1b0\u0026quot; ], \u0026quot;Params\u0026quot;: { \u0026quot;NTPServerInfo\u0026quot;: { \u0026quot;NTPServers\u0026quot;: \u0026quot;sms-ncn-w001\u0026quot;, \u0026quot;Port\u0026quot;: 123, \u0026quot;ProtocolEnabled\u0026quot;: true }, \u0026quot;SyslogServerInfo\u0026quot;: { \u0026quot;SyslogServers\u0026quot;: \u0026quot;sms-ncn-w001\u0026quot;, \u0026quot;Port\u0026quot;:514, \u0026quot;ProtocolEnabled\u0026quot;: true }, \u0026quot;SSHKey\u0026quot;: \u0026quot;xxxxyyyyzzzz\u0026quot;, \u0026quot;SSHConsoleKey\u0026quot;: \u0026quot;aaaabbbbcccc\u0026quot;, \u0026quot;BootOrder\u0026quot;: [\u0026quot;Boot0\u0026quot;,\u0026quot;Boot1\u0026quot;,\u0026quot;Boot2\u0026quot;,\u0026quot;Boot3\u0026quot;] } } To set parameters for the specified targets:\nncn-w001# cray scsd bmc loadcfg create PAYLOAD_FILE --format json { \u0026quot;Targets\u0026quot;: [ { \u0026quot;Xname\u0026quot;: \u0026quot;x0c0s0b0\u0026quot;, \u0026quot;StatusCode\u0026quot;: 200, \u0026quot;StatusMsg\u0026quot;: \u0026quot;OK\u0026quot; }, { \u0026quot;Xname\u0026quot;: \u0026quot;x0c0s1b0\u0026quot;, \u0026quot;StatusCode\u0026quot;: 405, \u0026quot;StatusMsg\u0026quot;: \u0026quot;Only GET operations permitted\u0026quot; } ] } Set Parameters for a Single BMC or Controller Set the BMC configuration for a single target using a specific xname. If no form data is specified, all network protocol data is returned for the target; otherwise, only the requested data is returned.\nThe following is an example payload file that was used to generate the output in the command below:\n{ \u0026quot;Force\u0026quot;: true, \u0026quot;Params\u0026quot;: { \u0026quot;NTPServerInfo\u0026quot;: { \u0026quot;NTPServers\u0026quot;: \u0026quot;sms-ncn-w001\u0026quot;, \u0026quot;Port\u0026quot;: 123, \u0026quot;ProtocolEnabled\u0026quot;: true }, \u0026quot;SyslogServerInfo\u0026quot;: { \u0026quot;SyslogServers\u0026quot;: \u0026quot;sms-ncn-w001\u0026quot;, \u0026quot;Port\u0026quot;:514, \u0026quot;ProtocolEnabled\u0026quot;: true }, \u0026quot;SSHKey\u0026quot;: \u0026quot;xxxxyyyyzzzz\u0026quot;, \u0026quot;SSHConsoleKey\u0026quot;: \u0026quot;aaaabbbbcccc\u0026quot;, \u0026quot;BootOrder\u0026quot;: [\u0026quot;Boot0\u0026quot;,\u0026quot;Boot1\u0026quot;,\u0026quot;Boot2\u0026quot;,\u0026quot;Boot3\u0026quot;] } } To set the parameters for a single BMC or controller:\nncn-m001# cray scsd bmc cfg create XNAME --format json { \u0026quot;StatusMsg\u0026quot;: \u0026quot;OK\u0026quot; } Set Redfish Credentials for Multiple Targets Use the following command to set Redfish credentials for BMCs and controllers. Note that this is different than SSH keys, which are only used on controllers. These credentials are for Redfish access, not SSH access into a controller.\nThe API allows for different credentials to be set for each target within one call. It is not possible to retrieve credentials with this command. Only setting them is allowed for security reasons.\nThe payload for this API is amenable to setting different credentials for different targets all in one call. To set credentials for a group of controllers, set up a group in HSM and use the group ID.\nThe following is an example payload file that was used to generate the output in the command below:\n{ \u0026quot;Force\u0026quot;: false, \u0026quot;Targets\u0026quot;: [ { \u0026quot;Xname\u0026quot;: \u0026quot;x0c0s0b0\u0026quot;, \u0026quot;Creds\u0026quot;: { \u0026quot;Username\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;Password\u0026quot;: \u0026quot;admin-pw\u0026quot; } }, { \u0026quot;Xname\u0026quot;: \u0026quot;x0c0s1b0\u0026quot;, \u0026quot;Creds\u0026quot;: { \u0026quot;Username\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;Password\u0026quot;: \u0026quot;admin-pw\u0026quot; } } ] } To set the Redfish credentials for multiple targets:\nncn-m001# cray scsd bmc discreetcreds create PAYLOAD_FILE --format json { \u0026quot;Targets\u0026quot;: [ { \u0026quot;Xname\u0026quot;: \u0026quot;x0c0s0b0\u0026quot;, \u0026quot;StatusCode\u0026quot;: 200, \u0026quot;StatusMsg\u0026quot;: \u0026quot;OK\u0026quot; }, { \u0026quot;Xname\u0026quot;: \u0026quot;x0c0s1b0\u0026quot;, \u0026quot;StatusCode\u0026quot;: 200, \u0026quot;StatusMsg\u0026quot;: \u0026quot;OK\u0026quot; } ] } Set Redfish Credentials for a Single Target Set Redfish credentials for a single target. This command is similar to the cray scsd bmc discreetcreds create command, except it cannot be used to set different credentials for multiple targets.\nThe following is an example payload file that was used to generate the output in the command below:\n{ \u0026quot;Force\u0026quot;: true, \u0026quot;Creds\u0026quot;: { \u0026quot;Username\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;Password\u0026quot;: \u0026quot;admin-pw\u0026quot; } } To set the Redfish credentials for a single target:\nncn-m001# cray scsd bmc creds create XNAME --format json { \u0026quot;StatusMsg\u0026quot;: \u0026quot;OK\u0026quot; } "
},
{
	"uri": "/docs-csm/en-11/operations/spire/restore_spire_postgres_without_a_backup/",
	"title": "Restore Spire Postgres Without An Existing Backup",
	"tags": [],
	"description": "",
	"content": "Restore Spire Postgres without an Existing Backup Reinstall the Spire helm chart in the event that spire-postgres databases cannot be restored from a backup.\nUninstall Spire   Uninstall the Spire helm chart.\nncn# helm uninstall -n spire spire   Wait for the pods in the Spire namespace to terminate. Once that is done, remove the spire-data-server pvcs.\nncn# kubectl get pvc -n spire | grep spire-data-spire-server | awk \u0026#39;{print $1}\u0026#39; | xargs kubectl delete -n spire pvc   Disable spire-agent on all of the Kubernetes NCNs and delete the join data.\nncn# for ncn in $(kubectl get nodes -o name | cut -d\u0026#39;/\u0026#39; -f2); do ssh \u0026#34;${ncn}\u0026#34; systemctl stop spire-agent; ssh \u0026#34;${ncn}\u0026#34; rm / root/spire/data/svid.key /root/spire/agent_svid.der /root/spire/bundle.der; done   Re-install the Spire Helm Chart The CSM release tarball is required as it contains the Spire helm chart.\n  Extract the current release tarball.\n## This example assumes the csm-1.0.0 release is currently running and the csm-1.0.0.tar.gz has been pulled down under /root ncn# cd /root ncn# tar -xzf csm-1.0.0.tar.gz ncn# rm csm-1.0.0.tar.gz ncn# PATH_TO_RELEASE=/root/csm-1.0.0   Get the current cached customizations.\nncn# kubectl get secrets -n loftsman site-init -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | base64 -d \u0026gt; customizations.yaml   Get the current cached sysmgmt manifest.\nncn# kubectl get cm -n loftsman loftsman-sysmgmt -o jsonpath=\u0026#39;{.data.manifest\\.yaml}\u0026#39; -o sysmgmt.yaml   Edit the sysmgmt.yaml spec.charts section to only include the spire chart and all its current data. (The resources specified above will be updated in the next step and the version may differ, because this is an example).\napiVersion: manifests/v1beta1 metadata: name: sysmgmt spec: charts: - name: spire namespace: spire source: csm values: server: fqdn: spire.local trustDomain: shasta version: 0.11.3 version: 0.12.0 sources: charts: - location: ./helm name: csm type: directory - location: ./helm name: csm-algol60 type: directory   Generate the manifest that will be used to redeploy the chart with the modified resources.\nncn# manifestgen -c customizations.yaml -i sysmgmt.yaml -o manifest.yaml   Update the helm chart path in the manifest.yaml file.\nncn# sed -i \u0026#34;s|./helm|${PATH_TO_RELEASE}/helm|\u0026#34; manifest.yaml   Validate that the manifest.yaml file only contains chart information for Spire, and that the sources charts location points to the directory the helm chart was extracted from to prepend to /helm.\n  Redeploy the Spire chart.\nncn# loftsman ship --manifest-path ${PWD}/manifest.yaml   Verify that all Spire pods have started.\nThis step may take a few minutes due to a number of pods requiring other pods to be up.\nncn# kubectl get pods -n spire   Restart all compute nodes and User Access Nodes (UANs).\n  Compute nodes and UANs get their join token on boot from the Boot Script Service (BSS). Their old SVID data is no longer valid and a reboot is required in order for them to re-join Spire.\n"
},
{
	"uri": "/docs-csm/en-11/operations/system_configuration_service/set_bmc_credentials/",
	"title": "Set BMC Credentials",
	"tags": [],
	"description": "",
	"content": "Set BMC Credentials Use the System Configuration Service (SCSD) to set the BMCs credentials to unique values, or set them all to the same value. Redfish BMCs get installed into the system with default credentials. Once the machine is shipped, the Redfish credentials must be changed on all BMCs. This is done using System Configuration Service (SCSD) through the Cray CLI.\nPrerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system.  Procedure   Determine the live BMCs in the system.\nThe list of BMCs must include air-cooled compute BMCs, air-cooled High Speed Network (HSN) switch BMCs, liquid-cooled compute BMCs, liquid-cooled switch BMCs, and liquid-cooled chassis BMCs.\nncn-m001# for fff in `cray hsm inventory redfishEndpoints list --format json \\ | jq \u0026#39;.RedfishEndpoints[] | select(.FQDN | contains(\u0026#34;-rts\u0026#34;) | not) | \\ select(.DiscoveryInfo.LastDiscoveryStatus == \u0026#34;DiscoverOK\u0026#34;) | select(.Enabled==true) \\ | .ID\u0026#39; | sed \u0026#39;s/\u0026#34;//g\u0026#39;`; do echo \u0026#34;Pinging ${fff}...\u0026#34; ; xxx=`curl -k https://${fff}/redfish/v1/` if [[ \u0026#34;${xxx}\u0026#34; != \u0026#34; ]]; then echo \u0026#34;PRESENT\u0026#34; else echo \u0026#34;NOT PRESENT\u0026#34; fi done   Create a new JSON file containing the BMC credentials for all BMCs returned in the previous step.\nSelect one of the options below to set the credentials for the BMCs:\n  Set all BMCs with the same credentials.\nncn-m001# vi bmc_creds_glb.json { \u0026#34;Force\u0026#34;: false, \u0026#34;Username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;new.root.password\u0026#34; \u0026#34;Targets\u0026#34;: [ \u0026#34;x0c0s0b0\u0026#34;, \u0026#34;x0c0s1b0\u0026#34; ] }   Set all BMCs with different credentials.\nncn-m001# vi bmc_creds_dsc.json { \u0026#34;Force\u0026#34;: true, \u0026#34;Targets\u0026#34;: [ { \u0026#34;Xname\u0026#34;: \u0026#34;x0c0s0b0\u0026#34;, \u0026#34;Creds\u0026#34;: { \u0026#34;Username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;pw-x0c0s0b0\u0026#34; } }, { \u0026#34;Xname\u0026#34;: \u0026#34;x0c0s0b1\u0026#34;, \u0026#34;Creds\u0026#34;: { \u0026#34;Username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;pw-x0c0s0b1\u0026#34; } } ] }     Apply the new BMC credentials.\nUse only one of the following options depending on how the credentials are being set:\n  Apply global credentials to BMCs with the same credentials.\nncn-m001# cray scsd bmc globalcreds create ./bmc_creds_glb.json   Apply discrete credentials to BMCs with different credentials.\nncn-m001# cray scsd bmc discreetcreds create ./bmc_creds_dsc.json   Troubleshooting: If either command has any components that do not have the status of OK, they must be retried until they work, or the retries are exhausted and noted as failures. Failed modules need to be taken out of the system until they are fixed.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/updating_the_liquid-cooled_ex_cabinet_default_credentials_after_a_cec_password_change/",
	"title": "Updating The Liquid-cooled Ex Cabinet Cec With Default Credentials After A Cec Password Change",
	"tags": [],
	"description": "",
	"content": "Updating the Liquid-Cooled EX Cabinet CEC with Default Credentials after a CEC Password Change This procedure changes the credential for liquid-cooled EX cabinet chassis controllers and node controller (BMCs) used by CSM services after the CECs have been set to a new global default credential.\nNOTE: This procedure does not provision Slingshot switch BMCs (RouterBMCs). Slingshot switch BMC default credentials must be changed using the procedures in the Slingshot product documentation. To update Slingshot switch BMCs, refer to \u0026ldquo;Change Rosetta Login and Redfish API Credentials\u0026rdquo; in the Slingshot Operations Guide (\u0026gt; 1.6.0).\nThis procedure provisions only the default Redfish root account passwords. It does not modify Redfish accounts that have been added after an initial system installation.\nPrerequisites  Perform procedures in Provisioning a Liquid-Cooled EX Cabinet CEC with Default Credentials on all CECs in the system. All of the CECs must be configured with the same global credential. The previous default global credential for liquid-cooled BMCs needs to be known.  Procedure 1. Update the Default credentials used by MEDS for new hardware. The MEDS sealed secret contains the default global credential used by MEDS when it discovers new liquid-cooled EX cabinet hardware.\n1.1 Acquire site-init. Before redeploying MEDS, update the customizations.yaml file in the site-init secret in the loftsman namespace.\n  If the site-init repository is available as a remote repository, then clone it on the host orchestrating the upgrade as described here.\nncn-m001# git clone \u0026#34;$SITE_INIT_REPO_URL\u0026#34; site-init   Acquire customizations.yaml from the currently running system:\nncn-m001# kubectl get secrets -n loftsman site-init -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | base64 -d \u0026gt; site-init/customizations.yaml   Review, add, and commit customizations.yaml to the local site-init repository as appropriate.\n NOTE: If site-init was cloned from a remote repository in step 1, there may not be any differences and hence nothing to commit. This is okay. If there are differences between what is in the repository and what was stored in the site-init, then it suggests settings were changed at some point.\n ncn-m001# cd site-init ncn-m001# git diff ncn-m001# git add customizations.yaml ncn-m001# git commit -m \u0026#39;Add customizations.yaml from site-init secret\u0026#39;   Acquire sealed secret keys:\nncn-m001# mkdir -p certs ncn-m001# kubectl -n kube-system get secret sealed-secrets-key -o jsonpath=\u0026#39;{.data.tls\\.crt}\u0026#39; | base64 -d \u0026gt; certs/sealed_secrets.crt ncn-m001# kubectl -n kube-system get secret sealed-secrets-key -o jsonpath=\u0026#39;{.data.tls\\.key}\u0026#39; | base64 -d \u0026gt; certs/sealed_secrets.key   1.2 Modify MEDS sealed secret to use new global default credential.   Inspect the original default credentials for MEDS:\nncn-m001# ./utils/secrets-decrypt.sh cray_meds_credentials ./certs/sealed_secrets.key ./customizations.yaml | jq .data.vault_redfish_defaults -r | base64 -d | jq { \u0026#34;Username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;bar\u0026#34; }   Specify the desired default credentials for MEDS to use with new hardware:\n Replace foobar with the root password configured on the CEC(s).\n ncn-m001# echo \u0026#39;{ \u0026#34;Username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;foobar\u0026#34; }\u0026#39; | base64 \u0026gt; creds.json.b64   Update and regenerate the cray_meds_credentials sealed secret:\nncn-m001# cat \u0026lt;\u0026lt; EOF | yq w - \u0026#39;data.vault_redfish_defaults\u0026#39; \u0026#34;$(\u0026lt;creds.json.b64)\u0026#34; | yq r -j - | ./utils/secrets-encrypt.sh | yq w -f - -i ./customizations.yaml \u0026#39;spec.kubernetes.sealed_secrets.cray_meds_credentials\u0026#39; { \u0026#34;kind\u0026#34;: \u0026#34;Secret\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;cray-meds-credentials\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;services\u0026#34;, \u0026#34;creationTimestamp\u0026#34;: null }, \u0026#34;data\u0026#34;: {} } EOF   Decrypt updated sealed secret for review. The sealed secret should match the credentials set on the CEC.\nncn-m001# ./utils/secrets-decrypt.sh cray_meds_credentials ./certs/sealed_secrets.key ./customizations.yaml | jq .data.vault_redfish_defaults -r | base64 -d | jq { \u0026#34;Username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;foobar\u0026#34; }   Update the site-init secret containing customizations.yaml for the system:\nncn-m001# kubectl delete secret -n loftsman site-init ncn-m001# kubectl create secret -n loftsman generic site-init --from-file=customizations.yaml   Check in changes made to customizations.yaml\nncn-m001# git diff ncn-m001# git add customizations.yaml ncn-m001# git commit -m \u0026#39;Update customizations.yaml with global default credential for MEDS\u0026#39;   Push to the remote repository as appropriate:\nncn-m001# git push   1.3 Redeploy MEDS to pick up the new sealed secret and push credentials into vault.   Determine the version of MEDS:\nncn-m001# MEDS_VERSION=$(kubectl -n loftsman get cm loftsman-core-services -o jsonpath=\u0026#39;{.data.manifest\\.yaml}\u0026#39; | yq r - \u0026#39;spec.charts.(name==cray-hms-meds).version\u0026#39;) ncn-m001# echo $MEDS_VERSION   Create meds-manifest.yaml:\nncn-m001# cat \u0026gt; meds-manifest.yaml \u0026lt;\u0026lt; EOF apiVersion: manifests/v1beta1 metadata: name: meds spec: charts: - name: cray-hms-meds version: $MEDS_VERSION namespace: services EOF   Merge customizations.yaml with meds-manifest.yaml:\nncn-m001# manifestgen -c customizations.yaml -i ./meds-manifest.yaml \u0026gt; ./meds-manifest.out.yaml   Redeploy the MEDS helm chart:\nncn-m001# loftsman ship \\  --charts-repo https://packages.local/repository/charts \\  --manifest-path meds-manifest.out.yaml   Wait for the MEDS Vault loader job to run to completion:\nncn-m001# kubectl wait -n services job cray-meds-vault-loader --for=condition=complete --timeout=5m   Verify the default credentials have changed in Vault:\nncn-m001# VAULT_PASSWD=$(kubectl -n vault get secrets cray-vault-unseal-keys -o json | jq -r \u0026#39;.data[\u0026#34;vault-root\u0026#34;]\u0026#39; | base64 -d) ncn-m001# kubectl -n vault exec -it cray-vault-0 -c vault -- env VAULT_TOKEN=$VAULT_PASSWD VAULT_ADDR=http://127.0.0.1:8200 vault kv get secret/meds-cred/global/ipmi ====== Data ====== Key Value --- ----- Password foobar Username root   2. Update credentials for existing EX hardware in the system   Set CRED_PASSWORD to the new updated password:\nncn-m001# CRED_PASSWORD=foobar   Update the credentials used by CSM services for all previously discovered EX cabinet BMCs to the new global default:\nncn-m001# \\ REDFISH_ENDPOINTS=$(cray hsm inventory redfishEndpoints list --type \u0026#39;!RouterBMC\u0026#39; --format json | jq .RedfishEndpoints[].ID -r | sort -V ) cray hsm state components list --format json \u0026gt; /tmp/components.json for RF in $REDFISH_ENDPOINTS; do echo \u0026#34;$RF: Checking...\u0026#34; CLASS=$(jq -r --arg XNAME \u0026#34;$RF\u0026#34; \u0026#39;.Components[] | select(.ID == $XNAME).Class\u0026#39; /tmp/components.json) if [[ \u0026#34;$CLASS\u0026#34; != \u0026#34;Mountain\u0026#34; ]]; then echo \u0026#34;$RFis not Mountain, skipping...\u0026#34; continue fi echo \u0026#34;$RF: Updating credentials\u0026#34; cray hsm inventory redfishEndpoints update ${RF} --user root --password ${CRED_PASSWORD} done It will take some time for the above bash script to run. It will take approximately 5 minutes to update all of the credentials for a single fully populated cabinet.\n Alternatively, use the following command on each BMC. Replace BMC_XNAME with the BMC xname to update the credentials:\nncn-m001# cray hsm inventory redfishEndpoints update BMC_XNAME --user root --password ${CRED_PASSWORD}    Wait for HSM to re-discover the updated RedfishEndpoints:\nncn-m001# sleep 180   Wait for all updated Redfish endpoints to become DiscoverOK:\nThe following bash script will find all Redfish endpoints for the liquid-cooled BMCs that are not in DiscoverOK, and display their last Discovery Status.\nncn-m001# \\ cray hsm inventory redfishEndpoints list --laststatus \u0026#39;!DiscoverOK\u0026#39; --type \u0026#39;!RouterBMC\u0026#39; --format json \u0026gt; /tmp/redfishEndpoints.json cray hsm state components list --format json \u0026gt; /tmp/components.json REDFISH_ENDPOINTS=$(jq .RedfishEndpoints[].ID -r /tmp/redfishEndpoints.json | sort -V) for RF in $REDFISH_ENDPOINTS; do CLASS=$(jq -r --arg XNAME \u0026#34;$RF\u0026#34; \u0026#39;.Components[] | select(.ID == $XNAME).Class\u0026#39; /tmp/components.json) if [[ \u0026#34;$CLASS\u0026#34; != \u0026#34;Mountain\u0026#34; ]]; then continue fi DISCOVERY_STATUS=$(jq -r --arg XNAME \u0026#34;$RF\u0026#34; \u0026#39;.RedfishEndpoints[] | select(.ID == $XNAME).DiscoveryInfo.LastDiscoveryStatus\u0026#39; /tmp/redfishEndpoints.json) echo \u0026#34;$RF: $DISCOVERY_STATUS\u0026#34; done Example output:\nx1001c0r5b0: HTTPsGetFailed x1001c1s0b0: HTTPsGetFailed x1001c1s0b1: HTTPsGetFailed x1001c2s0b1: DiscoveryStarted For each Redfish endpoint that is reported use the following to troubleshoot why it is not DiscoverOK or DiscoveryStarted:\n  If the Redfish endpoint is DiscoveryStarted, then that BMC is currently in the process of being inventoried by HSM. Wait a few minutes and re-try the bash script above to re-check the current discovery status of the RedfishEndpoints.\n The hms-discovery cronjob (if enabled) will trigger a discover on BMCs that are not currently in DiscoverOK or DiscoveryStarted every 3 minutes.\n   If the Redfish endpoint is HTTPsGetFailed, then HSM had issues contacting BMC.\n  Verify that the BMC xname is resolvable and pingable:\nncn-m001# ping x1001c1s0b0   If a NodeBMC is not pingable, then verify that the slot powering the BMC is powered on. If this is a ChassisBMC, skip this step. For example, the NodeBMC x1001c1s0b0 is in slot x1001c1s0.\nncn-m001# cray capmc get_xname_status create --xnames x1001c1s0 e = 0 err_msg = \u0026#34; on = [ \u0026#34;x1001c1s0b0\u0026#34;,] If the slot is off, power it on:\nncn-m001# cray capmc xname_on create --xnames x1001c1s0   If the BMC is reachable and in HTTPsGetFailed, then verify that the BMC is accessible with the new default global credential. Replace BMC_XNAME with the hostname of the Redfish Endpoint.\nncn-m001# curl -k -u root:$CRED_PASSWORD https://BMC_XNAME/redfish/v1/Managers | jq If the error message below is returned, then the BMC must have a StatefulReset action performed on it. The StatefulReset action clears previously user defined credentials that are taking precedence over the CEC supplied credential. It also clears NTP, Syslog, and SSH Key configurations on the BMC.\n{ \u0026#34;error\u0026#34;: { \u0026#34;@Message.ExtendedInfo\u0026#34;: [ { \u0026#34;@odata.type\u0026#34;: \u0026#34;#Message.v1_0_5.Message\u0026#34;, \u0026#34;Message\u0026#34;: \u0026#34;While attempting to establish a connection to /redfish/v1/Managers, the service was denied access.\u0026#34;, \u0026#34;MessageArgs\u0026#34;: [ \u0026#34;/redfish/v1/Managers\u0026#34; ], \u0026#34;MessageId\u0026#34;: \u0026#34;Security.1.0.AccessDenied\u0026#34;, \u0026#34;Resolution\u0026#34;: \u0026#34;Attempt to ensure that the URI is correct and that the service has the appropriate credentials.\u0026#34;, \u0026#34;Severity\u0026#34;: \u0026#34;Critical\u0026#34; } ], \u0026#34;code\u0026#34;: \u0026#34;Security.1.0.AccessDenied\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;While attempting to establish a connection to /redfish/v1/Managers, the service was denied access.\u0026#34; } } Perform a StatefulReset on the liquid-cooled BMC replace BMC_XNAME with the hostname of the BMC. The OLD_DEFAULT_PASSWORD must match the credential that was previously set on the BMC. This is mostly likely the previous global default credential for liquid-cooled BMCs.\nncn-m001# curl -k -u root:OLD_DEFAULT_PASSWORD -X POST -H \u0026#39;Content-Type: application/json\u0026#39; -d \\ \u0026#39;{\u0026#34;ResetType\u0026#34;: \u0026#34;StatefulReset\u0026#34;}\u0026#39; \\ https://BMC_XNAME/redfish/v1/Managers/BMC/Actions/Manager.Reset After the StatefulReset action has been issued, the BMC will be unreachable for a few minutes as it performs the StatefulReset.\n      3. Reapply BMC settings if a StatefulReset was performed on any BMC. This section only needs to be performed if any liquid-cooled Node or Chassis BMCs that had to be StatefulReset.\n  For each liquid-cooled BMC that the StatefulReset action was applied delete the BMC from HSM. Replace BMC_XNAME with the BMC xname to delete.\n ncn-m001# cray hsm inventory redfishEndpoints delete BMC_XNAME    Restart MEDS to re-setup the NTP and Syslog configuration the RedfishEndpoints:\nView Running MEDS pods:\nncn-m001# kubectl -n services get pods -l app.kubernetes.io/instance=cray-hms-meds NAME READY STATUS RESTARTS AGE cray-meds-6d8b5875bc-4jngc 2/2 Running 0 17d Restart MEDS:\nncn-m001# kubectl -n services rollout restart deployment cray-meds ncn-m001# kubectl -n services rollout status deployment cray-meds   Wait for MEDS to re-discover the deleted RedfishEndpoints:\nncn-m001# sleep 300   Verify all expected hardware has been discovered:\nThe following bash script will find all Redfish endpoints for the liquid-cooled BMCs that are not in DiscoverOK, and display their last Discovery Status.\nncn-m001# \\ cray hsm inventory redfishEndpoints list --laststatus \u0026#39;!DiscoverOK\u0026#39; --type \u0026#39;!RouterBMC\u0026#39; --format json \u0026gt; /tmp/redfishEndpoints.json cray hsm state components list --format json \u0026gt; /tmp/components.json REDFISH_ENDPOINTS=$(jq .RedfishEndpoints[].ID -r /tmp/redfishEndpoints.json | sort -V) for RF in $REDFISH_ENDPOINTS; do CLASS=$(jq -r --arg XNAME \u0026#34;$RF\u0026#34; \u0026#39;.Components[] | select(.ID == $XNAME).Class\u0026#39; /tmp/components.json) if [[ \u0026#34;$CLASS\u0026#34; != \u0026#34;Mountain\u0026#34; ]]; then continue fi DISCOVERY_STATUS=$(jq -r --arg XNAME \u0026#34;$RF\u0026#34; \u0026#39;.RedfishEndpoints[] | select(.ID == $XNAME).DiscoveryInfo.LastDiscoveryStatus\u0026#39; /tmp/redfishEndpoints.json) echo \u0026#34;$RF: $DISCOVERY_STATUS\u0026#34; done   Restore SSH Keys configured by cray-conman on liquid-cooled Node BMCs. Get the SSH Console private key from Vault:\nncn-m001# VAULT_PASSWD=$(kubectl -n vault get secrets cray-vault-unseal-keys \\ -o json | jq -r \u0026#39;.data[\u0026#34;vault-root\u0026#34;]\u0026#39; | base64 -d) ncn-m001# kubectl -n vault exec -t cray-vault-0 -c vault \\ -- env VAULT_TOKEN=$VAULT_PASSWD VAULT_ADDR=http://127.0.0.1:8200 \\ VAULT_FORMAT=json vault read transit/export/signing-key/mountain-bmc-console \\ | jq -r .data.keys[] \u0026gt; ssh-console.key   Generate the SSH public key:\nncn-m001# chmod 0600 ssh-console.key ncn-m001# export SCSD_SSH_CONSOLE_KEY=$(ssh-keygen -yf ssh-console.key) ncn-m001# echo $SCSD_SSH_CONSOLE_KEY   Delete the SSH Console private key from disk:\nncn-m001# rm ssh-console.key   Generate a payload for the SCSD service. The admin must be authenticated to the Cray CLI before proceeding.\nncn-m001# cat \u0026gt; scsd_cfg.json \u0026lt;\u0026lt;DATA { \u0026#34;Force\u0026#34;:false, \u0026#34;Targets\u0026#34;: $(cray hsm state components list --class Mountain --type NodeBMC --format json | jq -r \u0026#39;[.Components[] | .ID]\u0026#39;), \u0026#34;Params\u0026#34;:{ \u0026#34;SSHConsoleKey\u0026#34;:\u0026#34;$(echo $SCSD_SSH_CONSOLE_KEY)\u0026#34; } } DATA Alternatively create a scsd_cfg.json file with only the SSH Console key:\nncn-m001# cat \u0026gt; scsd_cfg.json \u0026lt;\u0026lt;DATA { \u0026#34;Force\u0026#34;:false, \u0026#34;Targets\u0026#34;:[ \u0026#34;x1000c0s0b0\u0026#34;, \u0026#34;x1000c0s0b0\u0026#34; ], \u0026#34;Params\u0026#34;:{ \u0026#34;SSHConsoleKey\u0026#34;:\u0026#34;$(echo $SCSD_SSH_CONSOLE_KEY)\u0026#34; } } DATA   Edit the \u0026ldquo;Targets\u0026rdquo; array to contain the NodeBMCs that have have had the StatefulReset action.\n  Inspect the generated scsd_cfg.json file. Ensure the following are true before running the cray scsd command below:\n The xname looks valid/appropriate. Limit the scsd_cfg.json file to NodeBMCs that have had the StatefulReset action applied to them. The SSHConsoleKey settings match the desired public key.    Apply SSH Console key to the NodeBMCs:\nncn-m001# cray scsd bmc loadcfg create scsd_cfg.json   Check the output to verify all hardware has been set with the correct keys. Passwordless SSH to the consoles should now function as expected.\n    "
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/system_security_and_authentication/",
	"title": "System Security And Authentication",
	"tags": [],
	"description": "",
	"content": "System Security and Authentication The system uses a number of mechanisms to ensure the security and authentication of internal and external requests.\n  API Gateway service - The Cray API Gateway service provides a common access gateway for all of the systems management REST APIs. Authentication is provided by an Identity and Access Management (IAM) service that integrates with Istio.\n  Keycloak - Keycloak is an open source Identity and Access Management (IAM) solution. It provides authentication and authorization services that are used to secure access to services on the system.\nTo learn more about Keycloak, refer to https://www.keycloak.org/.\n  JSON Web Tokens (JWT) - The approach for system management authentication and authorization is to leverage the OpenID Connect standard, as much as practical. OpenID Connect consists of a specific application of the OAuth v2.0 standard, which leverages the use of JSON Web Tokens (JWT).\n  All connections through the Istio ingress gateway require authentication with a valid JWT from Keycloak, except for the following endpoints accessed via the shasta external hostname:\n /keycloak /apis/tokens /vcs /spire-jwks- /spire-bundle /meta-data /user-data /phone-home /repository /v2 /service/rest /capsules/  Table of Contents  Manage System Passwords  Update NCN Passwords Change Root Passwords for Compute Nodes Change NCN Image Root Password and SSH Keys Change EX Liquid-Cooled Cabinet Global Default Password Provisioning a Liquid-Cooled EX Cabinet CEC with Default Credentials Updating the Liquid-Cooled EX Cabinet Default Credentials after a CEC Password Change   SSH Keys Authenticate an Account with the Command Line Default Keycloak Realms, Accounts, and Clients  Certificate Types Change the Keycloak Admin Password Create a Service Account in Keycloak Retrieve the Client Secret for Service Accounts Get a Long-Lived Token for a Service Account Access the Keycloak User Management UI Create Internal User Accounts in the Keycloak Shasta Realm Delete Internal User Accounts in the Keycloak Shasta Realm Create Internal User Groups in the Keycloak Shasta Realm Remove Internal Groups from the Keycloak Shasta Realm Remove the Email Mapper from the LDAP User Federation Re-Sync Keycloak Users to Compute Nodes Keycloak Operations Configure Keycloak for LDAP/AD authentication Configure the RSA Plugin in Keycloak Preserve Username Capitalization for Users Exported from Keycloak Change the LDAP Server IP Address for Existing LDAP Server Content Change the LDAP Server IP Address for New LDAP Server Content Remove the LDAP User Federation from Keycloak Add LDAP User Federation   Public Key Infrastructure (PKI)  PKI Certificate Authority (CA) Make HTTPS Requests from Sources Outside the Management Kubernetes Cluster Transport Layer Security (TLS) for Ingress Services PKI Services HashiCorp Vault Backup and Restore Vault Clusters Troubleshoot Common Vault Cluster Issues   Public Key Infrastructure (PKI)  PKI Certificate Authority (CA) Make HTTPS Requests from Sources Outside the Management Kubernetes Cluster Transport Layer Security (TLS) for Ingress Services PKI Services HashiCorp Vault Backup and Restore Vault Clusters Troubleshoot Common Vault Cluster Issues   Troubleshoot SPIRE Failing to Start on NCNs API Authorization  "
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/transport_layer_security_for_ingress_services/",
	"title": "Transport Layer Security (tls) For Ingress Services",
	"tags": [],
	"description": "",
	"content": "Transport Layer Security (TLS) for Ingress Services The Istio Secure Gateway and Keycloak Gatekeeper services utilize Cert-manager for their Transport Layer Security (TLS) certificate and private key. Certificate custom resource definitions are deployed as part of Helm Charts for these services.\nTo view properties of the Istio Secure Gateway certificate:\n# kubectl describe certificate -n istio-system ingress-gateway-cert To view the properties of the Keycloak Gatekeeper certificate:\n# kubectl describe certificate -n services keycloak-gatekeeper An outstanding bug in the Keycloak Gatekeeper service prevents it from updating its TLS certificate and key material upon Cert-manager renewal. Thus, it may be necessary to monitor the situation and proactively renew/force reload Keycloak Gatekeeper.\n"
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/troubleshoot_common_vault_cluster_issues/",
	"title": "Troubleshoot Common Vault Cluster Issues",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Common Vault Cluster Issues Search for underlying issues causing unhealthy Vault clusters. Check the Vault statefulset and various pod logs to determine what is impacting the health of the Vault.\nProcedure   Verify the Vault statefulset.\n# kubectl -n vault get statefulset --show-labels NAME READY AGE LABELS cray-vault 3/3 8d app.kubernetes.io/name=vault,vault_cr=cray-vault   Check the pod logs for the bank-vaults container for Vault statefulset pods.\n# kubectl logs -n vault cray-vault-0 --tail=-1 --prefix -c bank-vaults # kubectl logs -n vault cray-vault-1 --tail=-1 --prefix -c bank-vaults # kubectl logs -n vault cray-vault-0 --tail=-1 --prefix -c bank-vaults   Check the Vault container logs within the pod.\n# kubectl logs -n vault cray-vault-0 --tail=-1 --prefix -c vault # kubectl logs -n vault cray-vault-1 --tail=-1 --prefix -c vault # kubectl logs -n vault cray-vault-2 --tail=-1 --prefix -c vault   Check the Vault operator pod logs using ephemerally named pods.\n# kubectl logs -n vault cray-vault-operator-7dbbdbb68b-zvg2g --tail=-1 --prefix   "
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/troubleshoot_spire_failing_to_start_on_ncns/",
	"title": "Troubleshoot Spire Failing To Start On NCNs",
	"tags": [],
	"description": "",
	"content": "Troubleshoot SPIRE Failing to Start on NCNs The spire-agent service may fail to start on Kubernetes non-compute nodes (NCNs). A key indication of this failure is when logging errors occur with the journalctl command. The following are logging errors that will indicate if the spire-agent is failing to start:\n The join token does not exist or has already been used message is returned The last lines of the logs contain multiple lines of systemd[1]: spire-agent.service: Start request repeated too quickly.  Deleting the request-ncn-join-token daemonset pod running on the node may clear the issue.\nWhile the spire-agent systemctl service on the Kubernetes node should eventually restart cleanly, administrators may need to log in to the impacted nodes and restart the service. The easiest way to delete the appropriate pod is to create the following function and run it on the impacted node.\nfunction renewncnjoin() { if [ -z \u0026#34;$1\u0026#34; ]; then echo \u0026#34;usage: renewncnjoin NODE_XNAME\u0026#34; else for pod in $(kubectl get pods -n spire | grep request-ncn-join-token | awk \u0026#39;{print $1}\u0026#39;); do if kubectl describe -n spire pods $pod | grep -q \u0026#34;Node:.*$1\u0026#34;; then echo \u0026#34;Restarting $podrunning on $1\u0026#34;; kubectl delete -n spire pod \u0026#34;$pod\u0026#34;; fi done fi } Run the renewncnjoin function on the NCN where kubectl is running:\n# renewncnjoin NODE_XNAME The spire-agent service may also fail if an NCN was powered off for too long and its tokens expired. If this happens, delete /root/spire/agent_svid.der, /root/spire/bundle.der, and /root/spire/data/svid.key off the NCN before deleting the request-ncn-join-token daemonset pod.\n# rm /root/spire/agent_svid.der # rm /root/spire/bundle.der # rm /root/spire/data/svid.key "
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/update_ncn_passwords/",
	"title": "Update NCN Passwords",
	"tags": [],
	"description": "",
	"content": "Update NCN Passwords Change the passwords for non-compute nodes (NCNs) on the system using the rotate-pw-mgmt-nodes.yml Ansible playbook provided by CSM or by running the CSM site.yml playbook.\nThe NCNs deploy with a default password, which are changed during the system install. See Change NCN Image Root Password and SSH Keys for more information.\nIt is a recommended best practice for system security to change the root password after the install is complete.\nThe NCN root user password is stored in the HashiCorp Vault instance, and applied with the csm.password Ansible role via a CFS session.\n NOTE: The root password is also updated when applying the CSM configuration layer during NCN personalization using the site.yml playbook. See the Configure Non-Compute Nodes with CFS procedure for more information.\n Use the following procedure with the rotate-pw-mgmt-nodes.yml playbook to change the root password as a quick alternative to running a full NCN personalization.\nProcedure   Generate a new password hash for the root user. Replace PASSWORD with the root password that will be used.\nncn# openssl passwd -6 -salt $(\u0026lt; /dev/urandom tr -dc _A-Z-a-z-0-9 | head -c4) PASSWORD   Get the HashiCorp Vault root token:\nncn# kubectl get secrets -n vault cray-vault-unseal-keys -o jsonpath=\u0026#39;{.data.vault-root}\u0026#39; | base64 -d; echo   Write the password hash from step 1 to the HashiCorp Vault. The vault login command will request the token value from the output of step 2 above. The vault read command verifies the hash was stored correctly.\nNOTE: It is important to enclose the hash in single quotes to preserve any special characters.\nncn# kubectl exec -itn vault cray-vault-0 -- sh export VAULT_ADDR=http://cray-vault:8200 vault login vault write secret/csm/management_nodes root_password=\u0026#39;HASH\u0026#39; vault read secret/csm/management_nodes exit   Create a CFS configuration layer to run the password change Ansible playbook. Replace the branch name in the JSON below with the branch in the CSM configuration management Git repository that is in use. Alternatively, the branch key can be replaced with the commit key and the git commit id that is in use. See Use Branches in Configuration Layers for more information.\nncn# cat config.json { \u0026#34;layers\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;ncn-password-update\u0026#34;, \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;rotate-pw-mgmt-nodes.yml\u0026#34;, \u0026#34;branch\u0026#34;: \u0026#34;ADD BRANCH NAME HERE\u0026#34; } ] } ncn# cray cfs configurations update ncn-password-update --file ./config.json   Create a CFS configuration session to apply the password update.\nncn# cray cfs sessions create --name ncn-password-update-`date +%Y%m%d%H%M%S` --configuration-name ncn-password-update NOTE: Subsequent password changes need only update the password hash in HashiCorp Vault and create the CFS session as long as the branch of the CSM configuration management repository has not changed.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/resync_keycloak_users_to_compute_nodes/",
	"title": "Re-sync Keycloak Users To Compute Nodes",
	"tags": [],
	"description": "",
	"content": "Re-Sync Keycloak Users to Compute Nodes Resubmit the keycloak-users-localize job and run the keycloak-users-compute.yml Ansible play to sync the users and groups from Keycloak to the compute nodes. This procedure alters the /etc/passwd and /etc/group files used on compute nodes.\nUse this procedure to quickly synchronize changes made in Keycloak to the compute nodes.\nProcedure   Resubmit the keycloak-users-localize job.\nThe output might appear slightly different than in the example below.\nncn-w001# kubectl get job -n services -l app.kubernetes.io/name=cray-keycloak-users-localize \\ -ojson | jq \u0026#39;.items[0]\u0026#39; \u0026gt; keycloak-users-localize-job.json ncn-w001# cat keycloak-users-localize-job.json | jq \u0026#39;del(.spec.selector)\u0026#39; | \\ jq \u0026#39;del(.spec.template.metadata.labels)\u0026#39; | kubectl replace --force -f - job.batch \u0026#34;keycloak-users-localize-1\u0026#34; deleted job.batch/keycloak-users-localize-1 replaced   Watch the pod to check the status of the job.\nThe pod will go through the normal Kubernetes states. It will stay in a Running state for a while, and then it will go to Completed.\nncn-w001# kubectl get pods -n services | grep keycloak-users-localize keycloak-users-localize-1-sk2hn 0/2 Completed 0 2m35s   Check the pod\u0026rsquo;s logs.\nReplace the KEYCLOAK_POD_NAME value with the pod name from the previous step.\nncn-w001# kubectl logs -n services KEYCLOAK_POD_NAME keycloak-localize \u0026lt;logs showing it has updated the \u0026#34;s3\u0026#34; objects and ConfigMaps\u0026gt; 2020-07-20 18:26:15,774 - INFO - keycloak_localize - keycloak-localize complete   Sync the users and groups from Keycloak to the compute nodes.\n  Get the crayvcs password for pushing the changes.\nncn-w001# kubectl get secret -n services vcs-user-credentials \\ --template={{.data.vcs_password}} | base64 --decode   Checkout content from the cos-config-management VCS repository.\nncn-w001# git clone https://api-gw-service-nmn.local/vcs/cray/cos-config-management.git ncn-w001# cd cos-config-management ncn-w001# git checkout integration   Create the group_vars/Compute/keycloak.yaml file.\nThe file should contain the following values:\n--- keycloak_config_computes: True   Push the changes to VCS with the crayvcs username.\nncn-w001# git add group_vars/Compute/keycloak.yaml ncn-w001# git commit -m \u0026#34;Configure keycloak on computes\u0026#34; ncn-w001# git push origin integration   Do a reboot with the Boot Orchestration Service (BOS).\nncn-w001# cray bos session create --template-uuid BOS_TEMPLATE --operation reboot     "
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/remove_the_ldap_user_federation_from_keycloak/",
	"title": "Remove The LDAP User Federation From Keycloak",
	"tags": [],
	"description": "",
	"content": "Remove the LDAP User Federation from Keycloak Use the Keycloak UI or Keycloak REST API to remove the LDAP user federation from Keycloak.\nRemoving user federation is useful if the LDAP server was decommissioned or if the administrator would like to make changes to the Keycloak configuration using the Keycloak user localization tool.\nPrerequisites LDAP user federation is currently configured in Keycloak.\nProcedure Follow the steps in only one of the sections below depending on if it is preferred to use the Keycloak REST API or Keycloak administration console UI.\nUse the Keycloak Administration Console UI   Log in to the administration console.\nSee Access the Keycloak User Management UI for more information.\n  Click on User Federation under the Configure header of the navigation panel on the left side of the page.\n  Click on the Delete button on the line for the LDAP provider in the User Federation table.\n  Use the Keycloak REST API   Create a function to get a token as a Keycloak master administrator.\nMASTER_USERNAME=$(kubectl get secret -n services keycloak-master-admin-auth -ojsonpath=\u0026#39;{.data.user}\u0026#39; | base64 -d) MASTER_PASSWORD=$(kubectl get secret -n services keycloak-master-admin-auth -ojsonpath=\u0026#39;{.data.password}\u0026#39; | base64 -d) function get_master_token { curl -ks -d client_id=admin-cli -d username=$MASTER_USERNAME -d password=$MASTER_PASSWORD -d grant_type=password https://api-gw-service-nmn.local/keycloak/realms/master/protocol/openid-connect/token | python -c \u0026#34;import sys.json; print json.load(sys.stdin)[\u0026#39;access_token\u0026#39;]\u0026#34; }   Get the component ID for the LDAP user federation.\nncn-w001# COMPONENT_ID=$(curl -s -H \u0026#34;Authorization: Bearer $(get_master_token)\u0026#34; \\ https://api-gw-service-nmn.local/keycloak/admin/realms/shasta/components \\ | jq -r \u0026#39;.[] | select(.providerId==\u0026#34;ldap\u0026#34;).id\u0026#39;) ncn-w001# echo $COMPONENT_ID 57817383-e4a0-4717-905a-ea343c2b5722   Delete the LDAP user federation by performing a DELETE operation on the LDAP resource.\nThe HTTP status code should be 204.\nncn-w001# curl -i -XDELETE -H \u0026#34;Authorization: Bearer $(get_master_token)\u0026#34; \\ https://api-gw-service-nmn.local/keycloak/admin/realms/shasta/components/$COMPONENT_ID HTTP/2 204 ...   "
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/retrieve_an_authentication_token/",
	"title": "Retrieve An Authentication Token",
	"tags": [],
	"description": "",
	"content": "Retrieve an Authentication Token Retrieve a token for authenticating to the API gateway.\nThe following are important properties of authentication tokens:\n Keycloak access tokens remain valid for 365 days Secrets do not expire; they are persistent in Keycloak Tokens and/or secrets can be revoked at anytime by an admin  The API gateway uses OAuth2 for authentication. A token is required to authenticate with this gateway.\nProcedure   Retrieve a token.\nRetrieving a token depends on whether the request is based on a regular user (as defined directly in Keycloak or backed by LDAP) or a service account.\n  Resource owner password grant (user account) - In this case, the user account flow requires the username, password, and the client ID.\nIn the example below, replace myuser, mypass, and shasta in the cURL command with site-specific values. The shasta client is created during the SMS install process.\nIn the following example, the python -mjson.tool is not required, it is simply used to format the output for readability.\nncn-w001# curl -s \\  -d grant_type=password \\  -d client_id=shasta \\  -d username=myuser \\  -d password=mypass \\  https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | python -mjson.tool Expected output:\n{ \u0026#34;access_token\u0026#34;: \u0026#34;ey...IA\u0026#34;, \u0026lt;\u0026lt;-- Note this value \u0026#34;expires_in\u0026#34;: 300, \u0026#34;not-before-policy\u0026#34;: 0, \u0026#34;refresh_expires_in\u0026#34;: 1800, \u0026#34;refresh_token\u0026#34;: \u0026#34;ey...qg\u0026#34;, \u0026#34;scope\u0026#34;: \u0026#34;profile email\u0026#34;, \u0026#34;session_state\u0026#34;: \u0026#34;10c7d2f7-8921-4652-ad1e-10138ec6fbc3\u0026#34;, \u0026#34;token_type\u0026#34;: \u0026#34;bearer\u0026#34; } Use the value of access_token to make requests.\n  Client credentials (service account) - The client credentials flow requires a client ID and client secret.\nThere are a couple of ways to use a service account:\n  By creating a new service account.\n  By using the Keycloak client that was generated by the System Management Services (SMS) installation process. The client ID is admin-client. The client secret is generated during the install and put into a Kubernetes secret named admin-client-auth. Retrieve the client secret from this secret as follows:\nncn-w001# echo \u0026#34;$(kubectl get secrets admin-client-auth \\ -ojsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d)\u0026#34; 2b0d6df0-183b-40e6-93be-51c7854388a1   Given the client ID and secret, the user can retrieve a token by requesting one from Keycloak. In the example below, replace the string being assigned to client_secret with the actual client secret from the previous step.\nIn the following example, the python -mjson.tool is not required, it formats the output for readability.\nncn-w001# curl -s \\  -d grant_type=client_credentials \\  -d client_id=admin-client \\  -d client_secret=2b0d6df0-183b-40e6-93be-51c7854388a1 \\  https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | python -mjson.tool Expected output:\n{ \u0026#34;access_token\u0026#34;: \u0026#34;ey...DA\u0026#34;, \u0026lt;\u0026lt;-- Note this value \u0026#34;expires_in\u0026#34;: 300, \u0026#34;not-before-policy\u0026#34;: 0, \u0026#34;refresh_expires_in\u0026#34;: 1800, \u0026#34;refresh_token\u0026#34;: \u0026#34;ey...kg\u0026#34;, \u0026#34;scope\u0026#34;: \u0026#34;profile email\u0026#34;, \u0026#34;session_state\u0026#34;: \u0026#34;ca8ab15c-2378-40c1-8063-7a522274fce0\u0026#34;, \u0026#34;token_type\u0026#34;: \u0026#34;bearer\u0026#34; } Use the value of access_token to make requests.\n      Present the token.\nTo present the access token on the request, put it in the Authorization header on the request as a Bearer token.\nFor example:\nncn-w001# TOKEN=access_token ncn-w001# curl -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; \\ https://api-gw-service-nmn.local/apis/capmc/capmc/get_node_rules { \u0026#34;e\u0026#34;:0, \u0026#34;err_msg\u0026#34;:\u0026#34;, \u0026#34;latency_node_off\u0026#34;:60, \u0026#34;latency_node_on\u0026#34;:120, \u0026#34;latency_node_reinit\u0026#34;:180, \u0026#34;max_off_req_count\u0026#34;:-1, \u0026#34;max_off_time\u0026#34;:-1, \u0026#34;max_on_req_count\u0026#34;:-1, \u0026#34;max_reinit_req_count\u0026#34;:-1, \u0026#34;min_off_time\u0026#34;:-1 }   "
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/retrieve_the_client_secret_for_service_accounts/",
	"title": "Retrieve The Client Secret For Service Accounts",
	"tags": [],
	"description": "",
	"content": "Retrieve the Client Secret for Service Accounts Get the client secret that is generated by Keycloak when the client or service account was created. The secret can be regenerated any time with an administrative action.\nA client secret is needed to make requests using a new client or service account.\nPrerequisites A client or service account has been created. See Create a Service Account in Keycloak.\nProcedure Follow the steps in only one of the sections below depending on if it is preferred to use the Keycloak REST API or Keycloak administration console UI.\nUse the Keycloak Administration Console UI   Log in to the administration console.\nSee Access the Keycloak User Management UI for more information.\n  Click on Clients under the Configure header of the navigation panel on the left side of the page.\n  Click on the ID for the target client in the Clients table.\n  Switch to the Credentials tab.\n  Save the client secret stored in the Secret field.\n  Create a variable for the client secret.\nLeave the Keycloak UI and create a variable for the client secret on the system.\nReplace 8a91fdf2-f9c5-4c7f-8da8-49cfbb97680a with the client secret returned for the service account being used.\nncn-w001# export CLIENT_SECRET=8a91fdf2-f9c5-4c7f-8da8-49cfbb97680a   Use the Keycloak REST API   Create the get_master_token function to get a token as a Keycloak master administrator.\nMASTER_USERNAME=$(kubectl get secret -n services keycloak-master-admin-auth -ojsonpath=\u0026#39;{.data.user}\u0026#39; | base64 -d) MASTER_PASSWORD=$(kubectl get secret -n services keycloak-master-admin-auth -ojsonpath=\u0026#39;{.data.password}\u0026#39; | base64 -d) function get_master_token { curl -ks -d client_id=admin-cli -d username=$MASTER_USERNAME -d password=$MASTER_PASSWORD -d grant_type=password https://api-gw-service-nmn.local/keycloak/realms/master/protocol/openid-connect/token | python -c \u0026#34;import sys.json; print json.load(sys.stdin)[\u0026#39;access_token\u0026#39;]\u0026#34; }   Get a unique ID for a client from Keycloak.\nIn the example below, the client ID is my-test-client, which should be replaced with the client ID for the target client. The returned 82d009de-1e36-41b6-8c21-4c390a25c188 in the output is the unique ID of the client.\nncn-w001# CLIENT_ID=$(curl -s -H \u0026#34;Authorization: Bearer $(get_master_token)\u0026#34; \\ https://api-gw-service-nmn.local/keycloak/admin/realms/shasta/clients | jq -r \u0026#39;.[] \\ | select(.clientId==\u0026#34;my-test-client\u0026#34;).id\u0026#39;) ncn-w001# echo $CLIENT_ID 82d009de-1e36-41b6-8c21-4c390a25c188   Retrieve the client secret.\nIn the example below, the returned client secret is 8a91fdf2-f9c5-4c7f-8da8-49cfbb97680a.\nncn-w001# curl -s -H \u0026#34;Authorization: Bearer $(get_master_token)\u0026#34; \\ https://api-gw-service-nmn.local/keycloak/admin/realms/shasta/clients/$CLIENT_ID/client-secret | jq -r .value 8a91fdf2-f9c5-4c7f-8da8-49cfbb97680a   Create a variable for the client secret.\nReplace 8a91fdf2-f9c5-4c7f-8da8-49cfbb97680a with the client secret returned for the service account being used.\nncn-w001# export CLIENT_SECRET=8a91fdf2-f9c5-4c7f-8da8-49cfbb97680a   "
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/ssh_keys/",
	"title": "SSH Keys",
	"tags": [],
	"description": "",
	"content": "SSH Keys An SSH key is required by Ansible and several other system management services on the system. If an SSH key is not available, the Configuration Framework Service (CFS) is unusable, and Ansible cannot be invoked from ncn-w001. The SSH key is created in two parts, with a public and private key. The id_rsa.pub and id_rsa key values are located in the /root/.ssh directory on ncn-w001.\nImportant: Changing the SSH key can have serious implications. The loss of the private key can result in administrators no longer being able to connect to other nodes or containers via SSH. Ansible also uses SSH as the root user, so Ansible plays will fail as a result of the loss of the private key.\nThere is only one copy of the private part of the key, and that resides on ncn-w001. If that is lost, then passwordless SSH is no longer possible between ncn-w001 and the rest of the system.\n"
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/preserve_username_capitalization_for_users_exported_from_keycloak/",
	"title": "Preserve Username Capitalization For Users Exported From Keycloak",
	"tags": [],
	"description": "",
	"content": "Preserve Username Capitalization for Users Exported from Keycloak Keycloak converts all characters in a username to lowercase when users are exported. Use this procedure to update the keycloak-users-localize tool with a configuration option that enables administrators to preserve the username letter case when users are exported from Keycloak.\nThe LDAP server that provides password resolution and user account federation supports mixed case usernames. If the usernames are changed to lowercase when exported from Keycloak, it can cause issues.\nPrerequisites  The kubectl command is installed. Each user\u0026rsquo;s homeDirectory attribute has the exact username as the last element of the path.  Procedure   Update the user export setting in the customizations.yaml file.\nSet the userExportNameSource field to homeDirectory in the spec.kubernetes.services.cray-keycloak-users-localize field in the customizations.yaml file.\nncn-w001# vi customizations.yaml   Re-apply the cray-keycloak-users-localize Helm chart.\nRe-apply the cray-keycloak-users-localize Helm chart with the updated customizations.yaml file.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/provisioning_a_liquid-cooled_ex_cabinet_cec_with_default_credentials/",
	"title": "Provisioning A Liquid-cooled Ex Cabinet Cec With Default Credentials",
	"tags": [],
	"description": "",
	"content": "Provisioning a Liquid-Cooled EX Cabinet CEC with Default Credentials This procedure provisions a Glibc compatible SHA-512 administrative password hash to a cabinet environmental controller (CEC). This password becomes the Redfish default global credential to access the CMM controllers and node controllers (BMCs).\nThis procedure does not provision Slingshot switch BMCs. Slingshot switch BMC default credentials must be changed using the procedures in the Slingshot product documentation. Refer to \u0026ldquo;Change Rosetta Login and Redfish API Credentials\u0026rdquo; in the Slingshot Operations Guide (\u0026gt; 1.6.0).\nPrerequisites   Physical access to the CEC LCD panel to enable privileged command mode. The CEC does not enable users to set, display, or clear the password hash in restricted command mode.\n  A laptop with a terminal program such as Netcat (nc), telnet, or PuTTY that supports 10/100 IPv6 Ethernet connectivity to the CEC Ethernet port is required.\n  A generated SHA-512 hash for the CEC credentials:\n The passhash tool that is installed on the CMMs can be used to generate a SHA-512 password hash. This HPE tool is provided for convenience, but any tool that generates an SHA-512 hash that is compatible with glibc can be used. The salt portion must be between 8 and 16 chars inclusive. The CEC does not support the optional \u0026ldquo;rounds=\u0026rdquo; parameter in the hash. See the man 3 crypt page for a description: https://man7.org/linux/man-pages/man3/crypt.3.html  remote# passhash PASSWORD $6$v5YlqfghB$scBci.GbT8... Note: The example password hash is truncated to prevent using this example value. The password hash is a SHA-512 hash.\n  Procedure   Disconnect the CEC Ethernet cable from the Ethernet port.\n  Connect an Ethernet cable from an Apple Mac or Linux laptop to the CEC Ethernet port. The CEC Ethernet PHY will auto negotiate to either 10/100Mb speed and it supports auto crossover functionality. Any standard Ethernet patch cord should work for this.\n  Use the Right Arrow on the display controls to select the CEC Network Settings Menu. The IPv6 link local address is displayed on this menu.\n  Start the terminal program and use Netcat (nc), telnet, or PuTTY to connect to CEC command shell and provide the CEC IPv6 link local address.\n# nc -t -6 'fe80::a1:3e8:0%en14' 23 # telnet fe80::a1:3e8:0%eth0   en14 and eth0 in these examples are the Ethernet interfaces for the laptop.\n  Enter return a few times to start the connection.\n  Note: If the network connection to the CEC is lost, or if a CEC command does not return to the prompt, it may be necessary to reboot the CEC. Use the Right Arrow on the CEC control panel to display the Action menu, select Reset CEC, and press the green checkmark button to reboot the CEC. Then re-establish the nc or telnet connection.\n    From the CEC\u0026gt; prompt, enter help to view the list of commands.\nCEC\u0026gt; help Caution: Run only the CEC commands in this procedure. Do not change other CEC settings.\n  From the CEC\u0026gt; prompt, generate an unlock token for the CEC. Use the enable command (alias for unlock command) without arguments to display a random unlock token on the CEC front panel.\nCEC\u0026gt; enable ab12903c   Record the unlock token displayed on the CEC front panel.\nThe unlock code is valid as long as the remote shell connection is open to the CEC. If you enter the unlock token incorrectly, a new unlock token is displayed on the front panel.\n  Enter the enable command again but supply the token as an argument to unlock the CEC and enter privileged command mode.\nCEC\u0026gt; enable AB12903C EXE\u0026gt; If the token code is typed in incorrectly a new one is generated on screen. When unlocked, the LCD screen displays UNLOCKED and the shell prompt changes to EXE\u0026gt;.\nDo not use the get_hash command to display the password hash. If there is no password hash set, this command will not return to the prompt and the connection will be lost.\n  Enter set_hash and provide the password hash value as the argument.\nThe CEC validates the input syntax of the hash. Adding an extra char or omitting a character is flagged as an error. I a character is changed, the password entered in the serial console login shell or the Redfish root account will not work. If that happens, rerun the set_hash command on the CEC and reboot the CMMs.\nEXE\u0026gt; set_hash $6$v5YlqxKB$scBci.GbT8... Note: Example truncated to prevent accidental setting of production password hash to example values. The password hash is a SHA-512 hash.\n  Exit privileged command mode.\nEXE\u0026gt; lock CEC\u0026gt; The CEC remains in privileged mode until it is reset with the lock command or if the X button on the CEC front panel is pressed. Typing exit or terminating the connection exits privileged mode. There is no connection timeout.\n  Use the front panel Right Arrow to select the CEC Action menu.\n  Reset the CMMs 3, 2, 1, and 0.\nThe Reset CMM commands reboot either the even numbered, or odd numbered CMMs in the cabinet, depending on which CEC is issuing the commands.\n  Important!: Power cycle the compute blade slots in each chassis.\n  If Cray System Management (CSM) is provisioned, use CAPMC to power cycle the compute blade slots (example show cabinets 1000-1003). Note: If a chassis is not fully populated, specify each slot individually:\nncn-m001# cray capmc xname_off create --xnames x[1000-1003]c[0-7]s[0-7] --format json Check the power status:\nncn-m001# cray capmc get_xname_status create --xnames x[1000-1003]c[0-7] --format json Power on the compute chassis slots:\nncn-m001# cray capmc xname_on create --xnames x[1000-1003]c[0-7]s[0-7] --format json   If the cabinet has not been provisioned with CSM or other management software (bare-metal), the compute chassis slots are most likely powered off. To perform chassis power control operations, SSH to a CMM and and use the redfish -h command to display the power control commands:\n\u0026gt; ssh root@x9000c1 x9000c1:\u0026gt; redfish -h \u0026quot;redfish\u0026quot; -- redfish API debugging tool \u0026lt;snip\u0026gt; redfish chassis status redfish chassis power [on|off|forceoff] redfish [blade|perif] [0-7] [on|off|forceoff] redfish node status redfish node [0-1] [on|off|forceoff] \u0026lt;snip\u0026gt; x9000c1:\u0026gt;     To test the password, connect to the CMM serial console though the CEC. The IPv6 address is the same, but the port numbers are different as described below.\n#!/bin/bash trap \u0026quot;stty sane \u0026amp;\u0026amp; echo ''\u0026quot; EXIT stty -icanon -echo nc -6 'fe80::a1:2328:0%en14' 50000  The even numbered CEC manages the CMM serial console for chassis 0, 2, 4, 6 on TCP port numbers 50000-50003 respectively. The odd numbered CEC manages the CMM serial console for chassis 1, 3, 5, 7 on TCP port numbers 50000-50003 respectively. If using the script shown in the example to connect to the CMM console, type exit to return to the CMM login prompt and enter ctrl-c to close the console connection.    Perform this procedure for each CEC in all system cabinets.\n  HPE Cray EX3000 and EX4000 cabinets have two CECs per cabinet.\n  HPE Cray EX2000 cabinets have a single CEC per cabinet.\n    "
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/public_key_infrastructure_pki/",
	"title": "Public Key Infrastructure (PKI)",
	"tags": [],
	"description": "",
	"content": "Public Key Infrastructure (PKI) Public Key Infrastructure (PKI) represents the algorithms, infrastructure, policies, and processes required to leverage applied public key cryptography methods for operational security use cases. The Rivest-Shamir-Adleman (RSA) and Elliptic-curve (ECC) are some example algorithm systems.\nThe use of PKI for the system is in the Transport Layer Security (TLS) protocol, which is the successor of the now deprecated Secure Sockets Layer (SSL). This is where trusted chains of Certificate Authorities (CAs) are used to authenticate the identity of servers, and sometimes clients (for example, mutual TLS) for relying parties. This chain of trust is anchored by a root CA and is used to make assertions that a particular public and private key pair belong to a given party by assigning a certificate for the party. This party is still required to prove they actually own the key material through enciphering, deciphering, and digital signature operations that require private keys that are not shared amongst parties. However, public keys are shared through certificates and are policy bound in that respect.\nThe PKI implementation for the system, post-installation, is made up of Kubernetes services (illustrated in the \u0026ldquo;Public Key Infrastructure\u0026rdquo; figure). During installation, the platform can be directed to either generate certificate authorities (CAs), or a customer-supplied intermediate CA can be supplied. After installation, the CA material resides in a Kubernetes Secret, and ultimately in the HashiCorp Vault.\nRefer to PKI Services for more information on the services in the figure above.\nLimitations The following limitations exist within the PKI implementation:\n  An outstanding bug in the Keycloak Gatekeeper service prevents it from updating its TLS certificate and key material upon Cert-manager renewal\n It may be necessary to monitor the situation and proactively renew/force reload Keycloak Gatekeeper.    Supported Cryptography Suite(s)\n  RSA-based CAs and certificates are currently supported. CAs must have either a 3072- or 4096-bit modulus and use SHA256 as the signature algorithm. Installation paths are designed to force convention.\nPassword encrypted private keys are also not currently supported.\n    CA \u0026ldquo;Rotation\u0026rdquo;\n Changing the platform CA post-installation is not currently supported. Changing it requires a re-install.    Implications of Transitive Trust\n  If the platform is configured to generate a dynamic CA, then customer services or users that interact with the platform must trust the platform CA to validate TLS sessions. Thus, provided the platform has a disjoint DNS domain name (for example, shasta.acme.org), and the PKI trust realm is established at or \u0026lsquo;below\u0026rsquo; (subdomains) this FQDN, a compromise of platform CA material should be limited to the platform itself (subject to many nuances).\nIf a customer supplies a CA to the platform, and the CA is part of an expanded PKI trust realm, a compromise of platform CA material could be leveraged to compromise the broader environment through PKI APIs available on the system. Customers should consider this risk, and, if providing a CA is desired, maybe strictly limit the PKI trust realm established by the provided CA.\n    Abuse of PKI APIs to Sign Malicious Products\n  Compromise of a platform could lead to the generation of certificates for potentially malicious workloads.\nCurrent HashiCorp Vault policies that control legitimate signing activities are fairly broad in allowed certificate CSR properties. This is due largely to common name and SAN requirements for certificate workloads across the platform.\n    Security of CA Material\n  During installation, CA material is exposed:\n  When they are staged by the installer\n  By installation processes (e.g., shasta-cfg) After installation, CA material is exposed:\n  In a SealedSecret\n  In a Kubernetes Secret\n  In Kubernetes etcd backups and backups taken of the platform otherwise\n  To Vault\n  Through the creation of additional subordinate CAs for Spire\n      Revocation Lists and OSCP\n The platform does not provide revocation lists or access to a revocation service (OSCP).    Key Escrow\n The platform does not provide any key escrow services.    "
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/remove_internal_groups_from_the_keycloak_shasta_realm/",
	"title": "Remove Internal Groups From The Keycloak Shasta Realm",
	"tags": [],
	"description": "",
	"content": "Remove Internal Groups from the Keycloak Shasta Realm Remove a group in the Keycloak Shasta realm. Unused Keycloak groups can be removed.\nPrerequisites   This procedure assumes the user has already accessed Keycloak\u0026rsquo;s user management interface. See Access the Keycloak User Management UI for more information.\n  This procedure assumes that the password for the Keycloak admin account is known. The Keycloak password is set during the software installation process. The password can be obtained using the following command:\nncn-w001# kubectl get secret -n services keycloak-master-admin-auth \\ --template={{.data.password}} | base64 --decode   Procedure   Click the Groups text in the Manage section in the navigation area on the left side of the screen.\n  Search for the group and select the group in the groups table.\n  Click the Delete button at the top of the table.\n  Once the groups are removed from Keycloak, follow the instructions in Re-Sync Keycloak Users to Compute Nodes to update the groups on the compute nodes.\n"
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/remove_the_email_mapper_from_the_ldap_user_federation/",
	"title": "Remove The Email Mapper From The LDAP User Federation",
	"tags": [],
	"description": "",
	"content": "Remove the Email Mapper from the LDAP User Federation The email mapper is automatically added to the LDAP user federation in Keycloak, but it can be removed. The system does not use the user\u0026rsquo;s email for anything, so this function can be removed.\nIf there are duplicate email addresses for LDAP users, it can cause Keycloak to have issues syncing with LDAP. Removing the email mapper will fix this problem.\nProcedure   Create a function to get a token as a Keycloak master administrator.\nMASTER_USERNAME=$(kubectl get secret -n services keycloak-master-admin-auth -ojsonpath=\u0026#39;{.data.user}\u0026#39; | base64 -d) MASTER_PASSWORD=$(kubectl get secret -n services keycloak-master-admin-auth -ojsonpath=\u0026#39;{.data.password}\u0026#39; | base64 -d) function get_master_token { curl -ks -d client_id=admin-cli -d username=$MASTER_USERNAME -d password=$MASTER_PASSWORD -d grant_type=password https://api-gw-service-nmn.local/keycloak/realms/master/protocol/openid-connect/token | python -c \u0026#34;import sys.json; print json.load(sys.stdin)[\u0026#39;access_token\u0026#39;]\u0026#34; }   Verify that the new get_master_token function is working.\nIf the function is working, it will return a long string that is base64-encoded.\nncn-w001# get_master_token eyJhbGciO...YceX4Ig   Get the ID of the LDAP user federation.\nReplace SHASTA-USER-FEDERATION-LDAP in the command below with the name of the user federation being used.\nncn-w001# FEDERATION_ID=$(curl -s -H \u0026#34;Authorization: Bearer $(get_master_token)\u0026#34; \\ https://api-gw-service-nmn.local/keycloak/admin/realms/shasta/components?name=SHASTA-USER-FEDERATION-LDAP \\ | jq .[0].id -r) ncn-w001# echo $FEDERATION_ID e080d20a-51b0-40ad-8f21-98f7b752e39c   Get the ID of the email mapper.\nncn-w001# EMAIL_COMPONENT_ID=$(curl -s -H \u0026#34;Authorization: Bearer $(get_master_token)\u0026#34; \\ \u0026#34;https://api-gw-service-nmn.local/keycloak/admin/realms/shasta/components?name=email\u0026amp;parent=$FEDERATION_ID\u0026#34; \\ | jq .[0].id -r) ncn-w001# echo $EMAIL_COMPONENT_ID ba3cfe20-c2ed-4c92-aac0-3b6fc865989c   Delete the email mapper.\nncn-w001# curl -i -s -XDELETE -H \u0026#34;Authorization: Bearer $(get_master_token)\u0026#34; \\ https://api-gw-service-nmn.local/keycloak/admin/realms/shasta/components/$EMAIL_COMPONENT_ID   Verify in the Keycloak UI that there is no longer an email mapper for the LDAP user federation.\nFor more information on accessing the Keycloak UI, see Access the Keycloak User Management UI.\nThe email row shown in the image below should no longer be present.\n  Click the Synchronize all users button in the Settings tab for the LDAP user federation.\nThe Synchronize all users button will be at the bottom of the page.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/make_https_requests_from_sources_outside_the_management_kubernetes_cluster/",
	"title": "Make HTTPS Requests From Sources Outside The Management Kubernetes Cluster",
	"tags": [],
	"description": "",
	"content": "Make HTTPS Requests from Sources Outside the Management Kubernetes Cluster Clients lying outside the system\u0026rsquo;s management cluster need to trust the Certificate Authority (CA) certificate or host certificate in order to make requests to a non-compute node (NCN). Getting the client system to trust the CA certificate depends on the operating system.\nThis procedure shows an example of how to have a client trust the system\u0026rsquo;s CA certificate on a Mac OS X system.\nUpdating the default certificates and password of the Keycloak master admin is not supported.\nPrerequisites This procedure assumes that it is being carried out on a Mac OS X system.\nProcedure   Retrieve the CA public certificate file.\nThe scp command is used in the following example to copy the CA public certificate file to the current directory.\n$ scp sms-jones.us.cray.com:/etc/pki/trust/anchors/platform-ca-certs.crt . certificate_authority.crt 100% 989 1.4MB/s 00:00 Troubleshooting: Request a copy of the certificate file from a system administrator if it is not possible to log on to the NCN.\n  Use Finder to locate the .crt file.\n  Double-click the .crt file to add it to the system keychain.\nThe .crt file might still not be trusted, therefore, start up the Keychain Access utility to mark it as trusted for SSL.\n  Double-click the untrusted certificate.\n  Expand the Trust menu item to set Change Secure Sockets Layer (SSL) to Always Trust.\n  Quit Keychain Access.\n  Make secure requests to any NCN as needed.\nMaking a secure request to an NCN requires users to pass an authentication token. See Retrieve an Authentication Token to set the $TOKEN variable used in the example below.\nFor example:\n$ curl -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; \\ https://api.SYSTEM-NAME_DOMAIN-NAME/apis/capmc/capmc/get_node_rules { \u0026#34;e\u0026#34;:0, \u0026#34;err_msg\u0026#34;:\u0026#34;, \u0026#34;latency_node_off\u0026#34;:60, \u0026#34;latency_node_on\u0026#34;:120, \u0026#34;latency_node_reinit\u0026#34;:180, \u0026#34;max_off_req_count\u0026#34;:-1, \u0026#34;max_off_time\u0026#34;:-1, \u0026#34;max_on_req_count\u0026#34;:-1, \u0026#34;max_reinit_req_count\u0026#34;:-1, \u0026#34;min_off_time\u0026#34;:-1 }   "
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/manage_system_passwords/",
	"title": "Manage System Passwords",
	"tags": [],
	"description": "",
	"content": "Manage System Passwords Many system services require login credentials to gain access to them. The information below is a comprehensive list of system passwords and how to change them.\nContact HPE Cray service in order to obtain the default usernames and passwords for any of these components or services.\nKeycloak Default Keycloak admin user login credentials:\n  Username: admin\n  The password can be obtained with the following command:\nncn-w001# kubectl get secret -n services keycloak-master-admin-auth \\ --template={{.data.password}} | base64 --decode   To update the default password for the admin account, refer to Change the Keycloak Admin Password.\nTo create new accounts, refer to Create Internal User Accounts in the Keycloak Shasta Realm.\nGitea The initial Gitea login credentials for the crayvcs username are stored in three places:\n  vcs-user-credentials Kubernetes secret - This is used to initialize the other two locations, as well as providing a place where users can query for the password.\nThe password can be obtained using this command:\nncn-w001# kubectl get secret -n services vcs-user-credentials \\ --template={{.data.vcs_password}} | base64 --decode The password can be changed using this command:\nncn-w001# kubectl create secret generic vcs-user-credentials \\ --save-config --from-literal=vcs_username=\u0026#34;crayvcs\u0026#34; --from-literal=vcs_password=\u0026#34;NEW_PASSWORD\u0026#34; \\ --dry-run -o yaml | kubectl apply -f -   Gitea - These credentials are used when pushing to Git using the default username and password. The password should be changed through the Gitea UI.\n  Keycloak - These credentials are used to allow access to the Gitea UI. They must be changed through Keycloak.\n  IMPORTANT: These three sources of credentials are not currently synced by any mechanism, and so changing the default password requires that it be changed in all three places. Changing only one many result in difficulty determining the password at a later date, or may result in lost access to Gitea.\nSystem Management Health Service Contact HPE Cray service in order to obtain the default password for Grafana and Kiali. The default username is admin.\nManagement Network Switches Each rack type includes a different set of passwords. During different stages of installation, these passwords are subject to change. Contact HPE Cray service in order to obtain the default passwords.\nThe tables below include the default login credentials for each rack type. These passwords can be changed by going into the console on a given switch and changing it. However, if the user gets locked out attempting to change the password or the configuration gets corrupted for an individual switch, it can wipe out the entire network configuration for the system.\nLiquid Cooled Cabinet:\n   Name Role Switch IP Address Login     smn01 Leaf/Mgmt Dell S3048-ON 10.254.0.2 admin   smn02 Spine-001 Mellanox SN2100 10.254.0.1 admin   smn03 Spine-002 Mellanox SN2100 10.254.0.3 admin    Air Cooled Cabinet:\n   Name Role Switch IP Address Login     mtnsw01 Leaf/Mgmt Dell S3048-ON 10.254.0.2 admin    Coolant Distribution Unit (CDU):\n   Name Role Switch IP Address Login     cdu-s1 CDU Dell S4048T-ON 10.254.0.2 admin    ClusterStor:\n   Name Role Switch IP Address Login     Arista  DCS-7060CX-32S 172.16.249.10 admin   Sonexion Entry point to Arista CS-L300 172.30.49.178 admin    Redfish Credentials Redfish accounts are only valid with the Redfish API. They do not allow system logins via ssh or serial console. Three accounts are created by default:\n Root - Administrative account  Username: root Password:    Operator - Power components on/off, read values, and configure accounts  Username: operator Password:    ReadOnly - Log in, configure self, and read values  Username: guest Password:     Contact HPE Cray service in order to obtain the default passwords.\nThe account database is automatically saved to the non-volatile settings partition (/nvram/redfish/redfish-accounts) any time an account or account policy is modified. The file is stored as a redis command dump and is replayed (if it exists) anytime the core Redfish schema is loaded via the init script. If default accounts must be restored, delete the redis command dump and reboot the controller.\nList accounts:\nUse the following API path to list all accounts:\nGET /redfish/v1/AccountService/Accounts { \u0026#34;@odata.context\u0026#34;: \u0026#34;/redfish/v1/$metadata#ManagerAccountCollection.ManagerAccountCollection\u0026#34;, \u0026#34;@odata.etag\u0026#34;: \u0026#34;W/\\\u0026#34;1559675674\\\u0026#34;, \u0026#34;@odata.id\u0026#34;: \u0026#34;/redfish/v1/AccountService/Accounts\u0026#34;, \u0026#34;@odata.type\u0026#34;: \u0026#34;#ManagerAccountCollection.ManagerAccountCollection\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Collection for Manager Accounts\u0026#34;, \u0026#34;Members\u0026#34;: [ { \u0026#34;@odata.id\u0026#34;: \u0026#34;/redfish/v1/AccountService/Accounts/1\u0026#34; }, { \u0026#34;@odata.id\u0026#34;: \u0026#34;/redfish/v1/AccountService/Accounts/2\u0026#34; } ], \u0026#34;Members@odata.count\u0026#34;: 2, \u0026#34;Name\u0026#34;: \u0026#34;Accounts Collection\u0026#34; } Use the following API path to list a single account:\nGET /redfish/v1/AccountService/Accounts/1 { \u0026#34;@odata.context\u0026#34;: \u0026#34;/redfish/v1/$metadata#ManagerAccount.ManagerAccount(*)\u0026#34;, \u0026#34;@odata.etag\u0026#34;: \u0026#34;W/\u0026#34;1559675272\u0026#34;, \u0026#34;@odata.id\u0026#34;: \u0026#34;/redfish/v1/AccountService/Accounts/1\u0026#34;, \u0026#34;@odata.type\u0026#34;: \u0026#34;#ManagerAccount.v1_1_1.ManagerAccount\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Default Account\u0026#34;, \u0026#34;Enabled\u0026#34;: true, \u0026#34;Id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;Links\u0026#34;: { \u0026#34;Role\u0026#34;: { \u0026#34;@odata.id\u0026#34;: \u0026#34;/redfish/v1/AccountService/Roles/Administrator\u0026#34; } }, \u0026#34;Locked\u0026#34;: false, \u0026#34;Name\u0026#34;: \u0026#34;Default Account\u0026#34;, \u0026#34;RoleId\u0026#34;: \u0026#34;Administrator\u0026#34;, \u0026#34;UserName\u0026#34;: \u0026#34;root\u0026#34; } Add accounts:\nIf an account is successfully created, then the account information data structure will be returned. The most important bit returned is the Id because it is part of the URL used for any further manipulation of the account. Use the following API path to add accounts:\nPOST /redfish/v1/AccountService/Accounts Content-Type: application/json { \u0026#34;Name\u0026#34;: \u0026#34;Test Account\u0026#34;, \u0026#34;RoleId\u0026#34;: \u0026#34;Administrator\u0026#34;, \u0026#34;UserName\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;test123\u0026#34;, \u0026#34;Locked\u0026#34;: false, \u0026#34;Enabled\u0026#34;: true } Response: { \u0026#34;@odata.context\u0026#34;: \u0026#34;/redfish/v1/$metadataAccountService/Members/Accounts\u0026#34;, \u0026#34;@odata.etag\u0026#34;: \u0026#34;W/\u0026#34;1559679136\u0026#34;, \u0026#34;@odata.id\u0026#34;: \u0026#34;/redfish/v1/AccountService/Accounts\u0026#34;, \u0026#34;@odata.type\u0026#34;: \u0026#34;#ManagerAccount.v1_1_1.ManagerAccount\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Collection of Account Details\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;5\u0026#34;, **\u0026lt;\u0026lt;-- Note this value** \u0026#34;Links\u0026#34;: { \u0026#34;Role\u0026#34;: { \u0026#34;@odata.id\u0026#34;: \u0026#34;/redfish/v1/AccountService/Roles/Administrator\u0026#34; } }, \u0026#34;Enabled\u0026#34;: true, \u0026#34;Locked\u0026#34;: false, \u0026#34;Name\u0026#34;: \u0026#34;Test\u0026#34;, \u0026#34;RoleId\u0026#34;: \u0026#34;Administrator\u0026#34;, \u0026#34;UserName\u0026#34;: \u0026#34;test\u0026#34; } Delete accounts:\nDelete an account with the curl command:\n# curl -u root:xxx -X DELETE https://x0c0s0b0/redfish/v1/AccountService/Accounts/ACCOUNT_ID Update passwords:\nUpdate an account\u0026rsquo;s password with the curl command:\n# curl -u root:xxx -X PATCH \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{\u0026#34;Name\u0026#34;: \u0026#34;Test\u0026#34;}\u0026#39; \\ https://x0c0s0b0/redfish/v1/AccountService/Accounts/ACCOUNT_ID System Controllers For SSH access, the system controllers have the following default credentials:\n Node controller (nC)  Username: root Password:    Chassis controller (cC)  Username: root Password:    Switch controller (sC)  Username: root Password:    sC minimal recovery firmware image (rec)  Username: root Password:     Contact HPE Cray service in order to obtain the default passwords.\nPasswords for nC, cC, and sC controllers are all managed with the following process. The cfgsh tool is a configuration shell that can be used interactively or scripted. Interactively, it may be used as follows after logging in as root via ssh:\nx0c1# config x0c1(conf)# CURRENT_PASSWORD root NEW_PASSWORD x0c1(conf)# exit x0c1# copy running-config startup-config x0c1# exit It may be used non-interactively as well. This is useful for separating out several of the commands used for the initial setup. The shell utility returns non-zero on error.\n# cfgsh --config CURRENT_PASSWORD root NEW_PASSWORD # cfgsh copy running-config startup-config In both cases, a running-config must be saved out to non-volatile storage in a startup configuration file. If it is not, the password will revert to default on the next boot. This is the exact same behavior as standard managed Ethernet switches.\nGigabyte Contact HPE Cray service in order to obtain the default password for Gigabyte. The default username is admin.\n"
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/pki_certificate_authority_ca/",
	"title": "PKI Certificate Authority (ca)",
	"tags": [],
	"description": "",
	"content": "PKI Certificate Authority (CA) An instance of HashiCorp Vault, deployed via the Bitnami Bank-vaults operator, stores private and public Certificate Authority (CA) material, and serves APIs through a Public Key Infrastructure (PKI) engine instance.\nCA material is injected as a start-up secret into Vault through a SealedSecret that translates into a Kubernetes Secret.\nCA Certificate Distribution Trusted CA certificates are distributed via two channels:\n Cloud-init metadata Kubernetes ConfigMaps  Kubernetes-native workloads generally leverage ConfigMap-based distribution. The Cloud-init based method is used for non-compute node (NCN) OS distribution.\nOn NCNs, trusted certificates are installed by Cloud-init in the /etc/pki/trust/anchors/platform-ca-certs.crt file. Refer to Make HTTPS Requests from Sources Outside the Management Kubernetes Cluster for more information for clients that are outside the system\u0026rsquo;s management cluster.\nOn compute nodes (CNs), trusted certificates are installed at image build time by the Image Management Service (IMS), and are located in the /etc/cray/ca/certificate_authority.crt file.\nFor NCNs and CNs, the trusted certificates are added to the base OS trust store. The TrustedCerts Kubernetes Operator manages updates to trusted CA material across the noted channels.\n"
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/pki_services/",
	"title": "PKI Services",
	"tags": [],
	"description": "",
	"content": "PKI Services The services in this section are integral parts of the Public Key Infrastructure (PKI) implementation.\nHashiCorp Vault A deployment of HashiCorp Vault, managed via the Bitnami Bank-vaults operator, stores private and public Certificate Authority (CA) material, and serves APIs through a PKI engine instance. This instance also serves as a general secrets engine for the system.\nKubernetes service account authorization is utilized to authenticate access to Vault. The configuration of Vault, as deployed on the system, can be viewed with the following command:\n# kubectl get vault -n vault cray-vault -o yaml A Kubernetes operator manages the deployment of Vault, based on this definition. The resulting instance will also be deployed to the vault namespace.\nIMPORTANT: Changing the cray-vault custom resource definition is not supported unless directed by customer support.\nFor more information, refer to the following resources:\n HashiCorp Vault https://banzaicloud.com/docs/bank-vaults/overview/ https://www.vaultproject.io/docs  Jetstack Cert-manager A deployment of Jetstack Cert-manager provides a Kubernetes-native API to request x.509 certificate and key management operations.\nCert-manager is integrated with HashiCorp Vault for use as a CA. Cert-manager generates key material and a certificate signing request (CSR), and then submits the CSR to Vault for signature. Once Vault has signed the certificate, it is made available, along with other key materials, via a Kubernetes Secret. Kubernetes pods or other platform aware components can then source the resulting secret.\nCert-manager will also automatically manage renewal of certificates prior to their expiration time. Cert-manager is deployed on the system using namespace-specific (certificate) issuers.\nTo view Issuers:\n# kubectl get issuer -A -o wide To view certificates:\n# kubectl get certificate -A -o wide Once a certificate is ready, the resulting secret will contain the following data fields:\n   Field Description     ca.crt Contains trusted CA certificates   tls.crt Contains the generated certificate, along with trusted CA certificates in the trust chain   tls.key Contains the private key    To view certificate signing requests:\n# kubectl get certificaterequest -A -o wide The Cert-manager workload is deployed to the cert-manager namespace.\nFor more information, refer to the following resources:\n https://cert-manager.io/docs/  TrustedCerts Operator The TrustedCerts Operator is an HPE Kubernetes Operator. It acts on the TrustedCertificates custom resource definitions. Its function is to source CA certificates via use of a Vault API, and then distribute them.\nTo see the deployed TrustedCertificates resources:\n# kubectl get trustedcertificates -A These resources can be used to further examine the ConfigMap and Boot Script Service (BSS) destination references. The TrustedCerts workload is deployed to the pki-operator namespace.\n"
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/default_keycloak_realms_accounts_and_clients/",
	"title": "Default Keycloak Realms, Accounts, And Clients",
	"tags": [],
	"description": "",
	"content": "Default Keycloak Realms, Accounts, and Clients The following default realms, accounts, and clients are created when the system software is installed:\nDefault Realms  Master Shasta  Default Keycloak Accounts Username: admin\nThe password can be obtained with the following command:\nncn-w001# kubectl get secret -n services keycloak-master-admin-auth \\ --template={{.data.password}} | base64 --decode The password for the admin account can be changed. See Change the Keycloak Admin Password.\nDefault Keycloak Clients Users authenticate to Keycloak on behalf of a client. Keycloak clients own configurations, such as the mapping of Keycloak user information to data available to either the userinfo endpoint, or in the JWT token. Keycloak clients also own resources, such as URIs.\n   Client Type Descriptions     admin-client Private The admin-client client represents a service account that is used during the install to register the services with the API gateway. The secret for this account is generated during the software installation process.   shasta Public The shasta client is meant to be a generic client that can be used to access any Cray micro-service. The SMS install process creates the shasta client in the Shasta realm. The shasta client is public and has mappers set up so that the uidNumber, gidNumber, homeDirectory, and loginShell user attributes are included in the userinfo response. The shasta client has 2 roles created for authorization: admin and user.   gatekeeper Private The gatekeeper client is used by the keycloak-gatekeeper to authenticate web UIs using OAUTH.   system-compute-client Private The system-compute-client client is used by the Cray Operating System (COS) for compute nodes and some NCN services for boot orchestration and management.   system-pxe-client Private The system-pxe-client client is used by the cray-ipxe service to communicate with cray-bss to prepare boot scripts and other boot-related content.    "
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/delete_internal_user_accounts_from_the_keycloak_shasta_realm/",
	"title": "Delete Internal User Accounts In The Keycloak Shasta Realm",
	"tags": [],
	"description": "",
	"content": "Delete Internal User Accounts in the Keycloak Shasta Realm Manually delete a user account in the Keycloak Shasta realm. User accounts are maintained via the Keycloak user management UI.\nRemoving an account from Keycloak is a good way to revoke admin or user privileges.\nPrerequisites   This procedure assumes the user has already accessed Keycloak\u0026rsquo;s user management interface. See Access the Keycloak User Management UI for more information.\n  This procedure assumes that the password for the Keycloak admin account is known. The Keycloak password is set during the software installation process. The password can be obtained with the following command:\nncn-w001# kubectl get secret -n services keycloak-master-admin-auth \\ --template={{.data.password}} | base64 --decode   Procedure   Navigate to the Users tab.\n  Search for the username or ID of the account that is being deleted.\n  Click the Delete button in the Actions column of the table to remove the desired account.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/get_a_long-lived_token_for_a_service_account/",
	"title": "Get A Long-lived Token For A Service Account",
	"tags": [],
	"description": "",
	"content": "Get a Long-Lived Token for a Service Account et up a long-lived offline token for a service account using the Keycloak REST API. Keycloak implements the OpenID Connect protocol, so this is a standard procedure for any OpenID Connect server.\nRefer to https://www.keycloak.org/docs/latest/server_admin/index.html#_offline-access for more information.\nPrerequisites  A client or service account has been created. See Create a Service Account in Keycloak. The CLIENT_SECRET variable has been set up. See Retrieve the Client Secret for Service Accounts.  Procedure   Get a long-lived token for a service account.\nEnsure the following have been done before running the command below:\n Replace the my-test-client value in the command below with the ID of the target client The scope option should be set to offline_access The $CLIENT_SECRET variable is set  ncn-w001# curl -s -d grant_type=client_credentials -d client_id=my-test-client \\ -d client_secret=$CLIENT_SECRET -d scope=offline_access \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq { \u0026#34;access_token\u0026#34;: \u0026#34;eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJ6OE5xSFQ5YUZQUjI0a1RSZWtPU0VaX19WY0pJWXUybE5YXzBraVRjTGZZIn0.eyJqdGkiOiI0MjkyYmJlNC0yZTg3LTQ2YjUtYjgwNC02MjU3MWQwZDJhMzQiLCJleHAiOjE2MzI4NTE0OTQsIm5iZiI6MCwiaWF0IjoxNjAxMzE1NDk0LCJpc3MiOiJodHRwczovL2F1dGgudnNoYXN0YS5pby9rZXljbG9hay9yZWFsbXMvc2hhc3RhIiwiYXVkIjpbInNoYXN0YSIsImFjY291bnQiXSwic3ViIjoiNTMzMWMzM2ItYTVkNi00ODE3LThhOGEtODE4ZGZlOGZjYTM1IiwidHlwIjoiQmVhcmVyIiwiYXpwIjoibXktdGVzdC1jbGllbnQiLCJhdXRoX3RpbWUiOjAsInNlc3Npb25fc3RhdGUiOiI4MGE5NmUyMS0wOTQyLTQ0N2UtYjFkNy0yMWRhNTVkM2ZmNGEiLCJhY3IiOiIxIiwicmVhbG1fYWNjZXNzIjp7InJvbGVzIjpbIm9mZmxpbmVfYWNjZXNzIiwidW1hX2F1dGhvcml6YXRpb24iXX0sInJlc291cmNlX2FjY2VzcyI6eyJzaGFzdGEiOnsicm9sZXMiOlsiYWRtaW4iXX0sImFjY291bnQiOnsicm9sZXMiOlsibWFuYWdlLWFjY291bnQiLCJtYW5hZ2UtYWNjb3VudC1saW5rcyIsInZpZXctcHJvZmlsZSJdfX0sInNjb3BlIjoicHJvZmlsZSBvZmZsaW5lX2FjY2VzcyBlbWFpbCIsImNsaWVudEhvc3QiOiIxMC40NC4wLjAiLCJlbWFpbF92ZXJpZmllZCI6ZmFsc2UsImNsaWVudElkIjoibXktdGVzdC1jbGllbnQiLCJwcmVmZXJyZWRfdXNlcm5hbWUiOiJzZXJ2aWNlLWFjY291bnQtbXktdGVzdC1jbGllbnQiLCJjbGllbnRBZGRyZXNzIjoiMTAuNDQuMC4wIn0.jiYELnxC2r5OFu4mJMQIuuH5o9ktupEZNf9k5cT0P58jv0FamE8437Tie3Ix6o8kfwu_z3ASk-0NVZkxk9s28SNtntlYT3kaVUJJHNKLlv24RWq0sxc-oGGBAfYxWF52unr_VUxvJwB7YQ-DyHH71hjMrmJLKQTTT5OYhgiw2oJ5W7jKrqxtFO8wZYyCzCSJkOKIn48Cxd3KBfYyoT53h9yFF5tOONGNFntRbAtPc3tqLYP0ov0FJLzOU98HUUgxObyb_xvBHuexckvwQ2c3ndHJW72PtKkrATpSGsMmByNVbQdgkT50mCYjH5XDqAYD5308qGVQhqSQnd7jl4ghpA\u0026#34;, \u0026#34;expires_in\u0026#34;: 31536000, \u0026#34;refresh_expires_in\u0026#34;: 0, \u0026#34;refresh_token\u0026#34;: \u0026#34;eyJhbGciOiJIUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICI5ZGNmM2E1Ni0yMmY4LTRlNWYtOWZjNS05ZmEzY2Q3ZDhjYWYifQ.eyJqdGkiOiJhNWYxZmIzNi1mMDM5LTQzNTMtYmQ1Ni05NTUxNjJlNzdlM2IiLCJleHAiOjAsIm5iZiI6MCwiaWF0IjoxNjAxMzE1NDk0LCJpc3MiOiJodHRwczovL2F1dGgudnNoYXN0YS5pby9rZXljbG9hay9yZWFsbXMvc2hhc3RhIiwiYXVkIjoiaHR0cHM6Ly9hdXRoLnZzaGFzdGEuaW8va2V5Y2xvYWsvcmVhbG1zL3NoYXN0YSIsInN1YiI6IjUzMzFjMzNiLWE1ZDYtNDgxNy04YThhLTgxOGRmZThmY2EzNSIsInR5cCI6Ik9mZmxpbmUiLCJhenAiOiJteS10ZXN0LWNsaWVudCIsImF1dGhfdGltZSI6MCwic2Vzc2lvbl9zdGF0ZSI6IjgwYTk2ZTIxLTA5NDItNDQ3ZS1iMWQ3LTIxZGE1NWQzZmY0YSIsInJlYWxtX2FjY2VzcyI6eyJyb2xlcyI6WyJvZmZsaW5lX2FjY2VzcyIsInVtYV9hdXRob3JpemF0aW9uIl19LCJyZXNvdXJjZV9hY2Nlc3MiOnsic2hhc3RhIjp7InJvbGVzIjpbImFkbWluIl19LCJhY2NvdW50Ijp7InJvbGVzIjpbIm1hbmFnZS1hY2NvdW50IiwibWFuYWdlLWFjY291bnQtbGlua3MiLCJ2aWV3LXByb2ZpbGUiXX19LCJzY29wZSI6InByb2ZpbGUgb2ZmbGluZV9hY2Nlc3MgZW1haWwifQ.jxk_81lpFciAuU2z_mFXDWRbImUkNO3HF8Ug958U6xs\u0026#34;, \u0026#34;token_type\u0026#34;: \u0026#34;bearer\u0026#34;, \u0026#34;not-before-policy\u0026#34;: 0, \u0026#34;session_state\u0026#34;: \u0026#34;80a96e21-0942-447e-b1d7-21da55d3ff4a\u0026#34;, \u0026#34;scope\u0026#34;: \u0026#34;profile offline_access email\u0026#34; } Two things are important in the returned response compared to when requesting an \u0026ldquo;online\u0026rdquo; token:\n The refresh_expires_in value is 0. The refresh token will not expire and become invalid by itself. The refresh tokens can be revoked via administrative action in Keycloak. The refresh_token value can be used to get a fresh token any time and will be needed if the access token expires (which will happen in 31,536,000 seconds after the access token was issued).    Refresh the access token.\nEnsure the following have been done before running the command below:\n Replace the my-test-client value in the command below with the ID of the target client Replace the REFRESH_TOKEN value with the string returned in the previous step The grant_type option is set to refresh_token The $CLIENT_SECRET variable is set To refresh the access token, use a grant_type of refresh_token and provide the client ID, client secret, and refresh token.  ncn-w001# curl -s -d grant_type=refresh_token -d client_id=my-test-client \\ -d client_secret=$CLIENT_SECRET -d refresh_token=REFRESH_TOKEN \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq { \u0026#34;access_token\u0026#34;: \u0026#34;eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJ6OE5xSFQ5YUZQUjI0a1RSZWtPU0VaX19WY0pJWXUybE5YXzBraVRjTGZZIn0.eyJqdGkiOiI0ZGU4MGYwMi05ZjczLTRlMzItODEzMS1mYWQ3ZTA2MjY5NjgiLCJleHAiOjE2MzI4NTI0ODksIm5iZiI6MCwiaWF0IjoxNjAxMzE2NDg5LCJpc3MiOiJodHRwczovL2F1dGgudnNoYXN0YS5pby9rZXljbG9hay9yZWFsbXMvc2hhc3RhIiwiYXVkIjpbInNoYXN0YSIsImFjY291bnQiXSwic3ViIjoiNTMzMWMzM2ItYTVkNi00ODE3LThhOGEtODE4ZGZlOGZjYTM1IiwidHlwIjoiQmVhcmVyIiwiYXpwIjoibXktdGVzdC1jbGllbnQiLCJhdXRoX3RpbWUiOjAsInNlc3Npb25fc3RhdGUiOiI4MGE5NmUyMS0wOTQyLTQ0N2UtYjFkNy0yMWRhNTVkM2ZmNGEiLCJhY3IiOiIxIiwicmVhbG1fYWNjZXNzIjp7InJvbGVzIjpbIm9mZmxpbmVfYWNjZXNzIiwidW1hX2F1dGhvcml6YXRpb24iXX0sInJlc291cmNlX2FjY2VzcyI6eyJzaGFzdGEiOnsicm9sZXMiOlsiYWRtaW4iXX0sImFjY291bnQiOnsicm9sZXMiOlsibWFuYWdlLWFjY291bnQiLCJtYW5hZ2UtYWNjb3VudC1saW5rcyIsInZpZXctcHJvZmlsZSJdfX0sInNjb3BlIjoicHJvZmlsZSBvZmZsaW5lX2FjY2VzcyBlbWFpbCIsImNsaWVudEhvc3QiOiIxMC40NC4wLjAiLCJlbWFpbF92ZXJpZmllZCI6ZmFsc2UsImNsaWVudElkIjoibXktdGVzdC1jbGllbnQiLCJwcmVmZXJyZWRfdXNlcm5hbWUiOiJzZXJ2aWNlLWFjY291bnQtbXktdGVzdC1jbGllbnQiLCJjbGllbnRBZGRyZXNzIjoiMTAuNDQuMC4wIn0.UhFP_EZX6JTvVrqclyaDGbmxg5neKgZ_dn_DE42hR7MF8vToKVMPssKcogoYW5FUH5DpZFeveYj4mAPthblJVxCdW2lbDE_iX6HdvEl7Y64Fna3W3zZZRkkuUNuKhbzXPgfAs8_2tFsBwU6whrkwVetzUMax3_GKNNgKu-u6sPVhK0eJvSQ2Le2j88psT6RA8C2JKv9wmy5az-9vti67OmSDiDWGWsYOCNYPqxoINMC6xHo7LJooplxO2F8Q1rn2fnHUMn84MWyEiQNq1huKx5yN8W-b9_Bkd24ewh8rksPn9CvdCeP5SCVDA-amP0HGR3ojF3uxlzwIJ4WhfMiTCA\u0026#34;, \u0026#34;expires_in\u0026#34;: 31536000, \u0026#34;refresh_expires_in\u0026#34;: 0, \u0026#34;refresh_token\u0026#34;: \u0026#34;eyJhbGciOiJIUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICI5ZGNmM2E1Ni0yMmY4LTRlNWYtOWZjNS05ZmEzY2Q3ZDhjYWYifQ.eyJqdGkiOiJmOTE1ZWYzYi1mYzY5LTQ5NWYtYTllNC00Mjg3Yjg2YTFmNjYiLCJleHAiOjAsIm5iZiI6MCwiaWF0IjoxNjAxMzE2NDg5LCJpc3MiOiJodHRwczovL2F1dGgudnNoYXN0YS5pby9rZXljbG9hay9yZWFsbXMvc2hhc3RhIiwiYXVkIjoiaHR0cHM6Ly9hdXRoLnZzaGFzdGEuaW8va2V5Y2xvYWsvcmVhbG1zL3NoYXN0YSIsInN1YiI6IjUzMzFjMzNiLWE1ZDYtNDgxNy04YThhLTgxOGRmZThmY2EzNSIsInR5cCI6Ik9mZmxpbmUiLCJhenAiOiJteS10ZXN0LWNsaWVudCIsImF1dGhfdGltZSI6MCwic2Vzc2lvbl9zdGF0ZSI6IjgwYTk2ZTIxLTA5NDItNDQ3ZS1iMWQ3LTIxZGE1NWQzZmY0YSIsInJlYWxtX2FjY2VzcyI6eyJyb2xlcyI6WyJvZmZsaW5lX2FjY2VzcyIsInVtYV9hdXRob3JpemF0aW9uIl19LCJyZXNvdXJjZV9hY2Nlc3MiOnsic2hhc3RhIjp7InJvbGVzIjpbImFkbWluIl19LCJhY2NvdW50Ijp7InJvbGVzIjpbIm1hbmFnZS1hY2NvdW50IiwibWFuYWdlLWFjY291bnQtbGlua3MiLCJ2aWV3LXByb2ZpbGUiXX19LCJzY29wZSI6InByb2ZpbGUgb2ZmbGluZV9hY2Nlc3MgZW1haWwifQ.kT_xiLTFH-jXqdu9pydnr8ddIknC5hGbrAEuwi82iDs\u0026#34;, \u0026#34;token_type\u0026#34;: \u0026#34;bearer\u0026#34;, \u0026#34;not-before-policy\u0026#34;: 0, \u0026#34;session_state\u0026#34;: \u0026#34;80a96e21-0942-447e-b1d7-21da55d3ff4a\u0026#34;, \u0026#34;scope\u0026#34;: \u0026#34;profile offline_access email\u0026#34; }   "
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/hashicorp_vault/",
	"title": "Hashicorp Vault",
	"tags": [],
	"description": "",
	"content": "HashiCorp Vault A deployment of HashiCorp Vault, managed via the Bitnami Bank-vaults operator, stores private and public Certificate Authority (CA) material, and serves APIs through a PKI engine instance. This instance also serves as a general secrets engine for the system.\nKubernetes service account authorization is utilized to authenticate access to Vault. The configuration of Vault, as deployed on the system, can be viewed with the following command:\nncn# kubectl get vault -n vault cray-vault -o yaml A Kubernetes operator manages the deployment of Vault, based on this definition. The operator is deployed to the vault namespace. The resulting instance will also be deployed to the vault namespace.\nIMPORTANT: Changing the cray-vault custom resource definition or modifying data directly in Vault is not supported unless directed by customer support.\nFor more information, refer to the following resources:\n https://banzaicloud.com/docs/bank-vaults/overview/ https://www.vaultproject.io/docs  Storage Model In previous releases, Vault used etcd as a high-availability (HA) storage back-end. Currently, Vault uses HashiCorp\u0026rsquo;s Raft Implementation. Raft is now configured to run natively inside the Vault statefulset instead of as an independent deployment.\nUnseal Keys Vault requires unseal keys for start-up. If the unseal keys are not present, or are incorrect, Vault (by design) will not start. Unseal keys are stored in the cray-vault-unseal-keys Kubernetes Secret on a system, which is inside the vault namespace.\nAdmin Access Administrative access to Vault can be accomplished through the use of the unseal secret. The use of administrative access should be limited to situations where it is truly necessary. Otherwise, Kubernetes service account access should be used.\nTo obtain and use the root token:\nncn# export VAULT_TOKEN=$(kubectl get secrets cray-vault-unseal-keys \\ -n vault -o jsonpath={.data.vault-root} | base64 -d) ncn# kubectl exec -it -n vault -c vault cray-vault-0 -- sh -c \\ \u0026#34;export VAULT_ADDR=http://localhost:8200; export \\ VAULT_TOKEN=$VAULT_TOKEN; vault secrets list\u0026#34; Kubernetes Service Account Access Vault is configured to allow service account access from the services namespace (among others). This access is tied to a role, which is also subject to specific access policies.\nTo obtain and use the service account token:\nncn# SA_SECRET=$(kubectl -n services get serviceaccounts \\ default -o jsonpath=\u0026#39;{.secrets[0].name}\u0026#39;) ncn# SA_JWT=$(kubectl -n services get secret $SA_SECRET \\ -o jsonpath=\u0026#39;{.data.token}\u0026#39; | base64 --decode) ncn# VAULT_TOKEN=$(kubectl exec -it -n vault -c vault cray-vault-0 \\ -- sh -c \u0026#34;export VAULT_ADDR=http://localhost:8200; vault write \\ auth/kubernetes/login role=services jwt=$SA_JWT-format=json\u0026#34; \\ | jq \u0026#34;.auth.client_token\u0026#34; | sed -e \u0026#39;s/\u0026#34;//g\u0026#39;) ncn# kubectl exec -it -n vault -c vault cray-vault-0 -- sh -c \\ \u0026#34;export VAULT_ADDR=http://localhost:8200; export \\ VAULT_TOKEN=$VAULT_TOKEN; vault kv list secret/\u0026#34; Service account tokens will eventually expire.\nCheck the Status of Vault Clusters Check the status of Vault clusters with the following command:\nncn# for n in $(seq 0 2); do echo \u0026#34;======= Vault status from cray-vault-${n}======\u0026#34;; \\ kubectl exec -it -n vault -c vault cray-vault-${n} -- sh \\ -c \u0026#34;export VAULT_ADDR=http://localhost:8200; vault status\u0026#34;; done ======= Vault status from cray-vault-0 ====== Key Value --- ----- Seal Type shamir Initialized true Sealed false Total Shares 5 Threshold 3 Version 1.5.5 Cluster Name vault-cluster-e19b13b8 Cluster ID 3ea3b6a2-f3f8-fda3-d997-454795dc2be5 HA Enabled true HA Cluster https://cray-vault-1:8201 HA Mode standby Active Node Address http://cray-vault.vault:8200 Raft Committed Index 521 Raft Applied Index 521 ======= Vault status from cray-vault-1 ====== Key Value --- ----- Seal Type shamir Initialized true Sealed false Total Shares 5 Threshold 3 Version 1.5.5 Cluster Name vault-cluster-e19b13b8 Cluster ID 3ea3b6a2-f3f8-fda3-d997-454795dc2be5 HA Enabled true HA Cluster https://cray-vault-1:8201 HA Mode active Raft Committed Index 521 Raft Applied Index 521 ======= Vault status from cray-vault-2 ====== Key Value --- ----- Seal Type shamir Initialized true Sealed false Total Shares 5 Threshold 3 Version 1.5.5 Cluster Name vault-cluster-e19b13b8 Cluster ID 3ea3b6a2-f3f8-fda3-d997-454795dc2be5 HA Enabled true HA Cluster https://cray-vault-1:8201 HA Mode standby Active Node Address http://cray-vault.vault:8200 Raft Committed Index 521 Raft Applied Index 521 Healthy clusters will have one Vault pod in active HA mode, and two Vault pods in standby HA Mode. All instances should also be unsealed and initialized.\n"
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/keycloak_operations/",
	"title": "Keycloak Operations",
	"tags": [],
	"description": "",
	"content": "Keycloak Operations A service may need to access Keycloak to perform various tasks. These typical uses for a service to access Keycloak include creating a new service account, creating a new user, etc. These operations require Keycloak administrative access. As part of the System Management Services (SMS) installation process, Keycloak is initialized with a Master realm. An administrative client and user are created within this realm. The system installation process adds the information needed for the Keycloak administrator\u0026rsquo;s authentication into a Kubernetes secret that can be accessed by any pod. Using this information and the Keycloak REST API, a service can create an account in the Shasta realm. The Keycloak master administrative authentication information is located in the keycloak-master-admin-auth secret, which includes the following fields:\n  client-id - Client ID for administrative operations\n  user - Username for the Keycloak Master admin.\n  password - Password for the Keycloak Master admin.\n  internal_token_url - URL that can be used to get a token, such as https://istio-ingressgateway.istio-system.svc.cluster.local/keycloak/realms/master/protocol/openid-connect/token.\nThe pod in the following example gets a Keycloak Master admin token and makes a request to create a client with a user ID attribute mapper.\nncn# kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Pod metadata: name: kc-admin-example namespace: services spec: containers: - name: kc-admin-example image: alpine command: - sh - -c - \u0026gt;- apk update \u0026amp;\u0026amp; apk add --no-cache curl jq \u0026amp;\u0026amp; echo endpoint: \\$(cat /mnt/auth/internal_token_url) \u0026amp;\u0026amp; echo client_id: \\$(cat /mnt/auth/client-id) \u0026amp;\u0026amp; echo user: \\$(cat /mnt/auth/user) \u0026amp;\u0026amp; TOKEN=\\$(curl -s --cacert /mnt/shasta-ca/certificate_authority.crt -d grant_type=password -d client_id=\\$(cat /mnt/auth/client-id) -d username=\\$(cat /mnt/auth/user) -d password=\\$(cat /mnt/auth/password) \\$(cat /mnt/auth/internal_token_url) | jq -r .access_token) \u0026amp;\u0026amp; echo \u0026#34;=== Making request with token \\$(echo \\$TOKEN | head -c10)... ===\u0026#34; \u0026amp;\u0026amp; curl -is --cacert /mnt/shasta-ca/certificate_authority.crt -H \u0026#34;Authorization: Bearer \\$TOKEN\u0026#34; -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;clientId\u0026#34;: \u0026#34;example\u0026#34;, \u0026#34;publicClient\u0026#34;: true, \u0026#34;standardFlowEnabled\u0026#34;: false, \u0026#34;implicitFlowEnabled\u0026#34;: false, \u0026#34;directAccessGrantsEnabled\u0026#34;: true, \u0026#34;protocolMappers\u0026#34;: [ {\u0026#34;name\u0026#34;: \u0026#34;uid-user-attribute-mapper\u0026#34;, \u0026#34;protocolMapper\u0026#34;: \u0026#34;oidc-usermodel-attribute-mapper\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;openid-connect\u0026#34;, \u0026#34;config\u0026#34;: {\u0026#34;user.attribute\u0026#34;: \u0026#34;uid\u0026#34;, \u0026#34;claim.name\u0026#34;: \u0026#34;uid\u0026#34;, \u0026#34;access.token.claim\u0026#34;: false, \u0026#34;userinfo.token.claim\u0026#34;: true}}]}\u0026#39; https://istio-ingressgateway.istio-system.svc.cluster.local/keycloak/admin/realms/shasta/clients volumeMounts: - name: ca-vol mountPath: /mnt/shasta-ca - name: auth-vol mountPath: \u0026#39;/mnt/auth\u0026#39; readOnly: true volumes: - name: ca-vol configMap: name: cray-configmap-ca-public-key - name: auth-vol secret: secretName: keycloak-master-admin-auth restartPolicy: Never EOF ncn# kubectl logs -n services kc-admin-example fetch http://dl-cdn.alpinelinux.org/alpine/v3.8/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.8/community/x86_64/APKINDEX.tar.gz v3.8.1-115-ge3ed6b4e31 [http://dl-cdn.alpinelinux.org/alpine/v3.8/main] v3.8.1-112-g45bdd0edfb [http://dl-cdn.alpinelinux.org/alpine/v3.8/community] OK: 9546 distinct packages available fetch http://dl-cdn.alpinelinux.org/alpine/v3.8/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.8/community/x86_64/APKINDEX.tar.gz (1/7) Installing ca-certificates (20171114-r3) (2/7) Installing nghttp2-libs (1.32.0-r0) (3/7) Installing libssh2 (1.8.0-r3) (4/7) Installing libcurl (7.61.1-r1) (5/7) Installing curl (7.61.1-r1) (6/7) Installing oniguruma (6.8.2-r0) (7/7) Installing jq (1.6_rc1-r1) Executing busybox-1.28.4-r1.trigger Executing ca-certificates-20171114-r3.trigger OK: 7 MiB in 20 packages endpoint: https://istio-ingressgateway.istio-system.svc.cluster.local/keycloak/realms/master/protocol/openid-connect/token client_id: admin-cli user: admin === Making request with token eyJhbGciOi... === HTTP/1.1 201 Created Content-Length: 0 Connection: keep-alive Location: https://istio-ingressgateway.istio-system.svc.cluster.local/keycloak/admin/realms/shasta/clients/070c8537-6c46-43a4-b0bb-209b3c4b94c6 Date: Fri, 30 Nov 2018 20:07:39 GMT X-Kong-Upstream-Latency: 27 X-Kong-Proxy-Latency: 1 Via: kong/0.14.1   The new example client is now visible in the Keycloak administrative web application.\n"
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/configure_keycloak_for_ldapad_authentication/",
	"title": "Configure Keycloak For LDAP/ad Authentication",
	"tags": [],
	"description": "",
	"content": "Configure Keycloak for LDAP/AD authentication Keycloak enables users to be in an LDAP or Active Directory (AD) server. This allows users to get their tokens using their regular username and password, and use those tokens to perform operations on the system\u0026rsquo;s REST API.\nConfiguring Keycloak can be done using the admin GUI or through Keycloak\u0026rsquo;s web API.\nFor more information on setting up LDAP federation, see the Keycloak administrative documentation in a section titled https://www.keycloak.org/docs/latest/server_admin/index.html#_ldap.\nUsers are expected to have the following attributes:\n uidNumber gidNumber homeDirectory loginShell  These attributes are added to the users by adding a \u0026ldquo;User Attribute Mapper\u0026rdquo; to the LDAP User Federation object. For each of these, there should be a User Attribute Mapper that maps the \u0026ldquo;LDAP Attribute\u0026rdquo; in the LDAP Directory to the \u0026ldquo;User Model Attribute,\u0026rdquo; which will be uidNumber, gidNumber, and more.\nThe shasta client that is created during the install maps these specific user model attributes into the JWT token so that it is available to the REST APIs.\n"
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/configure_the_rsa_plugin_in_keycloak/",
	"title": "Configure The Rsa Plugin In Keycloak",
	"tags": [],
	"description": "",
	"content": "Configure the RSA Plugin in Keycloak Use Keycloak to configure a plugin that enables RSA token authentication.\nPrerequisites Access to the Keycloak UI is needed.\nProcedure   Verify the Shasta domain is being used.\nThis is indicated in the dropdown in the upper left of the UI.\n  Click on Authentication under the Configure header of the navigation area on the left side of the page.\n  Click on the Flows tab.\n  Click the dropdown button in the table header and switch to Browser.\n  Click the Copy button in the table header.\n  Enter RSA - Browser for the New Name type.\n  Click the Add execution button in the table header.\n  Switch the Provider to RSA and click Save.\n  Update the Requirement field.\nSet the table values to the following:\n   Field Requirement     RSA - Browser Forms REQUIRED   Username Password Form REQUIRED   RSA - Browser - Conditional OTP CONDITIONAL   Condition - User Configured DISABLED   OTP Form DISABLED   RSA REQUIRED      Click the Actions dropdown on the RSA line of the table, then select Config.\n  Enter the different configuration options:\n   Configuration Field Value     Alias Enter the desired alias. For example, \u0026ldquo;RSA\u0026rdquo; could be used.   RSA URL The base URL of the RSA API service. For example, https://rsa.mycompany.com:5555/   RSA Verify Endpoint /mfa/v1_1/authn/initialize   Keycloak Client ID The authentication agent. For example, rsa.mycompany.com. The value is from Access \u0026gt; Authentication Agents \u0026gt; Manage Existing in the RSA Console.   RSA Authentication Manager Client Key The key for the RSA API.      Set the Shared username if applicable.\nIf the usernames are the same in Keycloak and RSA, then this can be set to ON. This means the browser flow will not ask for the username for the RSA validation.\n  Click Save.\n    Return to the Flows tab on the Authentication page.\n  Click the dropdown button in the table header and switch to Direct Grant.\n  Click the Copy button in the table header.\n  Enter RSA - CLI for the New Name type.\n  Click the Add execution button in the table header.\n  Switch the Provider to RSA - CLI and click Save.\n  Update the Requirement field.\nSet the table values to the following:\n   Field Requirement     RSA - CLI REQUIRED   RSA - CLI Direct Grant - Conditional OTP DISABLED      Click Save.\n    Switch to the Bindings tab in the Authentication page.\n  Change Browser Flow to RSA - Browser.\n  Change Direct Grant Flow to RSA - CLI.\n  Click Save.\n    Here are a couple of things to try after this is set up to verify that it is working:\n  Point a browser at the following URL:\nhttp://auth.SYSTEM_DOMAIN_NAME/keycloak/realms/shasta/account The browser will be directed to the user login page. The first screen will ask for the username and password in Keycloak. After logging in this way, the next page will ask for the RSA username and token code.\n  Get a token using the direct grant flow.\nRun the following cURL command from ncn-w001. Replace USER with a user in Keycloak, PWD_NAME with the user\u0026rsquo;s password, RSA_USER with the user in RSA, and TOKEN_CODE with the token code:\nncn-w001# curl -i -d grant_type=password -d client_id=shasta -d username=USER \\ -d password=PWD_NAME -d rsa_username=RSA_USER -d rsa_otp=TOKEN_CODE \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token   "
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/create_a_service_account_in_keycloak/",
	"title": "Create A Service Account In Keycloak",
	"tags": [],
	"description": "",
	"content": "Create a Service Account in Keycloak Set up a Keycloak service account using the Keycloak administration console or the Keycloak REST API. A service account can be used to get a long-lived token that is used by automation tools.\nIn Keycloak, service accounts are associated with a client. See https://www.keycloak.org/docs/latest/server_admin/#_service_accounts for more information from the Keycloak documentation.\nProcedure Follow the steps in only one of the following sections depending on if it is preferred to use the Keycloak REST API or Keycloak administration console UI.\nUse the Keycloak Administration Console UI   Log in to the administration console.\nSee Access the Keycloak User Management UI for more information.\n  Click Clients under the Configure header of the navigation panel on the left side of the page.\n  Click the Create button at the top-right of the Clients table.\n  Enter a Client ID for the new client.\nThe Client Protocol must be openid-connect and the Root URL can be left blank.\n  Click the Save button.\n    Customize the new client.\nOnce the client is created, a new screen is displayed with more details for the client.\n  Change the Access type to confidential.\n  Change Stand Flow Enabled to OFF.\n  Change Direct Access Grants Enabled to OFF.\n  Change Service Accounts Enabled to ON.\n  Click the Save button.\n    Assign a role to the client for authorization.\n  Switch to the Mappers tab for the new client.\n  Click the Create button at the top-right of the Mappers table.\nA new form is displayed that asks for details for the mapper.\n  Enter a name.\nIn the image above, the example name is \u0026ldquo;admin-role.\u0026rdquo;\n  Change the Mapper Type to Hardcoded Role.\n  Set the Role to shasta.admin.\n  Click the Save button.\n    Use the Keycloak REST API  Create the get_master_token function to get a token as a Keycloak master administrator.  MASTER_USERNAME=$(kubectl get secret -n services keycloak-master-admin-auth -ojsonpath='{.data.user}' | base64 -d) MASTER_PASSWORD=$(kubectl get secret -n services keycloak-master-admin-auth -ojsonpath='{.data.password}' | base64 -d) function get_master_token { curl -ks -d client_id=admin-cli -d username=$MASTER_USERNAME -d password=$MASTER_PASSWORD -d grant_type=password https://api-gw-service-nmn.local/keycloak/realms/master/protocol/openid-connect/token | python -c \u0026quot;import sys,json; print json.load(sys.stdin)['access_token']\u0026quot; }  Create the client doing a POST call for a JSON object.\nThe clientId should be changed to the name for the new service account.\n  ncn-w001# curl -is -H \u0026quot;Authorization: Bearer $(get_master_token)\u0026quot; -H \u0026quot;Content-Type: application/json\u0026quot; -d ' { \u0026quot;clientId\u0026quot;: \u0026quot;my-test-client\u0026quot;, \u0026quot;standardFlowEnabled\u0026quot;: false, \u0026quot;implicitFlowEnabled\u0026quot;: false, \u0026quot;directAccessGrantsEnabled\u0026quot;: false, \u0026quot;serviceAccountsEnabled\u0026quot;: true, \u0026quot;publicClient\u0026quot;: false, \u0026quot;protocolMappers\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;admin-role\u0026quot;, \u0026quot;protocol\u0026quot;: \u0026quot;openid-connect\u0026quot;, \u0026quot;protocolMapper\u0026quot;: \u0026quot;oidc-hardcoded-role-mapper\u0026quot;, \u0026quot;consentRequired\u0026quot;: false, \u0026quot;config\u0026quot;: { \u0026quot;role\u0026quot;: \u0026quot;shasta.admin\u0026quot; } } ] } ' \\ https://api-gw-service-nmn.local/keycloak/admin/realms/shasta/clients Output similar to the following will be returned:\nHTTP/2 201 location: https://api-gw-service-nmn.local/keycloak/admin/realms/shasta/clients/bd8084d2-08bf-45cb-ab94-ee81e39921be content-length: 0 "
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/create_internal_groups_in_the_keycloak_shasta_realm/",
	"title": "Create Internal Groups In The Keycloak Shasta Realm",
	"tags": [],
	"description": "",
	"content": "Create Internal Groups in the Keycloak Shasta Realm Manually create a group in the Keycloak Shasta realm. New groups can be created with the Keycloak UI. On Shasta, Keycloak groups must have the cn and gidNumber attributes, otherwise the keycloak-users-localize tool will fail to export the groups.\nNew Keycloak groups can be used to group users for authentication.\nPrerequisites   This procedure assumes the user has already accessed Keycloak\u0026rsquo;s user management interface. See Access the Keycloak User Management UI\n  This procedure assumes that the password for the Keycloak admin account is known. The Keycloak password is set during the software installation process. The password can be obtained using the following command:\nncn-w001# kubectl get secret -n services keycloak-master-admin-auth \\ --template={{.data.password}} | base64 --decode   Procedure   Click the Groups text in the Manage section in the navigation area on the left side of the screen.\n  Click the New button in the groups table header.\n  Provide a unique name for the new group and click the Save button.\n  Navigate to the Attributes tab.\n  Add the cn attribute by setting the Key to cn, and the Value to the name of the group. Click the Add button on the row.\n  Add the gidNumber attribute by setting the Key to gidNumber, and the Value to the gidNumber of the group. Click the Add button on the row.\n  Click the Save button at the bottom of the page.\n  Once the groups are added to Keycloak, add users to the group and follow the instructions in Re-Sync Keycloak Users to Compute Nodes to update the groups on the compute nodes.\n"
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/create_internal_user_accounts_in_the_keycloak_shasta_realm/",
	"title": "Create Internal User Accounts In The Keycloak Shasta Realm",
	"tags": [],
	"description": "",
	"content": "Create Internal User Accounts in the Keycloak Shasta Realm The following manual procedure can be used to create a user in the Keycloak Shasta realm. New accounts can be created with the Keycloak UI.\nNew admin and user accounts are authenticated with Keycloak. Authenticated accounts are needed to use the Cray CLI.\nPrerequisites   This procedure assumes the user has already accessed Keycloak\u0026rsquo;s user management interface. See Access the Keycloak User Management UI for more information.\n  This procedure assumes that the password for the Keycloak admin account is known. The Keycloak password is set during the software installation process. The password can be obtained using the following command:\nncn-w001# kubectl get secret -n services keycloak-master-admin-auth \\ --template={{.data.password}} | base64 --decode   Procedure   Click the Add User button.\n  Enter the user name and other attributes as required.\n  Click the Save button.\n  On the Credentials tab enter a password for the user and change temporary option from ON to OFF.\n  Click the Reset Password button.\n  Click the red Change Password button on the Change password page.\n  Remove Update Password from the Required User Actions and on the user Details tab.\nThis step allows the user to authenticate and get a token without first needing to change the admin supplied password. It does not prevent the user from changing the password. The other option is to leave this setting requiring a password reset in Keycloak before making a token request.\n  Select the Save button.\n  Create a user and group ID for this user on the Attributes tab by performing the following steps for both the uid and gid attributes:\n  Add the attribute name to the Key column and its value to the Value column.\n  Click the Add button.\n  Select the Save button at the bottom once both the uid and gid attributes have been added.\n  In addition, other attributes can be added as needed by site-specific applications. The User Access Service (UAS) requires these attributes.\nUser accounts need the following attributes defined to create a User Access Instance (UAI):\n gidNumber homeDirectory loginShell uidNumber    Click on the Role Mappings tab to grant the user authority.\n  Click the Client Roles button.\n  Select shasta.\n  Set the assigned role to either admin or user.\n    Verify that the user account has been created in the Shasta realm using one or more of the following:\n Ensure that the new user is listed under Manage Users on the Administration Console page. Retrieve a token for the user. Log in to the Keycloak Shasta realm as the new user, which would verify the account\u0026rsquo;s validity and allow the user to reset their password. This functionally is supported for internal Keycloak accounts only.    Verify that the new local Keycloak account can authenticate to the Cray CLI.\nncn-w001# cray auth login --username USERNAME Password: Success!   Authorization Is Local to a Host: whenever you are using the CLI (cray command) on a host (e.g. a workstation or NCN) where it has not been used before, it is necessary to authenticate on that host using cray auth login. There is no mechanism to distribute CLI authorization amongst hosts.\n"
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/change_ncn_image_root_password_and_ssh_keys/",
	"title": "Change NCN Image Root Password And SSH Keys",
	"tags": [],
	"description": "",
	"content": "Change NCN Image Root Password and SSH Keys Customize the NCN image by changing the root password or adding different ssh keys for the root account. This procedure shows this process being done on the PIT node during a first time installation of the CSM software.\nThis process should be done for the \u0026ldquo;Kubernetes\u0026rdquo; image used by master and worker nodes and then repeated for the \u0026ldquo;ceph\u0026rdquo; image used by the utility storage nodes.\nKubernetes Image The Kubernetes image is used by the master and worker nodes.\n  Open the image.\nThe Kubernetes image will be of the form \u0026ldquo;kubernetes-0.1.69.squashfs\u0026rdquo; in /var/www/ephemeral/data/k8s, but the version number may be different.\npit# cd /var/www/ephemeral/data/k8s pit# unsquashfs kubernetes-0.1.69.squashfs   Chroot into the image root\npit# chroot ./squashfs-root   Change the password\nchroot-pit# passwd   Replace the ssh keys\nchroot-pit# cd root   Replace the default root public and private ssh keys with your own or generate a new pair with ssh-keygen(1)\nchroot-pit# mknod /dev/urandom c 1 9 chroot-pit# ssh-keygen \u0026lt;options\u0026gt; chroot-pit# rm /dev/urandom   Create the new SquashFS artifact\nchroot-pit# /srv/cray/scripts/common/create-kis-artifacts.sh   Exit the chroot\nchroot-pit# exit   Clean up the SquashFS creation\nThe Kubernetes image directory is /var/www/ephemeral/data/k8s.\npit# umount -v /var/www/ephemeral/data/k8s/squashfs-root/mnt/squashfs   Save old SquashFS image.\npit# mkdir -v old pit# mv -v *squashfs old   Move new SquashFS image, kernel, and initrd into place.\npit# mv -v squashfs-root/squashfs/* .   Update file permissions on initrd\npit# chmod -v 644 initrd.img.xz   Set the boot links.\npit# cd pit# set-sqfs-links.sh   The Kubernetes image will have the new password for the next boot.\nCeph Image The Ceph image is used by the utility storage nodes.\n  Open the image.\nThe Ceph image will be of the form \u0026ldquo;storage-ceph-0.1.69.squashfs\u0026rdquo; in /var/www/ephemeral/data/ceph, but the version number may be different.\npit# cd /var/www/ephemeral/data/ceph pit# unsquashfs storage-ceph-0.1.69.squashfs   Change into the image root\npit# chroot ./squashfs-root   Change the password\nchroot-pit# passwd   Replace the ssh keys\nchroot-pit# cd root   Replace the default root public and private ssh keys with your own or generate a new pair with ssh-keygen(1)\n  Create the new SquashFS artifact\nchroot-pit# /srv/cray/scripts/common/create-kis-artifacts.sh   Exit the chroot\nchroot-pit# exit   Clean up the SquashFS creation\nThe Ceph image directory is /var/www/ephemeral/data/ceph.\npit# umount -v /var/www/ephemeral/data/ceph/squashfs-root/mnt/squashfs   Save old SquashFS image.\npit# mkdir -v old pit# mv -v *squashfs old   Move new SquashFS image, kernel, and initrd into place.\npit# mv -v squashfs-root/squashfs/* .   Update file permissions on initrd\npit# chmod -v 644 initrd.img.xz   Set the boot links.\npit# cd pit# set-sqfs-links.sh   The Ceph image will have the new password for the next boot.\n"
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/change_root_passwords_for_compute_nodes/",
	"title": "Change Root Passwords For Compute Nodes",
	"tags": [],
	"description": "",
	"content": "Change Root Passwords for Compute Nodes Update the root password on the system for compute nodes.\nChanging the root password at least once is a recommended best practice for system security.\nPrerequisites The initial root password for compute nodes are not set. Use this procedure to set the initial and any subsequent password changes required.\nProcedure   Get an encrypted value for the new password.\nUse the passwd command to update the password and get the new passwd hash. The command will generate an encrypted value.\nIn the following example, mypasswd-example is the new password set by the admin, and demonstration is an example salt value, which is also configured by the admin. Refer to the openssl man pages for more information.\n# openssl passwd -6 -salt demonstration Password: mypasswd-example $6$demonstration$gbSD0NlKb2QTo7NRu/pUn4zTNjk5yhSysTS1tUruNIfbROX/a5H92T7CF8fovhORUkOtPrLUpGXmbqIEMmvrh/ Save the returned encrypted value to be used in the next step to configure the override of compute nodes password.\n  Use the encrypted value returned in the previous step to override the default passwords for compute nodes.\nRefer to Customize Configuration Values for more information.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/change_the_keycloak_admin_password/",
	"title": "Change The Keycloak Admin Password",
	"tags": [],
	"description": "",
	"content": "Change the Keycloak Admin Password Update the default password for the admin Keycloak account using the Keycloak user interface (UI). After updating the password in Keycloak, encrypt it on the system and verify that the change was made successfully.\nThis procedure uses SYSTEM_DOMAIN_NAME as an example for the DNS name of the non-compute node (NCN). Replace this name with the actual NCN\u0026rsquo;s DNS name while executing this procedure.\nProcedure   Log in to Keycloak with the default admin credentials.\nPoint a browser at https://auth.SYSTEM_DOMAIN_NAME/keycloak/admin, replacing SYSTEM_DOMAIN_NAME with the actual NCN\u0026rsquo;s DNS name.\nThe following is an example URL for a system:\nauth.system1.us.cray.com/keycloak/admin Use the following admin login credentials:\n  Username: admin\n  The password can be obtained with the following command:\nncn-w001# kubectl get secret -n services keycloak-master-admin-auth \\ --template={{.data.password}} | base64 --decode     Click the Admin drop-down menu in the upper-right corner of the page.\n  Select Manage Account.\n  Click the Password tab on the left side of the page.\n  Enter the existing password, new password and confirmation, and then click Save.\n  Log on to ncn-w001.\n  Change the password in the customizations.yaml file.\nThe Keycloak master admin password is also stored in the keycloak-master-admin-auth Secret in the services namespace. This needs to be updated so that clients that need to make requests as the master admin can authenticate with the new password.\nIn the customizations.yaml file, set the values for the keycloak_master_admin_auth keys in the spec.kubernetes.sealed_secrets field. The value in the data element where the name is password needs to be changed to the new Keycloak master admin password.\nFor example:\nkeycloak_master_admin_auth: generate: name: keycloak-master-admin-auth data: - type: static args: name: client-id value: admin-cli - type: static args: name: user value: admin - type: static args: name: password value: my_secret_password - type: static args: name: internal_token_url value: https://api-gw-service-nmn.local/keycloak/realms/master/protocol/openid-connect/token   Encrypt the values after changing the customizations.yaml file.\nncn-w001# utils/secrets-seed-customizations.sh customizations.yaml   Re-apply the cray-keycloak Helm chart with the updated customizations.yaml file.\nThis will update the keycloak-master-admin-auth SealedSecret which will cause the SealedSecret controller to update the Secret.\n  Verify that the Secret has been updated.\nGive the SealedSecret controller a few seconds to update the Secret, then run the following command to see the current value of the Secret:\nncn-w001# kubectl get secret -n services keycloak-master-admin-auth \\ --template={{.data.password}} | base64 --decode   "
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/change_the_ldap_server_ip_address_for_existing_ldap_server_content/",
	"title": "Change The LDAP Server Ip Address For Existing Ldap Server Content",
	"tags": [],
	"description": "",
	"content": "Change the LDAP Server IP Address for Existing LDAP Server Content The IP address that Keycloak is using for the LDAP server can be changed. In the case where the new LDAP server has the same contents as the previous LDAP server, edit the LDAP user federation to switch Keycloak to use the new LDAP server.\nRefer to Change the LDAP Server IP Address for New LDAP Server Content if the LDAP server is being replaced by a different LDAP server that has different content.\nPrerequisites The contents of the new LDAP server are the same as the previous LDAP server. For example, it is a replica or was restored from a backup.\nProcedure Follow the steps in only one of the sections below depending on if it is preferred to use the Keycloak REST API or Keycloak administration console UI.\nUse the Keycloak Administration Console UI   Log in to the administration console.\nSee Access the Keycloak User Management UI for more information.\n  Click on User Federation under the Configure header of the navigation panel on the left side of the page.\n  Click on the LDAP provider in the User Federation table.\nThis will bring up a form to edit the LDAP user federation.\n  Change the Connection URL value in the LDAP user federation form to use the new IP address.\n  Click the Save button at the bottom of the form.\n  Click the Synchronize all users button.\nThis may take a while depending on the number of users and groups in the LDAP server.\nWhen the synchronize process completes, the pop-up will show that the update was successful. There should be minimal or no changes because the contents of the servers are the same.\n  Use the Keycloak REST API   Create a function to get a token as a Keycloak master administrator.\nMASTER_USERNAME=$(kubectl get secret -n services keycloak-master-admin-auth -ojsonpath=\u0026#39;{.data.user}\u0026#39; | base64 -d) MASTER_PASSWORD=$(kubectl get secret -n services keycloak-master-admin-auth -ojsonpath=\u0026#39;{.data.password}\u0026#39; | base64 -d) function get_master_token { curl -ks -d client_id=admin-cli -d username=$MASTER_USERNAME -d password=$MASTER_PASSWORD -d grant_type=password https://api-gw-service-nmn.local/keycloak/realms/master/protocol/openid-connect/token | python -c \u0026#34;import sys.json; print json.load(sys.stdin)[\u0026#39;access_token\u0026#39;]\u0026#34; }   Get the component ID for the LDAP user federation.\nncn-w001# COMPONENT_ID=$(curl -s -H \u0026#34;Authorization: Bearer $(get_master_token)\u0026#34; \\ https://api-gw-service-nmn.local/keycloak/admin/realms/shasta/components \\ | jq -r \u0026#39;.[] | select(.providerId==\u0026#34;ldap\u0026#34;).id\u0026#39;) ncn-w001# echo $COMPONENT_ID 57817383-e4a0-4717-905a-ea343c2b5722   Get the current representation of the LDAP user federation.\nncn-w001# curl -s -H \u0026#34;Authorization: Bearer $(get_master_token)\u0026#34; \\ https://api-gw-service-nmn.local/keycloak/admin/realms/shasta/components/$COMPONENT_ID \\ | jq . \u0026gt; keycloak_ldap.json { \u0026#34;id\u0026#34;: \u0026#34;57817383-e4a0-4717-905a-ea343c2b5722\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;shasta-user-federation-ldap\u0026#34;, \u0026#34;providerId\u0026#34;: \u0026#34;ldap\u0026#34;, \u0026#34;providerType\u0026#34;: \u0026#34;org.keycloak.storage.UserStorageProvider\u0026#34;, \u0026#34;parentId\u0026#34;: \u0026#34;09580343-fc55-4951-84ee-1c73b3a7ad29\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;pagination\u0026#34;: [ \u0026#34;true\u0026#34; ], \u0026#34;fullSyncPeriod\u0026#34;: [ \u0026#34;-1\u0026#34; ], ... \u0026#34;connectionUrl\u0026#34;: [ \u0026#34;ldap://10.248.0.59\u0026#34; ], ...   Edit the keycloak_ldap.json file and set the connectionUrl string to the new URL with the new IP address.\nncn-w001# vi keycloak_ldap.json   Apply the updated keycloak_ldap.json file to the Keycloak server.\nThe output should show the response code is 204.\nncn-w001# curl -i -XPUT -H \u0026#34;Authorization: Bearer $(get_master_token)\u0026#34; -H \\ \u0026#34;Content-Type: application/json\u0026#34; -d @keycloak_ldap.json \\ https://api-gw-service-nmn.local/keycloak/admin/realms/shasta/components/$COMPONENT_ID HTTP/2 204 ...   "
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/change_the_ldap_server_ip_address_for_new_ldap_server_content/",
	"title": "Change The LDAP Server Ip Address For New Ldap Server Content",
	"tags": [],
	"description": "",
	"content": "Change the LDAP Server IP Address for New LDAP Server Content Delete the old LDAP user federation and create a new one. This procedure should only be done if the LDAP server is being replaced by a different LDAP server that has different contents.\nRefer to Change the LDAP Server IP Address for Existing LDAP Server Content if the new LDAP server content matches the previous LDAP server content.\nPrerequisites The LDAP server is being replaced by a different LDAP server that has different contents. For example, different users and groups.\nProcedure   Remove the LDAP user federation from Keycloak.\nFollow the procedure in Remove the LDAP User Federation from Keycloak.\n  Re-add the LDAP user federation in Keycloak.\nFollow the procedure in Add LDAP User Federation.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/add_ldap_user_federation/",
	"title": "Add LDAP User Federation",
	"tags": [],
	"description": "",
	"content": "Add LDAP User Federation Add LDAP user federation using the Keycloak localization tool.\nPrerequisites LDAP user federation is not currently configured in Keycloak. For example, if it was not configured in Keycloak when the system was initially installed or the LDAP user federation was removed.\nProcedure   Prepare to customize the customizations.yaml file.\nIf the customizations.yaml file is managed in an external Git repository (as recommended), then clone a local working tree. Replace the \u0026lt;URL\u0026gt; value in the following command before running it.\nncn-m001# git clone \u0026lt;URL\u0026gt; /root/site-init ncn-m001# cd /root/site-init If there is not a backup of site-init, perform the following steps to create a new one using the values stored in the Kubernetes cluster.\n  Create a new site-init directory using the CSM tarball.\nDetermine the location of the initial unpacked install tarball and set ${CSM_DISTDIR} accordingly.\n NOTE: If the unpacked set of CSM directories was copied, no untar action is required. If the tarball tgz file was copied, the command to unpack it is tar -zxvf CSM_RELEASE.tar.gz. Replace the CSM_RELEASE value before running the command to unpack the tarball.\n ncn-m001# cp -r ${CSM_DISTDIR}/shasta-cfg/* /root/site-init ncn-m001# cd /root/site-init   Extract customizations.yaml from the site-init secret.\nncn-m001# kubectl -n loftsman get secret site-init -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | base64 -d - \u0026gt; customizations.yaml   Extract the certificate and key used to create the sealed secrets.\nncn-m001# kubectl -n kube-system get secret sealed-secrets-key -o jsonpath=\u0026#39;{.data.tls\\.crt}\u0026#39; | base64 -d - \u0026gt; certs/sealed_secrets.crt ncn-m001# kubectl -n kube-system get secret sealed-secrets-key -o jsonpath=\u0026#39;{.data.tls\\.key}\u0026#39; | base64 -d - \u0026gt; certs/sealed_secrets.key      NOTE: All subsequent steps of this procedure should be performed within the /root/site-init directory created in this step.\n  Repopulate the keycloak_users_localize and cray-keycloak Sealed Secrets in the customizations.yaml file with the desired configuration.\nUpdate the LDAP settings with the desired configuration. LDAP connection information is stored in the keycloak-users-localize Secret in the customizations.yaml file.\n The ldap_connection_url key is required and is set to an LDAP URL. The ldap_bind_dn and ldap_bind_credentials keys are optional. If the LDAP server requires authentication. then the bind DN and credentials are set in these keys respectively.  For example:\ncray-keycloak: generate: name: keycloak-certs data: - type: static_b64 args: name: certs.jks value: /u3+7QAAAAIAAAAA5yXvSDt11bGXyBA9M2iy0/5i1Tg= keycloak_users_localize: generate: name: keycloak-users-localize data: - type: static args: name: ldap_connection_url value: \u0026#34;ldaps://my_ldap.my_org.test\u0026#34; - type: static args: name: ldap_bind_dn value: \u0026#34;cn=my_admin\u0026#34; - type: static args: name: ldap_bind_credentials value: \u0026#34;my_ldap_admin_password\u0026#34; The example above puts an empty certs.jks in the cray-keycloak Sealed Secret. The next step will generate certs.jks.\nOther LDAP configuration settings are set in the spec.kubernetes.services.cray-keycloak-users-localize field in the customizations.yaml file.\nThe fields are as follows:\n( Notes for the following table: format is * \u0026lt;cray-keycloak-users-localize chart option name\u0026gt; : \u0026lt;description\u0026gt; - default: \u0026lt;the default value if not overridden in customizations.yaml - type: \u0026lt;type that the value in customizations.yaml has to be. e.g., if type is string and a number is entered then you need to quote it\u0026gt; - allowed values: \u0026lt;if only certain values are allowed they are listed here\u0026gt; ) * ldapProviderId : The Keycloak provider ID for the component. This must be \u0026quot;ldap\u0026quot; - default: ldap - type: string * ldapFederationName : The name of the LDAP provider in Keycloak. If a provider with this name already exists then this tool will not create a new provider. - default: shasta-user-federation-ldap - type: string * ldapPriority : The priority of this provider when looking up users or adding a user. - default: 1 - type: string * ldapEditMode : If you want to be able to create or change users in Keycloak and have them created or modified in the LDAP server, and the LDAP server allows it, then this can be changed. - default: READ_ONLY - type: string - allowed values: READ_ONLY, WRITEABLE, or UNSYNCED * ldapSyncRegistrations : If true, then newly created users will be created in the LDAP server. - default: false - type: string - allowed values: true or false * ldapVendor: This determines some defaults for what mappers are created by default. - default: other - type: string - allowed values: Active Directory, Red Hat Directory Server, Tivoli, Novell eDirectory, or other * ldapUsernameLDAPAttribute: The LDAP attribute to map to the username in Keycloak. - default: uid - type: string * ldapRdnLDAPAttribute: The LDAP attribute being used as the users RDN. - default: uid - type: string * ldapUuidLDAPAttribute: The LDAP attribute being used as a unique ID. - default: uid - type: string * ldapUserObjectClasses: The object classes for user entries. - default: posixAccount - type: comma-separated string * ldapAuthType: Set to \u0026quot;none\u0026quot; if the LDAP server allows anonymous search for users and groups, otherwise set to \u0026quot;simple\u0026quot; to bind. - default: none - type: string - allowed values: none or simple * ldapSearchBase: The DN for the base entry to search for users and groups. - default: cn=default - type: string * ldapSearchScope: The search scope to use when searching for users or groups: 2 for subtree, 1 for onelevel - default: 2 - type: string - allowed values: 1 or 2 * ldapUseTruststoreSpi: Determines if the truststore is used to validate the server certificate when connecting to the server. - default: ldapsOnly - type: string - allowed values: ldapsOnly, always, never * ldapConnectionPooling: If true then Keycloak will use a connection pool of LDAP connections. - default: true - type: string - allowed values: true or false * ldapPagination: Set to true if the LDAP server supports or requires use of the paging extension. - default: true - type: string - allowed values: true or false * ldapAllowKerberosAuthentication: - Set to true to enable HTTP authentication of users with SPNEGO/Kerberos tokens. - default: false - type: string * ldapBatchSizeForSync: Count of LDAP users to be imported from LDAP to Keycloak in a single transaction. - default: 4000 - type: string * ldapFullSyncPeriod: If a positive number, this is the number of seconds between automatic full user synchronization operations; if negative then full user synchronization operations will not be done automatically. - default: -1 - type: string * ldapChangedSyncPeriod: f a positive number, this is the number of seconds between automatic changed user synchronization operations; if negative then changed user synchronization operations will not be done automatically. - default: -1 - type: string * ldapDebug: Set to true to enable extra logging of LDAP operations. - default: true - allowed values: true or false * ldapUserAttributeMappers: Extra attribute mappers to create so that users have attributes required by Shasta software. The Keycloak attribute that the LDAP attribute maps to will be the same. - default: [uidNumber, gidNumber, loginShell, homeDirectory] - type: list of strings * ldapUserAttributeMappersToRemove: These attribute mappers will be removed, to be used in the case where the default attribute mappers are not appropriate. For example, this could be used to remove the email mapper if email addresses are not unique. - default: [] - type: list of strings * ldapGroupNameLDAPAttribute: The LDAP attribute to map to the group name in Keycloak. - default: cn - type: string * ldapGroupObjectClass: The object classes for group entries. - default: posixGroup - type: comma-separated string * ldapPreserveGroupInheritance: Whether group inheritance should be propagated to Keycloak or not. - default: false - type: string - allowed values: true or false * ldapMembershipLDAPAttribute: Name of the LDAP attribute that refers to the group members. - default: memberUid - type: string * ldapMembershipAttributeType: If the member attribute contains the DN for the user, then set this to DN. If the member attribute is the UID of the entry then set this to UID. - default: UID - type: string - allowed values: UID or DN * ldapMembershipUserLDAPAttribute: If the ldapMembershipAttributeType is UID then this is the LDAP attribute containing the UID value, otherwise this is ignored. - default: uid - type: string * ldapGroupsLDAPFilter: Extra filter to include when searching for group entries. If this is not the empty string the value must start with the ( character and end with ). - default: \u0026quot; - type: string * ldapUserRolesRetrieveStrategy: Defines how to retrieve groups for a user. - default: LOAD_GROUPS_BY_MEMBER_ATTRIBUTE - type: string - allowed values: LOAD_GROUPS_BY_MEMBER_ATTRIBUTE, GET_GROUPS_FROM_USER_MEMBEROF_ATTRIBUTE, LOAD_GROUPS_BY_MEMBER_ATTRIBUTE_RECURSIVELY * ldapMappedGroupAttributes: Attributes of the group that will be added as attributes of the user. Some Shasta REST API operations require the user to have a gidNumber and this adds that attribute from the LDAP group. - default: cn,gidNumber,memberUid - type: comma-separated string * ldapDropNonExistingGroupsDuringSync: If true, groups that are not in LDAP will be deleted when synchronizing. - default: false - type: string - allowed values: true or false * ldapDoFullSync: Tells the HPE Cray EX Keycloak localization tool to perform an immediate full user synchronization after configuring the LDAP integration. - default: true - type: string * ldapRoleMapperDn: If this is an empty string then a role mapper is not created, otherwise this the the DN used as the search base to find role entries. - default: \u0026quot; - type: string * ldapRoleMapperRoleNameLDAPAttribute: The LDAP attribute to map to the role name in Keycloak. - default: cn - type: string * ldapRoleMapperRoleObjectClasses: The object classes for role entries. - default: groupOfNames - type: string * ldapRoleMapperLDAPAttribute: Name of the LDAP attribute that refers to the group members. - default: member - type: string * ldapRoleMapperMemberAttributeType: If the member attribute contains the DN for the user, then set this to DN. If the member attribute is the UID of the entry then set this to UID. - default: DN - type: string - allowed values: UID or DN * ldapRoleMapperUserLDAPAttribute: If the ldapRoleMapperMemberAttributeType is UID then this is the LDAP attribute containing the UID value, otherwise this is ignored. - default: sAMAccountName - type: string * ldapRoleMapperRolesLDAPFilter: Extra filter to include when searching for group entries. If this is not the empty string the value must start with the ( character and end with ). - default: \u0026quot; - type: string * ldapRoleMapperMode: Specifies how to retrieve roles for the user. - default: READ_ONLY - allowed values: READ_ONLY, LDAP_ONLY, or IMPORT * ldapRoleMapperStrategy: Defines how to retrieve roles for a user. - default: LOAD_ROLES_BY_MEMBER_ATTRIBUTE - type: string - allowed values: LOAD_ROLES_BY_MEMBER_ATTRIBUTE, GET_ROLES_FROM_MEMBEROF_ATTRIBUTE, or LOAD_ROLES_BY_MEMBER_ATTRIBUTE_RECURSIVELY * ldapRoleMapperMemberOfLDAPAttribute: Only used when ldapRoleMapperStrategy is GET_ROLES_FROM_MEMBEROF_ATTRIBUTE where it is the LDAP attribute in the user entry that contains the roles that the user has. - default: memberOf - type: string * ldapRoleMapperUseRealmRolesMapping: If true then LDAP role mappings will be mapped to realm role mappings in Keycloak, otherwise the LDAP role mappings will be mapped to client role mappings. - default: false - type: string - allowed values: true or false * ldapRoleMapperClientId: If ldapRoleMapperUseRealmRolesMapping is false then this is the client ID to apply the roles to. - default: shasta - type: string   (Optional) Add the LDAP CA certificate in the certs.jks section of customizations.yaml.\nIf LDAP requires TLS (recommended), update the cray-keycloak Sealed Secret value by supplying a base64 encoded Java KeyStore (JKS) that contains the CA certificate that signed the LDAP server\u0026rsquo;s host key. The password for the JKS file must be password.\nAdministrators may use the keytool command from the openjdk:11-jre-slim container image packaged with CSM to create a JKS file that includes a PEM-encoded CA certificate to verify the LDAP host(s).\n  Load the openjdk container image.\n NOTE: Requires a properly configured Docker or Podman environment.\n ncn-m001# ${CSM_DISTDIR}/hack/load-container-image.sh dtr.dev.cray.com/library/openjdk:11-jre-slim Troubleshooting:\n  If the output shows the skopeo.tar file cannot be found, ensure that the $CSM_DISTDIR directory looks correct, and contains the dtr.dev.cray.com directory that includes the originally installed docker images.\nThe following is an example of the skopeo.tar file not being found:\n++ podman load -q -i ./hack/../vendor/skopeo.tar ++ sed -e \u0026#39;s/^.*: //\u0026#39; + SKOPEO_IMAGE=   If the following overlay error is returned, it could be caused by an earlier podman invocation using a different configuration:\n\u0026quot;ERRO[0000] [graphdriver] prior storage driver overlay failed: 'overlay' is not supported over overlayfs, a mount_program is required: backing file system is unsupported for this graph driver\u0026quot; To recover podman, move the overlay directories to a backup folder as follows:\nncn-m001# mkdir /var/lib/containers/storage/backup ncn-m001# mv /var/lib/containers/storage/overlay* /var/lib/containers/storage/backup This should allow load-container-images.sh to succeed.\n    Create (or update) cert.jks with the PEM-encoded CA certificate for an LDAP host.\n IMPORTANT: Replace \u0026lt;ca-cert.pem\u0026gt; and \u0026lt;alias\u0026gt; before running the command.\n ncn-m001# podman run --rm -v \u0026#34;$(pwd):/data\u0026#34; dtr.dev.cray.com/library/openjdk:11-jre-slim keytool \\ -importcert -trustcacerts -file /data/\u0026lt;ca-cert.pem\u0026gt; -alias \u0026lt;alias\u0026gt; -keystore /data/certs.jks \\ -storepass password -noprompt   Set variables for the LDAP server.\nIn the following example, the LDAP server has the hostname dcldap2.us.cray.com and is using the port 636.\nncn-m001# export LDAP=dcldap2.us.cray.com ncn-m001# export PORT=636   Get the issuer certificate for the LDAP server at port 636. Use openssl s_client to connect and show the certificate chain returned by the LDAP host.\nncn-m001# openssl s_client -showcerts -connect $LDAP:${PORT} \u0026lt;/dev/null Either manually extract (cut/paste) the issuer\u0026rsquo;s certificate into cacert.pem, or try the following commands to create it automatically.\n NOTE: The following commands were verified using OpenSSL version 1.1.1d and use the -nameopt RFC2253 option to ensure consistent formatting of distinguished names (DNs). Unfortunately, older versions of OpenSSL may not support -nameopt on the s_client command or may use a different default format. As a result, mileage may vary; however, administrators should be able to extract the issuer certificate manually from the output of the above openssl s_client example if the following commands are unsuccessful.\n Observe the issuer\u0026rsquo;s DN.\nFor example:\nncn-m001# openssl s_client -showcerts -nameopt RFC2253 -connect $LDAP:${PORT} \u0026lt;/dev/null 2\u0026gt;/dev/null | grep issuer= | sed -e \u0026#39;s/^issuer=//\u0026#39; emailAddress=dcops@hpe.com,CN=Data Center,OU=HPC/MCS,O=HPE,ST=WI,C=US Then, extract the issuer\u0026rsquo;s certificate using the awk command:\n NOTE: The issuer DN is properly escaped as part of the awk pattern below. If the value being used is different, be sure to escape it properly!\n ncn-m001# openssl s_client -showcerts -nameopt RFC2253 -connect $LDAP:${PORT} \u0026lt;/dev/null 2\u0026gt;/dev/null | \\  awk \u0026#39;/s:emailAddress=dcops@hpe.com,CN=Data Center,OU=HPC\\/MCS,O=HPE,ST=WI,C=US/,/END CERTIFICATE/\u0026#39; | \\  awk \u0026#39;/BEGIN CERTIFICATE/,/END CERTIFICATE/\u0026#39; \u0026gt; cacert.pem   Verify the issuer\u0026rsquo;s certificate was properly extracted and saved in cacert.pem.\nncn-m001# cat cacert.pem Expected output looks similar to the following:\n-----BEGIN CERTIFICATE----- MIIDvTCCAqWgAwIBAgIUYxrG/PrMcmIzDuJ+U1Gh8hpsU8cwDQYJKoZIhvcNAQEL BQAwbjELMAkGA1UEBhMCVVMxCzAJBgNVBAgMAldJMQwwCgYDVQQKDANIUEUxEDAO BgNVBAsMB0hQQy9NQ1MxFDASBgNVBAMMC0RhdGEgQ2VudGVyMRwwGgYJKoZIhvcN AQkBFg1kY29wc0BocGUuY29tMB4XDTIwMTEyNDIwMzM0MVoXDTMwMTEyMjIwMzM0 MVowbjELMAkGA1UEBhMCVVMxCzAJBgNVBAgMAldJMQwwCgYDVQQKDANIUEUxEDAO BgNVBAsMB0hQQy9NQ1MxFDASBgNVBAMMC0RhdGEgQ2VudGVyMRwwGgYJKoZIhvcN AQkBFg1kY29wc0BocGUuY29tMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKC AQEAuBIZkKitHHVQHymtaQt4D8ZhG4qNJ0cTsLhODPMtVtBjPZp59e+PWzbc9Rj5 +wfjLGteK6/fNJsJctWlS/ar4jw/xBIPMk5pg0dnkMT2s7lkSCmyd9Uib7u6y6E8 yeGoGcb7I+4ZI+E3FQV7zPact6b17xmajNyKrzhBGEjYucYJUL5iTgZ6a7HOZU2O aQSXe7ctiHBxe7p7RhHCuKRrqJnxoohakloKwgHHzDLFQzX/5ADp1hdJcduWpaXY RMBu6b1mhmwo5vmc+fDnfUpl5/X4i109r9VN7JC7DQ5+JX8u9SHDGLggBWkrhpvl bNXMVCnwnSFfb/rnmGO7rdJSpwIDAQABo1MwUTAdBgNVHQ4EFgQUVg3VYExUAdn2 WE3e8Xc8HONy/+4wHwYDVR0jBBgwFoAUVg3VYExUAdn2WE3e8Xc8HONy/+4wDwYD VR0TAQH/BAUwAwEB/zANBgkqhkiG9w0BAQsFAAOCAQEAWLDQLB6rrmK+gwUY+4B7 0USbQK0JkLWuc0tCfjTxNQTzFb75PeH+GH21QsjUI8VC6QOAAJ4uzIEV85VpOQPp qjz+LI/Ej1xXfz5ostZQu9rCMnPtVu7JT0B+NV7HvgqidTfa2M2dw9yUYS2surZO 8S0Dq3Bi6IEhtGU3T8ZpbAmAp+nNsaJWdUNjD4ECO5rAkyA/Vu+WyMz6F3ZDBmRr ipWM1B16vx8rSpQpygY+FNX4e1RqslKhoyuzXfUGzyXux5yhs/ufOaqORCw3rJIx v4sTWGsSBLXDsFM3lBgljSAHfmDuKdO+Qv7EqGzCRMpgSciZihnbQoRrPZkOHUxr NA== -----END CERTIFICATE-----   Create certs.jks.\nncn-m001# podman run --rm -v \u0026#34;$(pwd):/data\u0026#34; dtr.dev.cray.com/library/openjdk:11-jre-slim keytool -importcert \\ -trustcacerts -file /data/cacert.pem -alias cray-data-center-ca -keystore /data/certs.jks \\ -storepass password -noprompt   Create certs.jks.b64 by base-64 encoding certs.jks.\nncn-m001# base64 certs.jks \u0026gt; certs.jks.b64   Inject and encrypt certs.jks.b64 into customizations.yaml.\nncn-m001# cat \u0026lt;\u0026lt;EOF | yq w - \u0026#39;data.\u0026#34;certs.jks\u0026#34;\u0026#39; \u0026#34;$(\u0026lt;certs.jks.b64)\u0026#34; | \\ yq r -j - | /root/site-init/utils/secrets-encrypt.sh | \\ yq w -f - -i /root/site-init/customizations.yaml \u0026#39;spec.kubernetes.sealed_secrets.cray-keycloak\u0026#39; { \u0026#34;kind\u0026#34;: \u0026#34;Secret\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;keycloak-certs\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;services\u0026#34;, \u0026#34;creationTimestamp\u0026#34;: null }, \u0026#34;data\u0026#34;: {} } EOF     Prepare to generate Sealed Secrets.\nSecrets are stored in customizations.yaml as SealedSecret resources (encrypted secrets), which are deployed by specific charts and decrypted by the Sealed Secrets operator. But first, those Secrets must be seeded, generated, and encrypted.\nncn-m001# ./utils/secrets-reencrypt.sh customizations.yaml ./certs/sealed_secrets.key ./certs/sealed_secrets.crt   Encrypt the static values in the customizations.yaml file after making changes.\nThe following command must be run within the site-init directory.\nncn-m001# ./utils/secrets-seed-customizations.sh customizations.yaml Expected output looks similar to:\nCreating Sealed Secret keycloak-certs Generating type static_b64... Creating Sealed Secret keycloak-master-admin-auth Generating type static... Generating type static... Generating type randstr... Generating type static... Creating Sealed Secret cray_reds_credentials Generating type static... Generating type static... Creating Sealed Secret cray_meds_credentials Generating type static... Creating Sealed Secret cray_hms_rts_credentials Generating type static... Generating type static... Creating Sealed Secret vcs-user-credentials Generating type randstr... Generating type static... Creating Sealed Secret generated-platform-ca-1 Generating type platform_ca... Creating Sealed Secret pals-config Generating type zmq_curve... Generating type zmq_curve... Creating Sealed Secret munge-secret Generating type randstr... Creating Sealed Secret slurmdb-secret Generating type static... Generating type static... Generating type randstr... Generating type randstr... Creating Sealed Secret keycloak-users-localize Generating type static...   Decrypt the Sealed Secret to verify it was generated correctly.\nncn-m001# ./utils/secrets-decrypt.sh keycloak_users_localize | jq -r \u0026#39;.data.ldap_connection_url\u0026#39; | base64 --decode ldaps://my_ldap.my_org.test   Re-apply the cray-keycloak Helm chart with the updated customizations.yaml file.\n  Retrieve the current platform.yaml manifest.\nncn-m001# kubectl -n loftsman get cm loftsman-platform -o jsonpath=\u0026#39;{.data.manifest\\.yaml}\u0026#39; \u0026gt; platform.yaml   Remove all charts from the platform.yaml except for cray-keycloak.\nEdit the platform.yaml file and delete all sections starting with -name: \u0026lt;chart_name\u0026gt;, except for the cray-keycloak section.\nThen, change the name of the manifest being deployed from platform to cray-keycloak:\nncn-m001:# sed -i \u0026#39;s/name: platform/name: cray-keycloak/\u0026#39; platform.yaml   Populate the platform manifest with data from the customizations.yaml file.\nncn-m001# manifestgen -i platform.yaml -c customizations.yaml -o new-platform.yaml   Re-apply the platform manifest with the updated cray-keycloak chart.\nncn-m001# loftsman ship --manifest-path ./new-platform.yaml --charts-repo https://packages.local/repository/charts   Wait for the keycloak-certs secret to reflect the new cert.jks.\nRun the following command until there is a non-empty value in the secret (this can take a minute or two):\nncn-m001# kubectl get secret -n services keycloak-certs -o yaml | grep certs.jks certs.jks: \u0026lt;REDACTED\u0026gt;   Restart the cray-keycloak-[012] pods.\nncn-m001# kubectl rollout restart statefulset -n services cray-keycloak   Wait for the Keycloak pods to restart before moving on to the next step.\nOnce the cray-keycloak-[012] pods have restarted, proceed to the next step.\nncn-m001# kubectl get po -n services | grep cray-keycloak     Re-apply the cray-keycloak-users-localize Helm chart with the updated customizations.yaml file.\n  Determine the cray-keycloak-users-localize chart version that is currently deployed.\nncn-m001# helm ls -A -a | grep cray-keycloak-users-localize | awk \u0026#39;{print $(NF-1)}\u0026#39; cray-keycloak-users-localize-1.5.6   Create a manifest file that will be used to reapply the same chart version.\nncn-m001# cat \u0026lt;\u0026lt; EOF \u0026gt; ./cray-keycloak-users-localize-manifest.yaml apiVersion: manifests/v1beta1 metadata: name: reapply-cray-keycloak-users-localize spec: charts: - name: cray-keycloak-users-localize namespace: services version: 1.5.6 EOF   Uninstall the current cray-keycloak-users-localize chart.\nncn-m001# helm del cray-keycloak-users-localize -n services   Populate the deployment manifest with data from the customizations.yaml file.\nncn-m001# manifestgen -i cray-keycloak-users-localize-manifest.yaml -c customizations.yaml -o deploy.yaml   Reapply the cray-keycloak-users-localize chart.\nncn-m001# loftsman ship --manifest-path ./deploy.yaml \\ --charts-repo https://packages.local/repository/charts   Watch the pod to check the status of the job.\nThe pod will go through the normal Kubernetes states. It will stay in a Running state for a while, and then it will go to Completed.\nncn-m001# kubectl get pods -n services | grep keycloak-users-localize keycloak-users-localize-1-sk2hn 0/2 Completed 0 2m35s   Check the pod\u0026rsquo;s logs.\nReplace the KEYCLOAK_POD_NAME value with the pod name from the previous step.\nncn-m001# kubectl logs -n services KEYCLOAK_POD_NAME keycloak-localize \u0026lt;logs showing it has updated the \u0026#34;s3\u0026#34; objects and ConfigMaps\u0026gt; 2020-07-20 18:26:15,774 - INFO - keycloak_localize - keycloak-localize complete     Sync the users and groups from Keycloak to the compute nodes.\n  Get the crayvcs password for pushing the changes.\nncn-m001# kubectl get secret -n services vcs-user-credentials \\ --template={{.data.vcs_password}} | base64 --decode   Checkout content from the cos-config-management VCS repository.\nncn-m001# git clone https://api-gw-service-nmn.local/vcs/cray/cos-config-management.git ncn-m001# cd cos-config-management ncn-m001# git checkout integration   Create the group_vars/Compute/keycloak.yaml file.\nThe file should contain the following values:\n--- keycloak_config_computes: True   Push the changes to VCS with the crayvcs username.\nncn-m001# git add group_vars/Compute/keycloak.yaml ncn-m001# git commit -m \u0026#34;Configure keycloak on computes\u0026#34; ncn-m001# git push origin integration   Update the Configuration Framework Service (CFS) configuration.\nncn-m001# cray cfs configurations update configurations-example \\ --file ./configurations-example.json --format json { \u0026#34;lastUpdated\u0026#34;: \u0026#34;2021-07-28T03:26:30:37Z\u0026#34;, \u0026#34;layers\u0026#34;: [ { \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/example-repo.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;\u0026lt;git commit id\u0026gt;\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;configurations-layer-example-1\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;configurations-example\u0026#34; }   Reboot with the Boot Orchestration Service (BOS).\nncn-m001# cray bos session create --template-uuid BOS_TEMPLATE --operation reboot     Validate that LDAP integration was added successfully.\n  Retrieve the admin password for Keycloak.\nncn-m001: # kubectl get secrets -n services keycloak-master-admin-auth -ojsonpath=\u0026#39;{.data.password}\u0026#39; | base64 -d   Login to the Keycloak UI using the admin user and the password obtained in the previous step.\nThe Keycloak UI URL is typically similar to the following:\nhttps://auth.\u0026lt;system_name\u0026gt;/keycloak   Click on the \u0026ldquo;Users\u0026rdquo; tab in the navigation pane on the left.\n  Click on the \u0026ldquo;View all users\u0026rdquo; button and verify the LDAP users appear in the table.\n  Verify a token can be retrieved from Keycloak using an LDAP user/password.\nIn the example below, replace myuser, mypass, and shasta in the cURL command with site-specific values. The shasta client is created during the SMS install process.\nIn the following example, the python -mjson.tool is not required; it is simply used to format the output for readability.\nncn-w001# curl -s \\  -d grant_type=password \\  -d client_id=shasta \\  -d username=myuser \\  -d password=mypass \\  https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | python -mjson.tool Expected output:\n{ \u0026#34;access_token\u0026#34;: \u0026#34;ey...IA\u0026#34;, \u0026lt;\u0026lt;-- NOTE this value, used in the following step \u0026#34;expires_in\u0026#34;: 300, \u0026#34;not-before-policy\u0026#34;: 0, \u0026#34;refresh_expires_in\u0026#34;: 1800, \u0026#34;refresh_token\u0026#34;: \u0026#34;ey...qg\u0026#34;, \u0026#34;scope\u0026#34;: \u0026#34;profile email\u0026#34;, \u0026#34;session_state\u0026#34;: \u0026#34;10c7d2f7-8921-4652-ad1e-10138ec6fbc3\u0026#34;, \u0026#34;token_type\u0026#34;: \u0026#34;bearer\u0026#34; }   Validate that the access_token looks correct.\nCopy the access_token from the previous step and open a browser window. Navigate to http://jwt.io, and paste the token in the \u0026ldquo;Encoded\u0026rdquo; field.\nVerify the preferred_username is the expected LDAP user and the role is admin (or other role based on the user).\n    "
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/authenticate_an_account_with_the_command_line/",
	"title": "Authenticate An Account With The Command Line",
	"tags": [],
	"description": "",
	"content": "Authenticate an Account with the Command Line Retrieve a token to authenticate to the Cray CLI using the command line. If the Cray CLI is needed before localization occurs and Keycloak is setup, an administrator can use this procedure to authenticate to the Cray CLI.\nProcedure   Retrieve the Kubernetes secret to be used for authentication.\nncn-w001# ADMIN_SECRET=$(kubectl get secrets admin-client-auth -ojsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d);   Modify the setup-token.json file so it is readable only by root.\nncn-w001# touch /tmp/setup-token.json ncn-w001# chmod 600 /tmp/setup-token.json   Retrieve a token for the new Keycloak account.\nncn-w001# curl -s -d grant_type=client_credentials -d client_id=admin-client -d client_secret=$ADMIN_SECRET \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token \u0026gt; /tmp/setup-token.json;   Set up the new account with the authenticated token.\nncn-w001# export CRAY_CREDENTIALS=/tmp/setup-token.json;   "
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/backup_and_restore_vault_clusters/",
	"title": "Backup And Restore Vault Clusters",
	"tags": [],
	"description": "",
	"content": "Backup and Restore Vault Clusters View the existing Vault backups on the system and use a completed backup to perform a restore operation.\nCAUTION: A restore operation should only be performed in extreme situations. Performing a restore from a backup may cause secrets stored in Vault to change to an earlier state or get out of sync.\n Velero is used to perform a nightly backup of Vault. The backup includes Kubernetes object state, in addition to pod volume data for the vault statefulset. For more information on Velero, refer to the https://velero.io/ external documentation.  Prerequisites   Access to a Kubernetes master or worker node.\nAll of the steps listed in this section should be performed from a Kubernetes master or worker node.\n  Ceph must be healthy to maximize the chance of a successful restore.\n  The kubectl command is installed.\n  View Backup Schedules and Complete Backups   View the backup schedules.\nncn# velero get schedule NAME STATUS CREATED SCHEDULE BACKUP TTL LAST BACKUP SELECTOR vault-daily-backup Enabled 2021-01-26 14:14:04 +0000 UTC 0 2 * * * 0s 19h ago vault_cr=cray-vault   View the completed backups.\nncn# velero get backup NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR vault-daily-backup-20210217020038 Completed 0 0 2021-02-17 02:00:38 +0000 UTC 29d default vault_cr=cray-vault vault-daily-backup-20210216020035 Completed 0 0 2021-02-16 02:00:35 +0000 UTC 28d default vault_cr=cray-vault vault-daily-backup-20210215020035 Completed 0 0 2021-02-15 02:00:35 +0000 UTC 27d default vault_cr=cray-vault ...   View the details of a completed backup.\nReplace the BACKUP_NAME value with the name of a backup returned in the previous step.\nncn# velero describe backup BACKUP_NAME --details Name: vault-daily-backup-20210217020038 Namespace: velero Labels: app.kubernetes.io/managed-by=Helm velero.io/schedule-name=vault-daily-backup velero.io/storage-location=default Annotations: velero.io/source-cluster-k8s-gitversion=v1.18.6 velero.io/source-cluster-k8s-major-version=1 velero.io/source-cluster-k8s-minor-version=18 Phase: Completed Errors: 0 Warnings: 0 Namespaces: Included: vault Excluded: \u0026lt;none\u0026gt; Resources: Included: pv, pvc, secret, sealedsecret, vault, configmap, deployment, service, statefulset, pod, ingress, replicaset Excluded: \u0026lt;none\u0026gt; Cluster-scoped: included Label selector: vault_cr=cray-vault Storage Location: default Velero-Native Snapshot PVs: auto TTL: 720h0m0s Hooks: \u0026lt;none\u0026gt; Backup Format Version: 1.1.0 Started: 2021-02-17 02:00:38 +0000 UTC Completed: 2021-02-17 02:00:52 +0000 UTC Expiration: 2021-03-19 02:00:38 +0000 UTC Total items to be backed up: 21 Items backed up: 21 Resource List: apps/v1/Deployment: - vault/cray-vault-configurer apps/v1/ReplicaSet: - vault/cray-vault-configurer-56df7f768d apps/v1/StatefulSet: - vault/cray-vault v1/ConfigMap: - vault/cray-vault-configurer - vault/cray-vault-statsd-mapping v1/PersistentVolume: - pvc-0ea5065b-d5e1-45f9-8b54-b8f56281b81b - pvc-34d11110-1ff3-4267-8e66-696045f35af4 - pvc-e3d07b75-1b27-4a55-b8d5-8e57857ad619 v1/PersistentVolumeClaim: - vault/vault-raft-cray-vault-0 - vault/vault-raft-cray-vault-1 - vault/vault-raft-cray-vault-2 v1/Pod: - vault/cray-vault-0 - vault/cray-vault-1 - vault/cray-vault-2 - vault/cray-vault-configurer-56df7f768d-z2wzn v1/Secret: - vault/cray-vault-unseal-keys v1/Service: - vault/cray-vault - vault/cray-vault-0 - vault/cray-vault-1 - vault/cray-vault-2 - vault/cray-vault-configurer Velero-Native Snapshots: \u0026lt;none included\u0026gt; Restic Backups: Completed: vault/cray-vault-0: vault-raft vault/cray-vault-1: vault-raft vault/cray-vault-2: vault-raft   Restore from a Backup  Verify the backup being restored contains a manifest of resources and Restic volume backups.\nObject names will vary.\nncn# velero describe backup BACKUP_NAME --details Name: vault-daily-backup-20210217020038 Namespace: velero Labels: app.kubernetes.io/managed-by=Helm velero.io/schedule-name=vault-daily-backup velero.io/storage-location=default Annotations: velero.io/source-cluster-k8s-gitversion=v1.18.6 velero.io/source-cluster-k8s-major-version=1 velero.io/source-cluster-k8s-minor-version=18 Phase: Completed Errors: 0 Warnings: 0 Namespaces: Included: vault Excluded: \u0026lt;none\u0026gt; Resources: Included: pv, pvc, secret, sealedsecret, vault, configmap, deployment, service, statefulset, pod, ingress, replicaset Excluded: \u0026lt;none\u0026gt; Cluster-scoped: included Label selector: vault_cr=cray-vault Storage Location: default Velero-Native Snapshot PVs: auto TTL: 720h0m0s Hooks: \u0026lt;none\u0026gt; Backup Format Version: 1.1.0 Started: 2021-02-17 02:00:38 +0000 UTC Completed: 2021-02-17 02:00:52 +0000 UTC Expiration: 2021-03-19 02:00:38 +0000 UTC Total items to be backed up: 21 Items backed up: 21 Resource List: apps/v1/Deployment: - vault/cray-vault-configurer apps/v1/ReplicaSet: - vault/cray-vault-configurer-56df7f768d apps/v1/StatefulSet: - vault/cray-vault v1/ConfigMap: - vault/cray-vault-configurer - vault/cray-vault-statsd-mapping v1/PersistentVolume: - pvc-0ea5065b-d5e1-45f9-8b54-b8f56281b81b - pvc-34d11110-1ff3-4267-8e66-696045f35af4 - pvc-e3d07b75-1b27-4a55-b8d5-8e57857ad619 v1/PersistentVolumeClaim: - vault/vault-raft-cray-vault-0 - vault/vault-raft-cray-vault-1 - vault/vault-raft-cray-vault-2 v1/Pod: - vault/cray-vault-0 - vault/cray-vault-1 - vault/cray-vault-2 - vault/cray-vault-configurer-56df7f768d-z2wzn v1/Secret: - vault/cray-vault-unseal-keys v1/Service: - vault/cray-vault - vault/cray-vault-0 - vault/cray-vault-1 - vault/cray-vault-2 - vault/cray-vault-configurer Velero-Native Snapshots: \u0026lt;none included\u0026gt; Restic Backups: Completed: vault/cray-vault-0: vault-raft vault/cray-vault-1: vault-raft vault/cray-vault-2: vault-raft   Scale the Vault operator down so that it will not attempt to reconcile the instance while the restore is in progress.\n  Scale the Vault operator down.\nncn# kubectl -n vault scale deployment cray-vault-operator --replicas=0 deployment.apps/cray-vault-operator scaled   Verify the changes were successfully made.\nncn# kubectl -n vault get deployment NAME READY UP-TO-DATE AVAILABLE AGE cray-vault-operator 0/0 0 0 19h     Delete the Vault instance to minimize the risk of Vault being in a partially restored state.\nVault will be inaccessible (if not already) after running the following commands.\nncn# kubectl -n vault delete vault -l vault_cr=cray-vault ncn# kubectl -n vault delete pvc -l vault_cr=cray-vault ncn# kubectl -n vault delete secret -l vault_cr=cray-vault   Submit the restore action.\nMonitor the progress of the restore job until it is in a completed phase. The progress can be viewed by using the logs command shown in the output.\nncn# velero restore create --from-backup BACKUP_NAME Restore request \u0026#34;vault-daily-backup-20210217100000\u0026#34; submitted successfully. Run `velero restore describe vault-daily-backup-20210217100000` or `velero restore logs vault-daily-backup-20210217100000` for more details.   Scale the Vault operator back to one replica.\n  Scale the Vault operator.\nncn# kubectl -n vault scale deployment cray-vault-operator --replicas=1 deployment.apps/cray-vault-operator scaled   Verify the changes were successfully made.\nncn# kubectl -n vault get deployment NAME READY UP-TO-DATE AVAILABLE AGE cray-vault-operator 1/1 1 1 19h     Delete the Vault pods and allow the operator to restart them.\nThe pods need to be manually restarted if the Vault statefulset pods are in CrashLoopBackOff after 5-10 minutes of performing the restore operation. The vault statefulset pods normally go through a number of restarts on a clean start-up.\n  Verify the pods are in a CrashLoopBackOff state.\nncn# kubectl -n vault get pod -o wide -l vault_cr=cray-vault NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES cray-vault-0 4/5 CrashLoopBackOff 9 30m 10.44.0.33 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-vault-1 4/5 CrashLoopBackOff 9 30m 10.42.0.10 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-vault-2 4/5 CrashLoopBackOff 9 30m 10.40.0.12 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-vault-configurer-56df7f768d-c228k 2/2 Running 0 30m 10.44.0.8 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   Delete the pods to restart them.\nncn# kubectl delete pod -n vault -l vault_cr=cray-vault pod \u0026#34;cray-vault-0\u0026#34; deleted pod \u0026#34;cray-vault-1\u0026#34; deleted pod \u0026#34;cray-vault-2\u0026#34; deleted pod \u0026#34;cray-vault-configurer-56df7f768d-c228k\u0026#34; deleted   Verify the pods are in a Running state.\nncn# kubectl get pod -n vault -l vault_cr=cray-vault NAME READY STATUS RESTARTS AGE cray-vault-0 5/5 Running 2 105s cray-vault-1 5/5 Running 2 67s cray-vault-2 5/5 Running 2 38s cray-vault-configurer-56df7f768d-c7mk2 2/2 Running 0 2m21s     "
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/certificate_types/",
	"title": "Certificate Types",
	"tags": [],
	"description": "",
	"content": "Certificate Types The system software installation process creates an X.509 Certificate Authority (CA) on the primary non-compute node (NCN) and uses the CA to create an NCN host X.509 certificate. This host certificate is used during the installation process to configure the API gateway for TLS so that communications to the gateway can use HTTPS.\nClients should use HTTPS to talk to services behind the API gateway and need to ensure that the NCN CA certificate is known by the client software when making requests.\nKeycloak, which is the Identity and Access Management (IAM) server, will also have a certificate created at install time for the Shasta realm. This certificate is known as the Shasta realm certificate and is used when signing a JSON Web Token (JWT). The Shasta realm certificate is registered with the API gateway and is used by the API gateway to validate that a JWT passed when requests are made actually originate from the IAM server.\nThis document does not cover the process for updating any of the certificates described below.\n  NCN CA certificate - The NCN CA is created by the installer and located at sms-1:/var/opt/cray/certificate_authority/certificate_authority.crt. The signature algorithm used is sha256WithRSAEncryption and the key length is 2048 bits. The CA Issuer is generated at the time of creation and therefore specific to each installation. This and other certificate details can be viewed from the NCN by executing:\n# openssl x509 -in /var/opt/cray/certificate_authority/certificate_authority.crt -noout -text   NCN host certificate - The NCN host certificate is created by the installer and located at sms-1:/var/opt/cray/certificate_authority/hosts/host.crt\nThe signature algorithm used is sha256WithRSAEncryption and the key length is 2048 bits. Additional certificate details can be viewed from the NCN by executing:\n# openssl x509 -in /var/opt/cray/certificate_authority/hosts/host.crt -noout -text   API Gateway TLS certificate - The API gateway is configured with the NCN host certificate and key to allow enabling TLS/HTTPS on the gateway. Configuration details are handled by the installer when the API gateway Kubernetes pods are created.\n  IAM Service Shasta realm JWT certificate - The IAM Service (Keycloak) Shasta realm contains an RSA certificate. This is known as the realm certificate. The realm certificate is used by the API gateway during JWT validation.\nThe installer adds a Shasta realm to the IAM service. The RSA realm certificate is created as part of that process. This certificate can be viewed from the Keycloak Admin Console by selecting the Shasta realm, then clicking on the Realm Settings link and then on the Keys tab. The certificate can be viewed by clicking the Public Key button for the RSA certificate. The key length is 2048 bits.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/change_ex_liquid-cooled_cabinet_global_default_password/",
	"title": "Change Cray Ex Liquid-cooled Cabinet Global Default Password",
	"tags": [],
	"description": "",
	"content": "Change Cray EX Liquid-Cooled Cabinet Global Default Password This procedure changes the global default credential on HPE Cray EX liquid-cooled cabinet embedded controllers (BMCs). The chassis management module (CMM) controller (cC), node controller, and Slingshot switch controller (sC) are generically referred to as \u0026ldquo;BMCs\u0026rdquo; in these procedures.\nPrerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray Command Line Interface (cray CLI) for more information. Review procedures in Manage System Passwords.  Procedure   If necessary, shut down compute nodes in each cabinet. Refer to Shut Down and Power Off Compute and User Access Nodes.\nncn-m001# sat bootsys shutdown --stage bos-operations \\ --bos-templates COS_SESSION_TEMPLATE   Perform procedures in Provisioning a Liquid-Cooled EX Cabinet CEC with Default Credentials.\n  Perform procedures in Updating the Liquid-Cooled EX Cabinet Default Credentials after a CEC Password Change.\n  To update Slingshot switch BMCs, refer to \u0026ldquo;Change Rosetta Login and Redfish API Credentials\u0026rdquo; in the Slingshot Operations Guide (\u0026gt; 1.6.0).\n  "
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/access_the_keycloak_user_management_ui/",
	"title": "Access The Keycloak User Management Ui",
	"tags": [],
	"description": "",
	"content": "Access the Keycloak User Management UI This procedure can be used to access the interface to manage Keycloak users. Users can be added with this interface (see Create Internal User Accounts in the Keycloak Shasta Realm).\nPrerequisites   This procedure uses SYSTEM_DOMAIN_NAME as an example for the DNS name of the non-compute node (NCN). Replace this name with the actual NCN\u0026rsquo;s DNS name while executing this procedure.\n  This procedure assumes that the password for the Keycloak admin account is known. The Keycloak password is set during the software installation process. The password can be obtained with the following command:\nncn-w001# kubectl get secret -n services keycloak-master-admin-auth \\ --template={{.data.password}} | base64 --decode   Procedure   Point a browser at https://auth.SYSTEM_DOMAIN_NAME/keycloak/, replacing SYSTEM_DOMAIN_NAME with the actual NCN\u0026rsquo;s DNS name.\nThe following is an example URL for a system:\nauth.system1.us.cray.com/keycloak The browser may return an error message similar to the following when auth.SYSTEM_DOMAIN_NAME/keycloak is launched for the first time:\nThis Connection Is Not Private This website may be impersonating \u0026quot;hostname\u0026quot; to steal your personal or financial information. You should go back to the previous page. See Make HTTPS Requests from Sources Outside the Management Kubernetes Cluster for more information on getting the Certificate Authority (CA) certificate on the system.\n  Click the Administration Console link.\n  Log in as the admin user for the Master realm.\n  Ensure that the selected Realm is Shasta.\n  Click the Users link under the Manage menu on the left side of the screen.\n  New users can be added with this interface (see Create Internal User Accounts in the Keycloak Shasta Realm).\n"
},
{
	"uri": "/docs-csm/en-11/operations/security_and_authentication/api_authorization/",
	"title": "Api Authorization",
	"tags": [],
	"description": "",
	"content": "API Authorization Authorization for REST API calls is only done at the API gateway. This is facilitated through policy checks to the Open Policy Agent (OPA). Every REST API call into the system is sent to the OPA to make an authorization decision. The decision is based on the Authenticated JSON Web Token (JWT) passed into the request.\nThe following is a list of available personas and the supported REST API endpoints for each:\n  admin\nAuthorized for every possible REST API endpoint.\n  user\nAuthorized for a subset of endpoints to allow users to create and use User Access Instances (UAI), run jobs, view job results, and use capsules.\nREST API endpoints for the user persona:\n# UAS {\u0026quot;method\u0026quot;: \u0026quot;GET\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/uas-mgr/v1/$`}, # Get UAS API Version {\u0026quot;method\u0026quot;: \u0026quot;GET\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/uas-mgr/v1/uas$`}, # List UAIs for current user {\u0026quot;method\u0026quot;: \u0026quot;POST\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/uas-mgr/v1/uas$`}, # Create a UAI for current user {\u0026quot;method\u0026quot;: \u0026quot;DELETE\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/uas-mgr/v1/uas$`}, # Delete a UAI(s) for current user {\u0026quot;method\u0026quot;: \u0026quot;GET\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/uas-mgr/v1/images$`}, # List Available UAI Images {\u0026quot;method\u0026quot;: \u0026quot;GET\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/uas-mgr/v1/mgr-info$`}, # Get UAS Service Version # PALS {\u0026quot;method\u0026quot;: \u0026quot;GET\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/pals/v1/.*$`}, # All PALs API Calls - GET {\u0026quot;method\u0026quot;: \u0026quot;PUT\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/pals/v1/.*$`}, # All PALs API Calls - PUT {\u0026quot;method\u0026quot;: \u0026quot;POST\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/pals/v1/.*$`}, # All PALs API Calls - POST {\u0026quot;method\u0026quot;: \u0026quot;DELETE\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/pals/v1/.*$`}, # All PALs API Calls - DELETE {\u0026quot;method\u0026quot;: \u0026quot;HEAD\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/pals/v1/.*$`}, # All PALs API Calls - HEAD {\u0026quot;method\u0026quot;: \u0026quot;PATCH\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/pals/v1/.*$`}, # All PALs API Calls - PATCH # Replicant {\u0026quot;method\u0026quot;: \u0026quot;GET\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/rm/v1/report/[\\d\\w|-]+$`}, # Get Report by id {\u0026quot;method\u0026quot;: \u0026quot;GET\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/rm/v1/reports$`}, # Get Reports # Analytics Capsules {\u0026quot;method\u0026quot;: \u0026quot;DELETE\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/capsules/.*$`}, # All Capsules API Calls - DELETE {\u0026quot;method\u0026quot;: \u0026quot;GET\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/capsules/.*$`}, # All Capsules API Calls - GET {\u0026quot;method\u0026quot;: \u0026quot;HEAD\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/capsules/.*$`}, # All Capsules API Calls - HEAD {\u0026quot;method\u0026quot;: \u0026quot;PATCH\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/capsules/.*$`}, # All Capsules API Calls - PATCH {\u0026quot;method\u0026quot;: \u0026quot;POST\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/capsules/.*$`}, # All Capsules API Calls - POST {\u0026quot;method\u0026quot;: \u0026quot;PUT\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/capsules/.*$`}, # All Capsules API Calls - PUT   system-pxe\nAuthorized for endpoints related to booting.\nREST API endpoints for the system-pxe persona:\n{\u0026quot;method\u0026quot;: \u0026quot;GET\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/bss/.*$`}, {\u0026quot;method\u0026quot;: \u0026quot;HEAD\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/bss/.*$`}, {\u0026quot;method\u0026quot;: \u0026quot;POST\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/bss/.*$`},   system-compute\nAuthorized for endpoints required by the Cray Operating System (COS) to manage compute nodes and NCN services.\nREST API endpoints for the system-compute persona:\n{\u0026quot;method\u0026quot;: \u0026quot;GET\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/cfs/.*$`}, {\u0026quot;method\u0026quot;: \u0026quot;HEAD\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/cfs/.*$`}, {\u0026quot;method\u0026quot;: \u0026quot;PATCH\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/cfs/.*$`}, {\u0026quot;method\u0026quot;: \u0026quot;GET\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/v2/cps/.*$`}, {\u0026quot;method\u0026quot;: \u0026quot;HEAD\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/v2/cps/.*$`}, {\u0026quot;method\u0026quot;: \u0026quot;POST\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/v2/cps/.*$`}, {\u0026quot;method\u0026quot;: \u0026quot;GET\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/hbtd/.*$`}, {\u0026quot;method\u0026quot;: \u0026quot;HEAD\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/hbtd/.*$`}, {\u0026quot;method\u0026quot;: \u0026quot;POST\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/hbtd/.*$`}, {\u0026quot;method\u0026quot;: \u0026quot;GET\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/v2/nmd/.*$`}, {\u0026quot;method\u0026quot;: \u0026quot;HEAD\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/v2/nmd/.*$`}, {\u0026quot;method\u0026quot;: \u0026quot;POST\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/v2/nmd/.*$`}, {\u0026quot;method\u0026quot;: \u0026quot;PUT\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/v2/nmd/.*$`}, {\u0026quot;method\u0026quot;: \u0026quot;GET\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/smd/.*$`}, {\u0026quot;method\u0026quot;: \u0026quot;HEAD\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/smd/.*$`}, {\u0026quot;method\u0026quot;: \u0026quot;GET\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/hmnfd/.*$`}, {\u0026quot;method\u0026quot;: \u0026quot;HEAD\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/hmnfd/.*$`}, {\u0026quot;method\u0026quot;: \u0026quot;PATCH\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/hmnfd/.*$`}, {\u0026quot;method\u0026quot;: \u0026quot;POST\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/hmnfd/.*$`}, {\u0026quot;method\u0026quot;: \u0026quot;DELETE\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/hmnfd/.*$`},   wlm\nAuthorized for endpoints related to the use of the Slurm or PBS workload managers.\nREST API endpoints for the wlm persona:\n# PALS - application launch {\u0026quot;method\u0026quot;: \u0026quot;GET\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/pals/.*$`}, {\u0026quot;method\u0026quot;: \u0026quot;HEAD\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/pals/.*$`}, {\u0026quot;method\u0026quot;: \u0026quot;POST\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/pals/.*$`}, {\u0026quot;method\u0026quot;: \u0026quot;DELETE\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/pals/.*$`}, # CAPMC - power capping {\u0026quot;method\u0026quot;: \u0026quot;GET\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/capmc/.*$`}, {\u0026quot;method\u0026quot;: \u0026quot;HEAD\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/capmc/.*$`}, {\u0026quot;method\u0026quot;: \u0026quot;POST\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/capmc/.*$`}, # BOS - node boot {\u0026quot;method\u0026quot;: \u0026quot;GET\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/bos/.*$`}, {\u0026quot;method\u0026quot;: \u0026quot;HEAD\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/bos/.*$`}, {\u0026quot;method\u0026quot;: \u0026quot;POST\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/bos/.*$`}, {\u0026quot;method\u0026quot;: \u0026quot;PATCH\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/bos/.*$`}, {\u0026quot;method\u0026quot;: \u0026quot;DELETE\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/bos/.*$`}, # SLS - hardware query {\u0026quot;method\u0026quot;: \u0026quot;GET\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/sls/.*$`}, {\u0026quot;method\u0026quot;: \u0026quot;HEAD\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/sls/.*$`}, # SMD - hardware state query {\u0026quot;method\u0026quot;: \u0026quot;GET\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/smd/.*$`}, {\u0026quot;method\u0026quot;: \u0026quot;HEAD\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/smd/.*$`}, # FC - VNI reservation {\u0026quot;method\u0026quot;: \u0026quot;GET\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/fc/.*$`}, {\u0026quot;method\u0026quot;: \u0026quot;HEAD\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/fc/.*$`}, {\u0026quot;method\u0026quot;: \u0026quot;POST\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/fc/.*$`}, {\u0026quot;method\u0026quot;: \u0026quot;PUT\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/fc/.*$`}, {\u0026quot;method\u0026quot;: \u0026quot;DELETE\u0026quot;, \u0026quot;path\u0026quot;: `^/apis/fc/.*$`},   "
},
{
	"uri": "/docs-csm/en-11/operations/river_endpoint_discovery_service/troubleshoot_common_error_messages_in_reds_logs/",
	"title": "Troubleshoot Common Error Messages In REDS Logs",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Common Error Messages in REDS Logs Examine logs for error messages that indicate common errors in the setup or running of the River Endpoint Discovery Service (REDS).\nLimitations This procedure does not cover all possible error messages. For help interpreting error messages not included here, contact the Cray customer account representative for this site.\nExamine REDS Logs Before viewing the REDS logs, set the following environment variable:\nncn-m001# REDSPOD=$(kubectl -n services get pods -l app.kubernetes.io/name=cray-reds \\ -o=custom-columns=:.metadata.name --no-headers) ncn-m001# echo $REDSPOD cray-reds-5854fdcd9d-ffgms   To view the logs and continue to monitor new messages as they are added:\nncn-m001# kubectl -n services logs --follow $REDSPOD cray-reds Press Ctrl-c to exit.\n  To view the logs but not continue to monitor:\nncn-m001# kubectl -n services logs $REDSPOD cray-reds   When examining the log for errors, begin at the beginning of the log and work downwards. In many cases, an error early on can cause other errors later, so it is important to correct the error first. The following table lists the most common error messages in REDS logs.\nThe Credentials in the Mapping do not Match the Credentials Configured on the Switch The following error message will be shown:\nWARNING: ${XNAME}:Getting Mac address table failed (VLANs): Received a report from the agent - UsmStatsDecryptionErrors(1.3.6.1.6.3.15.1.1.6.0) Solution:\n Verify that the credentials configured in the mapping file match those configured on the switch. Pay special attention to the protocols in use. After the credentials issue is corrected, Clear State and Restart REDS.  REDS did not Verify the HTTPS Certificate of the Datastore Service The following error message will be shown:\nWARNING: insecure https connection to datastore service Solution: Do nothing. This message is expected.\nThe Mapping File was not Uploaded The following error message will be shown:\nSNMP: Ignoring string on ignore list: NET-SNMP version ${VERSION} Solution: See System Layout Service (SLS).\n"
},
{
	"uri": "/docs-csm/en-11/operations/river_endpoint_discovery_service/troubleshoot_common_reds_issues/",
	"title": "Troubleshoot Common REDS Issues",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Common REDS Issues This procedure provides troubleshooting steps for two different scenarios:\n No nodes can be geolocated. One of the nodes can be geolocated.  Prerequisites  The River Endpoint Discovery Service (REDS) failed to geolocate one or more nodes. The Cray command line interface (CLI) tool is initialized and configured on the system.  Limitations This procedure does not cover the list of all possible issues that can be encountered while using REDS. contact the Cray customer account representative to resolve issues not covered here.\nProcedure   Use one of the following set of steps, depending on requirements/scenario.\n Perform the following set of steps if no nodes can be geolocated.   Check the status of the REDS pod to determine whether it is running.\nncn-m001# kubectl get pods -n services | grep reds cray-reds-66d99d895c-2cpk9 2/2 Running 0 48m   If no result is returned, start REDS manually.\nncn-m001# kubectl scale -n services --replicas=0 deployment cray-reds; \\ kubectl scale -n services --replicas=1 deployment cray-reds Wait at least three (3) minutes, then check the REDS pod status again. If REDS is now running, skip the rest of this procedure. If nodes still cannot be geolocated, restart this procedure.\n  If a result is returned but the third column does not indicate Running, see Troubleshoot Common Error Messages in REDS Logs.\n  If a result is returned and the third column indicates running, there must be some other problem, so continue to the next step.\n    Check if REDS is able to communicate with the System Layout Service (SLS).\n  Set an environment variable.\nncn-m001# REDSPOD=$(kubectl -n services get pods -l app.kubernetes.io/name=cray-reds \\ -o=custom-columns=:.metadata.name --no-headers) ncn-m001# echo $REDSPOD cray-reds-5854fdcd9d-ffgms   Check the logs.\nncn-m001# kubectl -n services logs -f $REDSPOD cray-reds   When finished viewing the log, press Ctrl-c to exit.\n  If the following error message is in the log, REDS cannot communicate with SLS. If nodes still cannot be geolocated, restart this procedure.\nWARNING: Unable to get new switch list:   If messages like the following are in the log, then REDS can communicate with SLS. There must be some other problem, so proceed to the next step.\nRunning periodic scan for XNAME     Check the logs to determine whether REDS configured the necessary artifacts in the artifact repository and BSS.\n  Set an environment variable.\nncn-m001# REDSPOD=$(kubectl -n services get pods -l app.kubernetes.io/name=cray-reds-init \\ -o=custom-columns=:.metadata.name --no-headers) ncn-m001# echo $REDSPOD cray-reds-5854fdcd9d-ffgms   Check the logs.\nncn-m001# kubectl -n services logs -f $REDSPOD cray-reds   When finished viewing the log, press Ctrl-c to exit.\n  If the last line of the log is the following, then REDS correctly configured artifacts in the artifact repository and BSS.\nFinished updates to BSS entry for Unknown-x86_64! There must be some other problem. Proceed to the next step.\n  If that line is not present in the log, then there may be an issue with the artifact repository and/or BSS.\n    Observe the boot process of a node to determine whether the node is able to pull the images. Attach a monitor to the video port on the node and watch it boot. If it does not boot to a Linux login prompt, determine whether the network is up and working correctly (for example, run ip a s on a non-compute node (NCN), or try pinging switches from an NCN).\n If the network is not up and working correctly, troubleshoot the network. If the network is up and working correctly, then there must be some other problem. Contact the Cray customer account representative for this site.     Perform the following set of steps if one of the nodes cannot be geolocated.   Look for the appropriate xname in the list of Redfish endpoints to determine whether the node has already been discovered.\nncn-m001# cray hsm inventory redfishEndpoints list   Check the logs for indicators of common issues. See Troubleshoot Common Error Messages in REDS Logs.\n  Determine whether the network is up and working correctly (for example, run ip a s on an NCN, or try pinging switches from an NCN).\n If the network is not up and working correctly, troubleshoot the network. If the network is up and working correctly, then there must be some other problem. Contact the Cray customer account representative for this site.        If the suggested troubleshooting steps do not resolve the issue, contact the Cray customer account representative for this site.\n"
},
{
	"uri": "/docs-csm/en-11/operations/river_endpoint_discovery_service/verify_node_removal/",
	"title": "Verify Node Removal",
	"tags": [],
	"description": "",
	"content": "Verify Node Removal Use this procedure to verify that a node has been successfully removed from the system.\nPrerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system. This procedure requires the xname of the removed node to be known.  Limitations The REDS mapping file is not automatically updated when the system is changed. It must be manually updated.\nProcedure   Ensure that the Redfish endpoint of the removed node\u0026rsquo;s BMC has been disabled.\n  View the list of Redfish endpoints to verify the removed node\u0026rsquo;s BMC Redfish endpoint has been disabled:\nncn-m001# cray hsm inventory redfishEndpoints list [[RedfishEndpoints]] Domain = \u0026#34; MACAddr = \u0026#34;a4bf012b71a9\u0026#34; UUID = \u0026#34;61b3843b-9d33-4986-ba03-1b8acd0bfd9c\u0026#34; IPAddress = \u0026#34;10.254.2.13\u0026#34; RediscoverOnUpdate = true Hostname = \u0026#34;10.254.2.13\u0026#34; Enabled = false \u0026lt;\u0026lt;-- The Redfish endpoint has been disabled FQDN = \u0026#34;10.254.2.13\u0026#34; User = \u0026#34;root\u0026#34; Password = \u0026#34; Type = \u0026#34;NodeBMC\u0026#34; ID = \u0026#34;x3000c0s19b4\u0026#34; [RedfishEndpoints.DiscoveryInfo] LastDiscoveryAttempt = \u0026#34;2020-04-03T12:37:48.833692Z\u0026#34; RedfishVersion = \u0026#34;1.1.0\u0026#34; LastDiscoveryStatus = \u0026#34;DiscoverOK\u0026#34; ...   Search the list of Redfish endpoints using the xname of the node\u0026rsquo;s BMC that it was disabled:\nncn-m001# cray hsm inventory redfishEndpoints list --id NODE_BMC_XNAME [[RedfishEndpoints]] Domain = \u0026#34; MACAddr = \u0026#34;a4bf012b71a9\u0026#34; UUID = \u0026#34;61b3843b-9d33-4986-ba03-1b8acd0bfd9c\u0026#34; IPAddress = \u0026#34;10.254.2.13\u0026#34; RediscoverOnUpdate = true Hostname = \u0026#34;10.254.2.13\u0026#34; Enabled = false \u0026lt;\u0026lt;-- The Redfish endpoint has been disabled FQDN = \u0026#34;10.254.2.13\u0026#34; User = \u0026#34;root\u0026#34; Password = \u0026#34; Type = \u0026#34;NodeBMC\u0026#34; ID = \u0026#34;x3000c0s19b4\u0026#34; [RedfishEndpoints.DiscoveryInfo] LastDiscoveryAttempt = \u0026#34;2020-04-03T12:37:48.833692Z\u0026#34; RedfishVersion = \u0026#34;1.1.0\u0026#34; LastDiscoveryStatus = \u0026#34;DiscoverOK\u0026#34;     If a River node will not be replaced, update the REDS mapping file. If the System Layout Service (SLS) is enabled, update the SLS input file to omit it.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/river_endpoint_discovery_service/clear_state_and_restart_reds/",
	"title": "Clear State And Restart REDS",
	"tags": [],
	"description": "",
	"content": "Clear State and Restart REDS Many solutions to common River Endpoint Discovery Service (REDS) errors require that the REDS state be deleted and the service restarted. Use this procedure if an error has occurred that requires a restart of REDS.\nPrerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system.  Procedure   Stop all processes in the REDS container and delete all non-persistent state.\nncn-m001# kubectl scale -n services --replicas=0 deployment cray-reds   Wait for the first command to finish.\nncn-m001# while [ -n \u0026#34;$(kubectl -n services get pods | grep reds | \\ grep -v -e etcd -e loader -e init )\u0026#34; ]; do sleep 1; done   Delete all persistent state.\nncn-m001# kubectl exec -it -n services $(kubectl get pods -n services | grep reds-etcd \\ | head -n 1 | awk \u0026#39;{print $1}\u0026#39;) -- /bin/sh -c \u0026#39;ETCDCTL_API=3 etcdctl del \u0026#34; --from-key=true\u0026#39;   Restart the REDS container, which will have no saved state.\nncn-m001# kubectl scale -n services --replicas=1 deployment cray-reds   Wait for REDS to fully come up.\nncn-m001# while [ -z \u0026#34;$(kubectl get pods -n services | grep reds | grep Running)\u0026#34; ] ; do sleep 1; done   Verify that the status of REDS is Running.\n$ kubectl get pods -n services | grep reds | grep Running If no results are returned, REDS is not running.\n  Upload the current REDS mapping file.\nWhen state information is deleted, mapping data is deleted as well, so it is necessary to upload the current mapping file to restore that data.\nncn-m001# cray reds port_xname_map update --map-file MAP_FILENAME   REDS has been restarted and has current mapping data.\n"
},
{
	"uri": "/docs-csm/en-11/operations/river_endpoint_discovery_service/configure_a_management_switch_for_reds/",
	"title": "Configure A Management Switch For REDS",
	"tags": [],
	"description": "",
	"content": "Configure a Management Switch for REDS Every switch listed in the River Endpoint Discovery Service (REDS) mapping file requires additional configuration for REDS to function properly. This procedure configures a switch to add a group named cray-reds-group, add a user to that group (username) with appropriate permissions, and create a view named cray-reds-view.\nThe IP address used in example commands in this procedure corresponds to the default IP address for the switch stack that is connected to the management networks (Node Management Network (NMN), High Speed Network (HSN), and ClusterStor network). Performing this procedure using the IP address for the switch stack configures both physical switches in that stack.\nThis configuration is done automatically during the software installation process. Perform this procedure manually as part of cold-starting a system, which must be done whenever there is a power outage or a new switch is added to the system, such as when a switch fails and needs to be replaced.\nThe following items used in the examples of this procedure should be replaced with actual values when this procedure is performed:\n username is used as an example for the SNMP user password1 is used as an example for the SNMP authentication password password2 is used as an example for the SNMP privacy password  Prerequisites Ability to connect to the switch using a serial terminal or network.\nLimitations If a new switch has been added to this system, contact the customer account representative for this site to help with full switch configuration because this procedure covers only the portion related to REDS.\nProcedure   Connect to the switch.\nThe IP address in the following example corresponds to the default IP address for the switch stack that is connected to the management networks (NMN, HSN, and ClusterStor).\nncn-m001# ssh admin@sw-leaf01 admin@sw-leaf01\u0026#39;s password: sw-smn1#   Enter configuration mode.\nsw-smn1# configure   Add a group named cray-reds-group.\nsw-smn1(conf)# snmp-server group cray-reds-group 3 noauth read cray-reds-view   Add a user named username to cray-reds-group with the following permissions.\nsw-smn1(conf)# snmp-server user username cray-reds-group 3 auth md5 password1 priv des56 password2   Add a view named cray-reds-view.\nThe following command specifies 1.3.6.1.2 as the object ID (OID) subtree to be included in the view, which is the same subtree used for this switch in all HPE Cray EX systems.\nsw-smn1(conf)# snmp-server view cray-reds-view 1.3.6.1.2 included   Exit configuration mode.\nsw-smn1(conf)# end   Verify the configuration.\nAdditional configuration lines may be present in the output of this command, depending on other configuration present on the switch. Hash values displayed in the example, such as 6b7c0616ef0a434c518012ef9d75691c, will be different on this system.\nsw-smn1# show running-config snmp ! snmp-server group cray-reds-group 3 noauth read cray-reds-view snmp-server user username cray-reds-group 3 encrypted auth md5 6b7c0616ef0a434c518012ef9d75691c priv des56 ad2b5b9f6af4f15a93c2d3c2956f5e5e snmp-server view cray-reds-view 1.3.6.1.2 included sw-smn1#   Write configuration to permanent storage.\nsw-smn1# write memory ! sw-smn1#   Exit switch configuration console.\nIn the following example, xxx.xxx.xxx.xx is the IP address of the client (ncn-m001 in the example) on the switch.\nsw-smn1# quit Session terminated for user admin on line vty 0 ( xxx.xxx.xxx.xx ) Connection to 10.4.255.254 closed. ncn-m001#   There may be another action to perform after switch configuration is complete. Please consult the use cases in River Endpoint Discovery Service (REDS).\n"
},
{
	"uri": "/docs-csm/en-11/operations/river_endpoint_discovery_service/initialize_and_geolocate_nodes/",
	"title": "Initialize And Geolocate Nodes",
	"tags": [],
	"description": "",
	"content": "Initialize and Geolocate Nodes This procedure initializes and geolocates compute nodes. Initialization provides base configuration for system hardware. Geolocation adds compute nodes to the Hardware State Manager (HSM) service.\nThis procedure is performed automatically during installation of the system\u0026rsquo;s software. Perform this procedure manually as part of cold-starting a system, which must be done whenever there is a power outage or when new compute nodes are added to the system.\nPrerequisites   The Cray command line interface (CLI) tool is initialized and configured on the system.\n  Management network switches have been configured for the River Endpoint Discovery Service (REDS). See Configure a Management Switch for REDS.\n  Check the REDS pod\u0026rsquo;s state by executing the following command to ensure that it is in the Running state.\nncn-m001# kubectl get pods -n services | grep reds cray-reds-66d99d895c-2cpk9 2/2 Running 0 48m If it is not running, see Scenario 1 of Troubleshoot Common REDS Issues.\n  Check to see if the pod for MEDS is in a Running state.\nncn-m001# kubectl get pods -n services | grep meds cray-meds-8b76c7566-bhtml 2/2 Running 0 12d   Procedure   View the Redfish endpoints inventory to determine whether any endpoints are already present.\nncn-m001# cray hsm inventory redfishEndpoints list If the endpoint for the node to be discovered is already present, skip the rest of this procedure. Otherwise, proceed to the next step.\n  Power on all nodes that need to be initialized and geolocated.\nThis can be done by pressing the power button on the machine or by using the BMC console interface (if BMC credentials are known). If the node is already powered on, power it off for a minimum of two minutes before powering it back on.\n  Wait about five minutes for all nodes to power on.\n  View the Redfish endpoints inventory again to verify that new endpoints have been added.\nncn-m001# cray hsm inventory redfishEndpoints list   The power-cycled compute nodes have been initialized and geolocated. The geolocation process powers nodes off when discovery is complete. Nodes that do not power down have had discovery issues.\nIf geolocation has failed for one or more compute nodes, see Troubleshoot Common REDS Issues.\n"
},
{
	"uri": "/docs-csm/en-11/operations/river_endpoint_discovery_service/river_endpoint_discovery_service/",
	"title": "River Endpoint Discovery Service (REDS)",
	"tags": [],
	"description": "",
	"content": "River Endpoint Discovery Service (REDS) The River Endpoint Discovery Service (REDS) performs geolocation and initialization of compute nodes, based on a mapping file that is provided with each system. In geolocation, names are assigned to compute nodes based on their physical location in the system, and the nodes are added to the Hardware State Manager (HSM). In initialization, hardware is assigned the required base configuration.\nREDS reads its configuration from the System Layout Service (SLS). Systems ship with management switches that are preconfigured, and compute nodes are initialized and geolocated as a part of the installation process. The procedures in this section are provided to customers for the following use cases.\nREDS Use Cases  Use Case: Switch replaced (for example, if a switch fails).\nEntry Point: Configure a Management Switch for REDS\n Use Case:\n A node has been removed/decommissioned. A node that is new to the system needs to be discovered. A node that was previously configured by the system needs to be discovered. The system wiring changed.  Entry Point:\n Update the SLS configuration Initialize and Geolocate Nodes   Use Case: Cold Start.\nEntry Point:\n If a switch is replaced, refer to Configure a Management Switch for REDS. Power up switches and non-compute nodes (NCN). If any nodes are added, or the wiring is changed, check the SLS configuration. Initialize and Geolocate Nodes.   Use Case: Geolocation has failed for one or more compute nodes.\nEntry Point: Troubleshoot Common REDS Issues\n"
},
{
	"uri": "/docs-csm/en-11/operations/resiliency/recreate_statefulset_pods_on_another_node/",
	"title": "Recreate Statefulset Pods On Another Node",
	"tags": [],
	"description": "",
	"content": "Recreate StatefulSet Pods on Another Node Some pods are members of StatefulSets, meaning that there is a very specific number of them, each likely running on a different node. Similar to DaemonSets, these pods will never be recreated on another node as long as they are sitting in a Terminating state.\nWarning: This procedure should only be done for pods that are known to no longer be running. Corruption may occur if the worker node is still running when deleting the deleting the StatefulSet pod in a Terminating state.\nThe design of StatefulSet pods prohibits the creation of new pods on another node. This is the expected behavior for StatefulSet pods. In many cases, the StatefulSet is a set of only one pod. If a StatefulSet pod needs to be recreated on another node, the Terminating pod needs to be force deleted and then it will automatically recreate.\nThis procedure prevents services from being taken out of service when a node goes down.\nPrerequisites  A StatefulSet pod failed to be moved to another healthy non-compute node (NCN) acting as a worker node. If the network is being brought down temporarily during testing, ignore any issues with StatefulSet pods until the testing is complete. These errors should be ignored because the nodes could have the network restored, and then the Terminating pod may go ACTIVE temporarily (causing a race and corruption) if it comes up at the same time as another StatefulSet running the pod. This is much more likely to occur during testing of a network outage.  Procedure   View the StatefulSet pods on the system.\nAny of the pods with x/1 in the READY column could be on a node that goes down, at which point that service will no longer be available.\nncn-w001# kubectl get statefulsets -A NAMESPACE NAME READY AGE backups benji-k8s-postgresql 1/1 2d14h nexus nexus 1/1 2d14h services cray-dhcp-kea-postgres 3/3 2d14h services cray-hms-badger-postgres 3/3 2d14h services cray-keycloak 3/3 2d14h services cray-rm-pals-postgres 3/3 2d14h services cray-shared-kafka-kafka 3/3 2d14h services cray-shared-kafka-zookeeper 3/3 2d14h services cray-sls-postgres 3/3 2d14h services cray-smd-postgres 3/3 2d14h services gitea-vcs-postgres 1/1 2d14h services keycloak-postgres 3/3 2d14h services slingshot-controllers-kafka 3/3 2d14h services slingshot-controllers-zookeeper 3/3 2d14h sma cluster-kafka 3/3 2d14h sma cluster-zookeeper 3/3 2d14h sma elasticsearch-master 3/3 2d14h sma mysql 1/1 2d14h sma sma-ldms-aggr-compute 1/1 2d14h sma sma-ldms-aggr-ncn 1/1 2d14h sma sma-monasca-agent 1/1 2d14h sma sma-monasca-mysql 1/1 2d14h sma sma-monasca-notification 1/1 2d14h sma sma-monasca-zoo-entrance 1/1 2d14h sma sma-postgres-cluster 2/2 96s sysmgmt-health alertmanager-cray-sysmgmt-health-promet-alertmanager 1/1 2d14h sysmgmt-health prometheus-cray-sysmgmt-health-promet-prometheus 1/1 2d14h vault cray-vault 3/3 2d14h   Describe a service to find the StatefulSet pod name.\nThe StatefulSet pod will have a name in the StatefulSet-0 format. Vault is the service being described in the example below. The StatefulSet pod name is cray-vault-0.\nncn-w001# kubectl get pods -A -o wide | grep SERVICE_NAME operators cray-vault-operator-57dbbb7db5-9lr6z 1/1 Running 0 5d18h 10.40.0.11 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; services cray-meds-vault-loader-bzgbd 0/2 Completed 0 5d18h 10.40.0.24 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; services cray-reds-vault-loader-d9cn9 0/2 Completed 0 5d18h 10.40.0.25 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; vault cray-vault-0 4/4 Terminating 7 5d18h 10.42.0.21 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; vault cray-vault-configurer-68754d5b69-knrsc 2/2 Running 0 92m 10.40.0.103 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; vault cray-vault-configurer-68754d5b69-mwmlr 2/2 Terminating 0 5d18h 10.42.0.22 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; vault cray-vault-etcd-4b2t84tp79 1/1 Terminating 0 5d18h 10.42.0.27 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; vault cray-vault-etcd-699ncq8s9c 1/1 Running 0 5d18h 10.40.0.17 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; vault cray-vault-etcd-hvmtrwjpsw 1/1 Running 0 92m 10.47.0.110 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; vault cray-vault-etcd-mx8qc596t9 1/1 Running 0 5d18h 10.47.0.23 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   Delete the StatefulSet pod in a Terminating state.\nThe StatefulSet (the controller) will recreate the pod on a working node when the pod or the node it sits on is deleted. The command below assumes the pod is located on the downed node and is currently in a Terminating state.\nncn-w001# kubectl delete pod -n NAMESPACE POD_NAME --force --grace-period 0 For example:\nncn-w001# kubectl delete pod -n vault cray-vault-0 --force --grace-period 0 The StatefulSet will then recreate cray-vault-0 on a node that is in Ready state.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/resiliency/resilience_of_system_management_services/",
	"title": "Resilience Of System Management Services",
	"tags": [],
	"description": "",
	"content": "Resilience of System Management Services HPE Cray EX systems are designed so that system management services (SMS) are fully resilient and that there is no single point of failure. The design of the system allows for resiliency in the following ways:\n Three non-compute nodes (NCNs) are configured as Kubernetes master nodes. When one master goes down, operations (such as jobs running across compute nodes) are expected to continue. At least three utility storage nodes provide persistent storage for the services running on the Kubernetes management nodes. When one of the utility storage nodes goes down, operations (such as jobs running across compute nodes) are expected to continue. At least three NCNs are configured as Kubernetes worker nodes. If one of only three Kubernetes worker nodes were to go down, it would be much more difficult for the remaining two NCN worker nodes to handle the total balance of pods. It is less significant to lose one of the NCN worker nodes if the system has more than three NCN worker nodes because there are more worker nodes able to handle the pod load. The state and configuration of the Kubernetes cluster are stored in an etcd cluster distributed across the Kubernetes master nodes. This cluster is also backed up on an interval, and backups are pushed to Ceph Rados Gateway (S3). A micro-service can run on any node that meets the requirements for that micro-service, such as appropriate hardware attributes, which are indicated by labels and taints. All micro-services have shared persistent storage so that they can be restarted on any NCN in the Kubernetes management cluster without losing state.  Kubernetes is designed to ensure that the wanted number of deployments of a micro-service are always running on one or more worker nodes. In addition, it ensures that if one worker node becomes unresponsive, the micro-services that were running on it are migrated to another NCN that is up and meets the requirements of those micro-services.\nSee Restore System Functionality if a Kubernetes Worker Node is Down for more information.\nResiliency Improvements To increase the overall resiliency of system management services and software within the system, the following improvements were made:\n Capsules services have implemented replicas for added resiliency. Added support for new storage class that supports Read-Write-Many and in doing so, eliminated some of the errors we encountered on pods which could not seamlessly start up on other worker NCNs upon termination (because of a PVC unmount error). Modified procedures for reloading DVS on NCNs to reduce DVS service interruptions. DVS now retries DNS queries in dvs_generate_map, which improves boot resiliency at scale. Additional retries implemented in the BOA, IMS, and CFS services for increased protection around outages of dependent services. Image-based installs emphasizing \u0026ldquo;non-special\u0026rdquo; node types eliminated single points of failure previously encountered with DNS, administrative and installation tooling, and gathering of Cray System Management (CSM) logging for Ceph and Kubernetes.  Expected Resiliency Behavior In addition, the following general criteria describe the expected behavior of the system if a single Kubernetes node (master, worker, or storage) goes down temporarily:\n  Once a job has been launched and is executing on the compute plane, it is expected that it will continue to run without interruption during planned or unplanned outages characterized by the loss of an NCN master, worker, or storage node. Applications launched through PALS may show error messages and lost output if a worker node goes down during application runtime.\n  If an NCN worker node goes down, it will take between 4 and 5 minutes before most of the pods which had been running on the downed NCN will begin terminating. This is a predefined Kubernetes behavior, not something inherent to HPE Cray EX.\n  Within around 20 minutes or less, it should be possible to launch a job using a UAI or UAN after planned or unplanned outages characterized by the loss of an NCN master, worker, or storage node.\n In the case of a UAN, the recovery time is expected to be quicker. However, launching a UAI after an NCN outage means that some UAI pods may need to relocate to other NCN worker nodes. The status of those new UAI pods will remain unready until all necessary content has been loaded on the new NCN that the UAI is starting up on. This process can take approximately 10 minutes.    Within around 20 minutes or less, it should be possible to boot and configure compute nodes after planned or unplanned outages characterized by the loss of an NCN master, worker, or storage node.\n  At least three utility storage nodes provide persistent storage for the services running on the Kubernetes management nodes. When one of the utility storage nodes goes down, critical operations such as job launch, app run, or compute boot are expected to continue to work.\n  Not all pods running on a downed NCN worker node are expected to migrate to a remaining NCN worker node. There are some pods which are configured with anti-affinity such that if the pod exists on another NCN worker node, it will not start another of those pods on that same NCN worker node. At this time, this mostly only applies to etcd clusters running in the cluster. It is optimal to have those pods balanced across the NCN worker nodes (and not have multiple etcd pods, from the same etcd cluster, running on the same NCN worker node). Thus, when an NCN worker node goes down, the etcd pods running on it will remain in terminated state and will not attempt to relocate to another NCN worker node. This should be fine as there should be at least two other etcd pods (from the cluster of 3) running on other NCN worker nodes. Additionally, any pods that are part of a stateful set will not migrate off a worker node when it goes down. Those are expected to stay on the node and also remain in the terminated state until the NCN worker nodes comes back up or unless deliberate action is taken to force that pod off the NCN worker node which is down.\n The cps-cm-pm pods are part of a daemonset and they only run on designated nodes. When the node comes back up the containers will be restarted and service restored. Refer to \u0026ldquo;Content Projection Service (CPS)\u0026rdquo; in the Cray Operating System (COS) product stream documentation for more information on changing node assignments.    After an NCN worker, storage, or master node goes down, if there are issues with launching a UAI session or booting compute nodes, that does not necessarily mean that the problem is due to a worker node being down. If possible, it is advised to also check the relevant \u0026ldquo;Compute Node Boot Troubleshooting Information\u0026rdquo; and User Access Service (specifically with respect to Troubleshoot UAS Issues) procedures. Those sections can give guidance around general known issues and how to troubleshoot them. For any customer support ticket opened on these issues, however, it would be an important piece of data to include in that ticket if the issue was encountered while one or more of the NCNs were down.\n  Known Issues and Workarounds Though an effort was made to increase the number of pod replicas for services that were critical to system operations such as booting computes, launching jobs, and running applications across the compute plane, there are still some services that remain with single copies of their pods. In general, this does not result in a critical issue if these singleton pods are on an NCN worker node that goes down. Most micro-services should (after being terminated by Kubernetes), simply be rescheduled onto a remaining NCN worker node. That assumes that the remaining NCN worker nodes have sufficient resources available and meet the hardware/network requirements of the pods.\nHowever, it is important to note that some pods, when running on a worker NCN that goes down, may require some manual intervention to be rescheduled. Note the workarounds in this section for such pods. Work is on-going to correct these issues in a future release.\n  Nexus pod\n  The nexus pod is a single pod deployment and serves as our image repository. If it is on an NCN worker node that goes down, it will attempt to start up on another NCN worker node. However, it is likely that it can also encounter the \u0026ldquo;Multi-Attach error for volume\u0026rdquo; error that can be seen in the kubectl describe output for the pod that is trying to come up on the new node.\n  To determine if this is happening, run the following:\n# kubectl get pods -n nexus | grep nexus   Describe the pod obtained in the previous bullet:\n# kubectl describe pod -n nexus NEXUS_FULL_POD_NAME   If the event data at the bottom of the describe command output indicates that a Multi-Attach PVC error has occurred. See the Troubleshoot Pods Failing to Restart on Other Worker Nodes procedure to unmount the PVC. This will allow the Nexus pod to begin successfully running on the new NCN worker node.\n    High-speed network resiliency after ncn-w001 goes down\n  The slingshot-fabric-manager pod running on one of NCNs does not rely on ncn-w001. If ncn-w001 goes down, the slingshot-fabric-manager pods should not be impacted as the pod is runs on other NCNs, such as ncn-w002.\n  The slingshot-fabric-manager pod relies on Kubernetes to launch the new pod on another NCN if the slingshot-fabric-manager pod is running on ncn-w001 when it is brought down.\nUse the following command and check the NODE column to check which NCN the pod is running on:\n# kubectl get pod -n services -o wide | awk \u0026#39;NR == 1 || /slingshot-fabric-manager/\u0026#39;   When the slingshot-fabric-manager pod goes down, the switches will continue to run. Even if the status of the switches changes, those changes will be picked up after slingshot-fabric-manager pod is brought back up and the sweeping process restarts.\n  The slingshot-fabric-manager relies on data in persistent storage. The data is persistent across upgrades but when the pods are deleted, the data is also deleted.\n    Future Resiliency Improvements In a future release, strides will be made to further improve the resiliency of the system. These improvements may include one or more of the following:\n Further emphasis on eliminating singleton system management pods. Reduce time delays for individual service responsiveness after its pods are terminated because of it running on a worker node that has gone down. Rebalancing of pods/workloads after an NCN worker node that was down, comes back up. Analysis/improvements wrt outages of the Node Management Network (NMN) and the impact to critical system management services. Expanded analysis/improvements of resiliency of noncritical services (those that are not directly related to job launch, application run, or compute boot).  "
},
{
	"uri": "/docs-csm/en-11/operations/resiliency/resiliency/",
	"title": "Resiliency",
	"tags": [],
	"description": "",
	"content": "Resiliency HPE Cray EX systems are designed so that system management services (SMS) are fully resilient and that there is no single point of failure.\n"
},
{
	"uri": "/docs-csm/en-11/operations/resiliency/resiliency_testing_procedure/",
	"title": "Resiliency Testing Procedure",
	"tags": [],
	"description": "",
	"content": "Resiliency Testing Procedure This document and the procedure contained within it is for the purposes of communicating the kind of testing done by the internal Cray System Management team to ensure a basic level of system resiliency in the event of the loss of a single NCN. It is assumed that some procedures are already known by admins and thus does not go into great detail or attempt to encompass every command necessary for execution. It is intended to be higher level guidance (with some command examples) to inform internal users and customers about our process.\nHigh Level Procedure Summary:  Preparation for Resiliency Testing Establish System Health Before Beginning Monitoring for Changes Launch a Non-Interactive Batch Job Shut Down an NCN Conduct Testing Power On the Downed NCN Execute Post-Boot Health Checks  Preparation for Resiliency Testing  xname mapping for each node on the system - this get dumped out by execution of the /opt/cray/platform-utils/ncnGetXnames.sh script. also note that metal.no-wipe=1 is set for each of the NCNs - via ouptput from running the ncnGetXnames.sh script ensure that you can run as an authorized user on the Cray CLI - login as a user account you have access to ncn# export CRAY_CONFIG_DIR=$(mktemp -d); echo $CRAY_CONFIG_DIR; cray init --configuration default --hostname https://api-gw-service-nmn.local /tmp/tmp.ShkBrUfhsJ Username: username Password: Success! Then validate the authorization by executing the \u0026lsquo;cray uas list\u0026rsquo; command, for example. For more information see the Validate UAI Creation section of Validate CSM Health.\n Verify that kubectl get nodes reports all Master and worker nodes are Ready. ncn# kubectl get nodes -o wide  Get a current list of pods that have a status of anything other than Running or Completed. Investigate any of concern. Save the list of pods for comparison once resiliency testing is completed and the system has been restored. ncn# kubectl get pods -o wide -A | grep -Ev \u0026#39;Running|Completed\u0026#39;  Note which pods are running on an NCN that will be taken down (as well as the total number of pods running). Here is an example that shows the listing of pods running on ncn-w001: ncn# kubectl get pods -o wide -A | grep ncn-w001 | awk \u0026#39;{print $2}\u0026#39; Note that the above would only apply to kubernetes nodes like master and worker nodes (ncn-m00x and ncn-w00x)\n Verify ipmitool can report power status for the NCN node to be shutdown ncn# ipmitool -I lanplus -U root -P \u0026lt;password\u0026gt; -H \u0026lt;ncn-node-name\u0026gt; chassis power status If ncn-m001 is the node to be brought down, note that it has the external connection so it will be important to establish that ipmitool commands will be able to be run from a node external to the system, in order to get the ipmitool power staus of ncn-m001.\n If ncn-m001 is the node to take down, establish CAN links to bypass ncn-m001 (since it will be down) in order to enable an external connection to one of the other master ncn nodes before, during, and after ncn-m001 is brought down. Verify BOS templates and create a new one(s) if needed (to be set-up for booting a specific compute node(s) after the targeted NCN node has been shutdown Ahead of shutting down the NCN and beginning resiliency testing, we want to verify that compute nodes identified for reboot validation can be successfully rebooted and configured. To see a listing of bos templates that exist on the system, run: ncn# cray bos v1 sessiontemplate list For more information regarding management of bos session templates, refer to Manage a Session Template.\n If a UAN is present on the system, log onto it and verify that the WLM (work load manager) is configured by running a command (example for slurm): uan# srun -N 4 hostname | sort   Establish System Health Before Beginning In order to ensure that the system is healthy before taking an NCN node down, run the Platform Health Checks section of Validate CSM Health.\nIf health issues are noted, it is best to address those before proceeding with the resiliency testing procedure. If it is believed (in the case of an internal Cray-HPE testing environment) that the issue is known/understood and will not impact the testing to be performed, then those health issues just need to be noted (so that it does not appear that they were caused by inducing the fault, in this case, powering off the NCN). There is an optional section of the platform health validation that deals with using the System Management monitoring tools to survey system health. If that optional validation is included, please note that the prometheus alert manager may show various alerts that would not prevent or block moving forward with this testing. For more information about prometheus alerts (and some that can be safely ignored), reference Troubleshooting Prometheus Alerts.\nPart of the data being returned via execution of the Platform Health Checks includes patronictl info for each postgres cluster. Each of the postgres clusters has a leader pod, and in the case of a resiliency test that involves bringing an NCN worker node down, it may be useful to take note of the postgres clusters that have their leader pods running on the NCN worker targeted for shutdown. The postgres-operator should handle re-establishment of a leader on another pod running in the cluster, but it is worth taking note of where leader re-elections are expected to occur so special attention can be given to those postgres clusters. The postgres health check is included in Validate CSM Health, but the script for dumping postgres data can be run at any time:\nncn# /opt/cray/platform-utils/ncnPostgresHealthChecks.sh Monitoring for Changes In order to keep watch on various items during and after the fault has been introduced (in this case, the shutdown of a single NCN node), the steps listed below can help give insight into changing health conditions. It is an eventual goal to strive for a monitoring dashboard which would help track these sort of things in a single or few automated views. Until that can be incorporated, these kinds of command prompt sessions can be useful.\n  It is important to ensure that critical services can ride through a fault, and as such, it is recommended to set up a \u0026lsquo;watch\u0026rsquo; command which will repeatedly run via the Cray CLI (that will hit the service API) and note that there is not more than a window of 5-10 minutes where a service that we are polling would, intermittently, fail to respond. In the examples below, the CLI commands are checking the bos and cps APIs. It may be desired to choose additional Cray CLI commands to run in this manner. The ultimate proof of system resiliency lies in the ability to perform system level use cases and to, further, prove that can be done at scale. If there are errors being returned, consistently (and without recovery), with respect to these commands, it is likely that business critical use cases (that utilize the same APIs) will also fail.\nIt may be useful to reference instructions for Configuring the Cray CLI.\nncn# watch -n 5 \u0026#34;date; cray cps contents\u0026#34; ncn# watch -n 5 \u0026#34;date; cray bos v1 session list\u0026#34;   Monitor ceph health, in a window, during the and after a single NCN node is taken down:\nncn# watch -n 5 \u0026#34;date; ceph -s\u0026#34;   The following commands (run in separate windows) can also help identify when pods (on a downed master or worker NCN) are no longer responding. This takes around 5 -6 minutes, and kubernetes will begin terminating pods so that new pods to replace them can start-up on another NCN. Pods that had been running on the downed NCN will remain in Terminated state until the NCN is back up. Pods that need to start-up on other nodes will be Pending until they start-up. Some pods that have anti-affinity configurations or that run as daemonsets will not be able to start up on another NCN. Those pods will remain in Pending state until the NCN is back up. Finally, it is helpful to have a window tracking the list of pods that are not in Completed or Running state to be able to determine how that list is changing once the NCN is downed and pods begin shifting around. This step offers a view of what is going on at the time that the NCN is brought down and once kubernetes detects an issue and begins remediation. It is not so important to capture everything that is happening during this step. It may be helpful for debugging. The output of these windows/commands becomes more interesting once you have brought the NCN down for a period of time and then bring it back up. At that point, the expectation is that everything can recover.\nncn# watch -n 5 \u0026#34;date; kubectl get pods -o wide -A | grep Termin\u0026#34; ncn# watch -n 10 \u0026#34;date; kubectl get pods -o wide -A | grep Pending\u0026#34; ncn# watch -n 5 \u0026#34;date; kubectl get pods -o wide -A | grep -v Completed | grep -v Running\u0026#34;   This command run in a window can also help detect the change in state of the various postgres instances running. Should there be a case of postgres status that deviates from Running, that would require further investigation and possibly remediation via Troubleshooting the Postgres Database.\nncn# watch -n 30 \u0026#34;date; kubectl get postgresql -A\u0026#34;   Launch a Non-Interactive Batch Job The purpose of this procedure is to launch a non-interactive, long-running batch job across computes via a UAI (or the UAN, if present) in order to ensure that even though the UAI pod used to launch the job is running on the NCN worker node being taken down, it will start up on another NCN worker (once kubernetes begins terminating pods). Additionally, it is important to verify that the batch job continued to run, uninterrupted through that process. If the target NCN for shutdown is not a worker node (where a uai would be running), then there is no need to pay attention to the steps, below, that discuss ensuring the uai can only be created on the NCN worker node that is targeted for shutdown. If executing a shut down of a master or storage NCN, the procedure can begin at creating a uai. It is still good to ensure that non-interactive batch jobs are uninterrupted, even with the uai they are launched from is not being disrupted.\nLaunching on a UAI  Create a UAI. If target node for shutdown is a worker NCN, force the UAI to be created on target worker NCN. The following steps will create labels to ensure that uai pods will only be able to start-up on the target worker NCN - in this example ncn-w002 is the target node for shutdown (on a system with only 3 NCN worker nodes). ncn# kubectl label node ncn-w001 uas=False --overwrite node/ncn-w001 labeled ncn# kubectl label node ncn-w003 uas=False --overwrite node/ncn-w003 labeled ncn# kubectl get nodes -l uas=False NAME STATUS ROLES AGE VERSION ncn-w001 Ready \u0026lt;none\u0026gt; 4d19h v1.18.2 ncn-w003 Ready \u0026lt;none\u0026gt; 4d19h v1.18.2 It is EXTREMELY IMPORTANT to note that after a uai has been created, that we clear out these labels or else when kubernetes terminates the running uai on the NCN being shut down, it will not be able to reschedule the uai on another pod. To create a uai, see the Validate UAI Creation section of Validate CSM Health and/or Create a UAI. To remove the labels set above after the uai has been created, run the following:\nncn# kubectl label node ncn-w001 uas- ncn# kubectl label node ncn-w003 uas-  Within the created uai, verify that WLM is configured with the appropriate workload manager. Verify the connection string of the created uai: ncn# jolt1-ncn-w001:~ # cray uas list [[results]] username = \u0026#34;uastest\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; uai_connect_string = \u0026#34;ssh uastest@172.30.48.49 -p 31137 -i ~/.ssh/id_rsa\u0026#34; uai_img = \u0026#34;bis.local:5000/cray/cray-uas-sles15sp1-slurm:latest\u0026#34; uai_age = \u0026#34;1m\u0026#34; uai_name = \u0026#34;uai-uastest-5653e9b9\u0026#34; Login to the created uai (example):\nncn# ssh uastest@172.30.48.49 -p 31137 -i ~/.ssh/id_rsa To verify the configuration of slurm, for example, within the uai:\nuastest@uai-uastest-5653e9b9:/lus/uastest\u0026gt; srun -N 4 hostname | sort nid000001 nid000002 nid000003 nid000004  Copy an MPI application source and WLM (workload manager) batch job files to the uai Within UAI compile an MPI application. Launch application as batch job (not interactive) on compute node(s) that have not been designated, already, for reboots once an NCN is shut down.  Verify that batch job is running and that application output is streaming to a file. Streaming output will be used to verify that the batch job is still running during resiliency testing. A batch job, when submitted, will designate a log file location. This log file can be accessed to be able to verify that the batch job is continuing to run after an NCN is brought down and once it is back online. Additionally, the squeue command can be used to verify that the job continues to run (for slurm). When all testing has been completed, a uai session can be deleted with:  ncn# cray uas delete --uai-list uai-uastest-5653e9b9   Launching on a UAN  Login to the UAN and verify that a WLM (workload manager) has been properly configured (in this case, slurm will be used) uan01# srun -N 4 hostname | sort nid000001 nid000002 nid000003 nid000004  Copy an MPI application source and WLM batch job files to UAN Within the UAN, compile an MPI application. Launch the application as interactive on compute node(s)that have not been designated, already, for either reboots (once an NCN is shut down) or that are not already running an MPI job via a uai. Verify that the job launched on the UAN is running and that application output is streaming to a file. Streaming output will be used to verify that the batch job is still running during resiliency testing. A batch job, when submitted, will designate a log file location. This log file can be accessed to be able to verify that the batch job is continuing to run after an NCN is brought down and once it is back online. Additionally, the squeue command can be used to verify that the job continues to run (for slurm).  Shut Down an NCN  Establish a console session to the NCN targeted for shutdown by executing the steps in Establish a Serial Connection to NCNs. Log onto target node and execute /sbin/shutdown -h 0  Note in target node\u0026rsquo;s console output the timestamp of the power off Once the target node is reported as being powered off, verify that the node\u0026rsquo;s power status with the ipmitool is reported as off  ncn# ipmitool -I lanplus -U root -P \u0026lt;password\u0026gt; -H \u0026lt;ncn-node-name\u0026gt; chassis power status  Note that at times in the past, an ipmitool command has been used to simply yank the power to an NCN. There have been times where this resulted in a longer recovery procedure under Shasta 1.5 (mostly due to issues with getting nodes physically booted up again), so the preference has been to simply use the shutdown command.   If the NCN shutdown is a master or worker node, within 5-6 minutes of the node being shut down, kubernetes will begin reporting \u0026ldquo;Terminating\u0026rdquo; pods on the target node and start rescheduling pods to other NCN nodes. New pending pods will be created for pods that can not be relocated off of the NCN shut down. Pods reported as \u0026ldquo;Terminating\u0026rdquo; will remain in that state until the NCN node has been powered back up. Take note of changes in the data being reported out of the many monitoring windows that were set-up in a previous step.  Conduct Testing  After the target NCN was shut down, assuming the command line windows that were set-up for ensuring API responsiveness are not encountering persistent failures, the next step will be to use a bos template to boot a pre-designated set of compute nodes. The timing of this test is recommended to be around 10 minutes after the NCN has gone down. That should give ample time for kubernetes to have terminated pods on the downed node (in the case of a master or worker NCN) and for them to have been rescheduled and in a healthy state on another NCN. Going too much earlier than 10 minutes runs the risk that there are still some critical pods that are settling out to reach a healthy state. ncn# cray bos v1 session create --template-uuid boot-nids-1-4 --operation reboot Issuing this reboot command will spit out a boa \u0026ldquo;jobId\u0026rdquo;, which can be used to find the new boa pod that has been created for the boot. And then the logs can be tailed to watch the compute boot proceed. The command kubectl get pods -o wide -A | grep \u0026lt;boa-job-id\u0026gt; can be used to find the boa job pod name. Then, the command kubectl logs -n services \u0026lt;boa-job-pod-name\u0026gt; -c boa -f can be used to watch the progress of the reboot of the compute(s). Failures or a timeout being reached in either the boot or cfs (post-boot configuration) phase will need investigation. For more information around accessing logs for the bos operations, see Check the Progress of BOS Session Operations.\n If the target node for shutdown was a worker NCN, verify that the uai launched on that node still exists. It should be running on another worker NCN.  Any prior ssh session established with the uai while it was running on the downed NCN worker node will be unresponsive. A new ssh session will need to be established once the uai pods has been successfully relocated to another worker NCN. Log back into the uai and verify that the WLM (workload manager) batch job is still running and streaming output. The log file created with the kick-off of the batch job should still be accessible and the squeue command can be used to verify that the job continues to run (for slurm).   If the workload manager batch job was launched on a UAN, log back into it and verify that the WLM (workload manager) batch job is still running and streaming output via the log file created with the batch job and/or the squeue command (if slurm is used as the WLM). Verify that new WLM jobs can be started on a compute node after the NCN is down (either via a uai or the UAN node). Look at any pods that, are at this point, in a state other than Running, Completed, Pending, or Terminating: ncn# kubectl get pods -o wide -A | grep -Ev \u0026#34;Running|Completed|Pending|Termin\u0026#34;  Compare what comes up in this list to the pod list that you collected before. If there are new pods that are in status ImagePullBackOff or CrashLoopBackOff, a kubectl describe as well as kubectl logs command should be run against them to collect additional data about what happened. Obviously, if there were pods in a bad state before the procedure started, then it should not be expected that bringing one of the NCNs down is going to fix that. Ignore anything that was already in a bad state before (that was deemed to be ok). It is also worth taking note of any pods in a bad state at this stage as this should be checked again after bringing the NCN back up - to see if those pods remain in a bad state or if they are cleared. Noting behaviors, collecting logs, and opening tickets throughout this process is recommended when behavior occurs that is not expected. When we see an issue that has not been encountered before, it may not be immediately clear if code changes/regressions are at fault or if it is simply an intermittent/timing kind of issue that has not previously surfaced. The recommendation at that point, given time/resources is to repeat the test to gain a sense of the repeatability of the behavior (in the case that the issue is not directly tied to a code-change). Additionally, it is as important to understand (and document) any work-around procedures needed to fix issues encountered. In addition to filing a bug for a permanent fix, work-around documentation can be very useful when written up - for both internal and external customers to access.    Power On the Downed NCN  Use the ipmitool command to power up the NCN. It will take several minutes for the NCN to reboot. Progress can be monitored over the connected serial console session. Wait to begin execution of the next steps until after it can be determined that the NCN has booted up and is back at the login prompt (when viewing the serial console log). ncn# ipmitool -I lanplus -U root -P \u0026lt;password\u0026gt; -H \u0026lt;hostname\u0026gt; chassis power on #example hostname is ncn-w003-mgmt  If the NCN being powered on is a master or worker, verify that \u0026ldquo;Terminating\u0026rdquo; pods on that NCN clear up. It may take several minutes. Watch the command prompt, previously set-up, that is displaying the Terminating pod list. If the NCN being powered on is a storage node, wait for Ceph to recover and again report a \u0026ldquo;HEALTH_OK\u0026rdquo; status. It may take several minutes for Ceph to resolve clock skew. This can be noted in the previously set-u window to watch ceph status. Finally, check that pod statuses have returned to the state that they were in at the beginning of this procedure, paying particular attention to any pods that were previously noted to be in a bad state while the NCN was down. Additionally, there is no concern if pods that were in a bad state at the beginning of the procedure, are still in a bad state. What is important to note is anything that is different from either the beginning of the test or from the time that the NCN was down.  Execute Post-Boot Health Checks  Re-run the Platform Health Checks section of Validate CSM Health noting any output that indicates output is not as expected. Note that in a future version of CSM, these checks will be further automated for better efficiency and pass/fail clarity. Ensure that after a downed NCN worker node (can ignore if not a worker node) has been powered up, a new uai can be created on that NCN. It may be necessary to label the nodes again, to ensure the uai gets created on the worker node that was just powered on. Refer to the section above for Launch a Non-Interactive Batch Job for the procedure. Do not forget to remove the labels after the uai has been created. Once the uai has been created, log into it and ensure a new workload manager job can be launched. Ensure tickets have been opened for any unexpected behavior along with associated logs and notes on work-arounds, if any were executed.  "
},
{
	"uri": "/docs-csm/en-11/operations/resiliency/restore_system_functionality_if_a_kubernetes_worker_node_is_down/",
	"title": "Restore System Functionality If A Kubernetes Worker Node Is Down",
	"tags": [],
	"description": "",
	"content": "Restore System Functionality if a Kubernetes Worker Node is Down Services running on Kubernetes worker nodes can be properly restored if downtime occurs. Use this procedure to ensure that if a Kubernetes worker node is lost or restored after being down, certain features the node was providing can also be restored or recovered on another node.\nCapture the metadata for the unhealthy node before bringing down the node. The pods will successfully terminate when the node goes down, which should resolve most pods in an error state. Once any remaining testing or validation work is complete, these pods can be restored with the file used to capture the metadata.\nPrerequisites This procedure requires administrative privileges.\nCollect Information Before Powering Down the Node   Check the Persistent Volume Claims (PVC) that have been created on the system.\n  View the PVCs in all namespaces.\nncn# kubectl get pvc –A NAMESPACE NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE jhub claim-user Bound pvc-3cf34569-4db4-11ea-b8e1-a4bf01581d70 10Gi RWO ceph-rbd-external 14d jhub claim-users Bound pvc-18b7155a-4dba-11ea-bf78-a4bf01684f9e 10Gi RWO ceph-rbd-external 14d jhub claim-user01 Bound pvc-c5df3ba1-4db3-11ea-b8e1-a4bf01581d70 10Gi RWO ceph-rbd-external 14d jhub hub-db-dir Bound pvc-b41675c6-4d4e-11ea-b8e1-a4bf01581d70 1Gi RWO ceph-rbd-external 15d loftsman loftsman-chartmuseum-data-pvc Bound pvc-7d45b88b-4575-11ea-bf78-a4bf01684f9e 1Gi RWO ceph-rbd-external 25d ...   Get a list of PVCs for a particular pod.\nncn# kubectl get pod POD_NAME -o \\ jsonpath=\u0026#39;{.spec.volumes[*].persistentVolumeClaim.claimName}{\u0026#34;\\n\u0026#34;}\u0026#39;     Verify the time is synced across all NCNs.\nncn# pdsh -w ncn-s00[1-3],ncn-m00[1-3],ncn-w00[1-3] date ncn-m001: Thu Feb 27 08:41:11 CST 2020 ncn-s002: Thu Feb 27 08:41:11 CST 2020 ncn-s003: Thu Feb 27 08:41:11 CST 2020 ncn-m002: Thu Feb 27 08:41:11 CST 2020 ncn-s001: Thu Feb 27 08:41:11 CST 2020 ncn-m003: Thu Feb 27 08:41:11 CST 2020 ncn-w001: Thu Feb 27 08:41:11 CST 2020 ncn-w003: Thu Feb 27 08:41:11 CST 2020 ncn-w002: Thu Feb 27 08:41:11 CST 2020   Generate a list of pods that are running on the node that will be taken down.\nThe example below is displaying all of the pods running on ncn-w001.\nncn# kubectl get pods -A -o wide | grep NODE_NAME default cray-dhcp-7b5c6496c6-76rst 1/1 Running 0 5d14h 10.252.1.1 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; default kube-keepalived-vip-mgmt-plane-nmn-local-vxldv 1/1 Running 1 25d 10.252.1.1 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ims cray-ims-57b4f98b-bc0d-422e-8891-808ab69bf158-create-nbd5c 0/2 Init:Error 0 6d21h 10.40.1.36 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ims cray-ims-60ed661b-acf2-45fd-af44-300baabfc299-customize-skswx 0/2 Completed 0 40h 10.40.1.38 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ims cray-ims-bc748ef4-49ad-4211-92db-993d097ac80e-create-6j6xv 0/2 Completed 0 6d21h 10.40.1.40 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ims cray-ims-c31232bc-d4e0-4491-90fb-e3d2fd3ca0ce-create-gzbvk 0/2 Init:Error 0 6d21h 10.40.1.45 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ims cray-ims-ccacd186-854e-4057-acfa-192ccea16ad0-customize-6qkkg 0/2 Completed 0 46h 10.40.1.52 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; istio-system istio-pilot-9d769b86c-mzshz 2/2 Running 0 4m54s 10.40.1.38 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; istio-system istio-pilot-9d769b86c-t8mtg 2/2 Running 0 5m10s 10.40.1.51 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; istio-system istio-sidecar-injector-b887db765-td7db 1/1 Running 0 12d 10.40.0.173 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ... Take note of any pods that are not in a Running or Completed state, or have another state that is not considered to be healthy. This will help identify after the node is brought back up what new issues have occurred.\nTo view the pods in an unhealthy state:\nncn# kubectl get pods -A -o wide | grep -v -e Completed -e Running NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES backups benji-k8s-backup-backups-namespace-1594161300-gk72h 0/1 Error 0 5d20h 10.45.0.109 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; backups benji-k8s-backup-backups-namespace-1594161600-kqprj 0/1 Error 0 5d20h 10.45.0.126 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; backups benji-k8s-backup-backups-namespace-1594161900-6rqcx 0/1 Error 0 5d20h 10.45.0.125 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; loftsman helm-1594233719 0/1 Error 0 5d 10.36.0.192 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; loftsman helm-1594312289 0/1 Error 0 4d2h 10.36.0.186 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; loftsman helm-1594329768 0/1 Error 0 3d21h 10.36.0.164 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; loftsman helm-1594400189 0/1 Error 0 3d1h 10.36.0.191 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; loftsman helm-1594400232 0/1 Error 0 3d1h 10.36.0.191 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; loftsman shipper-1594216843-v5hh4 0/1 Error 0 5d4h 10.36.0.179 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; loftsman shipper-1594216923-z5clh 0/1 Error 0 5d4h 10.36.0.179 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; loftsman shipper-1594217083-w5mzk 0/1 Error 0 5d4h 10.36.0.179 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; loftsman shipper-1594312344-87555 0/1 Error 0 4d2h 10.36.0.194 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; loftsman shipper-1594317770-z8z7q 0/1 Error 0 4d 10.36.0.186 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; loftsman shipper-1594328454-khsfc 0/1 Error 0 3d21h 10.36.0.190 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; loftsman shipper-1594329426-75shp 0/1 Error 0 3d21h 10.36.0.195 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; loftsman shipper-1594329510-q8bsj 0/1 Error 0 3d21h 10.36.0.164 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; services boa-8b1687b2-2c5b-4c92-8cd7-a44965fef41a-mbfsj 0/2 Error 0 6d 10.42.0.152 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; services boa-8b1687b2-2c5b-4c92-8cd7-a44965fef41a-q7cr4 0/2 Error 0 6d 10.42.0.154 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; services boa-cf973765-92e7-4c5e-b52a-e904088976b8-cplj6 0/2 Error 0 5d23h 10.42.0.158 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; services boa-f6f86426-58bf-4c6f-b3cd-e25010aa9ff6-s7zph 0/2 Error 0 4d2h 10.36.0.191 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; services cfs-0336105c-e697-4d9d-a129-badde6da3218-vn6n4 0/3 Error 0 6d20h 10.42.0.98 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ...   View the status of the node before taking it down.\nncn# kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME ncn-m001 Ready master 27d v1.18.2 10.252.0.10 \u0026lt;none\u0026gt; SUSE Linux Enterprise Server 15 SP1 4.12.14-197.45-default containerd://1.3.3 ncn-m002 Ready master 27d v1.18.2 10.252.0.11 \u0026lt;none\u0026gt; SUSE Linux Enterprise Server 15 SP1 4.12.14-197.45-default containerd://1.3.3 ncn-m003 Ready master 27d v1.18.2 10.252.0.12 \u0026lt;none\u0026gt; SUSE Linux Enterprise Server 15 SP1 4.12.14-197.45-default containerd://1.3.3 ncn-w001 Ready \u0026lt;none\u0026gt; 27d v1.18.2 10.252.0.4 \u0026lt;none\u0026gt; SUSE Linux Enterprise Server 15 SP1 4.12.14-197.45-default containerd://1.3.3 ncn-w002 Ready \u0026lt;none\u0026gt; 27d v1.18.2 10.252.0.5 \u0026lt;none\u0026gt; SUSE Linux Enterprise Server 15 SP1 4.12.14-197.45-default containerd://1.3.3 ncn-w003 Ready \u0026lt;none\u0026gt; 27d v1.18.2 10.252.0.6 \u0026lt;none\u0026gt; SUSE Linux Enterprise Server 15 SP1 4.12.14-197.45-default containerd://1.3.3   Collect Information After Powering Down the Node   Shut down the node.\nncn# export USERNAME=root ncn# export IPMI_PASSWORD=changeme ncn# ipmitool -H BMC_IP_ADDRESS -v -I lanplus -U $USERNAME -E chassis power off   View the node status after the node is taken down.\nncn# kubectl get nodes   View the pods on the system to see if their states have changed.\n  View all of the pods on the system.\nThe following are important things to look for when viewing the pods:\n Check for any pods that are still running on the node that was brought down, and if there are still some, make sure those are expected View the status for all pods before looking for any new error states  ncn# kubectl get pods -A -o wide   Take note of any pods that are in a Pending state.\nncn# kubectl get pods -A -o wide | grep Pending   Capture the details for any pod that is in an unexpected state.\nncn# kubectl describe pod POD_NAME     Collect Information After the Node is Powered On   Power the node back on.\nncn# export USERNAME=root ncn# export IPMI_PASSWORD=changeme ncn# ipmitool -H BMC_IP_ADDRESS -v -I lanplus -U $USERNAME -E chassis power on   Record the status of the pods again.\nThe pods will go to an Unknown state as the node is coming up and taking inventory of its assigned pods. The number of Unknown states will increase as it does this inventory, and then they will decrease as it finds their actual state and puts them back to Running, Terminated, or another state.\n  View all the pods on the system.\nncn# kubectl get pods --all-namespaces -o wide   Take note of any pods that are in a Pending or Error state.\nncn# kubectl get pods -A -o wide | grep -e \u0026#39;Pending|Error\u0026#39;   Capture the details for any pod that is in an unexpected state.\nncn# kubectl describe pod POD_NAME     The node that encountered issues should now be returned to a healthy state.\n"
},
{
	"uri": "/docs-csm/en-11/operations/resiliency/ntp_resiliency/",
	"title": "NTP Resiliency",
	"tags": [],
	"description": "",
	"content": "NTP Resiliency Sync the time on all non-compute nodes (NCNs) via Network Time Protocol (NTP). Avoid a single point of failure for NTP when testing system resiliency.\nPrerequisites This procedure requires administrative privileges.\nProcedure   Set the date manually if the time on NCNs is off by more than an a few hours, days, or more.\nFor example:\nncn-m001# timedatectl set-time \u0026#34;2021-02-19 15:04:00\u0026#34;   Configure NTP on the Pre-install Toolkit (PIT).\nncn-m001# /root/bin/configure-ntp.sh   Sync NTP on all other nodes.\nIf more than nine NCNs are in use on the system, update the for loop in the following command accordingly.\nncn-m002# for i in ncn-{w,s}00{1..3} ncn-m00{2..3}; do echo \\ \u0026#34;------$i--------\u0026#34;; ssh $i \u0026#39;/srv/cray/scripts/metal/set-ntp-config.sh\u0026#39;; done ------ncn-w001-------- CURRENT TIME SETTINGS rtc: 2021-01-16 15:04:57.593224+00:00 sys: 2021-01-16 15:05:07.449052+0000 200 OK 200 OK NEW TIME SETTINGS rtc: 2021-02-19 13:02:28.811921+00:00 sys: 2021-02-19 13:02:28.924540+0000 ------ncn-w002-------- CURRENT TIME SETTINGS rtc: 2021-01-16 15:05:35.390043+00:00 sys: 2021-01-16 15:05:45.838885+0000 200 OK 200 OK NEW TIME SETTINGS rtc: 2021-02-19 13:03:06.515083+00:00 sys: 2021-02-19 13:03:07.184846+0000 ... ...   Reboot each node.\nIf manually adjust the time is not effective, reboot each node to trigger the NTP script to run on first boot from cloud-init.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/power_management/power_management/",
	"title": "Power Management",
	"tags": [],
	"description": "",
	"content": "Power Management HPE Cray System Management (CSM) software manages and controls power out-of-band through Redfish APIs. Note that power management features are \u0026ldquo;asynchronous,\u0026rdquo; in that the client must determine whether the component status has changed after a power management API call returns.\nIn-band power management features are not supported in v1.4.\nHPE supports Slurm as a workload manager which reports job energy usage and records it in the ITDB for system accounting purposes.\nsma-postgres-cluster A time-series PostgreSQL database (sma-postgres-cluster) contains the power telemetry data and tracks job start/end times, job-id, application-id, user-id, and application node allocation data. This data is made available through the System Monitoring Framework (SMF). Power monitoring and job data in sma-postgres-cluster enable out-of-band power profiling on the management nodes. Slurm interfaces with the PostgreSQL database through a plug-in.\nHPE Cray EX EX Systems Cabinet-level power/energy data from compute blades, switch blades, and chassis rectifiers is collected by each Chassis Management Module (CMM) and provided on system management network. This power telemetry can be monitored by the SMF. Cabinet-level power data is collected and forwarded to the management nodes. The management nodes store the telemetry in the power management database.\nPM Counters The blade-level and node-level accumulated energy telemetry is point-in-time power data. Blade accumulated energy data is collected out-of-band and is made available via workload managers. Users have access to the data in-band at the node-level via a special sysfs files in /sys/cray/pm_counters on the node.\nHPE Cray EX Standard Rack Systems Rack systems support 2 intelligent power distribution units (iPDUs). The power/energy telemetry, temperature, and humidity measurements (supported by optional probes), are accessible through the iPDU HTTP interface.\nNode-level accumulated energy data is point-in-time power and accumulated energy data collected via Redfish through the server BMC.\n"
},
{
	"uri": "/docs-csm/en-11/operations/power_management/system_power_off_procedures/",
	"title": "System Power Off Procedures",
	"tags": [],
	"description": "",
	"content": "System Power Off Procedures The procedures in section detail the high-level tasks required to power off an HPE Cray EX system.\n  The Cray Advanced Platform Monitoring and Control (CAPMC) service controls power to major components. CAPMC sequences the power off tasks in the correct order, but does not gracefully shut down software services.\n  The Boot Orchestration Service (BOS) manages proper shutdown and power off tasks for compute nodes and User Access Nodes (UANs).\n  The System Admin Toolkit (SAT) automates shutdown services by stage, for example:\nsat bootsys shutdown --stage platform-services sat bootsys shutdown --stage bos-operations   Prepare the System for Power Off To make sure that the system is healthy before power off and all the information is collected, Prepare the System for Power Off.\nShut Down Compute Nodes and UANs To shut down compute nodes and User Access Nodes (UANs) see Shut Down and Power Off Compute and User Access Nodes.\nSave Management Network Switch Settings To save management switch configuration settings, see Save Management Network Switch Configuration Settings.\nPower Off System Cabinets To power off standard rack and liquid-cooled cabinet PDUs, see Power Off Compute and IO Cabinets.\nShut Down the Management Kubernetes Cluster Shut down the management Kubernetes cluster, see Shut Down and Power Off the Management Kubernetes Cluster.\nPower Off the External Lustre File System Power off the external Lustre file system (ClusterStor), see Power Off the External Lustre File System.\nLockout Tagout Facility Power If facility power must be removed from a single cabinet or cabinet group for maintenance, follow proper lockout-tagout procedures for the site.\n"
},
{
	"uri": "/docs-csm/en-11/operations/power_management/system_power_on_procedures/",
	"title": "System Power On Procedures",
	"tags": [],
	"description": "",
	"content": "System Power On Procedures The procedures in section detail the high-level tasks required to power on an HPE Cray EX system.\nImportant: If an emergency power off (EPO) event occurred, see Recover from a Liquid Cooled Cabinet EPO Event for recovery procedures.\n  The Cray Advanced Platform Monitoring and Control (CAPMC) service controls power to major components. CAPMC sequences the power on tasks in the correct order, but does not determine if the required software services are running on the components.\n  The Boot Orchestration Service (BOS) manages and configures power on and boot tasks.\n  The System Admin Toolkit (SAT) automates boot and shutdown services by stage, for example:\nsat bootsys boot --stage platform-services sat bootsys boot --stage bos-operations   Get User IDs and Passwords   Obtain user ID and password for all the system management network switches, for example:\nsw-spine-001.mtl sw-spine-002.mtl sw-leaf-001.mtl sw-leaf-002.mtl sw-cdu-001.mtl sw-cdu-002.mtl   Obtain the user ID and password for the ClusterStor primary management node. For example, cls01234n000.\n  Obtain the user ID and password for all edge switches.\n  Determine the correct Boot Orchestration Service (BOS) templates that should be used to boot compute nodes and UANs, for example:\n Compute nodes: slurm UANs: uan-slurm    Power on Cabinet Circuit Breakers and PDUs Always use the cabinet power-on sequence for the customer site.\nThe management cabinet is the first part of the system that must be powered on and booted. Management network and Slingshot fabric switches power on and boot when cabinet power is applied. After cabinets are powered on, wait at least 10 minutes for systems to initialize.\n To power on all liquid-cooled cabinet CDUs and cabinet PDUs, see Power On Compute and IO Cabinets. To power on all remaining system cabinet CDUs and PDUs.  After all the system cabinets are powered on, be sure that all management network and Slingshot network switches are powered on and there are no error LEDS or hardware failures.\nPower on and Boot the Kubernetes Management Cluster To power on the management cabinet and bring up the management Kubernetes cluster, refer to Power On and Start the Management Kubernetes Cluster.\nPower on the External Lustre File System To power an external Lustre file system (ClusterStor), see Power On the External Lustre File System.\nBring up the Slingshot Fabric To bring up the Slingshot fabric, see:\n The Slingshot Administration Guide for Cray EX systems PDF. The Slingshot Troubleshooting Guide PDF.  Power On and Boot Compute Nodes and User Access Nodes (UANs) To power on and boot compute nodes and UANs, refer to Power On and Boot Compute and User Access Nodes and make nodes available to customers.\nRun System Health Checks After power on, refer to Validate CSM Health to check system health and status.\n"
},
{
	"uri": "/docs-csm/en-11/operations/power_management/user_access_to_compute_node_power_data/",
	"title": "User Access To Compute Node Power Data",
	"tags": [],
	"description": "",
	"content": "User Access to Compute Node Power Data Shasta Liquid Cooled AMD EPYC compute node power management data available to users.\nShasta Liquid Cooled compute blade power management counters (pm_counters) enable users access to energy usage over time for billing and job profiling.\nThe blade-level and node-level accumulated energy telemetry is point-in-time power data. Blade accumulated energy data is collected out-of-band and is made available via workload managers. Users have access to the data in-band at the node-level via a special sysfs files in /sys/cray/pm\\_counters on the node.\nTime-stamped energy data from each node can be captured for a specific job before, during, and after the job to generate a power profile about the job. This energy usage data can be used in conjunction with current energy costs to assign a monetary value to the job.\nThe node CPU vendor provides specific in-band and out-of-band interfaces for controlling power management. In-band interfaces are accessed from the node OS through /sys/cray/pm\\_counters. Out-of-band interfaces are accessed from a node BMC or Redfish API.\nNote that each node has a power supply that can support a fixed number of Watts. The combined power consumption of the CPU and the accelerator can never exceed this limit, thus, power to either the CPU or the accelerator must be capped so as not to exceed the total amount of power available.\npm_counters Access to compute node power and energy data is provided by a set of files located in /sys/cray/pm\\_counters/ on the node. All pm_counters are accompanied by a timestamp.\n   File Description     power Point-in-time power (Watts). When accelerators are present, includes accel_power. See limitation below on data collection from accelerators.   energy Accumulated energy, in joules. When accelerators are present, includes accel_energy. See limitation below on data collection from accelerators.   cpu_power Point-in-time power (Watts) used by the CPU domain.   cpu_energy The total energy (Joules) used by the CPU domain.   cpu_temp Temperature reading (Celsius) of the CPU domain.   memory_power Point-in-time power (Watts) used by the memory domain.   memory_energy The total energy (Joules) used by the memory domain.   accel_energy Accumulated accelerator energy (Joules). The data is non-zero only when an accelerator is present on the node.   accel_power Accelerator point-in-time power (Watts). The data is non-zero only when an accelerator is present on the node.   generation A counter that increments each time a power cap value is changed.   startup Startup counter.   freshness Free-running counter that increments at a rate of approximately 10Hz.   version Version number for power management counter support.   power_cap Current power cap limit in Watts; 0 indicates no capping. When accelerators are present, includes accel_power_cap.   raw_scan_hz The power management scanning rate for all data in pm_counters.    "
},
{
	"uri": "/docs-csm/en-11/operations/power_management/save_management_network_switch_configurations/",
	"title": "Save Management Network Switch Configuration Settings",
	"tags": [],
	"description": "",
	"content": "Save Management Network Switch Configuration Settings Switches must be powered on and operating. This procedure is optional if switch configurations have not changed.\nOptional Task: Save management spine and leaf switch configurations, and other network switch configurations before removing power from cabinets or the CDU. Management switch names are listed in the /etc/hosts file.\nProcedure   Connect to all management network Dell leaf switches and write memory configuration to startup.xml.\nThe management switches in CDU cabinets are leaf switches.\nDell leaf switches, for example sw-leaf-001:\nncn-m001# ssh admin@sw-leaf-001 admin@sw-leaf-001s password: sw-leaf-001# write memory sw-leaf-001# dir config sw-leaf-001# exit Use a for loop:\nncn-m001# for sw in sw-leaf-001 sw-leaf-002 sw-cdu-001 sw-cdu-002; \\ do ssh admin@$sw; done   Connect to all management network Mellanox spine switches and write memory configuration.\nMellanox spine switches, for example sw-spine-001.nmn:\nncn-m001# ssh admin@sw-spine-001.nmn admin@sw-spine-001 password: sw-spine-001# enable sw-spine-001# write memory sw-spine-001# exit   Connect to all management network Aruba switches and write memory configuration.\nncn-m001# ssh admin@sw-spine-001.nmn admin@sw-spine-001 password: sw-spine-001# write memory sw-spine-001# exit   Save configuration settings on link aggregation group (LAG) switches that connect customer storage networks to the Slingshot network.\nLAG switches are accessible from the ClusterStor management network.\nncn-m001# ssh admin@cls01053n00 admin@cls01053n00 password: cls01053n00# ssh r0-100gb-sw01 r0-100gb-sw01# enable r0-100gb-sw01# write memory r0-100gb-sw01# exit   "
},
{
	"uri": "/docs-csm/en-11/operations/power_management/set_the_turbo_boost_limit/",
	"title": "Set The Turbo Boost Limit",
	"tags": [],
	"description": "",
	"content": "Set the Turbo Boost Limit Turbo boost limiting is supported on the Intel® and AMD® processors. Because processors have a high degree of variability in the amount of turbo boost each processor can supply, limiting the amount of turbo boost can reduce performance variability and reduce power consumption.\nTurbo boost can be limited by setting the turbo_boost_limit kernel parameter to one of these values:\n 0 - Disable turbo boost 999 - (default) No limit is applied.  The following values are not supported in COS v1.4:\n 100 - Limits turbo boost to 100 MHz 200 - Limits turbo boost to 200 MHz 300 - Limits turbo boost to 300 MHz 400 - Limits turbo boost to 400 MHz  The limit applies only when a high number of cores are active. On an N-core processor, the limit is in effect when the active core count is N, N-1, N-2, or N-3. For example, on a 12-core processor, the limit is in effect when 12, 11, 10, or 9 cores are active.\nSet or Change the Turbo Boost Limit Parameter Modify the Boot Orchestration Service (BOS) template for the node(s). This example below disables turbo boost. The default setting is 999 (no limit).\n{ \u0026quot;boot_sets\u0026quot;: { \u0026quot;boot_set61\u0026quot;: { \u0026quot;boot_ordinal\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;ims_image_id\u0026quot;: \u0026quot;efdfe6fc-af3f-40f0-9053-dd1ad6c359d3\u0026quot;, \u0026quot;kernel_parameters\u0026quot;: \u0026quot;console=tty0 console=ttyS0,115200n8 root=crayfs imagename=/SLES15 selinux=0 rd.shell rd.net.timeout.carrier=40 rd.retry=40 ip=dhcp rd.neednet=1 crashkernel=256M turbo_boost_limit=0\u0026quot;, \u0026quot;network\u0026quot;: \u0026quot;nmn\u0026quot;, \u0026quot;node_groups\u0026quot;: [ \u0026quot;group1\u0026quot;, \u0026quot;group2\u0026quot; ], \u0026quot;node_list\u0026quot;: [ \u0026quot;x0c0s28b0n0\u0026quot;, \u0026quot;node2\u0026quot;, \u0026quot;node3\u0026quot; ], \u0026quot;node_roles_groups\u0026quot;: [ \u0026quot;compute\u0026quot; ], \u0026quot;rootfs_provider\u0026quot;: \u0026quot;, \u0026quot;rootfs_provider_passthrough\u0026quot;: \u0026quot; }, }, \u0026quot;cfs_branch\u0026quot;: \u0026quot;my-test-branch\u0026quot;, \u0026quot;cfs_url\u0026quot;: \u0026quot;https://api-gw-service-nmn.local/vcs/cray/config-management.git\u0026quot;, \u0026quot;enable_cfs\u0026quot;: \u0026quot;true\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;st6\u0026quot;, \u0026quot;partition\u0026quot;: \u0026quot;p1\u0026quot; } "
},
{
	"uri": "/docs-csm/en-11/operations/power_management/shut_down_and_power_off_compute_and_user_access_nodes/",
	"title": "Shut Down And Power Off Compute And User Access Nodes",
	"tags": [],
	"description": "",
	"content": "Shut Down and Power Off Compute and User Access Nodes Shut down and power off compute and user access nodes (UANs). This procedure powers off all compute nodes in the context of an entire system shutdown.\nPrerequisites The cray and sat commands must be initialized and authenticated with valid credentials for Keycloak. If these have not been prepared, then see Configure the Cray Command Line Interface (cray CLI) and refer to \u0026ldquo;SAT Authentication\u0026rdquo; in the System Admin Toolkit (SAT) product documentation.\nProcedure   List detailed information about the available boot orchestration service (BOS) session template names.\nIdentify the BOS session template names such as \u0026quot;cos-2.0.x\u0026quot;, uan-slurm, and choose the appropriate compute and UAN node templates for the shutdown.\nncn-m001# cray bos sessiontemplate list [[results]] name = \u0026#34;cos-2.0.x\u0026#34; description = \u0026#34;BOS session template for booting compute nodes, generated by the installation\u0026#34; . . . name = \u0026#34;slurm\u0026#34; description = \u0026#34;BOS session template for booting compute nodes, generated by the installation\u0026#34; . . . name = \u0026#34;uan-slurm\u0026#34; description = \u0026#34;Template for booting UANs with Slurm\u0026#34;   To display more information about a session template, for example cos-2.0.x, use the describe option.\nncn-m001# cray bos sessiontemplate describe cos-2.0.x   Use sat bootsys shutdown to shut down and power off UANs and compute nodes.\nAttention: Specify the required session templates for COS_SESSION_TEMPLATE and UAN_SESSION_TEMPLATE in the example.\nAn optional --loglevel debug can be used to provide more information as the system shuts down. If used, it must be added after sat but before bootsys.\nncn-m001# sat bootsys shutdown --stage bos-operations \\ --bos-templates COS_SESSION_TEMPLATE,UAN_SESSION_TEMPLATE Started boot operation on BOS session templates: cos-2.0.x, uan. Waiting up to 600 seconds for sessions to complete. Waiting for BOA k8s job with id boa-a1a697fc-e040-4707-8a44-a6aef9e4d6ea to complete. Session template: uan. To monitor the progress of this job, run the following command in a separate window: \u0026#39;kubectl -n services logs -c boa -f --selector job-name=boa-a1a697fc-e040-4707-8a44-a6aef9e4d6ea\u0026#39; Waiting for BOA k8s job with id boa-79584ffe-104c-4766-b584-06c5a3a60996 to complete. Session template: cos-2.0.0. To monitor the progress of this job, run the following command in a separate window: \u0026#39;kubectl -n services logs -c boa -f --selector job-name=boa-79584ffe-104c-4766-b584-06c5a3a60996\u0026#39; . . . All BOS sessions completed.   Use the Job ID strings (for example, the cos-2.0.0 session, boa-79584ffe-104c-4766- b584-06c5a3a60996) from the previous command to monitor the progress of the boot of the compute nodes.\nThe command to run is displayed in the output of the sat bootsys shutdown command.\nncn-m001# kubectl logs -n services -c boa -f \\ --selector job-name=boa-boa-79584ffe-104c-4766-b584-06c5a3a60996 2020-08-21 17:27:02,358 - DEBUG - cray.boa - BOA starting 2020-08-21 17:27:02,358 - DEBUG - cray.boa - Boot Agent Image: created. 2020-08-21 17:27:02,358 - INFO - cray.boa - Boot Session: boa-79584ffe-104c-4766-b584-06c5a3a60996 2020-08-21 17:27:02,371 - INFO - cray.boa.connection - Reattempting GET request for \u0026#39;http://cray-cfs-api/apis/cfs/sessions\u0026#39; 2020-08-21 17:27:02,373 - INFO - cray.boa.connection - Reattempting GET request for \u0026#39;http://cray-cfs-api/apis/cfs/sessions\u0026#39; 2020-08-21 17:27:02,395 - INFO - cray.boa.connection - Reattempting GET request for \u0026#39;http://cray-cfs-api/apis/cfs/sessions\u0026#39; 2020-08-21 17:27:02,437 - INFO - cray.boa.connection - Reattempting GET request for \u0026#39;http://cray-cfs-api/apis/cfs/sessions\u0026#39; 2020-08-21 17:27:02,519 - INFO - cray.boa.connection - Reattempting GET request for \u0026#39;http://cray-cfs-api/apis/cfs/sessions\u0026#39; 2020-08-21 17:27:02,681 - INFO - cray.boa.connection - Reattempting GET request for \u0026#39;http://cray-cfs-api/apis/cfs/sessions\u0026#39; 2020-08-21 17:27:03,003 - INFO - cray.boa.connection - Reattempting GET request for \u0026#39;http://cray-cfs-api/apis/cfs/sessions\u0026#39; 2020-08-21 17:27:03,645 - INFO - cray.boa.connection - Reattempting GET request for \u0026#39;http://cray-cfs-api/apis/cfs/sessions\u0026#39; The BOS shutdown session may or may not power off compute nodes depending on the session template being used.\n  In another shell window, use a similar command to monitor the UAN session.\nncn-m001# kubectl -n services logs -c boa -f --selector job-name=boa-a1a697fc-e040-4707-8a44-a6aef9e4d6ea   Check the status of UAN and compute nodes to verify they are Off.\nThere may be delay in nodes reaching the Off state in the hardware state manager (HSM).\nncn-m001# sat status +----------------+------+----------+-------+------+---------+------+----------+-------------+----------+ | xname | Type | NID | State | Flag | Enabled | Arch | Class | Role | Net Type | +----------------+------+----------+-------+------+---------+------+----------+-------------+----------+ | x1000c0s0b0n0 | Node | 1001 | Off | OK | True | X86 | Mountain | Compute | Sling | | x1000c0s0b0n1 | Node | 1002 | Off | OK | True | X86 | Mountain | Compute | Sling | | x1000c0s0b1n0 | Node | 1003 | Off | OK | True | X86 | Mountain | Compute | Sling | | x1000c0s0b1n1 | Node | 1004 | Off | OK | True | X86 | Mountain | Compute | Sling | | x1000c0s1b0n0 | Node | 1005 | Off | OK | True | X86 | Mountain | Compute | Sling | | x1000c0s1b0n1 | Node | 1006 | Off | OK | True | X86 | Mountain | Compute | Sling | | x1000c0s1b1n0 | Node | 1007 | Off | OK | True | X86 | Mountain | Compute | Sling | | x1000c0s1b1n1 | Node | 1008 | Off | OK | True | X86 | Mountain | Compute | Sling | | x1000c1s0b0n0 | Node | 1033 | Off | OK | True | X86 | Mountain | Compute | Sling | | x1000c1s0b0n1 | Node | 1034 | Off | OK | True | X86 | Mountain | Compute | Sling | | x1000c1s0b1n0 | Node | 1035 | Off | OK | True | X86 | Mountain | Compute | Sling | . . .   "
},
{
	"uri": "/docs-csm/en-11/operations/power_management/shut_down_and_power_off_the_management_kubernetes_cluster/",
	"title": "Shut Down And Power Off The Management Kubernetes Cluster",
	"tags": [],
	"description": "",
	"content": "Shut Down and Power Off the Management Kubernetes Cluster Shut down management services and power off the HPE Cray EX management Kubernetes cluster.\nUnderstand the following concepts before powering off the management non-compute nodes (NCNs) for the Kubernetes cluster and storage:\n The etcd cluster provides storage for the state of the management Kubernetes cluster. The three node etcd cluster runs on the same nodes that are configured as Kubernetes Master nodes. The management cluster state must be frozen when powering off the Kubernetes cluster. When one member is unavailable, the two other members continue to provide full access to the data. When two members are down, the remaining member will switch to only providing read-only access to the data. Avoid Unnecessary Data Movement with Ceph - The Ceph cluster runs not only on the dedicated storage nodes, but also on the nodes configured as Kubernetes Master nodes. Specifically, the mon processes. If one of the storage nodes goes down, Ceph can rebalance the data onto the remaining nodes and object storage daemons (OSDs) to regain full protection. Avoid Spinning up Replacement Pods on Worker Nodes - Kubernetes keeps all pods running on the management cluster. The kubelet process on each node retrieves information from the etcd cluster about what pods must be running. If a node becomes unavailable for more than five minutes, Kubernetes creates replacement pods on other management nodes. High-Speed Network (HSN) - When the management cluster is shut down the HSN is also shut down.  The sat bootsys command automates the shutdown of Ceph and the Kubernetes management cluster and performs these tasks:\n Stops etcd and which freezes the state of the Kubernetes cluster on each management node. Stops and disables the kubelet on each management and worker node. Stops all containers on each management and worker node. Stop containerd on each management and worker node. Stops Ceph from rebalancing on the management node that is running a mon process.  Prerequisites An authentication token is required to access the API gateway and to use the sat command. See the System Security and Authentication and \u0026ldquo;SAT Authentication\u0026rdquo; in the Shasta Admin Toolkit (SAT) product stream documentation.\n  A work-around may be required to set an SSH key when running the sat bootsys shutdown --stage platform-services command below if performing a re-install of a 1.4.x system.\n  If the following error message is displayed:\nERROR: Failed to create etcd snapshot on ncn-m001: Failed to connect to ncn-m001: Server 'ncn-m001' not found in known_hosts ERROR: Fatal error in step \u0026quot;Create etcd snapshot on all Kubernetes manager NCNs.\u0026quot; of platform services stop: Failed to create etcd snapshot on hosts: ncn-m001   Then SSH to the management NCN that reported the problem, in this case ncn-m001 and add the host\u0026rsquo;s key to known_hosts file and repeat the preceding command.\nncn-m001# ssh ncn-m001 The authenticity of host \u0026#39;ncn-m001 (10.252.1.4)\u0026#39; can\u0026#39;t be established. ECDSA key fingerprint is SHA256:Mh43bU2iYDkOnQuI7Y067nV7no4btIE/OuYeHLh+n/4. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added \u0026#39;ncn-m001,10.252.1.4\u0026#39; (ECDSA) to the list of known hosts. Last login: Thu Jul 22 16:38:58 2021 from 172.25.66.163 ncn-m001# ncn-m001# logout Connection to ncn-m001 closed   Procedure CHECK HEALTH OF THE MANAGEMENT CLUSTER\n  To check the health and status of the management cluster before shutdown, see the \u0026ldquo;Platform Health Checks\u0026rdquo; section in Validate CSM Health.\n  Check the health and backup etcd clusters:\na. Determine what etcd clusters must be backed up and if they are healthy. Review Check the Health and Balance of etcd Clusters.\nb. Backup etcd clusters. See Backups for etcd-operator Clusters.\n  Check the status of NCN no wipe settings. Make sure metal.no-wipe=1. If a management NCN is set to metal.no-wipe==wipe, review Check and Set the metal.no-wipe Setting on NCNs before proceeding.\nncn-m001# /opt/cray/platform-utils/ncnGetXnames.sh +++++ Get NCN Xnames +++++ === Can be executed on any worker or master ncn node. === === Executing on ncn-m001, Thu Mar 18 20:58:04 UTC 2021 === === NCN node xnames and metal.no-wipe status === === metal.no-wipe=1, expected setting - the client === === already has the right partitions and a bootable ROM. === === Requires CLI to be initialized === === NCN Master nodes: ncn-m001 ncn-m002 ncn-m003 === === NCN Worker nodes: ncn-w001 ncn-w002 ncn-w003 === === NCN Storage nodes: ncn-s001 ncn-s002 ncn-s003 === Thu Mar 18 20:58:06 UTC 2021 ncn-m001: x3000c0s1b0n0 - metal.no-wipe=1 ncn-m002: x3000c0s2b0n0 - metal.no-wipe=1 ncn-m003: x3000c0s3b0n0 - metal.no-wipe=1 ncn-w001: x3000c0s4b0n0 - metal.no-wipe=1 ncn-w002: x3000c0s5b0n0 - metal.no-wipe=1 ncn-w003: x3000c0s6b0n0 - metal.no-wipe=1 ncn-s001: x3000c0s7b0n0 - metal.no-wipe=1 ncn-s002: x3000c0s8b0n0 - metal.no-wipe=1 ncn-s003: x3000c0s9b0n0 - metal.no-wipe=1 root@ncn-m001 2021-03-18 20:58:21 ~ #   SHUT DOWN THE KUBERNETES MANAGEMENT CLUSTER\n Shut down platform services.\nncn-m001# sat bootsys shutdown --stage platform-services The following Non-compute Nodes (NCNs) will be included in this operation: managers: - ncn-m001 storage: - ncn-s001 - ncn-s002 - ncn-s003 workers: - ncn-w001 - ncn-w002 - ncn-w003 Are the above NCN groupings correct? [yes,no] yes Executing step: Create etcd snapshot on all Kubernetes manager NCNs. Executing step: Stop etcd on all Kubernetes manager NCNs. Executing step: Stop and disable kubelet on all Kubernetes NCNs. Executing step: Stop containers running under containerd on all Kubernetes NCNs. WARNING: One or more \u0026#34;crictl stop\u0026#34; commands timed out on ncn-w003 WARNING: One or more \u0026#34;crictl stop\u0026#34; commands timed out on ncn-w002 ERROR: Failed to stop 1 container(s) on ncn-w003. Execute \u0026#34;crictl ps -q\u0026#34; on the host to view running containers. ERROR: Failed to stop 2 container(s) on ncn-w002. Execute \u0026#34;crictl ps -q\u0026#34; on the host to view running containers. WARNING: One or more \u0026#34;crictl stop\u0026#34; commands timed out on ncn-w001 ERROR: Failed to stop 4 container(s) on ncn-w001. Execute \u0026#34;crictl ps -q\u0026#34; on the host to view running containers. WARNING: Non-fatal error in step \u0026#34;Stop containers running under containerd on all Kubernetes NCNs.\u0026#34; of platform services stop: Failed to stop containers on the following NCN (s): ncn-w001, ncn-w002, ncn-w003 Continue with platform services stop? [yes,no] no Aborting. In the preceding example, the commands to stop containers timed out on all the worker nodes and reported WARNING and ERROR messages. A summary of the issue displays and prompts the user to continue or stop. Respond no stop the shutdown. Then review the containers running on the nodes.\nncn-m001# for ncn in ncn-w00{1,2,3}; do echo \u0026#34;$ncn\u0026#34;; ssh $ncn \u0026#34;crictl ps\u0026#34;; echo; done ncn-w001 CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID 032d69162ad24 302d9780da639 54 minutes ago Running cray-dhcp-kea 0 e4d1c01818a5a 7ab8021279164 2ad3f16035f1f 3 hours ago Running log-forwarding 0 a5e89a366f5a3 ncn-w002 CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID 1ca9d9fb81829 de444b360808f 4 hours ago Running cray-uas-mgr 0 902287a6d0393 ncn-w003 CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID Run the sat command again and enter yes at the prompt about the etcd snapshot not being created:\nncn-m001# sat bootsys shutdown --stage platform-services The following Non-compute Nodes (NCNs) will be included in this operation: managers: - ncn-m001 storage: - ncn-s001 - ncn-s002 - ncn-s003 workers: - ncn-w001 - ncn-w002 - ncn-w003 Are the above NCN groupings correct? [yes,no] yes Executing step: Create etcd snapshot on all Kubernetes manager NCNs. WARNING: Failed to create etcd snapshot on ncn-m001: The etcd service is not active on ncn-m001 so a snapshot cannot be created. WARNING: Failed to create etcd snapshot on ncn-m002: The etcd service is not active on ncn-m002 so a snapshot cannot be created. WARNING: Failed to create etcd snapshot on ncn-m003: The etcd service is not active on ncn-m003 so a snapshot cannot be created. WARNING: Non-fatal error in step \u0026#34;Create etcd snapshot on all Kubernetes manager NCNs.\u0026#34; of platform services stop: Failed to create etcd snapshot on hosts: ncn-m001, ncn-m00 2, ncn-m003 Continue with platform services stop? [yes,no] yes Continuing. Executing step: Stop etcd on all Kubernetes manager NCNs. Executing step: Stop and disable kubelet on all Kubernetes NCNs. Executing step: Stop containers running under containerd on all Kubernetes NCNs. Executing step: Stop containerd on all Kubernetes NCNs. Executing step: Check health of Ceph cluster and freeze state. If the process continues to report errors due to Failed to stop containers, iterate on the above step. Each iteration should reduce the number of containers running. If necessary, containers can be manually stopped using crictl stop CONTAINER. If containers are stopped manually, re-run the above procedure to complete any final steps in the process.\n  Shut down and power off all management NCNs except ncn-m001.\nncn-m001# sat bootsys shutdown --stage ncn-power Proceed with shutdown of other management NCNs? [yes,no] yes Proceeding with shutdown of other management NCNs. IPMI username: root IPMI password: The following Non-compute Nodes (NCNs) will be included in this operation: managers: - ncn-m002 - ncn-m003 storage: - ncn-s001 - ncn-s002 - ncn-s003 workers: - ncn-w001 - ncn-w002 - ncn-w003 The following Non-compute Nodes (NCNs) will be excluded from this operation: managers: - ncn-m001 storage: [] workers: [] Are the above NCN groupings and exclusions correct? [yes,no] yes   Use tail to monitor the log files in /var/log/cray/console_logs for each NCN.\nAlternately attach to the screen session (screen sessions real time, but not saved):\nncn-m001# screen -ls There are screens on: 26745.SAT-console-ncn-m003-mgmt (Detached) 26706.SAT-console-ncn-m002-mgmt (Detached) 26666.SAT-console-ncn-s003-mgmt (Detached) 26627.SAT-console-ncn-s002-mgmt (Detached) 26589.SAT-console-ncn-s001-mgmt (Detached) 26552.SAT-console-ncn-w003-mgmt (Detached) 26514.SAT-console-ncn-w002-mgmt (Detached) 26444.SAT-console-ncn-w001-mgmt (Detached) ncn-m001# screen -x 26745.SAT-console-ncn-m003-mgmt   Use ipmitool to check the power off status of management nodes.\nncn-m001# export USERNAME=root ncn-m001# export IPMI_PASSWORD=changeme ncn-m001# for ncn in ncn-m00{2,3} ncn-w00{1,2,3} ncn-s00{1,2,3}; do echo -n \u0026#34;$ncn: \u0026#34;; ipmitool -U $USERNAME -H ${ncn}-mgmt -E -I lanplus chassis power status; done   From a remote system, activate the serial console for ncn-m001.\nremote$ ipmitool -I lanplus -U $USERNAME -E -H NCN-M001_BMC_HOSTNAME sol activate ncn-m001 login: root Password:   From the serial console, shut down Linux.\nncn-m001# shutdown -h now   Wait until the console indicates that the node has shut down.\n  From a remote system that has access to the management plane, use IPMI tool to power off ncn-m001.\nremote$ ipmitool -I lanplus -U $USERNAME -E -H NCN-M001_BMC_HOSTNAME chassis power status remote$ ipmitool -I lanplus -U $USERNAME -E -H NCN-M001_BMC_HOSTNAME chassis power off remote$ ipmitool -I lanplus -U $USERNAME -E -H NCN-M001_BMC_HOSTNAME chassis power status CAUTION: The modular coolant distribution unit (MDCU) in a liquid-cooled HPE Cray EX2000 cabinet (also referred to as a Hill or TDS cabinet) typically receives power from its management cabinet PDUs. If the system includes a EX2000 cabinet, do not power off the management cabinet PDUs, Powering off the MDCU will cause an emergency power off (EPO) of the cabinet and may result in data loss or equipment damage.\n  (Optional) If a liquid-cooled EX2000 cabinet is not receiving MCDU power from this management cabinet, power off the PDU circuit breakers or disconnect the PDUs from facility power and follow lockout/tagout procedures for the site.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/power_management/standard_rack_node_power_management/",
	"title": "Standard Rack Node Power Management",
	"tags": [],
	"description": "",
	"content": "Standard Rack Node Power Management HPE Cray EX standard EIA rack node power management is supported by the server vendor BMC firmware. The BMC exposes the power control API for a node through the node\u0026rsquo;s Redfish ChassisPower schema.\nOut-of-band power management data is polled by a collector and published on a Kafka bus for entry into the Power Management Database. The Cray Advanced Platform Management and Control (CAPMC) API facilitates power control and enables power aware WLMs such as Slurm to perform power management and power capping tasks.\nImportant: Always use the Boot Orchestration Service (BOS) to power off or power on compute nodes.\nRedfish API The Redfish API for rack-mounted nodes is the node\u0026rsquo;s Chassis Power resource which is presented by the BMC. OEM properties may be used to augment the Power schema and allow for feature parity with previous Cray system power management capabilities. A PowerControl resource presents the various power management capabilities for the node.\nEach node has a node power control resource. The power control of the node must be enabled and may require additional licenses to use.\nCAPMC does not enable power capping on all standard rack nodes because each server vendor has a different implementation. The Activate and Deactivate commands that follow apply to Gigabyte nodes only.\nGet Node Power Limit Settings\n# curl -k -u $login:$pass -H \u0026#34;Content-Type: application/json\u0026#34; \\ -X GET https://$BMC_IP/redfish/v1/Chassis/Self/Power 2\u0026gt;/dev/null | python -m json.tool | egrep \u0026#39;LimitInWatts\u0026#39; Use the Cray CLI to get the node power limit settings:\n# cray capmc get_power_cap create --nids 100006 --format json | jq { \u0026#34;e\u0026#34;: 0, \u0026#34;err_msg\u0026#34;: \u0026#34;, \u0026#34;groups\u0026#34;: [ { \u0026#34;powerup\u0026#34;: 0, \u0026#34;host_limit_min\u0026#34;: 0, \u0026#34;supply\u0026#34;: 65535, \u0026#34;host_limit_max\u0026#34;: 0, \u0026#34;controls\u0026#34;: [ { \u0026#34;max\u0026#34;: 0, \u0026#34;min\u0026#34;: 0, \u0026#34;name\u0026#34;: \u0026#34;Chassis Power Control\u0026#34;, \u0026#34;desc\u0026#34;: \u0026#34;Chassis Power Control\u0026#34; } ], \u0026#34;nids\u0026#34;: [ 100006 ], \u0026#34;static\u0026#34;: 0, \u0026#34;desc\u0026#34;: \u0026#34;3_AuthenticAMD_64c_244GiB_3200MHz_NoAccel\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;3_AuthenticAMD_64c_244GiB_3200MHz_NoAccel\u0026#34; } ] } Set Node Power Limit\n# curl -k -u $login:$pass -H \u0026#34;Content-Type: application/json\u0026#34; \\ -H \u0026#39;If-Match: W/\u0026#34;\u0026#39;${o_data}\u0026#39;\u0026#34;\u0026#39; -X PATCH https://$BMC_IP/redfish/v1/Chassis/Self/Power \\ --data \u0026#39;{\u0026#34;PowerControl\u0026#34;: [{\u0026#34;PowerLimit\u0026#34;: {\u0026#34;LimitInWatts\u0026#34;: \u0026#39;$LimitValue\u0026#39;}}]}\u0026#39; Set the node power limit to 600 Watts:\n# cray capmc set_power_cap create --nids 1,2,3 --node 600 Get Node Energy Counter\n# curl -k -u $login:$pass -H \u0026#34;Content-Type: application/json\u0026#34; \\ -X GET https://$BMC_IP/redfish/v1/Chassis/Self/Power 2\u0026gt;/dev/null \\ | python -m json.tool | egrep \u0026#39;PowerConsumedWatts\u0026#39; Activate Node Power Limit\n# curl -k -u $login:$pass -H \u0026#34;Content-Type: application/json\u0026#34; \\ -X POST https://$BMC_IP/redfish/v1/Chassis/Self/Power/Actions/LimitTrigger \\ --data \u0026#39;{\u0026#34;PowerLimitTrigger\u0026#34;: \u0026#34;Activate\u0026#34;}\u0026#39; Deactivate Node Power Limit\n# curl -k -u $login:$pass -H \u0026#34;Content-Type: application/json\u0026#34; \\ -X POST https://$BMC_IP/redfish/v1/Chassis/Self/Power/Actions/LimitTrigger \\ --data \u0026#39;{\u0026#34;PowerLimitTrigger\u0026#34;: \u0026#34;Deactivate\u0026#34;}\u0026#39; "
},
{
	"uri": "/docs-csm/en-11/operations/power_management/power_on_and_boot_compute_nodes_and_user_access_nodes/",
	"title": "Power On And Boot Compute And User Access Nodes",
	"tags": [],
	"description": "",
	"content": "Power On and Boot Compute and User Access Nodes Use Boot Orchestration Service (BOS) and choose the appropriate session template to power on and boot compute and UANs.\nThis procedure boots all compute nodes and user access nodes (UANs) in the context of a full system power-up.\nPrerequisites  All compute cabinet PDUs, servers, and switches must be powered on. An authentication token is required to access the API gateway and to use the sat command. See the System Security and Authentication and \u0026ldquo;SAT Authentication\u0026rdquo; in the Shasta Admin Toolkit (SAT) product stream documentation.  Procedure   Obtain the authorization key.\nSee System Security and Authentication, Authenticate an Account with the Command Line, and \u0026ldquo;SAT Authentication\u0026rdquo; in the Shasta Admin Toolkit (SAT) product documentation.\n  List detailed information about the available boot orchestration service (BOS) session template names.\nIdentify the BOS session template names such as \u0026quot;cos-2.0.x\u0026quot;, slurm, uan-slurm, and choose the appropriate compute and UAN node templates for the power on and boot.\nncn-m001# cray bos v1 sessiontemplate list [[results]] name = \u0026#34;cos-2.0.x\u0026#34; description = \u0026#34;BOS session template for booting compute nodes, generated by the installation\u0026#34; . . . name = \u0026#34;slurm\u0026#34; description = \u0026#34;BOS session template for booting compute nodes, generated by the installation\u0026#34; . . . name = \u0026#34;uan-slurm\u0026#34; description = \u0026#34;Template for booting UANs with Slurm\u0026#34;   To display more information about a session template, for example cos-2.0.0, use the describe option.\nncn-m001# cray bos v1 sessiontemplate describe cos-2.0.x   Use sat bootsys boot to power on and boot UANs and compute nodes.\nAttention: Specify the required session template name for COS_SESSION_TEMPLATE and UAN_SESSION_TEMPLATE in the following command line.\nUse --loglevel debug command line option to provide more information as the system boots.\nncn-m001# sat bootsys boot --stage bos-operations \\ --bos-templates COS_SESSION_TEMPLATE,UAN_SESSION_TEMPLATE Started boot operation on BOS session templates: cos-2.0.x, uan. Waiting up to 900 seconds for sessions to complete. Waiting for BOA k8s job with id boa-a1a697fc-e040-4707-8a44-a6aef9e4d6ea to complete. Session template: uan. To monitor the progress of this job, run the following command in a separate window: \u0026#39;kubectl -n services logs -c boa -f --selector job-name=boa-a1a697fc-e040-4707-8a44-a6aef9e4d6ea\u0026#39; Waiting for BOA k8s job with id boa-79584ffe-104c-4766-b584-06c5a3a60996 to complete. Session template: cos-2.0.0. To monitor the progress of this job, run the following command in a separate window: \u0026#39;kubectl -n services logs -c boa -f --selector job-name=boa-79584ffe-104c-4766-b584-06c5a3a60996\u0026#39; . . . All BOS sessions completed. Note the returned job Id for each session, for example: \u0026quot;boa-caa15959-2402-4190-9243-150d568942f6\u0026quot;.\n  Use the Job ID strings to monitor the progress of the boot job.\nTip: The commands needed to monitor the progress of the job are provided in the output of the sat bootsys boot command.\nncn-m001# kubectl -n services logs -c boa -f --selector job-name=boa-caa15959-2402-4190-9243-150d568942f6   In another shell window, use a similar command to monitor the UAN session.\nncn-m001# kubectl -n services logs -c boa -f --selector job-name=boa-a1a697fc-e040-4707-8a44-a6aef9e4d6ea   Wait for compute nodes and UANs to boot and check the Configuration Framework Service (CFS) log for errors.\n  Verify that nodes have booted and indicate Ready.\nncn-m001# sat status +----------------+------+----------+-------+------+---------+------+----------+-------------+----------+ | xname | Type | NID | State | Flag | Enabled | Arch | Class | Role | Net Type | +----------------+------+----------+-------+------+---------+------+----------+-------------+----------+ | x1000c0s0b0n0 | Node | 1001 | Ready | OK | True | X86 | Mountain | Compute | Sling | | x1000c0s0b0n1 | Node | 1002 | Ready | OK | True | X86 | Mountain | Compute | Sling | | x1000c0s0b1n0 | Node | 1003 | Ready | OK | True | X86 | Mountain | Compute | Sling | | x1000c0s0b1n1 | Node | 1004 | Ready | OK | True | X86 | Mountain | Compute | Sling | | x1000c0s1b0n0 | Node | 1005 | Ready | OK | True | X86 | Mountain | Compute | Sling | . . .   Make nodes available to customers and refer to Validate CSM Health to check system health and status.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/power_management/power_on_and_start_the_management_kubernetes_cluster/",
	"title": "Power On And Start The Management Kubernetes Cluster",
	"tags": [],
	"description": "",
	"content": "Power On and Start the Management Kubernetes Cluster Power on and start management services on the HPE Cray EX management Kubernetes cluster.\nPrerequisites  All management rack PDUs are connected to facility power and facility power is ON. An authentication token is required to access the API gateway and to use the sat command. See the System Security and Authentication and \u0026ldquo;SAT Authentication\u0026rdquo; in the Shasta Admin Toolkit product documentation.  Procedure First run sat bootsys boot --stage ncn-power to power on and boot the management NCNs. Then the run sat bootsys boot --stage platform-services to start platform services on the system.\n  If necessary, power on the management cabinet CDU and chilled doors.\n  Set all management cabinet PDU circuit breakers to ON (all cabinets that contain Kubernetes master nodes, worker nodes, or storage nodes).\n  Power on the HPE Cray EX cabinets and standard rack cabinet PDUs.\nSee Power On Compute and IO Cabinets.\nBe sure that management switches in all racks and CDU cabinets are powered on and healthy.\n  From a remote system, start the Lustre file system, if it was stopped.\n  Activate the serial console window to ncn-m001.\nremote$ export USERNAME=root remote$ export IPMI_PASSWORD=changeme remote$ ipmitool -I lanplus -U $USERNAME -E -H NCN_M001_BMC_HOSTNAME sol activate   In a separate window, power on the master node 1 (ncn-m001) chassis using IPMI tool.\nremote$ ipmitool -I lanplus -U $USERNAME -E -H NCN_M001_BMC_HOSTNAME chassis power on Wait for the login prompt.\nIf the m001 node boots into the PIT (ncn-m001-pit), Set Boot Order to boot from disk, shutdown the PIT node and power cycle again to boot to into ncn-m001.\nncn-m001-pit:~ # shutdown -h now remote$ ipmitool -I lanplus -U $USERNAME -E -H NCN_M001_BMC_HOSTNAME chassis power on   Wait for the ncn-m001 node to boot, then ping the node to check status.\nremote$ ping NCN_M001_HOSTNAME   Log in to ncn-m001 as root.\nremote$ ssh root@NCN_M001_HOSTNAME   POWER ON ALL OTHER MANAGEMENT NCNs\n Power on and boot other management NCNs.\nncn-m001# sat bootsys boot --stage ncn-power IPMI username: root IPMI password: The following Non-compute Nodes (NCNs) will be included in this operation: managers: - ncn-m002 - ncn-m003 storage: - ncn-s001 - ncn-s002 - ncn-s003 workers: - ncn-w001 - ncn-w002 - ncn-w003 The following Non-compute Nodes (NCNs) will be excluded from this operation: managers: - ncn-m001 storage: [] workers: [] Are the above NCN groupings and exclusions correct? [yes,no] yes Powering on NCNs and waiting up to 300 seconds for them to be reachable via SSH: ncn-m002, ncn-m003 Waiting for condition \u0026#34;Hosts accessible via SSH\u0026#34; timed out after 300 seconds ERROR: Unable to reach the following NCNs via SSH after powering them on: ncn-m003, ncn-s002.. Troubleshoot the issue and then try again. In the preceding example, the SSH command to the NCN nodes timed out and reported ERROR messages. Iterate on the above step until you see Succeeded with boot of other management NCNs. Each iteration should get further in the process.\n  Use tail to monitor the log files in /var/log/cray/console_logs for each NCN.\nAlternately attach to the screen session (screen sessions real time, but not saved):\nncn-m001# screen -ls There are screens on: 26745.SAT-console-ncn-m003-mgmt (Detached) 26706.SAT-console-ncn-m002-mgmt (Detached) 26666.SAT-console-ncn-s003-mgmt (Detached) 26627.SAT-console-ncn-s002-mgmt (Detached) 26589.SAT-console-ncn-s001-mgmt (Detached) 26552.SAT-console-ncn-w003-mgmt (Detached) 26514.SAT-console-ncn-w002-mgmt (Detached) 26444.SAT-console-ncn-w001-mgmt (Detached) ncn-m001# screen -x 26745.SAT-console-ncn-m003-mgmt   VERIFY ACCESS TO LUSTRE FILE SYSTEM\nVerify that the Lustre file system is available from the management cluster.  START KUBERNETES (k8s)\n Use sat bootsys to start the k8s cluster. Note that the default timeout for Ceph to become healthy is 600 seconds, which is excessive. To work around this issue, set the timeout to a more reasonable value like 60 seconds using the --ceph-timeout option as shown below.\nncn-m001# sat bootsys boot --stage platform-services --ceph-timeout 60 The following Non-compute Nodes (NCNs) will be included in this operation: managers: - ncn-m001 storage: - ncn-s001 - ncn-s002 - ncn-s003 workers: - ncn-w001 - ncn-w002 - ncn-w003 Are the above NCN groupings correct? [yes,no] yes   The previous step may fail with a message like the following:\nExecuting step: Start inactive Ceph services, unfreeze Ceph cluster and wait for Ceph health. Waiting up to 60 seconds for Ceph to become healthy after unfreeze Waiting for condition \u0026#34;Ceph cluster in healthy state\u0026#34; timed out after 60 seconds ERROR: Fatal error in step \u0026#34;Start inactive Ceph services, unfreeze Ceph cluster and wait for Ceph health.\u0026#34; of platform services start: Ceph is not healthy. Please correct Ceph health and try again. If a failure like the above occurs, see the info-level log messages for details about the Ceph health check failure. Depending on the configured log level for sat, the log messages may appear in stderr or only in the log file. For example:\nncn-m001# grep \u0026#34;fatal Ceph health warnings\u0026#34; /var/log/cray/sat/sat.log | tail -n 1 2021-08-04 17:28:21,945 - INFO - sat.cli.bootsys.ceph - Ceph is not healthy: The following fatal Ceph health warnings were found: POOL_NO_REDUNDANCY The particular Ceph health warning may vary. In this example, it is POOL_NO_REDUNDANCY. See Manage Ceph Services for Ceph troubleshooting steps, which may include restarting Ceph services as described below for convenience.\nFrom storage node ncn-s001, restart Ceph services.\nncn-s001# ansible ceph_all -m shell -a \u0026#34;systemctl restart ceph-osd.target\u0026#34; ansible ceph_all -m shell -a \u0026#34;systemctl restart ceph-radosgw.target\u0026#34; ansible ceph_all -m shell -a \u0026#34;systemctl restart ceph-mon.target\u0026#34; ansible ceph_all -m shell -a \u0026#34;systemctl restart ceph-mgr.target\u0026#34; ansible ceph_all -m shell -a \u0026#34;systemctl restart ceph-mds.target\u0026#34; Once Ceph is healthy, repeat the previous step to finish starting the Kubernetes cluster.\n  Check the space available on the Ceph cluster.\nncn-m001# ceph df RAW STORAGE: CLASS SIZE AVAIL USED RAW USED %RAW USED ssd 63 TiB 60 TiB 2.8 TiB 2.8 TiB 4.45 TOTAL 63 TiB 60 TiB 2.8 TiB 2.8 TiB 4.45 POOLS: POOL ID STORED OBJECTS USED %USED MAX AVAIL cephfs_data 1 40 MiB 382 124 MiB 0 18 TiB cephfs_metadata 2 262 MiB 117 787 MiB 0 18 TiB .rgw.root 3 3.5 KiB 8 384 KiB 0 18 TiB default.rgw.buckets.data 4 71 GiB 27.07k 212 GiB 0.38 18 TiB default.rgw.control 5 0 B 8 0 B 0 18 TiB default.rgw.buckets.index 6 7.7 MiB 13 7.7 MiB 0 18 TiB default.rgw.meta 7 21 KiB 111 4.2 MiB 0 18 TiB default.rgw.log 8 0 B 207 0 B 0 18 TiB kube 9 67 GiB 26.57k 197 GiB 0.35 18 TiB smf 10 806 GiB 271.69k 2.4 TiB 4.12 18 TiB default.rgw.buckets.non-ec 11 0 B 0 0 B 0 18 TiB   If %USED for any pool approaches 80% used, resolve the space issue.\nTo resolve the space issue, see Troubleshoot Ceph OSDs Reporting Full.\n  Monitor the status of the management cluster and which pods are restarting as indicated by either a Running or Completed state.\nncn-m001# kubectl get pods -A -o wide | grep -v -e Running -e Completed The pods and containers are normally restored in approximately 10 minutes.\nBecause no containers are running, all pods first transition to an Error state. The error state indicates that their containers were stopped. The kubelet on each node restarts the containers for each pod. The RESTARTS column of the kubectl get pods -A command increments as each pod progresses through the restart sequence.\nIf there are pods in the MatchNodeSelector state, delete these pods. Then verify that the pods restart and are in the Running state.\n  Check the status of the slurmctld and slurmdbd pods to determine if they are starting:\nncn-m001# kubectl describe pod -n user -lapp=slurmctld ncn-m001# kubectl describe pod -n user -lapp=slurmdbd Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedCreatePodSandBox 29m kubelet, ncn-w001 Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \u0026#34;314ca4285d0706ec3d76a9e953e412d4b0712da4d0cb8138162b53d807d07491\u0026#34;: Multus: Err in tearing down failed plugins: Multus: error in invoke Delegate add - \u0026#34;macvlan\u0026#34;: failed to allocate for range 0: no IP addresses available in range set: 10.252.2.4-10.252.2.4 Warning FailedCreatePodSandBox 29m kubelet, ncn-w001 Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox ... If the preceding error is displayed, then remove all files in the following directories on all worker nodes:\n /var/lib/cni/networks/macvlan-slurmctld-nmn-conf /var/lib/cni/networks/macvlan-slurmdbd-nmn-conf    Check that spire pods have started.\nncn-m001# kubectl get pods -n spire -o wide | grep spire-jwks spire-jwks-6b97457548-gc7td 2/3 CrashLoopBackOff 9 23h 10.44.0.117 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; spire-jwks-6b97457548-jd7bd 2/3 CrashLoopBackOff 9 23h 10.36.0.123 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; spire-jwks-6b97457548-lvqmf 2/3 CrashLoopBackOff 9 23h 10.39.0.79 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   If spire pods indicate CrashLoopBackOff, then restart the spire pods.\nncn-m001# kubectl rollout restart -n spire deployment spire-jwks   Check if any pods are in CrashLoopBackOff due to errors connecting to vault. If so, restart the vault operator, the vault pods and finally the pod which is in CrashLoopBackOff. For example:\nncn-m001# kubectl get pods -A | grep CrashLoopBackOff services cray-console-node-1 2/3 CrashLoopBackOff 206 6d21h ncn-m001# kubectl -n services logs cray-console-node-1 cray-console-node | grep \u0026#34;connection failure\u0026#34; | grep vault 2021/08/26 16:39:28 Error: \u0026amp;api.ResponseError{HTTPMethod:\u0026#34;PUT\u0026#34;, URL:\u0026#34;http://cray-vault.vault:8200/v1/auth/kubernetes/login\u0026#34;, StatusCode:503, RawError:true, Errors:[]string{\u0026#34;upstream connect error or disconnect/reset before headers. reset reason: connection failure\u0026#34;}} panic: Error: \u0026amp;api.ResponseError{HTTPMethod:\u0026#34;PUT\u0026#34;, URL:\u0026#34;http://cray-vault.vault:8200/v1/auth/kubernetes/login\u0026#34;, StatusCode:503, RawError:true, Errors:[]string{\u0026#34;upstream connect error or disconnect/reset before headers. reset reason: connection failure\u0026#34;}} # Restart the vault-operator ncn-m001# kubectl delete pods -n vault -l app.kubernetes.io/name=vault-operator # Wait for the operator pod to restart with 2/2 Ready and Running - for example: ncn-m001# kubectl get pods -n vault -l app.kubernetes.io/name=vault-operator NAME READY STATUS RESTARTS AGE cray-vault-operator-69b4b6887-dfn2f 2/2 Running 2 1m # Restart the cray-vault pods ncn-m001# kubectl rollout restart statefulset cray-vault -n vault # Wait for the cray-vault pods to restart with 5/5 Ready and Running - for example: ncn-m001# kubectl get pods -n vault -l app.kubernetes.io/name=vault NAME READY STATUS RESTARTS AGE cray-vault-0 5/5 Running 1 2m cray-vault-1 5/5 Running 1 2m cray-vault-2 5/5 Running 2 2m # Restart cray-console-node-1 ncn-m001# kubectl delete pod cray-console-node-1 -n services # Wait for cray-console-node-1 to restart with 3/3 Ready and Running - for example: ncn-m001# kubectl get pods -n services | grep cray-console-node-1 cray-console-node-1 3/3 Running 0 2m   Determine whether the cfs-state-reporter service is failing to start on each manager/master and worker NCN while trying to contact CFS.\nncn-m001# pdsh -w ncn-m00[1-3],ncn-w00[1-3] systemctl status cfs-state-reporter ncn-w001: cfs-state-reporter.service - cfs-state-reporter reports configuration level of the system ncn-w001: Loaded: loaded (/usr/lib/systemd/system/cfs-state-reporter.service; enabled; vendor preset: disabled) ncn-w001: Active: activating (start) since Thu 2021-03-18 22:29:15 UTC; 21h ago ncn-w001: Main PID: 5192 (python3) ncn-w001: Tasks: 1 ncn-w001: CGroup: /system.slice/cfs-state-reporter.service ncn-w001: └─5192 /usr/bin/python3 -m cfs.status_reporter ncn-w001: ncn-w001: Mar 19 19:33:19 ncn-w001 python3[5192]: Expecting value: line 1 column 1 (char 0) ncn-w001: Mar 19 19:33:49 ncn-w001 python3[5192]: Attempt 2482 of contacting CFS... ncn-w001: Mar 19 19:33:49 ncn-w001 python3[5192]: Unable to contact CFS to report component status: CFS returned a non-json response: Unauthorized Request ncn-w001: Mar 19 19:33:49 ncn-w001 python3[5192]: Expecting value: line 1 column 1 (char 0) ncn-w001: Mar 19 19:34:19 ncn-w001 python3[5192]: Attempt 2483 of contacting CFS... ncn-w001: Mar 19 19:34:20 ncn-w001 python3[5192]: Unable to contact CFS to report component status: CFS returned a non-json response: Unauthorized Request ncn-w001: Mar 19 19:34:20 ncn-w001 python3[5192]: Expecting value: line 1 column 1 (char 0) ncn-w001: Mar 19 19:34:50 ncn-w001 python3[5192]: Attempt 2484 of contacting CFS... ncn-w001: Mar 19 19:34:50 ncn-w001 python3[5192]: Unable to contact CFS to report component status: CFS returned a non-json response: Unauthorized Request ncn-w001: Mar 19 19:34:50 ncn-w001 python3[5192]: Expecting value: line 1 column 1 (char 0) pdsh@ncn-m001: ncn-w001: ssh exited with exit code 3   On each NCN where cfs-state-reporter is stuck in \u0026ldquo;activating\u0026rdquo; as shown in the preceding error messages, restart the cfs-state-reporter service. For example:\nncn-m001# systemctl restart cfs-state-reporter   Check the status again.\nncn-m001# pdsh -w ncn-m00[1-3],ncn-w00[1-3] systemctl status cfs-state-reporter     VERIFY BGP PEERING SESSIONS\n Check the status of the Border Gateway Protocol (BGP). For more information, see Check BGP Status and Reset Sessions.\n  Check the status and health of etcd clusters, see Check the Health and Balance of etcd Clusters.\n  CHECK CRON JOBS\n Display all the k8s cron jobs.\nncn-m001# kubectl get cronjobs.batch -A NAMESPACE NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE kube-system kube-etcdbackup */10 * * * * False 0 2d1h 29d operators kube-etcd-defrag 0 0 * * * False 0 18h 29d operators kube-etcd-defrag-cray-hbtd-etcd 0 */4 * * * False 0 178m 29d operators kube-etcd-periodic-backup-cron 0 * * * * False 0 58m 29d services cray-dns-unbound-manager */3 * * * * False 0 63s 18h services hms-discovery */3 * * * * False 1 63s 18h services hms-postgresql-pruner */5 * * * * False 0 3m3s 18h services sonar-sync */1 * * * * False 0 63s 18h sma sma-pgdb-cron 10 4 * * * False 0 14h 27d Attention: It is normal for the hms-discovery service to be suspended at this point if liquid-cooled cabinets have not been powered on. The hms-discovery service is un-suspended during the liquid-cooled cabinet power on procedure. Do not re-create the hms-discovery cron job at this point.\n  Check for cron jobs that have a LAST SCHEDULE time that is older than the SCHEDULE time. These cron jobs must be restarted.\n  Check any cron jobs in question for errors.\nncn-m001# kubectl describe cronjobs.batch -n kube-system kube-etcdbackup | egrep -A 15 Events Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedNeedsStart 4m15s (x15156 over 42h) cronjob-controller Cannot determine if job needs to be \\  started: too many missed start time (\u0026gt; 100). \\  Set or decrease .spec.startingDeadlineSeconds \\  or check clock skew   For any cron jobs producing errors, get the YAML representation of the cron job and edit the YAML file:\nncn-m001# cd ~/k8s ncn-m001# kubectl get cronjobs.batch -n NAMESPACE CRON_JOB_NAME -o yaml \u0026gt; CRON_JOB_NAME-cronjob.yaml ncn-m001# vi CRON_JOB_NAME-cronjob.yaml   Delete all lines that contain uid:.\n  Delete the entire status: section including the status key.\n  Save the file and quit the editor.\n    Delete the cron job.\nncn-m001# kubectl delete -f CRON_JOB_NAME-cronjob.yaml   Apply the cron job.\nncn-m001# kubectl apply -f CRON_JOB_NAME-cronjob.yaml   Verify that the cron job has been scheduled.\nncn-m001# kubectl get cronjobs -n backups benji-k8s-backup-backups-namespace NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE kube-etcdbackup */10 * * * * False 0 92s 29   CHECK THE HSM INVENTORY STATUS OF NCNs\n Use the sat command to check for management NCNs in an Off state.\nncn-m001# sat status --filter role=management +----------------+------+----------+-------+---------+---------+------+-------+-------------+----------+ | xname | Type | NID | State | Flag | Enabled | Arch | Class | Role | Net Type | +----------------+------+----------+-------+---------+---------+------+-------+-------------+----------+ | x3000c0s10b0n0 | Node | 100001 | On | OK | True | X86 | River | Management | Sling | | x3000c0s12b0n0 | Node | 100002 | Off | OK | True | X86 | River | Management | Sling | | x3000c0s14b0n0 | Node | 100003 | On | Warning | True | X86 | River | Management | Sling | | x3000c0s16b0n0 | Node | 100004 | Ready | OK | True | X86 | River | Management | Sling | | x3000c0s18b0n0 | Node | 100005 | Ready | OK | True | X86 | River | Management | Sling | | x3000c0s20b0n0 | Node | 100006 | Off | OK | True | X86 | River | Management | Sling | | x3000c0s22b0n0 | Node | 100007 | On | OK | True | X86 | River | Management | Sling | | x3000c0s24b0n0 | Node | 100008 | On | OK | True | X86 | River | Management | Sling | | x3000c0s26b0n0 | Node | 100009 | On | OK | True | X86 | River | Management | Sling | Attention: When the NCNs are brought back online after a power outage or planned shutdown, sat status may report them as being Off.\n  If NCNs are listed as OFF, run a manual discovery of NCNs in the Off state.\nncn-m001# cray hsm inventory discover create --xnames x3000c0s12b0,x3000c0s20b0 [[results]] URI = \u0026#34;/hsm/v2/Inventory/DiscoveryStatus/0\u0026#34;   Check for NCN status.\nncn-m001# sat status --filter Role=Management +----------------+------+--------+-------+------+---------+------+-------+------------+----------+ | xname | Type | NID | State | Flag | Enabled | Arch | Class | Role | Net Type | +----------------+------+--------+-------+------+---------+------+-------+------------+----------+ | x3000c0s10b0n0 | Node | 100001 | On | OK | True | X86 | River | Management | Sling | | x3000c0s12b0n0 | Node | 100002 | On | OK | True | X86 | River | Management | Sling | | x3000c0s14b0n0 | Node | 100003 | On | OK | True | X86 | River | Management | Sling | | x3000c0s16b0n0 | Node | 100004 | Ready | OK | True | X86 | River | Management | Sling | | x3000c0s18b0n0 | Node | 100005 | Ready | OK | True | X86 | River | Management | Sling | | x3000c0s20b0n0 | Node | 100006 | On | OK | True | X86 | River | Management | Sling | | x3000c0s22b0n0 | Node | 100007 | On | OK | True | X86 | River | Management | Sling | | x3000c0s24b0n0 | Node | 100008 | On | OK | True | X86 | River | Management | Sling | | x3000c0s26b0n0 | Node | 100009 | On | OK | True | X86 | River | Management | Sling | +----------------+------+--------+-------+------+---------+------+-------+------------+----------+   To check the health and status of the management cluster after a power cycle, refer to the \u0026ldquo;Platform Health Checks\u0026rdquo; section in Validate CSM Health.\n  If NCNs must have access to Lustre, start the Lustre file system. See Power On the External Lustre File System.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/power_management/power_on_the_external_lustre_file_system/",
	"title": "Power On The External Lustre File System",
	"tags": [],
	"description": "",
	"content": "Power On the External Lustre File System Use this procedure as a general guide to power on an external ClusterStor system. Refer to the detailed procedures that support each ClusterStor hardware and software release:\n ClusterStor E1000 Administration Guide 4.2 - S-2758 for ClusterStor E1000 systems ClusterStor Administration Guide 3.4 - S-2756 for ClusterStor L300, L300N systems ClusterStor Administration Guide - S-2755 for Legacy ClusterStor systems  Power up storage nodes in the following sequence:\n Storage Management Unit (SMU) nodes Metadata server MGS/MDS nodes Object storage server (OSS) nodes  Prerequisites  Facility power must be connected to the PDUs the PDU circuit breakers are set to ON. This procedure assumes that power switches on all storage equipment are set to OFF.  Procedure   Set the System Management Unit (SMU) chassis power switches to ON.\n  Set the Metadata Unit (MDU) chassis power switches to ON.\n  Set the Metadata Management Unit (MMU) or Advanced Metadata Management Unit (AMMU) chassis power switches to ON.\n  Set the object storage server (OSS), scalable storage unit (SSU), extension storage unit (ESU), and Scalable Flash Unit (SFU) chassis power switches to ON.\n  SSH to the primary management node.\nFor example, on system cls01234.\nremote$ ssh -l admin cls01234n000.systemname.com   Check that the shared storage targets are available for the management nodes.\n[n000]$ pdsh -g mgmt cat /proc/mdstat | dshbak -c ---------------- cls01234n000 ---------------- Personalities : [raid1] [raid6] [raid5] [raid4] [raid10] md64 : active raid10 sda[0] sdc[3] sdw[2] sdl[1] 1152343680 blocks super 1.2 64K chunks 2 near-copies [4/4] [UUUU] bitmap: 2/9 pages [8KB], 65536KB chunk md127 : active raid1 sdy[0] sdz[1] 439548848 blocks super 1.0 [2/2] [UU] unused devices: \u0026lt;none\u0026gt; ---------------- cls01234n001 ---------------- Personalities : [raid1] [raid6] [raid5] [raid4] [raid10] md67 : active raid1 sdi[0] sdt[1] 576171875 blocks super 1.2 [2/2] [UU] bitmap: 0/5 pages [0KB], 65536KB chunk md127 : active raid1 sdy[0] sdz[1] 439548848 blocks super 1.0 [2/2] [UU] unused devices: \u0026lt;none\u0026gt;   Check HA status once the node is up and HA configuration has been established.\n[n000]$ sudo crm_mon -1r The output indicates that all resources have started and are balanced between two nodes.\n  In cases when all resources started on a single node (for example, all resources have started on node 00 and did not fail back to node 01, run the failback operation:\n[n000]$ cscli failback –n primary_MGMT_node   As root on the primary management node, power on the MGS and MDS nodes, for example:\n[n000]# cscli power_manage -n cls01234n[02-03] --power-on   Power on the OSS nodes and, if present, the ADU nodes.\n[n000]# cscli power_manage -n oss_adu_nodes --power-on   Check the status of the nodes.\n[n000]# pdsh -a date cls01234n000: Thu Aug 7 01:29:28 PDT 2014 cls01234n003: Thu Aug 7 01:29:28 PDT 2014 cls01234n002: Thu Aug 7 01:29:28 PDT 2014 cls01234n001: Thu Aug 7 01:29:28 PDT 2014 cls01234n007: Thu Aug 7 01:29:28 PDT 2014 cls01234n006: Thu Aug 7 01:29:28 PDT 2014 cls01234n004: Thu Aug 7 01:29:28 PDT 2014 cls01234n005: Thu Aug 7 01:29:28 PDT 2014   Check the health of the system.\n[n000]# cscli csinfo [n000]# cscli show_nodes [n000]# cscli fs_info   Check resources before mounting the file system.\n[n000]# ssh cls01234n000 crm_mon -r1 | grep fsys [n000]# ssh cls01234n002 crm_mon -r1 | grep fsys [n000]# ssh cls01234n004 crm_mon -r1 | grep fsys [n000]# ssh cls01234n006 crm_mon -r1 | grep fsys [n000]# ssh cls01234n008 crm_mon -r1 | grep fsys [n000]# ssh cls01234n010 crm_mon -r1 | grep fsys [n000]# ssh cls01234n012 crm_mon -r1 | grep fsys   Mount the file system.\n[n000]# cscli mount -f cls01234   "
},
{
	"uri": "/docs-csm/en-11/operations/power_management/prepare_the_system_for_power_off/",
	"title": "Prepare The System For Power Off",
	"tags": [],
	"description": "",
	"content": "Prepare the System for Power Off This procedure prepares the system to remove power from all system cabinets. Be sure the system is healthy and ready to be shut down and powered off.\nThe sat bootsys shutdown and sat bootsys boot commands are used to shut down the system.\nPrerequisites An authentication token is required to access the API gateway and to use the sat command. See the System Security and Authentication and \u0026ldquo;SAT Authentication\u0026rdquo; in the Shasta Admin Toolkit (SAT) product documentation.\nProcedure   Obtain the user ID and passwords for system components:\n  Obtain user ID and passwords for all the system management network switches. For example:\nsw-leaf-001 sw-leaf-002 sw-spine-001.nmn sw-spine-002.nmn sw-cdu-001 sw-cdu-002 User id: admin\nPassword: PASSWORD\n  If necessary, obtain the user ID and password for the ClusterStor primary management node. For example, cls01053n00.\nUser id: admin\nPassword: PASSWORD\n  If the Slingshot network includes edge switches, obtain the user ID and password for these switches.\n    Determine which Boot Orchestration Service (BOS) templates to use to shut down compute nodes and UANs. You can list all the session templates using cray bos v1 sessiontemplate list. If you are unsure of which template is in use, you can call sat status to find the xname, then use cray cfs components describe XNAME to find the bos_session, and use cray bos v1 session describe BOS_SESSION to find the templateUuid. Then finally use cray bos v1 sessiontemplate describe TEMPLATE_UUID to determine the list of xnames associated with a given template. For example:\nncn-m001# sat status | grep \u0026#34;Compute\\|Application\u0026#34; | x3000c0s19b1n0 | Node | 1 | On | OK | True | X86 | River | Compute | Sling | | x3000c0s19b2n0 | Node | 2 | On | OK | True | X86 | River | Compute | Sling | | x3000c0s19b3n0 | Node | 3 | On | OK | True | X86 | River | Compute | Sling | | x3000c0s19b4n0 | Node | 4 | On | OK | True | X86 | River | Compute | Sling | | x3000c0s27b0n0 | Node | 49169248 | On | OK | True | X86 | River | Application | Sling | ncn-m001# cray cfs components describe x3000c0s19b1n0 | grep bos_session bos_session = \u0026#34;e98cdc5d-3f2d-4fc8-a6e4-1d301d37f52f\u0026#34; ncn-m001# cray bos v1 session describe e98cdc5d-3f2d-4fc8-a6e4-1d301d37f52f | grep templateUuid templateUuid = \u0026#34;compute-nid1-4-sessiontemplate\u0026#34; ncn-m001# cray bos v1 sessiontemplate describe Nid1-4session-compute | grep node_list node_list = [ \u0026#34;x3000c0s19b1n0\u0026#34;, \u0026#34;x3000c0s19b2n0\u0026#34;, \u0026#34;x3000c0s19b3n0\u0026#34;, \u0026#34;x3000c0s19b4n0\u0026#34;,] ncn-m001# cray cfs components describe x3000c0s27b0n0 | grep bos_session bos_session = \u0026#34;b969c25a-3811-4a61-91d5-f1c194625748\u0026#34; # cray bos v1 session describe b969c25a-3811-4a61-91d5-f1c194625748 | grep templateUuid templateUuid = \u0026#34;uan-sessiontemplate\u0026#34; Compute nodes: compute-nid1-4-sessiontemplate\nUANs: uan-sessiontemplate\n  Use sat auth to authenticate to the API gateway within SAT.\nSee System Security and Authentication, Authenticate an Account with the Command Line, and \u0026ldquo;SAT Authentication\u0026rdquo; in the Shasta Admin Toolkit (SAT) product documentation.\n  Use sat to capture state of the system before the shutdown.\nncn-m001# sat bootsys shutdown --stage capture-state | tee sat.capture-state   Optional system health checks.\n  Use the System Dump Utility (SDU) to capture current state of system before the shutdown.\nImportant: SDU takes about 15 minutes to run on a small system (longer for large systems).\nncn-m001# sdu --scenario triage --start_time \u0026#39;-4 hours\u0026#39; \\ --reason \u0026#34;saving state before powerdown/up\u0026#34;   Capture the state of all nodes.\nncn-m001# sat status | tee sat.status.off   Capture the list of disabled nodes.\nncn-m001# sat status --filter Enabled=false | tee sat.status.disabled   Capture the list of nodes that are off.\nncn-m001# sat status --filter State=Off | tee sat.status.off   Capture the state of nodes in the workload manager, for example, if the system uses Slurm.\nncn-m001# ssh uan01 sinfo | tee uan01.sinfo   Capture the list of down nodes in the workload manager and the reason.\nncn-m001# ssh nid000001-nmn sinfo --list-reasons | tee sinfo.reasons   Check Ceph status.\nncn-m001# ceph -s | tee ceph.status   Check k8s pod status for all pods.\nncn-m001# kubectl get pods -o wide -A | tee k8s.pods Additional k8s status check examples :\nncn-m001# kubectl get pods -o wide -A | egrep \u0026#34;CrashLoopBackOff\u0026#34; \u0026gt; k8s.pods.CLBO ncn-m001# kubectl get pods -o wide -A | egrep \u0026#34;ContainerCreating\u0026#34; \u0026gt; k8s.pods.CC ncn-m001# kubectl get pods -o wide -A | egrep -v \u0026#34;Run|Completed\u0026#34; \u0026gt; k8s.pods.errors   Check HSN status.\nDetermine the name of the slingshot-fabric-manager pod:\nncn-m001# kubectl get pods -l app.kubernetes.io/name=slingshot-fabric-manager -n services NAME READY STATUS RESTARTS AGE slingshot-fabric-manager-5dc448779c-d8n6q 2/2 Running 0 4d21h Run fmn_status in the slingshot-fabric-manager pod and save the output to a file:\nncn-m001# kubectl exec -it -n services slingshot-fabric-manager-5dc448779c-d8n6q \\ -c slingshot-fabric-manager -- fmn_status --details | tee fabric.status   Check management switches to verify they are reachable (switch host names depend on system configuration).\nncn-m001# for switch in sw-leaf-00{1,2}.mtl sw-spine-00{1,2}.mtl sw-cdu-00{1,2}.mtl; \\ do while true; do ping -c 1 $switch \u0026gt; /dev/null; if [[ $? == 0 ]]; then echo \\ \u0026#34;switch $switchis up\u0026#34;; break; else echo \u0026#34;switch $switchis not yet up\u0026#34;; fi; sleep 5; done; done | tee switches   Check Lustre server health.\nncn-m001# ssh admin@cls01234n00.us.cray.com admin@cls01234n00 ~]$ cscli show_nodes   From a node which has the Lustre file system mounted.\nuan01:~ # lfs check servers uan01:~ # lfs df     Check for running sessions.\nncn-m001# sat bootsys shutdown --stage session-checks | tee sat.session-checks Checking for active BOS sessions. Found no active BOS sessions. Checking for active CFS sessions. Found no active CFS sessions. Checking for active CRUS upgrades. Found no active CRUS upgrades. Checking for active FAS actions. Found no active FAS actions. Checking for active NMD dumps. Found no active NMD dumps. Checking for active SDU sessions. Found no active SDU sessions. No active sessions exist. It is safe to proceed with the shutdown procedure. If active sessions are running, either wait for them to complete or shut down/cancel/delete the session.\n  Coordinate with the site to prevent new sessions from starting in the services listed.\nIn version 1.4.x, there is no method to prevent new sessions from being created as long as the service APIs are accessible on the API gateway.\n  Follow the vendor workload manager documentation to drain processes running on compute nodes. For Slurm, the see scontrol man page and for PBS Professional, see the pbsnodes man page.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/power_management/recover_from_a_liquid_cooled_cabinet_epo_event/",
	"title": "Recover From A Liquid Cooled Cabinet Epo Event",
	"tags": [],
	"description": "",
	"content": "Recover from a Liquid Cooled Cabinet EPO Event Identify an emergency power off (EPO) has occurred and restore cabinets to a healthy state.\nCAUTION: Verify the reason why the EPO occurred and resolve that problem before clearing the EPO state.\nIf a Cray EX liquid-cooled cabinet or cooling group experiences an EPO event, the compute nodes may not boot. Use CAPMC to force off all the chassis affected by the EPO event.\nProcedure   Verify that the EPO event did not damage the system hardware.\n  From ncn-m001, check the status of the chassis.\nncn-m001# cray capmc get_xname_status create --xnames x9000c[1,3] e = 0 err_msg = \u0026#34; off = [ \u0026#34;x9000c1\u0026#34;, \u0026#34;x9000c3\u0026#34;,]   Check the Chassis Controller Module (CCM) log for Critical messages and the EPO event.\nA cabinet has eight chassis.\nncn-m001# kubectl logs -n services -l app.kubernetes.io/name=cray-capm \\ -c cray-capmc --tail -1 | grep EPO -A 10 2019/10/24 02:37:30 capmcd.go:805: Message: Can not issue Enclosure Chassis.Reset \u0026#39;On\u0026#39;|\u0026#39;Off\u0026#39; while in EPO state 2019/10/24 02:37:30 capmcd.go:808: ExtendedInfo.Message: Can not issue Enclosure Chassis.Reset \u0026#39;On\u0026#39;|\u0026#39;Off\u0026#39; while in EPO state 2019/10/24 02:37:30 capmcd.go:809: ExtendedInfo.Resolution: Verify physical hardware, issue Enclosure Chassis.Reset --\u0026gt; \u0026#39;ForceOff\u0026#39;, and resubmit the request 2019/10/24 02:37:31 capmcd.go:136: Info: \u0026lt;-- Bad Request (400) POST https://x1000c7b0/redfish/v1/ Chassis/Enclosure/Actions/Chassis.Reset (1.045967005s) 2019/10/24 02:37:31 capmcd.go:799: POST https://x1000c7b0/redfish/v1/Chassis/Enclosure/Actions/Chassis.Reset !HTTP Error!   Disable the hms-discovery Kubernetes cron job.\nncn-m001# kubectl -n services patch cronjobs hms-discovery -p \u0026#39;{\u0026#34;spec\u0026#34; : {\u0026#34;suspend\u0026#34; : true }}\u0026#39; CAUTION: Do not power the system on until it is safe to do so. Determine why the EPO event occurred before clearing the EPO state.\n  If it is safe to power on the hardware, clear all chassis in the EPO state in the cooling group.\nAll chassis in cabinets 1000-1003 are forced off in this example. Power off all chassis in a cooling group simultaneously, or the EPO condition may persist.\nncn-m001# cray capmc xname_off create --xnames x[1000-1003]c[0-7] --force true e = 0 err_msg = \u0026#34; The HPE Cray EX EX TDS cabinet contains only two chassis: 1 (bottom) and 3 (top).\nncn-m001# cray capmc xname_off create --xnames x9000c[1,3] --force true e = 0 err_msg = \u0026#34;   Restart the hms-discovery cron job.\nncn-m001# kubectl -n services patch cronjobs hms-discovery -p '{\u0026quot;spec\u0026quot; : {\u0026quot;suspend\u0026quot; : false }}' About 5 minutes after hms-discovery restarts, the service will power on the chassis enclosures, switches, and compute blades. If components are not being powered back on, then power them on manually.\nncn-m001# cray capmc xname_on create \\ --xnames x[1000-1003]c[0-7]r[0-7],x[1000-1003]c[0-7]s[0-7] --prereq true --continue true e = 0 err_msg = \u0026#34;   After the components have powered on, boot the nodes using the Boot Orchestration Services (BOS).\nSee Power On and Boot Compute and User Access Nodes.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/power_management/ignore_nodes_with_capmc/",
	"title": "Ignore Nodes With CAPMC",
	"tags": [],
	"description": "",
	"content": "Ignore Nodes with CAPMC Update the Cray Advanced Platform Monitoring and Control (CAPMC) configmap to ignore non-compute nodes (NCNs) and ensure they cannot be powered off or reset.\nModifying the CAPMC configmap to ignore nodes can prevent them from accidentally being power cycled.\nNodes can also be locked with the Hardware State Manager (HSM) API. Refer to Lock and Unlock Management Nodes for more information.\nPrerequisites This procedure requires administrative privileges.\nProcedure   Edit the CAPMC configmap.\nncn-m001# kubectl -n services edit configmaps cray-capmc-configuration   Uncomment the # BlockRole = [\u0026ldquo;Management\u0026rdquo;] value in the [PowerControls.Off], [PowerControls.On], and [PowerControls.ForceRestart] sections.\n  Save and quit the configmap.\nCAPMC restarts using the new configmap.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/power_management/liquid_cooled_node_card_power_management/",
	"title": "Liquid Cooled Node Power Management",
	"tags": [],
	"description": "",
	"content": "Liquid Cooled Node Power Management Liquid Cooled AMD EPYC compute blade node card power capabilities and capping.\nLiquid Cooled cabinet node card power features are supported by the node controller (nC) firmware and CPU vendor. The nC exposes the power control API for each node via the node\u0026rsquo;s Redfish ChassisPower schema. Out-of-band power management data is produced and collected by the nC hardware and firmware. This data can be published to a collector using the Redfish EventService, or retrieved on-demand from the Redfish ChassisSensors resource.\nThe Cray Advanced Platform Management and Control (CAPMC) API facilitates power control and enables power-aware WLMs such as Slurm to perform power management and power capping tasks.\nAlways use the Boot Orchestration Service (BOS) to power off or power on compute nodes.\nRedfish API The Redfish API for Liquid Cooled compute blades is the node\u0026rsquo;s Chassis Power resource which is presented by the nC. OEM properties are used to augment the Power schema and allow for feature parity with previous Cray system power management capabilities. A PowerControl resource presents the various power management capabilities for the node.\nEach node has three or more power control resources:\n Node power control CPU power control Memory power control Accelerator power control (one resource per accelerator connected to the node)  The PowerControl resources will only manifest in the nC\u0026rsquo;s Redfish endpoint after a node has been powered on and background processes have discovered the node\u0026rsquo;s power management capabilities.\nPower Capping CAPMC power capping controls for compute nodes can query component capabilities and manipulate the node power constraints. This functionality enables external software to establish an upper bound, or estimate a minimum bound, on the amount of power a system or a select subset of the system may consume.\nCAPMC API calls provide means for third party software to implement advanced power management strategies and JSON functionality can send and receive customized JSON data structures.\nThe AMD EPYC node card supports these power capping and monitoring API calls:\n get_power_cap_capabilities get_power_cap set_power_cap get_node_energy get_node_energy_stats get_node_energy_counter get_system_power get_system_power_details  Cray CLI Examples for Liquid Cooled Compute Node Power Management   Get Node Energy\nncn-m001# cray capmc get\\_node\\_energy create --nids NID\\_LIST \\\\ --start-time '2020-03-04 12:00:00' --end-time '2020-03-04 12:10:00' --format json   Get Node Energy Stats\nncn-m001# cray capmc get\\_node\\_energy\\_stats create --nids NID\\_LIST \\\\ --start-time '2020-03-04 12:00:00' --end-time '2020-03-04 12:10:00' --format json   Get Node Energy Counter\nncn-m001# cray capmc get\\_node\\_energy\\_counter create --nids NID\\_LIST \\\\ --time '2020-03-04 12:00:00' --format json   Get Node Power Control and Limit Settings\nncn-m001# cray capmc get\\_power\\_cap create –-nids NID\\_LIST \\\\ --format json   Get System Power\nncn-m001# cray capmc get\\_system\\_power create \\\\ --start-time '2020-03-04 12:00:00' --window-len 30 --format json ncn-m001# cray capmc get\\_system\\_power\\_details create \\\\ --start-time '2020-03-04 12:00:00' --window-len 30 --format json   Get Power Capping Capabilities\nncn-m001# cray capmc get\\_power\\_cap\\_capabilities create –-nids NID\\_LIST \\\\ --format json   Set Node Power Limit\nncn-m001# cray capmc set\\_power\\_cap create –-nids NID\\_LIST \\\\ --node 225 --format json   Remove Node Power Limit (Set to Default)\nncn-m001# cray capmc set\\_power\\_cap create –-nids NID\\_LIST \\\\ --node 0 --format json   "
},
{
	"uri": "/docs-csm/en-11/operations/power_management/power_off_compute_and_io_cabinets/",
	"title": "Power Off Compute And Io Cabinets",
	"tags": [],
	"description": "",
	"content": "Power Off Compute and IO Cabinets Power off HPE Cray EX liquid-cooled and standard racks.\nLiquid-cooled Cabinets - HPE Cray EX liquid-cooled cabinet CDU and PDU circuit breakers are controlled manually.\nWhen the PDU breakers are switched to OFF, the Chassis Management Modules (CMMs) and Cabinet Environmental Controllers (CECs) are also powered off.\nWarning: The cabinet 480VAC power bus bars remain energized. Facility power must be disconnected to completely remove power from the cabinet. Follow lockout-tagout procedures for the site before maintenance.\nStandard Racks - HPE Cray standard EIA racks typically include 2 redundant PDUs. Some PDU models may require a flat-blade screw driver to open or close the PDU circuit breakers.\nWarning: The cabinet PDUs remain energized when circuit breakers are OFF. Facility power must be disconnected or the PDUs must be unplugged to completely remove power from the rack. Follow lockout-tagout procedures for the site before maintenance.\nPrerequisites  An authentication token is required to access the API gateway. See the System Security and Authentication for more information. This procedure assumes all system software and user jobs were shut down using the Shut Down and Power Off Compute and User Access Nodes (UAN) procedure.  Procedure  If the system does not include Cray EX liquid-cooled cabinets, proceed to step 9.  POWER OFF CRAY EX LIQUID-COOLED CABINETS\n Check CDU control panel for alerts or warnings and resolve any issues before continuing.\n  Check the power status before shutdown, this example shows cabinets 1000-1003.\nncn-m001# cray capmc get_xname_status create --xnames x[1000-1003]c[0-7] --format json   Use sat bootsys shutdown to shut down services and power off liquid-cooled cabinets.\nncn-m001# sat bootsys shutdown --stage cabinet-power This command suspends the hms-discovery cron job and recursively powers off the liquid-cooled cabinet chassis.\n  Verify that the hms-discovery cron job has been suspended (SUSPEND column = true).\nncn-m001# kubectl get cronjobs -n services hms-discovery NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE^M hms-discovery */3 * * * * True 0 117s 15d   Check the power off status, this example shows cabinets 1000-1003.\nncn-m001# cray capmc get_xname_status create --xnames x[1000-1003]c[0-7] --format json Rectifiers (PSUs) should indicate that DC power is OFF (AC OK is on).\n  Set the cabinet PDU circuit breakers to OFF for each shelf.\nThe AC OK LED on each PSU will remain amber for about 30 seconds (AC lost) until the system de-energizes, then extinguish.\nNote: If the TDS cabinet rack-mounted coolant distribution unit (MCDU) is receiving power from the PDUs in the management cabinet, the MCDU may stay on after the TDS cabinet PDU circuit breakers are set to OFF. This is expected.\nCAUTION: Do not power off the CDU if it is actively cooling other equipment.\n  If other systems are not being cooled by the floor-standing CDU, open the CDU rear door to access the control panel and set the circuit breakers to OFF.\n  POWER OFF STANDARD RACK PDU CIRCUIT BREAKERS\n  Check the power status before shutdown, this example shows nodes in cabinets 3001-3003.\nncn-m001# cray capmc get_xname_status create --xnames x300[1-3]c0s[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35]b[1-4]n0 --format json The get_xname_status command requires that the list of components be explicitly listed. In this example, the system includes only 2U servers and there are no state manager entries for even-numbered U-positions (slots), which would return an error.\nThe command does not filter nonexistent component names (xnames) and displays an error when invalid component names are specified. Use --filter show_all option to filter all the output:\nncn-m001# cray capmc get_xname_status create --filter show_all { \u0026#34;e\u0026#34;: 0, \u0026#34;err_msg\u0026#34;: \u0026#34;, \u0026#34;off\u0026#34;: [ \u0026#34;x3000c0s19b0n0\u0026#34;, \u0026#34;x3000c0s20b0n0\u0026#34;, \u0026#34;x3000c0s22b0n0\u0026#34;, \u0026#34;x3000c0s24b0n0\u0026#34;, \u0026#34;x3000c0s27b1n0\u0026#34;, \u0026#34;x3000c0s27b2n0\u0026#34;, \u0026#34;x3000c0s27b3n0\u0026#34;, \u0026#34;x3000c0s27b4n0\u0026#34; ], \u0026#34;on\u0026#34;: [ \u0026#34;x3000c0r15e0\u0026#34;, \u0026#34;x3000c0s2b0n0\u0026#34;, \u0026#34;x3000c0s3b0n0\u0026#34;, \u0026#34;x3000c0s4b0n0\u0026#34;, \u0026#34;x3000c0s5b0n0\u0026#34;, \u0026#34;x3000c0s6b0n0\u0026#34;, \u0026#34;x3000c0s7b0n0\u0026#34;, \u0026#34;x3000c0s8b0n0\u0026#34;, \u0026#34;x3000c0s9b0n0\u0026#34; ] }   Use CAPMC to power off non-management nodes HPE Cray standard racks.\nCAUTION: Do not power off the management cabinet. Verify the components names (xnames) specified in the following command line do not accidentally power off management cabinets.\nThis example shuts down racks 3001-3003.\nncn-m001# cray capmc xname_off create --xnames x300[1-3]c0s[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35]b[1-4]n0   Check the status of the CAPMC power off command.\nncn-m001# cray capmc get_xname_status create --xnames x300[1-3]c0s[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35]b[1-4]n0 --format json   Set each cabinet PDU circuit breaker to off.\nA slotted screw driver may be required to open PDU circuit breakers.\n  To power off Motivair liquid-cooled chilled doors and CDU, locate the power off switch on the CDU control panel and set it to OFF as shown in step 8.\nRefer to vendor documentation for the chilled-door cooling system for power control procedures when chilled doors are installed on standard racks.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/power_management/power_off_the_external_lustre_file_system/",
	"title": "Power Off The External Lustre File System",
	"tags": [],
	"description": "",
	"content": "Power Off the External Lustre File System General procedure for powering off an external ClusterStor system.\nUse this procedure as a general guide to power off an external ClusterStor system. Refer to the detailed procedures in the appropriate ClusterStor administration guide:\n   Title Model     ClusterStor E1000 Administration Guide 4.2 - S-2758 ClusterStor E1000   ClusterStor Administration Guide 3.4 - S-2756 ClusterStor L300/L300N   ClusterStor Administration Guide - S-2755 Legacy ClusterStor    Procedure   SSH to the primary MGMT node as admin.\nremote$ ssh -l admin cls01234n00.us.cray.com   Change to root user.\nadmin@n000$ sudo su –   Collect status information for the system before shutdown.\nn000# cscli csinfo n000# cscli show_nodes n000# cscli fs_info n000# crm_mon -1r   Check resources before unmounting the file system.\nn000# ssh cls01234n002 crm_mon -r1 | grep fsys n000# ssh cls01234n004 crm_mon -r1 | grep fsys n000# ssh cls01234n006 crm_mon -r1 | grep fsys n000# ssh cls01234n008 crm_mon -r1 | grep fsys n000# ssh cls01234n010 crm_mon -r1 | grep fsys n000# ssh cls01234n012 crm_mon -r1 | grep fsys . . .   Stop the Lustre file system.\n[n000]# cscli unmount -f FILESYSTEM_NAME   Verify that resources have been stopped by running the following on all even-numbered nodes:\n[n000]# ssh NODENAME crm_mon -r1 | grep fsys cls01234n006_md0-fsys (ocf::heartbeat:XYMNTR): Stopped cls01234n006_md1-fsys (ocf::heartbeat:XYMNTR): Stopped cls01234n006_md2-fsys (ocf::heartbeat:XYMNTR): Stopped cls01234n006_md3-fsys (ocf::heartbeat:XYMNTR): Stopped cls01234n006_md4-fsys (ocf::heartbeat:XYMNTR): Stopped cls01234n006_md5-fsys (ocf::heartbeat:XYMNTR): Stopped cls01234n006_md6-fsys (ocf::heartbeat:XYMNTR): Stopped cls01234n006_md7-fsys (ocf::heartbeat:XYMNTR): Stopped   SSH to the MGS node.\nMGS# ssh MGS_NODE   To determine if Resource Group md65-group is stopped, use the crm_mon utility to monitor the status of the MGS and MDS nodes.\nShows MGS and MDS nodes in a partial stopped state.\n[MGS]# crm_mon -1r | grep fsys cls01234n003_md66-fsys (ocf::heartbeat:XYMNTR): Stopped cls01234n003_md65-fsys (ocf::heartbeat:XYMNTR): Started   If the node is not stopped, issue the stop_xyraid command and verify that the node is stopped:\n[MGS]# stop_xyraid nodename_md65-group [MGS]# crm\\_mon -1r | grep fsys cls01234n003_md66-fsys (ocf::heartbeat:XYMNTR): Stopped cls01234n003_md65-fsys (ocf::heartbeat:XYMNTR): Stopped   Exit the MGS node.\nMGS# exit   Power off the non-MGMT diskless nodes.\nn000# cscli power_manage -n DISKLESS_NODES[XX-YY --power-off   Check power state of all non-MGMT nodes and list the node hostnames (in this example cls01234n[02-15]) before power off.\nn000# pm –q on: cls01234n[000-001] on: cls01234n[002-015] unknown:   Power off all non-MGMT nodes.\n[n00]$ cscli power_manage -n cls01234n[02-15] --power-off   Check the power status of the nodes.\nn000# pm –q on: cls01234n[000-001] off: cls01234n[002-015] unknown:   Repeat step 14 until all non-MGMT nodes are powered off.\n  From the primary MGMT node, power off the MGMT nodes:\nn000# cscli power_manage -n cls01234n[000-001] --power-off   Shut down the primary management node.\nn000# shutdown -h now   "
},
{
	"uri": "/docs-csm/en-11/operations/power_management/power_on_compute_and_io_cabinets/",
	"title": "Power On Compute And Io Cabinets",
	"tags": [],
	"description": "",
	"content": "Power On Compute and IO Cabinets Power on liquid-cooled and standard rack cabinet PDUs.\nLiquid-cooled Cabinets - HPE Cray EX liquid-cooled cabinet CDU and PDU circuit breakers are controlled manually.\nAfter the CDU is switched on and healthy, the liquid-cooled PDU circuit breakers can be switched ON. With PDU breakers ON, the Chassis Management Modules (CMM) and Cabinet Environmental Controllers (CEC) power on and boot. These devices can then communicate with the management cluster and larger system management network. HVDC power remains OFF on liquid-cooled chassis until environmental conditions are normal and the CMMs receive a chassis power-on command from Cray System Management (CSM) software.\nStandard Racks - HPE Cray standard EIA racks include redundant PDUs. Some PDU models may require a flat-blade screw driver to open or close the PDU circuit breakers.\nPrerequisites  The cabinet PDUs and coolant distribution units are connected to facility power and are healthy. An authentication token is required to access the API gateway and to use the sat command. See the System Security and Authentication and \u0026ldquo;SAT Authentication\u0026rdquo; in the Shasta Admin Toolkit (SAT) product documentation.  Procedure   Verify with site management that it is safe to power on the system.\n  If the system does not have Cray EX liquid-cooled cabinets, proceed to step 7.\n  POWER ON CRAY EX LIQUID-COOLED CABINET CIRCUIT BREAKERS\n Power on the CDU for the cabinet cooling group.\n  Open the rear door of the CDU.\n  Set the control panel circuit breakers to ON.\n    Set the PDU circuit breakers to on in each Cray EX cabinet.\n  Verify the status LEDs on the PSU are OK.\n  Use the System Admin Toolkit (sat) to power on liquid-cooled cabinets chassis and slots.\nncn-m001# sat bootsys boot --stage cabinet-power This command resumes the hms-discovery job which initiates power-on of the liquid-cooled cabinets. The --stage cabinet-power option controls power only to liquid-cooled cabinets.\nIf sat bootsys fails to schedule hms-discovery with the following message, then delete and recreate the cron job.\nERROR: The cronjob hms-discovery in namespace services was not scheduled within expected window after being resumed. If sat bootsys fails to power on the cabinets through hms-discovery, use CAPMC to manually power on the cabinet chassis, compute blade slots, and all populated switch blade slots (1, 3, 5, and 7). This example shows cabinets 1000-1003.\nncn-m001# cray capmc xname_on create --xnames x[1000-1003]c[0-7] --format json ncn-m001# cray capmc xname_on create --xnames x[1000-1003]c[0-7]s[0-7] --format json ncn-m001# cray capmc xname_on create --xnames x[1000-1003]c[0-7]r[1,3,5,7] --format json   POWER ON STANDARD RACK PDU CIRCUIT BREAKERS\n Switch the standard rack compute and I/O cabinet PDU circuit breakers to ON.\nThis applies power to the server BMCs connects them to the management network. Compute and I/O nodes do not power on and boot automatically. The Boot Orchestration Service (BOS) brings up compute nodes and User Access Nodes (UANs).\nIf necessary, use IPMI commands to power on individual servers as needed.\n  Make sure all system management network switches and Slingshot network switches are powered on in each rack and there are no error LEDS or hardware failures.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/power_management/bring_up_the_slingshot_fabric/",
	"title": "Bring Up The Slingshot Fabric",
	"tags": [],
	"description": "",
	"content": "Bring Up the Slingshot Fabric This procedure assumes the Slingshot fabric is installed an configured. The slingshot-fabric-manager software controls the fabric. On systems running Kubernetes, the slingshot-fabric-manager pod controls the fabric.\nPrerequisites The fabric manager software is installed and configured.\nProcedure   From the Kubernetes management node, determine the name of the fabric manager pod (FMN).\nncn-m001# kubectl get pods -l app.kubernetes.io/name=slingshot-fabric-manager -n services NAME READY STATUS RESTARTS AGE slingshot-fabric-manager-5dc448779c-d8n6q 2/2 Running 0 4d21h   Open a shell to access the fabric manager pod (in this example, slingshot-fabric-manager-5dc448779c-d8n6q).\nncn-m001# kubectl exec -it slingshot-fabric-manager-5dc448779c-d8n6q -n services -- /bin/bash slingshot-fabric-manager:#   Bring up the Slingshot fabric.\nslingshot-fabric-manager:# fmn_fabric_bringup -c   Check the status of the fabric.\nFrom within in the fabric manager pod:\nslingshot-fabric-manager:# fmn_status --details To save fabric status to a file from a management node:\nncn-m001# kubectl exec -it -n services slingshot-fabric-manager-5dc448779c-d8n6q \\ -c slingshot-fabric-manager -- fmn_status --details \u0026gt; fabric.status   "
},
{
	"uri": "/docs-csm/en-11/operations/power_management/cray_advanced_platform_monitoring_and_control_capmc/",
	"title": "Cray Advanced Platform Monitoring And Control (CAPMC)",
	"tags": [],
	"description": "",
	"content": "Cray Advanced Platform Monitoring and Control (CAPMC) CAPMC provides remote monitoring and hardware on/off control.\nThe Cray Advanced Platform Monitoring and Control (CAPMC) API enables direct hardware control of power on/off, power monitoring, or system-wide power telemetry and configuration parameters from Redfish. CAPMC implements a simple interface for powering on/off compute nodes, querying node state information, and querying site-specific service usage rules. These controls enable external software to more intelligently manage system-wide power consumption or configuration parameters.\nRefer to the CAPMC API documentation for detailed API information.\nThe current release of CAPMC supports the following power control features:\n Retrieve Redfish power status and power management capabilities of components Control single components via NID or xname Control grouped components Control the entire system (all or s0) Can specify ancestors (\u0026ndash;prereq) and descendants (\u0026ndash;recursive) of single component Provide a \u0026ndash;force option for immediate power off Power capping  Power sequencing using CAPMC assumes that all cabinets and PDUs have been plugged in, breakers are on, and PDU controllers, BMCs, and other embedded controllers are on and available. CAPMC provides a default order for components to powering on, but the power sequence can be configured.\nPower management strategies may vary and can be simple or complex using 3rd party software. A simple power management strategy is to power off idle compute nodes, then power on nodes when demand increases.\nThe cray CLI can be used from any system that has HTTPS access to System Management Services. Refer to the CAPMC API documentation for detailed information about API options and features.\nThe cray capmc command (see --help) can be used to control power to specific components by specifying the component NID, xname, or group.\nComponents that Can be Controlled with CAPMC in Shasta v1.4 Air Cooled Cabinets\n Compute Nodes  Liquid Cooled Cabinets\n Chassis Slingshot Switch blades Compute blades Compute nodes  Component Groups CAPMC uses xnames to specify entire cabinets or specific components throughout the system. By default, CAPMC controls power to only one component at a time. A --recursive option can be passed to CAPMC using the cray CLI. When the --recursive option is included in a request, all of the sub-components of the target component are included in the power command.\nThe cabinet naming convention assigns a number to each cabinet in the system. Cabinets can be located anywhere on the computer room floor, although manufacturing typically follows a sequential cabinet numbering scheme:\n Liquid Cooled cabinet numbers: x1000–x2999 Air Cooled cabinet numbers: x3000–x4999 Liquid Cooled TDS cabinet numbers: x5000–5999  Cabinet numbers can range from 0-9999 and contain from 1–4 digits only.\nFull system: s0, all\nCabinet numbers: x1000, x3000, x5000\nChassis numbers 0-7: x1000c7, x3500c0 (Air Cooled cabinets are always chassis 0)\nCompute Blade Slots:x1000c7s3, x3500c0s15 (U15)\nPower Capping CAPMC power capping controls for compute nodes can query component capabilities and manipulate the node power constraints. This functionality enables external software to establish an upper bound, or estimate a minimum bound, on the amount of power a system may consume.\nCAPMC API calls provide means for third party software to implement advanced power management strategies and JSON functionality can send and receive customized JSON data structures.\nAir Cooled nodes support these power capping and monitoring API calls:\n get_power_cap_capabilities get_power_cap set_power_cap get_node_energy get_node_energy_stats get_system_power  Examples for Liquid Cooled Compute Node Power Management Get Node Energy\nncn-m001# cray capmc get\\_node\\_energy create --nids NID\\_LIST --start-time '2020-03-04 12:00:00' \\\\ --end-time '2020-03-04 12:10:00' --format json Get Node Energy Stats\nncn-m001# cray capmc get\\_node\\_energy\\_stats create --nids NID\\_LIST --start-time \\\\ '2020-03-04 12:00:00' --end-time '2020-03-04 12:10:00' --format json Get Node Power Control and Limit Settings\nncn-m001# cray capmc get\\_power\\_cap create –-nids NID\\_LIST --format json Get System Power\nncn-m001# cray capmc get\\_system\\_power create --start-time \\\\ '2020-03-04 12:00:00' --window-len 30 --format json Get Power Capping Capabilities\nThe supply field contains the Max limit for the node.\nncn-m001# cray capmc get\\_power\\_cap\\_capabilities create –-nids NID\\_LIST --format json Set Node Power Limit\nncn-m001# cray capmc set\\_power\\_cap create –-nids NID\\_LIST --node 225 --format json Remove Node Power Limit (Set to Default)\nncn-m001# cray capmc set\\_power\\_cap create –-nids NID\\_LIST --node 0 --format json Activate Node Power Limit\n# curl -k -u $login:$pass -H \u0026quot;Content-Type: application/json\u0026quot; \\\\ -X POST https://$BMC\\_IP/redfish/v1/Chassis/Self/Power/Actions/LimitTrigger --date '\\{\u0026quot;PowerLimitTrigger\u0026quot;: \u0026quot;Activate\u0026quot;\\}' Deactivate Node Power Limit\n# curl -k -u $login:$pass -H \u0026quot;Content-Type: application/json\u0026quot; \\\\ -X POST https://$BMC\\_IP/redfish/v1/Chassis/Self/Power/Actions/LimitTrigger --data '\\{\u0026quot;PowerLimitTrigger\u0026quot;: \u0026quot;Deactivate\u0026quot;\\}' Power On/Off Examples Power Off a Cabinet\nncn-m001# cray capmc xname\\_off create --xnames x1000 --recursive --format json Power Off a Chassis 0 and Its Descendents\nncn-m001# cray capmc xname\\_off create --xnames x1000c0 --recursive --format json Power Off Node 0 in Cabinet 1000, Chassis, 0, Slot 0, Node Card 0\nncn-m001# cray capmc xname\\_off create --xnames x1000c0s0b0n0 --format json Emergency Power Off (EPO) CLI Command\nncn-m001# cray capmc emergency\\_power\\_off –-xnames LIST\\_OF\\_CHASSIS --force --format json To recover or \u0026ldquo;reset\u0026rdquo; the components after a software EPO, set the chassis to a known hardware state (off). The cabinet(s) can then be powered on normally after the EPO is cleared. For a complete procedure, see Recover from a Liquid Cooled Cabinet EPO Event.\nncn-m001# cray capmc xname\\_off create --xnames LIST\\_OF\\_CHASSIS --force true e = 0 err_msg = \u0026quot; "
},
{
	"uri": "/docs-csm/en-11/operations/package_repository_management/repair_yum_repository_metadata/",
	"title": "Repair Yum Repository Metadata",
	"tags": [],
	"description": "",
	"content": "Repair Yum Repository Metadata Nexus may have trouble (re)generating repository metadata (for example, repodata/repomd.xml), especially for larger repositories. Configure the Repair - Rebuild Yum repository metadata (repodata) task in Nexus to create the metadata if the standard generation fails. This is not typically needed, which makes it a repair task.\nThe example in this procedure is for creating a repair task to rebuild Yum metadata for the mirror-1.3.0-opensuse-leap-15 repository.\nSee the Nexus documentation on Tasks for more details: https://help.sonatype.com/repomanager3/system-configuration/tasks\nPrerequisites The system is fully installed.\nProcedure   Log in to the Nexus web UI.\nUse the hostname set in istio.ingress.hosts.ui.authority to connect to Nexus over the Customer Access Network (CAN) using a web browser. For example:\nhttps://nexus.{{network.dns.external}}/ Users will be redirected to Keycloak to log in, and based on the default OPA policy, only admin users will be authorized. Scripts may connect by first obtaining a JWT token from Keycloak and passing it in the HTTP Authorization header.\n  Click on the gear icon at the top of the page.\nClicking on the gear will open up the repository administration page.\n  Click on Tasks in the navigation bar on the left-hand side of the page.\nThe Tasks button is under the System heading.\n  Click the Create Task button.\nClicking the Create Task button will open up the following page. Select the type of task. For this example, the Create Repair - Rebuild Yum repository metadata (repodata) option would be selected.\n  Enter the required information for the task, such as the task name, repository, and task frequency.\nClick the Create task at the bottom of the page after entering all required information about the task.\nThe new task will now be available on the main Tasks page.\n  Click on the newly created task.\n  Click the Run button at the top of the page.\nSelect Yes when the confirmation pop-up appears.\nThere will now be information about the run that was just scheduled on the main Tasks page.\n  Click on the box icon in the navigation bar at the top of the page, then click Browse in the navigation bar on the left-hand side of the page..\n  Track down the name of the repository being repaired.\nIn this example, mirror-1.3.0-opensuse-leap-15 is used.\n  View the repodata for the repository.\n  View the log file on the system.\nEven though the Nexus logs contains messages pertaining to tasks, it can be difficult to track messages for a specific task, especially because rebuilding Yum metadata takes a long time.\n  Retrieve the Nexus pod name.\nncn# kubectl -n nexus get pods | grep nexus nexus-55d8c77547-65k6q 2/2 Running 1 22h   Use kubectl exec to access the running nexus pod.\nncn# kubectl -n nexus exec -ti nexus-55d8c77547-65k6q \\ -c nexus -- ls -ltr /nexus-data/log/tasks total 8 -rw-r--r-- 1 nexus nexus 1763 Aug 23 00:50 repository.yum.rebuild.metadata-20200822235306934.log -rw-r--r-- 1 nexus nexus 1525 Aug 23 01:00 repository.cleanup-20200823010000013.log If multiple repositories are being rebuilt, search the logs for the specific repository to find the latest corresponding log file. The example below is for mirror-1.3.0-opensuse-leap-15:\nncn# kubectl -n nexus exec -ti nexus-55d8c77547-65k6q \\ -c nexus -- grep -R \u0026#39;Rebuilding yum metadata for repository mirror-1.3.0-opensuse-leap-15\u0026#39; \\ /nexus-data/log/tasks /nexus-data/log/tasks/repository.yum.rebuild.metadata-20200822235306934.log:2020-08-22 23:53:06,936+0000 INFO [event-12-thread-797] *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl - Rebuilding yum metadata for repository mirror-1.3.0-opensuse-leap-15   View the log file for the rebuild.\nThe log file for a successful rebuild will look similar to the following:\nncn# kubectl -n nexus exec -ti nexus-55d8c77547-65k6q \\ -c nexus -- cat /nexus-data/log/tasks/repository.yum.rebuild.metadata-20200822235306934.log 2020-08-22 23:53:06,934+0000 INFO [quartz-9-thread-20] *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask - Task information: 2020-08-22 23:53:06,935+0000 INFO [quartz-9-thread-20] *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask - ID: 35536bcd-3947-4ba9-8d6d-43dcadbb87ad 2020-08-22 23:53:06,935+0000 INFO [quartz-9-thread-20] *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask - Type: repository.yum.rebuild.metadata 2020-08-22 23:53:06,935+0000 INFO [quartz-9-thread-20] *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask - Name: Rebuild Yum metadata - mirror-1.3.0-opensuse-leap-15 2020-08-22 23:53:06,935+0000 INFO [quartz-9-thread-20] *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask - Description: Rebuild metadata for mirror-1.3.0-opensuse-leap-15 2020-08-22 23:53:06,936+0000 INFO [quartz-9-thread-20] *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask - Task log: /nexus-data/log/tasks/repository.yum.rebuild.metadata-20200822235306934.log 2020-08-22 23:53:06,936+0000 INFO [event-12-thread-797] *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl - Rebuilding yum metadata for repository mirror-1.3.0-opensuse-leap-15 2020-08-22 23:53:06,936+0000 INFO [quartz-9-thread-20] *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask - Task complete 2020-08-23 00:50:47,468+0000 INFO [event-12-thread-797] *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl - Finished rebuilding yum metadata for repository mirror-1.3.0-opensuse-leap-15 The returned Finished rebuilding yum metadata for repository without any other ERROR or WARN messages indicates the rebuild has completed successfully.\nIn this case, it took nearly 58 minutes to finish. The time it takes to run is related to the size of the repository, so expect the mirror-1.3.0- repositories to take a while.\n    Check the repodata for the repository again in the web UI.\n  Troubleshooting: When a rebuild fails, expect to see ERROR and WARN messages around the same time as the Finished rebuilding yum metadata for repository message. For example, consider the log from a failed rebuild of mirror-1.3.0-opensuse-leap-15:\nncn# kubectl -n nexus exec -ti nexus-55d8c77547-65k6q -c nexus -- \\ cat /nexus-data/log/tasks/repository.yum.rebuild.metadata-20200822231259523.log 2020-08-22 23:12:59,523+0000 INFO [quartz-9-thread-20] *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask - Task information: 2020-08-22 23:12:59,526+0000 INFO [quartz-9-thread-20] *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask - ID: 35536bcd-3947-4ba9-8d6d-43dcadbb87ad 2020-08-22 23:12:59,526+0000 INFO [quartz-9-thread-20] *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask - Type: repository.yum.rebuild.metadata 2020-08-22 23:12:59,526+0000 INFO [quartz-9-thread-20] *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask - Name: Rebuild Yum metadata - mirror-1.3.0-opensuse-leap-15 2020-08-22 23:12:59,527+0000 INFO [quartz-9-thread-20] *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask - Description: Rebuild metadata for mirror-1.3.0-opensuse-leap-15 2020-08-22 23:12:59,529+0000 INFO [quartz-9-thread-20] *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask - Task log: /nexus-data/log/tasks/repository.yum.rebuild.metadata-20200822231259523.log 2020-08-22 23:12:59,529+0000 INFO [event-12-thread-780] *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl - Rebuilding yum metadata for repository mirror-1.3.0-opensuse-leap-15 2020-08-22 23:12:59,531+0000 INFO [quartz-9-thread-20] *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask - Task complete 2020-08-22 23:24:16,974+0000 INFO [Thread-1948 \u0026lt;command\u0026gt;sql.select from asset where (component IS NOT NULL AND attributes.yum.asset_kind = :p0) and (bucket=#59:1)\u0026lt;/command\u0026gt;] *SYSTEM com.orientechnologies.common.profiler.OProfilerStub - $ANSI{green {db=component}} [TIP] Query \u0026#39;SELECT FROM asset WHERE (component IS NOT NULL AND attributes.yum.asset_kind = \u0026#34;RPM\u0026#34; ) AND (bucket = #59:1 )\u0026#39; returned a result set with more than 10000 records. Check if you really need all these records, or reduce the resultset by using a LIMIT to improve both performance and used RAM 2020-08-22 23:29:57,700+0000 INFO [event-12-thread-780] *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl - Finished rebuilding yum metadata for repository mirror-1.3.0-opensuse-leap-15 2020-08-22 23:29:57,701+0000 ERROR [event-12-thread-780] *SYSTEM com.google.common.eventbus.EventBus.nexus.async - Could not dispatch event org.sonatype.nexus.repository.yum.internal.createrepo.YumMetadataInvalidationEvent@75b487e7 to subscriber org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl$$EnhancerByGuice$$9db995@93053b8 method [public void org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl.on(org.sonatype.nexus.repository.yum.internal.createrepo.YumMetadataInvalidationEvent)] org.sonatype.nexus.repository.InvalidContentException: Invalid RPM: external/noarch/redeclipse-data-1.5.6-lp151.2.5.noarch.rpm at org.sonatype.nexus.repository.yum.internal.rpm.YumRpmParser.parse(YumRpmParser.java:108) at org.sonatype.nexus.repository.yum.internal.rpm.YumRpmParser.parse(YumRpmParser.java:76) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.writeAssetToMetadata(CreateRepoServiceImpl.java:651) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.appendRpm(CreateRepoServiceImpl.java:511) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.loopAllRpmsAndAppend(CreateRepoServiceImpl.java:499) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.writeMetadata(CreateRepoServiceImpl.java:477) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.convertDirectoriesToMetadata(CreateRepoServiceImpl.java:180) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.buildMetadata(CreateRepoServiceImpl.java:150) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.buildMetadata(CreateRepoServiceImpl.java:134) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.buildMetadataWithoutCaching(CreateRepoServiceImpl.java:125) at org.sonatype.nexus.transaction.TransactionalWrapper.proceedWithTransaction(TransactionalWrapper.java:57) at org.sonatype.nexus.transaction.TransactionInterceptor.proceedWithTransaction(TransactionInterceptor.java:66) at org.sonatype.nexus.transaction.TransactionInterceptor.invoke(TransactionInterceptor.java:55) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl.buildMetadata(CreateRepoFacetImpl.java:196) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl.on(CreateRepoFacetImpl.java:178) at sun.reflect.GeneratedMethodAccessor125.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at com.google.common.eventbus.Subscriber.invokeSubscriberMethod(Subscriber.java:87) at com.google.common.eventbus.Subscriber$SynchronizedSubscriber.invokeSubscriberMethod(Subscriber.java:144) at com.google.common.eventbus.Subscriber$1.run(Subscriber.java:72) at org.sonatype.nexus.thread.internal.MDCAwareRunnable.run(MDCAwareRunnable.java:40) at org.apache.shiro.subject.support.SubjectRunnable.doRun(SubjectRunnable.java:120) at org.apache.shiro.subject.support.SubjectRunnable.run(SubjectRunnable.java:108) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Caused by: java.net.SocketTimeoutException: Read timed out at java.net.SocketInputStream.socketRead0(Native Method) at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) at java.net.SocketInputStream.read(SocketInputStream.java:171) at java.net.SocketInputStream.read(SocketInputStream.java:141) at org.apache.http.impl.io.SessionInputBufferImpl.streamRead(SessionInputBufferImpl.java:137) at org.apache.http.impl.io.SessionInputBufferImpl.read(SessionInputBufferImpl.java:198) at org.apache.http.impl.io.ContentLengthInputStream.read(ContentLengthInputStream.java:176) at org.apache.http.conn.EofSensorInputStream.read(EofSensorInputStream.java:135) at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82) at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180) at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82) at com.amazonaws.services.s3.internal.S3AbortableInputStream.read(S3AbortableInputStream.java:125) at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82) at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82) at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82) at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180) at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82) at com.amazonaws.util.LengthCheckInputStream.read(LengthCheckInputStream.java:107) at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82) at java.io.BufferedInputStream.fill(BufferedInputStream.java:246) at java.io.BufferedInputStream.read1(BufferedInputStream.java:286) at java.io.BufferedInputStream.read(BufferedInputStream.java:345) at com.google.common.io.CountingInputStream.read(CountingInputStream.java:63) at java.security.DigestInputStream.read(DigestInputStream.java:161) at java.io.FilterInputStream.read(FilterInputStream.java:107) at com.google.common.io.ByteStreams.exhaust(ByteStreams.java:273) at org.sonatype.nexus.repository.yum.internal.rpm.YumRpmParser.parse(YumRpmParser.java:97) ... 26 common frames omitted 2020-08-22 23:30:06,427+0000 WARN [Thread-1948 \u0026lt;command\u0026gt;sql.select from asset where (component IS NOT NULL AND attributes.yum.asset_kind = :p0) and (bucket=#59:1)\u0026lt;/command\u0026gt;] *SYSTEM org.sonatype.nexus.repository.storage.OrientAsyncHelper$QueueFeedingResultListener - Timed out adding query result to queue 28dee0bf after 60 seconds, aborting query 2020-08-22 23:31:06,430+0000 WARN [Thread-1948 \u0026lt;command\u0026gt;sql.select from asset where (component IS NOT NULL AND attributes.yum.asset_kind = :p0) and (bucket=#59:1)\u0026lt;/command\u0026gt;] *SYSTEM org.sonatype.nexus.repository.storage.OrientAsyncHelper$QueueFeedingResultListener - Timed out adding end marker to queue 28dee0bf after 60 seconds Any SQL warnings or notifications indicate the rebuild may have failed. Examine repodata/*.xml.gz file attributes, such as file size and last modified time, to determine if they are new compared to the timestamp on the Finished rebuilding yum metadata for repository message.\n2020-08-22 23:24:16,974+0000 INFO [Thread-1948 \u0026lt;command\u0026gt;sql.select from asset where (component IS NOT NULL AND attributes.yum.asset_kind = :p0) and (bucket=#59:1)\u0026lt;/command\u0026gt;] *SYSTEM com.orientechnologies.common.profiler.OProfilerStub - $ANSI{green {db=component}} [TIP] Query \u0026#39;SELECT FROM asset WHERE (component IS NOT NULL AND attributes.yum.asset_kind = \u0026#34;RPM\u0026#34; ) AND (bucket = #59:1 )\u0026#39; returned a result set with more than 10000 records. Check if you really need all these records, or reduce the resultset by using a LIMIT to improve both performance and used RAM ... 2020-08-22 23:30:06,427+0000 WARN [Thread-1948 \u0026lt;command\u0026gt;sql.select from asset where (component IS NOT NULL AND attributes.yum.asset_kind = :p0) and (bucket=#59:1)\u0026lt;/command\u0026gt;] *SYSTEM org.sonatype.nexus.repository.storage.OrientAsyncHelper$QueueFeedingResultListener - Timed out adding query result to queue 28dee0bf after 60 seconds, aborting query 2020-08-22 23:31:06,430+0000 WARN [Thread-1948 \u0026lt;command\u0026gt;sql.select from asset where (component IS NOT NULL AND attributes.yum.asset_kind = :p0) and (bucket=#59:1)\u0026lt;/command\u0026gt;] *SYSTEM org.sonatype.nexus.repository.storage.OrientAsyncHelper$QueueFeedingResultListener - Timed out adding end marker to queue 28dee0bf after 60 seconds However, seeing an ERROR with a JVM stack trace is a key indication that the rebuild failed:\n2020-08-22 23:29:57,701+0000 ERROR [event-12-thread-780] *SYSTEM com.google.common.eventbus.EventBus.nexus.async - Could not dispatch event org.sonatype.nexus.repository.yum.internal.createrepo.YumMetadataInvalidationEvent@75b487e7 to subscriber org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl$$EnhancerByGuice$$9db995@93053b8 method [public void org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl.on(org.sonatype.nexus.repository.yum.internal.createrepo.YumMetadataInvalidationEvent)] org.sonatype.nexus.repository.InvalidContentException: Invalid RPM: external/noarch/redeclipse-data-1.5.6-lp151.2.5.noarch.rpm at org.sonatype.nexus.repository.yum.internal.rpm.YumRpmParser.parse(YumRpmParser.java:108) at org.sonatype.nexus.repository.yum.internal.rpm.YumRpmParser.parse(YumRpmParser.java:76) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.writeAssetToMetadata(CreateRepoServiceImpl.java:651) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.appendRpm(CreateRepoServiceImpl.java:511) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.loopAllRpmsAndAppend(CreateRepoServiceImpl.java:499) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.writeMetadata(CreateRepoServiceImpl.java:477) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.convertDirectoriesToMetadata(CreateRepoServiceImpl.java:180) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.buildMetadata(CreateRepoServiceImpl.java:150) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.buildMetadata(CreateRepoServiceImpl.java:134) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.buildMetadataWithoutCaching(CreateRepoServiceImpl.java:125) at org.sonatype.nexus.transaction.TransactionalWrapper.proceedWithTransaction(TransactionalWrapper.java:57) at org.sonatype.nexus.transaction.TransactionInterceptor.proceedWithTransaction(TransactionInterceptor.java:66) at org.sonatype.nexus.transaction.TransactionInterceptor.invoke(TransactionInterceptor.java:55) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl.buildMetadata(CreateRepoFacetImpl.java:196) at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl.on(CreateRepoFacetImpl.java:178) at sun.reflect.GeneratedMethodAccessor125.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at com.google.common.eventbus.Subscriber.invokeSubscriberMethod(Subscriber.java:87) at com.google.common.eventbus.Subscriber$SynchronizedSubscriber.invokeSubscriberMethod(Subscriber.java:144) at com.google.common.eventbus.Subscriber$1.run(Subscriber.java:72) at org.sonatype.nexus.thread.internal.MDCAwareRunnable.run(MDCAwareRunnable.java:40) at org.apache.shiro.subject.support.SubjectRunnable.doRun(SubjectRunnable.java:120) at org.apache.shiro.subject.support.SubjectRunnable.run(SubjectRunnable.java:108) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Caused by: java.net.SocketTimeoutException: Read timed out at java.net.SocketInputStream.socketRead0(Native Method) at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) at java.net.SocketInputStream.read(SocketInputStream.java:171) at java.net.SocketInputStream.read(SocketInputStream.java:141) at org.apache.http.impl.io.SessionInputBufferImpl.streamRead(SessionInputBufferImpl.java:137) at org.apache.http.impl.io.SessionInputBufferImpl.read(SessionInputBufferImpl.java:198) at org.apache.http.impl.io.ContentLengthInputStream.read(ContentLengthInputStream.java:176) at org.apache.http.conn.EofSensorInputStream.read(EofSensorInputStream.java:135) at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82) at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180) at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82) at com.amazonaws.services.s3.internal.S3AbortableInputStream.read(S3AbortableInputStream.java:125) at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82) at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82) at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82) at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180) at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82) at com.amazonaws.util.LengthCheckInputStream.read(LengthCheckInputStream.java:107) at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82) at java.io.BufferedInputStream.fill(BufferedInputStream.java:246) at java.io.BufferedInputStream.read1(BufferedInputStream.java:286) at java.io.BufferedInputStream.read(BufferedInputStream.java:345) at com.google.common.io.CountingInputStream.read(CountingInputStream.java:63) at java.security.DigestInputStream.read(DigestInputStream.java:161) at java.io.FilterInputStream.read(FilterInputStream.java:107) at com.google.common.io.ByteStreams.exhaust(ByteStreams.java:273) at org.sonatype.nexus.repository.yum.internal.rpm.YumRpmParser.parse(YumRpmParser.java:97) ... 26 common frames omitted "
},
{
	"uri": "/docs-csm/en-11/operations/package_repository_management/restrict_admin_privileges_in_nexus/",
	"title": "Restrict Admin Privileges In Nexus",
	"tags": [],
	"description": "",
	"content": "Restrict Admin Privileges in Nexus Prior to making the system available to users, change the ingress settings to disable connections to packages.local and registry.local from automatically gaining admin privileges.\nConnections to packages.local and registry.local automatically login clients as the admin user. Admin privileges enable any user to make anonymous writes to Nexus, which means unauthenticated users can perform arbitrary actions on Nexus itself through the REST API, as well as in repositories by uploading or deleting assets.\nProduct installers currently do not expect to authenticate to Nexus, so it is necessary to retain the default ingress settings during installation.\nPrerequisites The system is fully installed.\nProcedure   Verify that the registry repository has docker.forceBasicAuth set to true.\nncn# curl -sS https://packages.local/service/rest/beta/repositories \\ | jq \u0026#39;.[] | select(.name == \u0026#34;registry\u0026#34;) | .docker.forceBasicAuth = true\u0026#39; \\ | curl -sSi -X PUT \u0026#39;https://packages.local/service/rest/beta/repositories/docker/hosted/registry\u0026#39; \\ -H \u0026#34;Content-Type: application/json\u0026#34; -d @-   Patch the nexus VirtualService resource in the nexus namespace to remove the X-WEBAUTH-USER request header when the authority matches packages.local or registry.local.\nReplace SYSTEM_DOMAIN_NAME in the following command before running it.\nncn# kubectl patch virtualservice -n nexus nexus --type merge --patch \\ \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;http\u0026#34;:[{\u0026#34;match\u0026#34;:[{\u0026#34;authority\u0026#34;:{\u0026#34;exact\u0026#34;:\u0026#34;packages.local\u0026#34;}}],\\ \u0026#34;route\u0026#34;:[{\u0026#34;destination\u0026#34;:{\u0026#34;host\u0026#34;:\u0026#34;nexus\u0026#34;,\u0026#34;port\u0026#34;:{\u0026#34;number\u0026#34;:80}},\u0026#34;headers\u0026#34;:{\\ \u0026#34;request\u0026#34;:{\u0026#34;remove\u0026#34;:[\u0026#34;X-WEBAUTH-USER\u0026#34;]}}}]},{\u0026#34;match\u0026#34;:[{\u0026#34;authority\u0026#34;:\\ {\u0026#34;exact\u0026#34;:\u0026#34;registry.local\u0026#34;}}],\u0026#34;route\u0026#34;:[{\u0026#34;destination\u0026#34;:{\u0026#34;host\u0026#34;:\u0026#34;nexus\u0026#34;,\\ \u0026#34;port\u0026#34;:{\u0026#34;number\u0026#34;:5003}},\u0026#34;headers\u0026#34;:{\u0026#34;request\u0026#34;:{\u0026#34;remove\u0026#34;:[\u0026#34;X-WEBAUTH-USER\u0026#34;]}}}]},\\ {\u0026#34;match\u0026#34;:[{\u0026#34;authority\u0026#34;:{\u0026#34;exact\u0026#34;:\u0026#34;nexus.SYSTEM_DOMAIN_NAME\u0026#34;}}],\u0026#34;route\u0026#34;:\\ [{\u0026#34;destination\u0026#34;:{\u0026#34;host\u0026#34;:\u0026#34;nexus\u0026#34;,\u0026#34;port\u0026#34;:{\u0026#34;number\u0026#34;:80}},\u0026#34;headers\u0026#34;:\\ {\u0026#34;request\u0026#34;:{\u0026#34;add\u0026#34;:{\u0026#34;X-WEBAUTH-USER\u0026#34;:\u0026#34;admin\u0026#34;},\u0026#34;remove\u0026#34;:[\u0026#34;Authorization\u0026#34;]}}}]}]}}\u0026#39; The following is an example of the nexus VirtualService resource before the patch:\nspec: http: - match: - authority: exact: packages.local route: - destination: host: nexus port: number: 80 headers: request: add: X-WEBAUTH-USER: admin remove: - Authorization - match: - authority: exact: registry.local route: - destination: host: nexus port: number: 5003 headers: request: add: X-WEBAUTH-USER: admin remove: - Authorization The patch will update the information to the following:\nspec: http: - match: - authority: exact: packages.local route: - destination: host: nexus port: number: 80 headers: request: remove: - X-WEBAUTH-USER - match: - authority: exact: registry.local route: - destination: host: nexus port: number: 5003 headers: request: remove: - X-WEBAUTH-USER   Troubleshooting: If the patch needs to be removed for maintenance activities or any other purpose, run the following command:\nReplace SYSTEM_DOMAIN_NAME in the following command before running it.\nncn# kubectl patch virtualservice -n nexus nexus --type merge \\ --patch \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;http\u0026#34;:[{\u0026#34;match\u0026#34;:[{\u0026#34;authority\u0026#34;:{\u0026#34;exact\u0026#34;:\u0026#34;packages.local\u0026#34;}}]\\ ,\u0026#34;route\u0026#34;:[{\u0026#34;destination\u0026#34;:{\u0026#34;host\u0026#34;:\u0026#34;nexus\u0026#34;,\u0026#34;port\u0026#34;:{\u0026#34;number\u0026#34;:80}},\u0026#34;headers\u0026#34;:\\ {\u0026#34;request\u0026#34;:{\u0026#34;add\u0026#34;:{\u0026#34;X-WEBAUTH-USER\u0026#34;:\u0026#34;admin\u0026#34;},\u0026#34;remove\u0026#34;:[\u0026#34;Authorization\u0026#34;]}}}]},\\ {\u0026#34;match\u0026#34;:[{\u0026#34;authority\u0026#34;:{\u0026#34;exact\u0026#34;:\u0026#34;registry.local\u0026#34;}}],\u0026#34;route\u0026#34;:[{\u0026#34;destination\u0026#34;:\\ {\u0026#34;host\u0026#34;:\u0026#34;nexus\u0026#34;,\u0026#34;port\u0026#34;:{\u0026#34;number\u0026#34;:5003}},\u0026#34;headers\u0026#34;:{\u0026#34;request\u0026#34;:{\u0026#34;add\u0026#34;:\\ {\u0026#34;X-WEBAUTH-USER\u0026#34;:\u0026#34;admin\u0026#34;},\u0026#34;remove\u0026#34;:[\u0026#34;Authorization\u0026#34;]}}}]},{\u0026#34;match\u0026#34;:\\ [{\u0026#34;authority\u0026#34;:{\u0026#34;exact\u0026#34;:\u0026#34;nexus.SYSTEM_DOMAIN_NAME\u0026#34;}}],\u0026#34;route\u0026#34;:\\ [{\u0026#34;destination\u0026#34;:{\u0026#34;host\u0026#34;:\u0026#34;nexus\u0026#34;,\u0026#34;port\u0026#34;:{\u0026#34;number\u0026#34;:80}},\u0026#34;headers\u0026#34;:\\ {\u0026#34;request\u0026#34;:{\u0026#34;add\u0026#34;:{\u0026#34;X-WEBAUTH-USER\u0026#34;:\u0026#34;admin\u0026#34;},\u0026#34;remove\u0026#34;:[\u0026#34;Authorization\u0026#34;]}}}]}]}}\u0026#39; "
},
{
	"uri": "/docs-csm/en-11/operations/package_repository_management/troubleshoot_nexus/",
	"title": "Troubleshoot Nexus",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Nexus General Nexus Troubleshooting Topics\n lookup registry.local: no such host lookup registry.local: Temporary failure in name resolution initiating layer upload\u0026hellip; in registry.local not ready: https://packages.local  Error lookup registry.local: no such host The following error may occur when running ./lib/setup-nexus.sh:\ntime=\u0026quot;2021-02-23T19:55:54Z\u0026quot; level=fatal msg=\u0026quot;Error copying tag \\\u0026quot;dir:/image/grafana/grafana:7.0.3\\\u0026quot;: Error writing blob: Head \\\u0026quot;https://registry.local/v2/grafana/grafana/blobs/sha256:cf254eb90de2dc62aa7cce9737ad7e143c679f5486c46b742a1b55b168a736d3\\\u0026quot;: dial tcp: lookup registry.local: no such host\u0026quot; + return Or a similar error:\ntime=\u0026quot;2021-03-04T22:45:07Z\u0026quot; level=fatal msg=\u0026quot;Error copying ref \\\u0026quot;dir:/image/cray/cray-ims-load-artifacts:1.0.4\\\u0026quot;: Error trying to reuse blob sha256:1ec886c351fa4c330217411b0095ccc933090aa2cd7ae7dcd33bb14b9f1fd217 at destination: Head \\\u0026quot;https://registry.local/v2/cray/cray-ims-load-artifacts/blobs/sha256:1ec886c351fa4c330217411b0095ccc933090aa2cd7ae7dcd33bb14b9f1fd217\\\u0026quot;: dial tcp: lookup registry.local: Temporary failure in name resolution\u0026quot; + return These errors are most likely intermittent and running ./lib/setup-nexus.sh again is expected to succeed.\nError initiating layer upload \u0026hellip; in registry.local: received unexpected HTTP status: 200 OK The following error may occur when running ./lib/setup-nexus.sh:\ntime=\u0026quot;2021-02-07T20:25:22Z\u0026quot; level=info msg=\u0026quot;Copying image tag 97/144\u0026quot; from=\u0026quot;dir:/image/jettech/kube-webhook-certgen:v1.2.1\u0026quot; to=\u0026quot;docker://registry.local/jettech/kube-webhook-certgen:v1.2.1\u0026quot; Getting image source signatures Copying blob sha256:f6e131d355612c71742d71c817ec15e32190999275b57d5fe2cd2ae5ca940079 Copying blob sha256:b6c5e433df0f735257f6999b3e3b7e955bab4841ef6e90c5bb85f0d2810468a2 Copying blob sha256:ad2a53c3e5351543df45531a58d9a573791c83d21f90ccbc558a7d8d3673ccfa time=\u0026quot;2021-02-07T20:25:33Z\u0026quot; level=fatal msg=\u0026quot;Error copying tag \\\u0026quot;dir:/image/jettech/kube-webhook-certgen:v1.2.1\\\u0026quot;: Error writing blob: Error initiating layer upload to /v2/jettech/kube-webhook-certgen/blobs/uploads/ in registry.local: received unexpected HTTP status: 200 OK\u0026quot; + return This error is most likely intermittent and running ./lib/setup-nexus.sh again is expected to succeed.\nerror: not ready: https://packages.local The error: not ready: https://packages.local indicates that from the caller\u0026rsquo;s perspective, Nexus is not ready to receive writes. However, it most likely indicates that a Nexus setup utility was unable to connect to Nexus via the packages.local name. Because the install does not attempt to connect to packages.local until Nexus has been successfully deployed, the error does not usually indicate something is actually wrong with Nexus. Instead, it is most commonly a network issue with name resolution (i.e., DNS), IP routes from the PIT node, switch misconfiguration, or Istio ingress.\nVerify that packages.local resolves to ONLY the load balancer IP address for the istio-ingressgateway service in the istio-system namespace, typically 10.92.100.71. If name resolution returns addresses on other networks (such as HMN) this must be corrected. Prior to DNS/DHCP hand-off to Unbound, these settings are controlled by dnsmasq. Unbound settings are based on SLS settings in sls_input_file.json and must be updated via the Unbound manager.\nIf packages.local resolves to the correct addresses, verify basic connectivity using ping. If ping packages.local is unsuccessful, verify the IP routes from the PIT node to the NMN load balancer network. The typical ip route configuration is 10.92.100.0/24 via 10.252.0.1 dev vlan002. If pings are successful, try checking the status of Nexus by running curl -sS https://packages.local/service/rest/v1/status/writable. If the connection times out, it indicates there is a more complex connection issue. Verify switches are configured properly and BGP peering is operating correctly, see Update BGP Neighbors for more information. Lastly, check Istio and OPA logs to see if connections to packages.local are not reaching Nexus, perhaps because of an authorization issue.\nIf https://packages.local/service/rest/v1/status/writable returns an HTTP code other than 200 OK, it indicates there is an issue with Nexus. Verify that the loftsman ship deployment of the nexus.yaml manifest was successful. If helm status -n nexus cray-nexus indicates the status is NOT deployed, then something is most likely wrong with the Nexus deployment and additional diagnosis is required. In this case, the current Nexus deployment probably needs to be uninstalled and the nexus-data PVC removed before attempting to deploy again.\n"
},
{
	"uri": "/docs-csm/en-11/operations/package_repository_management/nexus_configuration/",
	"title": "Nexus Configuration",
	"tags": [],
	"description": "",
	"content": "Nexus Configuration Expect each product to create and use its own File type blob store. For example, the Cray System Management (CSM) product uses csm.\nThe default blob store is also available, but Cray products are discouraged from using it.\nRepositories CSM creates the registry (format docker) and charts (format helm) repositories for managing container images and Helm charts across all Cray products. However, each product\u0026rsquo;s release may contain a number of RPM repositories that are added to Nexus. RPM repositories are created in Nexus as raw repositories to support signed repository metadata and to enable client GPG checks.\nRepository Naming Conventions RPM repositories should be named in the \u0026lt;product\u0026gt;[-\u0026lt;product-version\u0026gt;]-\u0026lt;os-dist\u0026gt;-\u0026lt;os-version\u0026gt;[-compute][-\u0026lt;arch\u0026gt;] format. The following is a description of each component in an RPM repository name:\n \u0026lt;product\u0026gt; indicates the product. For example, cos, csm, and sma. -\u0026lt;product-version\u0026gt; indicates the product version. For example, -1.4.0, -latest, -stable.  Type hosted repositories must specify -\u0026lt;product-version\u0026gt; relative to the patch release. Type group or proxy repositories whose sole member is a hosted repository (for instance, it serves as an alias) may use a more generic version, such as -1.4, or omit -\u0026lt;product-version\u0026gt; altogether if it represents the currently active version.   -\u0026lt;os-dist\u0026gt; indicates the OS distribution, such as -sle. -\u0026lt;os-version\u0026gt; indicates the OS version, such as -15sp1 or -15sp2. -compute must be specified if the repository contains RPMs specific to compute nodes and omitted otherwise. There is no suffix for repositories containing NCN RPMs. -\u0026lt;arch\u0026gt; must be specified if the repository contains RPMs specific to a system architecture other than x86_64, such as -aarch64.  "
},
{
	"uri": "/docs-csm/en-11/operations/package_repository_management/nexus_deployment/",
	"title": "Nexus Deployment",
	"tags": [],
	"description": "",
	"content": "Nexus Deployment Nexus is deployed with the cray-nexus chart to the nexus namespace as part of the Cray System Management (CSM) release. Nexus is deployed after critical platform services are up and running. Product installers configure and populate Nexus blob stores and repositories using the cray-nexus-setup container image. As a result, there is no singular product that provides all Nexus repositories or assets; instead, individual products must be installed. However, CSM configures the charts Helm repository and the registry Docker repository, which all products may use.\nCustomizations For a complete set of available settings, consult the values.yaml file for the cray-nexus chart. The most common customizations to set are specified in the following table. They must be set in the customizations.yaml file under the spec.kubernetes.services.cray-nexus setting.\n   Customization Default Description     istio.ingress.hosts.ui.enabled true Enables ingress from the CAN (default chart value is false)   istio.ingress.hosts.ui.authority nexus.{{ network.dns.external }} Sets the CAN hostname (default chart value is nexus.local)   sonatype-nexus.persistence.storageSize 1000Gi Nexus storage size, may be increased after installation; critical if spec.kubernetes.services.cray-nexus-setup.s3.enabled is false    Common Nexus Deployments A typical deployment will look similar to the following:\n# kubectl -n nexus get all NAME READY STATUS RESTARTS AGE pod/cray-precache-images-6tp2c 2/2 Running 0 20d pod/cray-precache-images-dnwdx 2/2 Running 0 20d pod/cray-precache-images-jgvx8 2/2 Running 0 20d pod/cray-precache-images-n2clw 2/2 Running 0 20d pod/cray-precache-images-v8ntg 2/2 Running 0 17d pod/cray-precache-images-xmg6d 2/2 Running 0 20d pod/nexus-55d8c77547-xcc2f 2/2 Running 0 19d NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/nexus ClusterIP 10.23.120.95 \u0026lt;none\u0026gt; 80/TCP,5003/TCP 19d NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/cray-precache-images 6 6 6 6 6 \u0026lt;none\u0026gt; 20d NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nexus 1/1 1 1 19d NAME DESIRED CURRENT READY AGE replicaset.apps/nexus-55d8c77547 1 1 1 19dd The cray-precache-images DaemonSet is used to keep select container images resident in the image cache on each worker node to ensure Nexus resiliency. It is deployed as a critical platform component prior to Nexus.\nWarning: The cray-nexus chart deploys Nexus with a single replica and the corresponding nexus-data PVC with RWX access mode. Nexus should NEVER be scaled to more than one replica; otherwise, the instance data in nexus-data PV will most likely be corrupted. Using RWX access mode enables Nexus to quickly restart on another worker node in the event of a node failure and avoid additional delay because of volume multi-attach errors.\nBootstrap Registry During installation, a Nexus instance is run on the PIT node at port 8081 to facilitate cluster bootstrap. It is only configured with a docker registry available at http://pit.nmn:5000, which is populated with container images included in the CSM release.\nBy default, http://pit.nmn:5000 is the default mirror configured in /etc/containerd/config.toml. However, once the PIT node is rebooted as ncn-m001, it will no longer be available.\nProduct installers Product installers vendor the dtr.dev.cray.com/cray/cray-nexus-setup:0.4.0 container image, which includes helper scripts for working with the Nexus REST API to update and modify repositories. Product release distributions will include the nexus-blobstores.yaml and nexus-repositories.yaml files, which define the Nexus blob stores and repositories required for that version of the product. Also, expect to find directories that include specific types of assets:\n rpm/ - RPM repositories docker/ - Container images helm/ - Helm Charts  Prior to deploying Helm charts to the system management Kubernetes cluster, product installers will setup repositories in Nexus and then upload assets to them. Typically, all of this is automated in the beginning of a product\u0026rsquo;s install.sh script, and will look something like the following:\nROOTDIR=\u0026#34;$(dirname \u0026#34;${BASH_SOURCE[0]}\u0026#34;)\u0026#34;source\u0026#34;${ROOTDIR}/lib/version.sh\u0026#34;source\u0026#34;${ROOTDIR}/lib/install.sh\u0026#34;# Load vendored tools into install environment load-install-deps # Upload the contents of an RPM repository named $repo nexus-upload raw \u0026#34;${ROOTDIR}/rpm/${repo}\u0026#34;${RELEASE_NAME}-${RELEASE_VERSION}-${repo}\u0026#34;# Setup Nexus nexus-setup blobstores \u0026#34;${ROOTDIR}/nexus-blobstores.yaml\u0026#34; nexus-setup repositories \u0026#34;${ROOTDIR}/nexus-repositories.yaml\u0026#34;# Upload container images to registry.local skopeo-sync \u0026#34;${ROOTDIR}/docker\u0026#34;# Upload charts to the \u0026#34;charts\u0026#34; repository nexus-upload helm \u0026#34;${ROOTDIR}/helm\u0026#34; charts # Remove vendored tools from install environment clean-install-deps Product installers also load and clean up the install tools used to facilitate installation. By convention, vendored tools will be in the vendor directory. In case something goes wrong, it may be useful to manually load them into the install environment to help with debugging.\n"
},
{
	"uri": "/docs-csm/en-11/operations/package_repository_management/package_repository_management/",
	"title": "Package Repository Management",
	"tags": [],
	"description": "",
	"content": "Package Repository Management Repositories are added to systems to extend the system functionality beyond what is initially delivered. The Sonatype Nexus Repository Manager is the primary method for repository management. Nexus hosts the Yum, Docker, raw, and Helm repositories for software and firmware content.\nRefer to the following for more information about Nexus:\n The official Sonatype documentation: https://help.sonatype.com/repomanager3 Nexus REST API and Web UI management: Manage Repositories with Nexus  "
},
{
	"uri": "/docs-csm/en-11/operations/package_repository_management/package_repository_management_with_nexus/",
	"title": "Package Repository Management With Nexus",
	"tags": [],
	"description": "",
	"content": "Package Repository Management with Nexus Overview of RPM repositories and container registry in Nexus.\nRPM Repositories in Nexus Repositories are available at https://packages.local/repository/REPO_NAME. For example, to configure the csm-sle-15sp2 repository on a non-compute node (NCN):\nncn# zypper addrepo -fG https://packages.local/repository/csm-sle-15sp2 csm-sle-15sp2 Adding repository \u0026#39;csm-sle-15sp2\u0026#39; .................................................................................................[done] Warning: GPG checking is disabled in configuration of repository \u0026#39;csm-sle-15sp2\u0026#39;. Integrity and origin of packages cannot be verified. Repository \u0026#39;csm-sle-15sp2\u0026#39; successfully added URI : https://packages.local/repository/csm-sle-15sp2 Enabled : Yes GPG Check : No Autorefresh : Yes Priority : 99 (default priority) Repository priorities are without effect. All enabled repositories share the same priority. ncn# zypper ref csm-sle-15sp2 Retrieving repository \u0026#39;csm-sle-15sp2\u0026#39; metadata ....................................................................................[done] Building repository \u0026#39;csm-sle-15sp2\u0026#39; cache .........................................................................................[done] Specified repositories have been refreshed. The -G option is used in this example to disable GPG checks. However, if the named repository is properly signed, it is not recommended to use the -G option.\nContainer Registry Container registry is available at https://registry.local on the NCNs or compute nodes. By default, access to the container registry is not available over the Customer Access Network (CAN). If desired, a corresponding route may be added to the nexus VirtualService resource in the nexus namespace:\nWarning: If access to the container registry in Nexus is exposed over CAN, it is strongly recommended to setup and configure fine-grained access control. However, as mentioned above, the default setup assumes the OPA policy only permits admin users access.\nncn# kubectl -n nexus get vs nexus NAME GATEWAYS HOSTS AGE nexus [services/services-gateway] [packages.local registry.local nexus.odin.dev.cray.com] 21d The only way to add images to the container registry is with the Docker API. Use a client (such as Skopeo, Podman, or Docker) to push images. By default, product installers use Podman with a vendor version of the Skopeo image to sync container images included in a release distribution to registry.local.\nThe Cray System Management (CSM) product adds a recent version of quay.io/skopeo/stable to the container registry, and it may be used to copy images into registry.local. For example, to update the version of quay.io/skopeo/stable:\nncn# podman run --rm registry.local/skopeo/stable copy \\ --dest-tls-verify=false docker://quay.io/skopeo/stable docker://registry.local/skopeo/stable Getting image source signatures Copying blob sha256:85a74b04b5b84b45c763e9763cc0f62269390bb30058d3e2b2545d820d3558f7 Copying blob sha256:ab9d1e8c4764f52ed5041c38bd3d64b6ae9c27d0f436be50f658ece38440a97b Copying blob sha256:e5c8e56645c4d70308640ede3f72f76386b466cf5d97010b9c2f31054caf30a5 Copying blob sha256:bcf471c5e964dc3ce3e7249bd2b1493acf3dd103a28af0cfe5af70351ad399d0 Copying blob sha256:d62975d5ffa72581b912ee3e1a850e2ac14435a4238253a8ebf80f5d10f2df4c Copying blob sha256:8c87d899c1ab2cc2d25708ba0ff9a1726fe6b57bf415c8fdc7de973e6b185f63 Copying config sha256:49f2b6d9790b48aadb2ac29f5bfef56ebb2fccec6319b3981639d04452887848 Writing manifest to image destination Storing signatures Kubernetes Pods are expected to rely on the registry mirror configuration in /etc/containerd/config.toml to automatically fetch container images from it using upstream references. By default, the following upstream registries are automatically redirected to registry.local:\n dtr.dev.cray.com docker.io (and registry-1.docker.io) quay.io gcr.io k8s.gcr.io  Warning: The registry mirror configuration in /etc/containerd/config.toml only applies to the CRI. When using the ctr command or another container runtime (For example, podman or docker), the admin must explicitly reference registry.local.\nThe following is an example of pulling dtr.dev.cray.com/baseos/alpine:3.12.0 using CRI:\nncn# crictl pull dtr.dev.cray.com/baseos/alpine:3.12.0 Image is up to date for sha256:5779738096ecb47dd7192d44ceef7032110edd38204f66c9ca4e35fca952975c Using containerd or Podman tooling requires changing dtr.dev.cray.com to registry.local to guarantee the runtime fetches the image from the container registry in Nexus.\nThe following is an example for containerd:\nncn# ctr image pull registry.local/baseos/alpine:3.12.0 registry.local/baseos/alpine:3.12.0: resolved |++++++++++++++++++++++++++++++++++++++| manifest-sha256:e25f4e287fad9c0ee0a47af590e999f9ff1f043fb636a9dc7a61af6d13fc40ca: done |++++++++++++++++++++++++++++++++++++++| layer-sha256:3ab6766f6281be4c2349e2122bab3b4d1ba1b524236b85fce0784453e759b516: done |++++++++++++++++++++++++++++++++++++++| layer-sha256:df20fa9351a15782c64e6dddb2d4a6f50bf6d3688060a34c4014b0d9a752eb4c: done |++++++++++++++++++++++++++++++++++++++| layer-sha256:62694d7552ccd2338f8a4d775bef09ea56f6d2bcfdfafb9e2a4e0241f360fca5: done |++++++++++++++++++++++++++++++++++++++| config-sha256:5779738096ecb47dd7192d44ceef7032110edd38204f66c9ca4e35fca952975c: done |++++++++++++++++++++++++++++++++++++++| elapsed: 0.2 s total: 0.0 B (0.0 B/s) unpacking linux/amd64 sha256:e25f4e287fad9c0ee0a47af590e999f9ff1f043fb636a9dc7a61af6d13fc40ca... done The following is an example for Podman:\nncn# podman pull registry.local/baseos/alpine:3.12.0 Trying to pull registry.local/baseos/alpine:3.12.0... Getting image source signatures Copying blob df20fa9351a1 [--------------------------------------] 0.0b / 0.0b Copying blob 3ab6766f6281 [--------------------------------------] 0.0b / 0.0b Copying blob 62694d7552cc [--------------------------------------] 0.0b / 0.0b Copying config 5779738096 done Writing manifest to image destination Storing signatures 5779738096ecb47dd7192d44ceef7032110edd38204f66c9ca4e35fca952975c "
},
{
	"uri": "/docs-csm/en-11/operations/package_repository_management/manage_repositories_with_nexus/",
	"title": "Manage Repositories With Nexus",
	"tags": [],
	"description": "",
	"content": "Manage Repositories with Nexus This section describes how to connect to Nexus with the Web UI, as well as how to access the REST API from non-compute nodes (NCNs) or compute nodes to manage repositories.\nAccess Nexus with the Web UI Use the hostname set in istio.ingress.hosts.ui.authority (see below) to connect to Nexus over the Customer Access Network (CAN) using a web browser. For example:\nhttps://nexus.{{network.dns.external}}/ Users will be redirected to Keycloak to log in, and based on the default OPA policy, only admin users will be authorized. Scripts may connect by first obtaining a JWT token from Keycloak and passing it in the HTTP Authorization header.\nAccess Nexus with the REST API The REST API is available from NCNs or compute nodes at https://packages.local/service/rest, as well as over the CAN at https://nexus.{{network.dns.external}}/service/rest (requires JWT token in the HTTP Authorization header).\nDownload the Open API document at /service/rest/swagger.json for details about the API, including specific options to available endpoints. By default, the REST API endpoints return (or accept) JSON.\nThe examples in the following sections use curl to exercise the REST API endpoints and jq to parse and manipulate the output. It is reasonable to use curl and jq to facilitate management tasks when necessary, but more complex actions may warrant development of more full-featured tools.\nThe following actions are described in this section:\n Check the Status of Nexus List Repositories List Assets Create a Repository Update a Repository Delete a Repository Create a Blob Store Delete a Blob Store  Pagination Various API endpoints use the external pagination too to return results. When a continuationToken included in the results is non-null, it indicates additional items are available.\nThe following is some example output:\n{ \u0026#34;items\u0026#34;: [...], \u0026#34;continuationToken\u0026#34;: \u0026#34;0a1b9d05d7162aa85d7747eaa75f171c\u0026#34; } In this example, the next set of results may be obtained by re-requesting the same URL with the added query parameter continuationToken=0a1b9d05d7162aa85d7747eaa75f171c.\nVarious examples in the following sections may use the paginate helper function to iterate over paginated results:\nfunction paginate() { local url=\u0026#34;$1\u0026#34; local token { token=\u0026#34;$(curl -sSk \u0026#34;$url\u0026#34; | tee /dev/fd/3 | jq -r \u0026#39;.continuationToken // null\u0026#39;)\u0026#34;; } 3\u0026gt;\u0026amp;1 until [[ \u0026#34;$token\u0026#34; == \u0026#34;null\u0026#34; ]]; do { token=\u0026#34;$(curl -sSk \u0026#34;$url\u0026amp;continuationToken=${token}\u0026#34; | tee /dev/fd/3 | jq -r \u0026#39;.continuationToken // null\u0026#39;)\u0026#34;; } 3\u0026gt;\u0026amp;1 done } Check the Status of Nexus Send an HTTP GET request to /service/rest/v1/status to check the operating status of Nexus. An HTTP 200 OK response indicates it is healthy:\n# curl -sSi https://packages.local/service/rest/v1/status HTTP/2 200 date: Sat, 06 Mar 202117:27:56 GMT server: istio-envoy x-content-type-options: nosniff content-length: 0 x-envoy-upstream-service-time: 6 Before attempting to write to Nexus, it is recommended to check that Nexus is writable by sending an HTTP GET request to /service/rest/v1/status/writable:\n# curl -sSi https://packages.local/service/rest/v1/status/writable HTTP/2 200 date: Sat, 06 Mar 202117:28:34 GMT server: istio-envoy x-content-type-options: nosniff content-length: 0 x-envoy-upstream-service-time: 6 List Repositories Use the /service/rest/v1/repositories endpoint to get a basic listing of available repositories:\n# curl -sSk https://packages.local/service/rest/v1/repositories | jq -r \u0026#39;.[] | .name\u0026#39; The /service/rest/beta/repositories endpoint provides a more detailed listing of available repositories. For example, the object returned for the csm-sle-15sp2 repository is:\n# curl -sSk https://packages.local/service/rest/beta/repositories \\ | jq -r \u0026#39;.[] | select(.name == \u0026#34;csm-sle-15sp2\u0026#34;)\u0026#39; { \u0026#34;name\u0026#34;: \u0026#34;csm-sle-15sp2\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;raw\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://packages.local/repository/csm-sle-15sp2\u0026#34;, \u0026#34;online\u0026#34;: true, \u0026#34;storage\u0026#34;: { \u0026#34;blobStoreName\u0026#34;: \u0026#34;csm\u0026#34;, \u0026#34;strictContentTypeValidation\u0026#34;: false }, \u0026#34;group\u0026#34;: { \u0026#34;memberNames\u0026#34;: [ \u0026#34;csm-0.8.0-sle-15sp2\u0026#34; ] }, \u0026#34;type\u0026#34;: \u0026#34;group\u0026#34; } Neither the v1 or beta/repositories endpoints are paginated.\nList Assets Use the /service/rest/v1/components endpoint to list the assets in a specific repository (REPO_NAME). The /service/rest/v1/components endpoint is paginated.\n# paginate \u0026#39;https://packages.local/service/rest/v1/components?repository=REPO_NAME\u0026#39; \\ | jq -r \u0026#39;.items[] | .name\u0026#39; For example, to list the names of all components in the csm-sle-15sp2 repository:\n# paginate \u0026#34;https://packages.local/service/rest/v1/components?repository=csm-sle-15sp2\u0026#34; \\ | jq -r \u0026#39;.items[] | .name\u0026#39; | sort -u noarch/basecamp-1.0.1-20210126131805_a665272.noarch.rpm noarch/csm-testing-1.3.2-20210205160852_e012960.noarch.rpm noarch/docs-csm-1.7.4-20210206165423_2fae6fa.noarch.rpm noarch/dracut-metal-dmk8s-1.4.7-20210129115153_7a86571.noarch.rpm noarch/dracut-metal-luksetcd-1.0.2-20210129115153_b34f9a5.noarch.rpm noarch/dracut-metal-mdsquash-1.4.20-20210201222655_e20e2ee.noarch.rpm noarch/goss-servers-1.3.2-20210205160852_e012960.noarch.rpm noarch/hpe-csm-goss-package-0.3.13-20210127124704_aae8d77.noarch.rpm noarch/hpe-csm-scripts-0.0.4-20210125173103_a527e49.noarch.rpm noarch/hpe-csm-yq-package-3.4.1-20210127134802_789be45.noarch.rpm noarch/metal-ipxe-1.4.33-20210127152038_ef91cc8.noarch.rpm noarch/metal-net-scripts-0.0.1-20210204114016_95ab47a.noarch.rpm noarch/nexus-0.5.2-1.20210115090713_aef3950.noarch.rpm noarch/platform-utils-0.1.2-20210115162116_1139af5.noarch.rpm noarch/platform-utils-0.1.5-20210203170424_ca869e9.noarch.rpm repodata/2aadc798a4f7e12e99be79e0faa8bb2c2fe05871295edda8a4045fd371e7a568-primary.xml.gz repodata/452f91a378fa64c52534c984b90cf492e546334732c5b940b8fe5cfe2aebde29-filelists.sqlite.bz2 repodata/8dbbe7d1fceb13ccbae981aa9abe8575004df7bb3c0a74669502b5ea53a5455c-other.xml.gz repodata/a4e95cc8a79f42b150d6c505c3f8e6bf242ee69de7849a2973dd19e0c1d8f07a-filelists.xml.gz repodata/d3a16a9bceebf92fd640d689a8c015984d2963e4c11d7a841ec9b24cc135e99a-primary.sqlite.bz2 repodata/e9e8163a7c956f38eb37d6af3f1ac1bdae8079035843c9cd22ced9e824498da0-other.sqlite.bz2 repodata/repomd.xml x86_64/cfs-state-reporter-1.4.4-20201204120230_c198848.x86_64.rpm x86_64/cfs-state-reporter-1.4.6-20210128142236_6bb340b.x86_64.rpm x86_64/cfs-trust-1.0.2-20201216135115_58f3d86.x86_64.rpm x86_64/cfs-trust-1.0.3-20210125135157_2a234cb.x86_64.rpm x86_64/craycli-0.40.8-20210122153602_8689250.x86_64.rpm x86_64/craycli-wrapper-0.40.6-20210122144319_0608411.x86_64.rpm x86_64/cray-cmstools-crayctldeploy-0.8.18-20210129163615_eebc7e8.x86_64.rpm x86_64/cray-metal-basecamp-1.1.0-20210203221641_1de4aa6.x86_64.rpm x86_64/cray-nexus-0.9.0-2.20210204104035_080a74e.x86_64.rpm x86_64/cray-site-init-1.5.18-20210204150234_b3ed304.x86_64.rpm x86_64/cray-switchboard-1.2.0-20201204104325_4cadbeb.x86_64.rpm x86_64/cray-uai-util-1.0.5-20201210081525_e7edeef.x86_64.rpm x86_64/csm-ssh-keys-1.0.3-20201211133315_1c474e9.x86_64.rpm x86_64/csm-ssh-keys-1.0.4-20210125120351_1193c0c.x86_64.rpm x86_64/csm-ssh-keys-roles-1.0.3-20201211133315_1c474e9.x86_64.rpm x86_64/csm-ssh-keys-roles-1.0.4-20210125120351_1193c0c.x86_64.rpm x86_64/hms-ct-test-crayctldeploy-1.3.2-20210203105451_5db2610.x86_64.rpm x86_64/loftsman-1.0.3.3-20210111191208_5322725.x86_64.rpm x86_64/manifestgen-1.3.1-20210201191257_7a2f66d.x86_64.rpm Each component item has the following structure:\n{ \u0026#34;id\u0026#34;: \u0026#34;Y3NtLXNsZS0xNXNwMjowYTFiOWQwNWQ3MTYyYWE4NWQ3NzQ3ZWFhNzVmMTcxYw\u0026#34;, \u0026#34;repository\u0026#34;: \u0026#34;csm-sle-15sp2\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;raw\u0026#34;, \u0026#34;group\u0026#34;: \u0026#34;/noarch\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;noarch/csm-testing-1.3.2-20210205160852_e012960.noarch.rpm\u0026#34;, \u0026#34;version\u0026#34;: null, \u0026#34;assets\u0026#34;: [ { \u0026#34;downloadUrl\u0026#34;: \u0026#34;https://packages.local/repository/csm-sle-15sp2/noarch/csm-testing-1.3.2-20210205160852_e012960.noarch.rpm\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;noarch/csm-testing-1.3.2-20210205160852_e012960.noarch.rpm\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;Y3NtLXNsZS0xNXNwMjpiZDdmNzllMTk2NzMwNTA4NjQ1OTczNzQwYTMwZTRjMg\u0026#34;, \u0026#34;repository\u0026#34;: \u0026#34;csm-sle-15sp2\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;raw\u0026#34;, \u0026#34;checksum\u0026#34;: { \u0026#34;sha1\u0026#34;: \u0026#34;daecc7f20e1ddd5dd50b8b40351203882e2ad1c4\u0026#34;, \u0026#34;sha512\u0026#34;: \u0026#34;5343a189a7fb10bd43033f6b36e13cb85d75e705de2fab63a18c7cda4e3e57233ee3bfe55450e497aa0fbbdf2f2d024fb2ef2c3081e529a0bde9fa843d06a288\u0026#34;, \u0026#34;sha256\u0026#34;: \u0026#34;f7f779126031bcbc266c81d5f1546852aee0fb08890b7fba07b6fafd23e79d3b\u0026#34;, \u0026#34;md5\u0026#34;: \u0026#34;2a600edec22b34cbf5886db725389ed0\u0026#34; } } ] } For example, to list the download URLs for each asset in the csm-sle-15sp2 repository:\n# paginate \u0026#34;https://packages.local/service/rest/v1/components?repository=csm-sle-15sp2\u0026#34; \\ | jq -r \u0026#39;.items[] | .assets[] | .downloadUrl\u0026#39; | sort -u https://packages.local/repository/csm-sle-15sp2/noarch/basecamp-1.0.1-20210126131805_a665272.noarch.rpm https://packages.local/repository/csm-sle-15sp2/noarch/csm-testing-1.3.2-20210205160852_e012960.noarch.rpm https://packages.local/repository/csm-sle-15sp2/noarch/docs-csm-1.7.4-20210206165423_2fae6fa.noarch.rpm https://packages.local/repository/csm-sle-15sp2/noarch/dracut-metal-dmk8s-1.4.7-20210129115153_7a86571.noarch.rpm https://packages.local/repository/csm-sle-15sp2/noarch/dracut-metal-luksetcd-1.0.2-20210129115153_b34f9a5.noarch.rpm https://packages.local/repository/csm-sle-15sp2/noarch/dracut-metal-mdsquash-1.4.20-20210201222655_e20e2ee.noarch.rpm https://packages.local/repository/csm-sle-15sp2/noarch/goss-servers-1.3.2-20210205160852_e012960.noarch.rpm https://packages.local/repository/csm-sle-15sp2/noarch/hpe-csm-goss-package-0.3.13-20210127124704_aae8d77.noarch.rpm https://packages.local/repository/csm-sle-15sp2/noarch/hpe-csm-scripts-0.0.4-20210125173103_a527e49.noarch.rpm https://packages.local/repository/csm-sle-15sp2/noarch/hpe-csm-yq-package-3.4.1-20210127134802_789be45.noarch.rpm https://packages.local/repository/csm-sle-15sp2/noarch/metal-ipxe-1.4.33-20210127152038_ef91cc8.noarch.rpm https://packages.local/repository/csm-sle-15sp2/noarch/metal-net-scripts-0.0.1-20210204114016_95ab47a.noarch.rpm https://packages.local/repository/csm-sle-15sp2/noarch/nexus-0.5.2-1.20210115090713_aef3950.noarch.rpm https://packages.local/repository/csm-sle-15sp2/noarch/platform-utils-0.1.2-20210115162116_1139af5.noarch.rpm https://packages.local/repository/csm-sle-15sp2/noarch/platform-utils-0.1.5-20210203170424_ca869e9.noarch.rpm https://packages.local/repository/csm-sle-15sp2/repodata/2aadc798a4f7e12e99be79e0faa8bb2c2fe05871295edda8a4045fd371e7a568-primary.xml.gz https://packages.local/repository/csm-sle-15sp2/repodata/452f91a378fa64c52534c984b90cf492e546334732c5b940b8fe5cfe2aebde29-filelists.sqlite.bz2 https://packages.local/repository/csm-sle-15sp2/repodata/8dbbe7d1fceb13ccbae981aa9abe8575004df7bb3c0a74669502b5ea53a5455c-other.xml.gz https://packages.local/repository/csm-sle-15sp2/repodata/a4e95cc8a79f42b150d6c505c3f8e6bf242ee69de7849a2973dd19e0c1d8f07a-filelists.xml.gz https://packages.local/repository/csm-sle-15sp2/repodata/d3a16a9bceebf92fd640d689a8c015984d2963e4c11d7a841ec9b24cc135e99a-primary.sqlite.bz2 https://packages.local/repository/csm-sle-15sp2/repodata/e9e8163a7c956f38eb37d6af3f1ac1bdae8079035843c9cd22ced9e824498da0-other.sqlite.bz2 https://packages.local/repository/csm-sle-15sp2/repodata/repomd.xml https://packages.local/repository/csm-sle-15sp2/x86_64/cfs-state-reporter-1.4.4-20201204120230_c198848.x86_64.rpm https://packages.local/repository/csm-sle-15sp2/x86_64/cfs-state-reporter-1.4.6-20210128142236_6bb340b.x86_64.rpm https://packages.local/repository/csm-sle-15sp2/x86_64/cfs-trust-1.0.2-20201216135115_58f3d86.x86_64.rpm https://packages.local/repository/csm-sle-15sp2/x86_64/cfs-trust-1.0.3-20210125135157_2a234cb.x86_64.rpm https://packages.local/repository/csm-sle-15sp2/x86_64/craycli-0.40.8-20210122153602_8689250.x86_64.rpm https://packages.local/repository/csm-sle-15sp2/x86_64/craycli-wrapper-0.40.6-20210122144319_0608411.x86_64.rpm https://packages.local/repository/csm-sle-15sp2/x86_64/cray-cmstools-crayctldeploy-0.8.18-20210129163615_eebc7e8.x86_64.rpm https://packages.local/repository/csm-sle-15sp2/x86_64/cray-metal-basecamp-1.1.0-20210203221641_1de4aa6.x86_64.rpm https://packages.local/repository/csm-sle-15sp2/x86_64/cray-nexus-0.9.0-2.20210204104035_080a74e.x86_64.rpm https://packages.local/repository/csm-sle-15sp2/x86_64/cray-site-init-1.5.18-20210204150234_b3ed304.x86_64.rpm https://packages.local/repository/csm-sle-15sp2/x86_64/cray-switchboard-1.2.0-20201204104325_4cadbeb.x86_64.rpm https://packages.local/repository/csm-sle-15sp2/x86_64/cray-uai-util-1.0.5-20201210081525_e7edeef.x86_64.rpm https://packages.local/repository/csm-sle-15sp2/x86_64/csm-ssh-keys-1.0.3-20201211133315_1c474e9.x86_64.rpm https://packages.local/repository/csm-sle-15sp2/x86_64/csm-ssh-keys-1.0.4-20210125120351_1193c0c.x86_64.rpm https://packages.local/repository/csm-sle-15sp2/x86_64/csm-ssh-keys-roles-1.0.3-20201211133315_1c474e9.x86_64.rpm https://packages.local/repository/csm-sle-15sp2/x86_64/csm-ssh-keys-roles-1.0.4-20210125120351_1193c0c.x86_64.rpm https://packages.local/repository/csm-sle-15sp2/x86_64/hms-ct-test-crayctldeploy-1.3.2-20210203105451_5db2610.x86_64.rpm https://packages.local/repository/csm-sle-15sp2/x86_64/loftsman-1.0.3.3-20210111191208_5322725.x86_64.rpm https://packages.local/repository/csm-sle-15sp2/x86_64/manifestgen-1.3.1-20210201191257_7a2f66d.x86_64.rpm Create a Repository Repositories are created by an HTTP POST request to the /service/rest/beta/repositories/\u0026lt;format\u0026gt;/\u0026lt;type\u0026gt; endpoint with an appropriate body that defines the repository settings.\nFor example, to create a hosted yum repository for RPMs using the default blob store, HTTP POST the following body (replace NAME as appropriate) to /service/rest/beta/repositories/yum/hosted:\n{ \u0026#34;name\u0026#34;: \u0026#34;NAME\u0026#34;, \u0026#34;online\u0026#34;: true, \u0026#34;storage\u0026#34;: { \u0026#34;blobStoreName\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;strictContentTypeValidation\u0026#34;: true, \u0026#34;writePolicy\u0026#34;: \u0026#34;ALLOW_ONCE\u0026#34; }, \u0026#34;cleanup\u0026#34;: null, \u0026#34;yum\u0026#34;: { \u0026#34;repodataDepth\u0026#34;: 0, \u0026#34;deployPolicy\u0026#34;: \u0026#34;STRICT\u0026#34; }, \u0026#34;format\u0026#34;: \u0026#34;yum\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;hosted\u0026#34; } The storage and yum options are used to control repository behavior.\nTo create a proxy repository to an upstream repository given by URL, HTTP POST the following body (replace NAME and URL as appropriate) to the /service/rest/beta/repositories/raw/proxy endpoint:\n{ \u0026#34;cleanup\u0026#34;: null, \u0026#34;format\u0026#34;: \u0026#34;raw\u0026#34;, \u0026#34;httpClient\u0026#34;: { \u0026#34;authentication\u0026#34;: null, \u0026#34;autoBlock\u0026#34;: false, \u0026#34;blocked\u0026#34;: false, \u0026#34;connection\u0026#34;: null }, \u0026#34;name\u0026#34;: \u0026#34;NAME\u0026#34;, \u0026#34;negativeCache\u0026#34;: { \u0026#34;enabled\u0026#34;: false, \u0026#34;timeToLive\u0026#34;: 0 }, \u0026#34;online\u0026#34;: true, \u0026#34;proxy\u0026#34;: { \u0026#34;contentMaxAge\u0026#34;: 1440, \u0026#34;metadataMaxAge\u0026#34;: 5, \u0026#34;remoteUrl\u0026#34;: \u0026#34;URL\u0026#34; }, \u0026#34;routingRule\u0026#34;: null, \u0026#34;storage\u0026#34;: { \u0026#34;blobStoreName\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;strictContentTypeValidation\u0026#34;: false }, \u0026#34;type\u0026#34;: \u0026#34;proxy\u0026#34; } The proxy, httpClient, and negativeCache options impact the proxy behavior. It may be helpful to create a repository via the Web UI, then retrieve its configuration through the /service/rest/beta/repositories endpoint in order to discover how to set appropriate settings.\nInstallers typically define Nexus repositories in nexus-repositories.yaml and rely on the nexus-repositories-create helper script included in the cray/cray-nexus-setup container image to facilitate creation.\nUpdate a Repository Update the configuration for a repository by sending an HTTP PUT request to the /service/rest/beta/repositories/FORMAT/TYPE/NAME endpoint.\nFor example, if the yum hosted repository test is currently online and it needs to be updated to be offline instead. Send an HTTP PUT request to the /service/rest/beta/repositories/yum/hosted/test endpoint after getting the current configuration and setting the online attribute to true:\n# curl -sS https://packages.local/service/rest/beta/repositories \\ | jq \u0026#39;.[] | select(.name == \u0026#34;test\u0026#34;)\u0026#39; { \u0026#34;name\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://packages.local/repository/test\u0026#34;, \u0026#34;online\u0026#34;: true, \u0026#34;storage\u0026#34;: { \u0026#34;blobStoreName\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;strictContentTypeValidation\u0026#34;: true, \u0026#34;writePolicy\u0026#34;: \u0026#34;ALLOW_ONCE\u0026#34; }, \u0026#34;cleanup\u0026#34;: null, \u0026#34;yum\u0026#34;: { \u0026#34;repodataDepth\u0026#34;: 0, \u0026#34;deployPolicy\u0026#34;: \u0026#34;STRICT\u0026#34; }, \u0026#34;format\u0026#34;: \u0026#34;yum\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;hosted\u0026#34; } # curl -sS https://packages.local/service/rest/beta/repositories \\ | jq \u0026#39;.[] | select(.name == \u0026#34;test\u0026#34;) | .online = false\u0026#39; | curl -sSi -X PUT \\ \u0026#39;https://packages.local/service/rest/beta/repositories/yum/hosted/test\u0026#39; -H \u0026#34;Content-Type: application/json\u0026#34; -d @- HTTP/2 204 date: Sat, 06 Mar 202117:55:57 GMT server: istio-envoy x-content-type-options: nosniff x-envoy-upstream-service-time: 9 # curl -sS https://packages.local/service/rest/beta/repositories \\ | jq \u0026#39;.[] | select(.name == \u0026#34;test\u0026#34;)\u0026#39; { \u0026#34;name\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://packages.local/repository/test\u0026#34;, \u0026#34;online\u0026#34;: false, \u0026#34;storage\u0026#34;: { \u0026#34;blobStoreName\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;strictContentTypeValidation\u0026#34;: true, \u0026#34;writePolicy\u0026#34;: \u0026#34;ALLOW_ONCE\u0026#34; }, \u0026#34;cleanup\u0026#34;: null, \u0026#34;yum\u0026#34;: { \u0026#34;repodataDepth\u0026#34;: 0, \u0026#34;deployPolicy\u0026#34;: \u0026#34;STRICT\u0026#34; }, \u0026#34;format\u0026#34;: \u0026#34;yum\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;hosted\u0026#34; } Delete a Repository To delete a repository, send an HTTP DELETE request to the /service/rest/beta/repositories/NAME.\nFor example:\n# curl -sfkSL -X DELETE \u0026#34;https://packages.local/service/rest/beta/repositories/NAME\u0026#34; Create a Blob Store A File type blob store may be created by sending an HTTP POST request to the /service/rest/beta/blobstores/file with the following body (replace NAME as appropriate):\n{ \u0026#34;name\u0026#34;: \u0026#34;NAME\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/nexus-data/blobs/NAME\u0026#34;, \u0026#34;softQuota\u0026#34;: null } Installers typically define Nexus blob stores in nexus-blobstores.yaml and rely on the nexus-blobstores-create helper script included in the cray/cray-nexus-setup container image to facilitate creation.\nDelete a Blob Store To delete a blob store, send an HTTP DELETE request to the /service/rest/v1/blobstores/NAME endpoint.\nFor example:\n# curl -sfkSL -X DELETE \u0026#34;https://packages.local/service/rest/v1/blobstores/NAME\u0026#34; "
},
{
	"uri": "/docs-csm/en-11/operations/node_management/update_the_gigabyte_server_bios_time/",
	"title": "Update The Gigabyte Server Bios Time",
	"tags": [],
	"description": "",
	"content": "Update the Gigabyte Server BIOS Time Check and set the time for Gigabyte compute nodes.\nIf the console log indicates the time between the rest of the system and the compute nodes is off by several hours, it prevents the spire-agent from getting a valid certificate, causing the node boot to drop into the dracut emergency shell.\nProcedure   Enter the \u0026ldquo;Del\u0026rdquo; key during the POST call.\n  Update the \u0026ldquo;System Date\u0026rdquo; field to match the time on the system.\nThe \u0026ldquo;System Date\u0026rdquo; field is located under the \u0026ldquo;Main\u0026rdquo; tab in the navigation bar.\n  Enter the \u0026ldquo;F10\u0026rdquo; key followed by the \u0026ldquo;Enter\u0026rdquo; key to save the BIOS time.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/node_management/use_the_physical_kvm/",
	"title": "Use The Physical KVM",
	"tags": [],
	"description": "",
	"content": "Use the Physical KVM For those who prefer to stand in front of the system and use a physically connected keyboard, mouse, and monitor, Cray provides a rack-mount-extendable KVM unit installed in rack unit slot 23 (RU23) of the management cabinet. It is connected to the first non-compute node (NCN) by default.\nTo use it, pull it out and raise the lid.\nTo bring up the main menu (shown in following figure), press Prnt Scrn.\nEach node in the system (except ClusterStor) appears in the main menu associated with a port. The first NCN is port 01, the other three NCNs are ports 02–04, and the compute nodes are 05–08.\nTo move to any node in the system, use the arrow keys and press Enter. The login screen for that node will appear.\n"
},
{
	"uri": "/docs-csm/en-11/operations/node_management/verify_accuracy_of_the_system_clock/",
	"title": "Verify Accuracy Of The System Clock",
	"tags": [],
	"description": "",
	"content": "Verify Accuracy of the System Clock When a non-compute node (NCN) is rebooted, there may be cases when the node\u0026rsquo;s time is not synchronized, or set correctly. Use this procedure to ensure that the time is set correctly on a rebooted node.\nAn inaccurate system clock can result in a number of problems, such as issues with Kubernetes operations, etcd, node\u0026rsquo;s responsiveness, and more.\nPrerequisites This procedure requires root privileges.\nProcedure   Log in as root to an NCN.\nThis procedure assumes that it is being carried out on an NCN Kubernetes worker, as indicated by the command prompts. Prompts will depend on the actual NCN that the procedure is performed on.\n  Run the Ansible play for NTP.\nThe NTP Ansible playbook may also be re-run if a bad NTP is suspected.\nncn-w001# ansible-playbook /opt/cray/crayctl/ansible_framework/main/ntp.yml   Verify that the time is set correctly on each of the NCNs.\nncn-w001# pdsh -w ncn-s00[1-3],ncn-m00[1-3],ncn-w00[1-3] date ncn-s001: Mon Dec 2 15:59:20 CST 2019 ncn-s003: Mon Dec 2 15:59:20 CST 2019 ncn-s002: Mon Dec 2 15:59:20 CST 2019 ncn-m003: Mon Dec 2 15:59:20 CST 2019 ncn-m001: Mon Dec 2 15:59:20 CST 2019 ncn-m002: Mon Dec 2 15:59:20 CST 2019 ncn-w001: Mon Dec 2 15:59:20 CST 2019 ncn-w003: Mon Dec 2 15:59:21 CST 2019 ncn-w002: Mon Dec 2 15:59:21 CST 2019   "
},
{
	"uri": "/docs-csm/en-11/operations/node_management/view_bios_logs_for_liquid_cooled_nodes/",
	"title": "View Bios Logs For Liquid Cooled Nodes",
	"tags": [],
	"description": "",
	"content": "View BIOS Logs for Liquid Cooled Nodes SSH to a Liquid Cooled node and view the BIOS logs. The BIOS logs for Liquid Cooled node controllers (nC) are stored in the /var/log/n0/current and var/log/n1/current directories.\nThe BIOS logs for Liquid Cooled nodes are helpful for troubleshooting boot-related issues.\nPrerequisites This procedure requires administrative privileges.\nProcedure   Log in to the node.\nSSH into the node controller for the host xname. For example, if the host xname (as defined in /etc/hosts) is x5000c1s0b0n0, the node controller would be x5000c1s0b0.\nncn-w001# ssh XNAME   Confirm the hostname is correct for the node being used.\n# hostname x1000c2s5b0   Change to the /var/log/n0 directory.\n# cd /var/log/n0   View the logs for n0.\nn0 is node 0 on the BMC.\n# tail current   Change to the /var/log/n1 directory.\n# cd /var/log/n1   View the logs for n1.\nn1 is node 1 on the BMC.\n# tail current   "
},
{
	"uri": "/docs-csm/en-11/operations/node_management/tls_certificates_for_redfish_bmcs/",
	"title": "TLS Certificates For Redfish BMCs",
	"tags": [],
	"description": "",
	"content": "TLS Certificates for Redfish BMCs Redfish HTTP communications are capable of using TLS certificates and Certificate Authority (CA) trust bundles to improve security. Several Hardware Management Services (HMS) have been modified to enable the HTTP transports used for Redfish communications to use a CA trust bundle.\nThe following services communicate with Redfish BMCs:\n State Manager Daemon (SMD) Cray Advanced Platform Monitoring and Control (CAPMC) Firmware Action Service (FAS) HMS Collector River Endpoint Discovery Service (REDS) Mountain Endpoint Discovery Service (MEDS)  Each Redfish BMC must have a TLS certificate in order to be useful. The certificates will come from the same PKI that issues the CA trust bundle. The Vault PKI is used to create the TLS certs. Services will get the CA trust bundle either directly from the Vault PKI, or it can be read in via a Kubernetes ConfigMap.\nTLS Certificate Creation TLS certificates are created by the System Configuration Service (SCSD) tool and are stored in Vault secure storage for later retrieval. Each certificate can be created at any level or domain from the individual BMC, all the way up to a cabinet-level. Any certificate created above the BMC domain will contain enough Subject Alternative Names (SANs) to cover that domain. The goal and intent is to create certificates at the cabinet domain, containing the SANs for every possible BMC in that cabinet.\nRefer to Add TLS Certificates to BMCs for the SCSD commands used to create and store the certificates.\nOnce the certificates are created, they can be placed on the target BMCs again using SCSD. Only liquid-cooled BMCs can have TLS certificates set in them.\nCA Bundle Usage By Services Services will use a CA trust bundle when creating secured/validated HTTP clients and transports for use in Redfish operations. Any services that communicate with other services must not use this same client/transport because these services are within the service mesh and do not use TLS certificates. Thus, most services will need different HTTP clients/transports for Redfish and for inter-service communications.\nThe CA trust bundle is placed into a file visible by each HMS service. The Helm chart for each service will specify where this file is located. In addition, there is an environment variable (CA_URI) that comes from a value in customizations.yaml and will direct the service to point to either the Vault PKI\u0026rsquo;s CA bundle or to the ConfigMap bundle.\nIf the CA_URI variable is an empty string, it means that the customizations.yaml has no special entry for it. In this event, each service is set up to not use a CA bundle for Redfish HTTP clients/transports. Thus, this implementation is very backward compatible.\n"
},
{
	"uri": "/docs-csm/en-11/operations/node_management/troubleshoot_interfaces_with_ip_address_issues/",
	"title": "Troubleshoot Interfaces With Ip Address Issues",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Interfaces with IP Address Issues Correct NCNs that are failing to assigning a static IP address or detect a duplicate IP address.\nThe Wicked network manager tool will fail to bring an interface up if its assigned IP address already exists in the respective LAN. This can be detected by checking for signs of duplicate IP messages in the log.\nPrerequisites An NCN has an interface that is failing to assign a static IP address or that has a duplicate IP address.\nProcedure   Use one of the following workarounds to correct NCNs that are failing to assigning a static IP address or detect a duplicate IP address:\n  Check the logs in /var/log/* to see what MAC address is being used for the IP address.\nncn-w001# grep duplicate /var/log/* warn:2020-08-04T19:22:02.434775+00:00 ncn-w001 wickedd[2188]: bond0: IPv4 duplicate address 10.1.1.1 detected (in use by 00:30:48:bb:e8:d2)!   Add an IP address that is not found or commonly assigned on the respective network.\n  Edit the /etc/sysconfig/network/ifcfg-FILENAME file.\nncn-w001# vi /etc/sysconfig/network/ifcfg-FILENAME   Reload the interface.\nUse the following command to safely reload the interface:\nncn-w001# wicked ifreload INTERFACE_NAME If that does not work, attempt to forcefully add it:\nncn-w001# systemctl restart wickedd-nanny     Add the duplicate IP address with the ip command.\n  Add the duplicate IP.\nThe command below will bypass Wicked and will not honor the system preference:\nncn-w001# ip a a IP_ADDRESS/MASK dev INTERFACE_NAME For example:\nncn-w001# ip a a 10.1.1.1/16 dev bond0   View the bond.\nncn-w001# ip a s bond0 8: bond0: \u0026lt;BROADCAST,MULTICAST,MASTER,UP,LOWER_UP\u0026gt; mtu 9238 qdisc noqueue state UP group default qlen 1000 link/ether b8:59:9f:c7:11:12 brd ff:ff:ff:ff:ff:ff inet 10.1.1.1/16 brd 10.1.255.255 scope global bond0 valid_lft forever preferred_lft forever inet6 fe80::ba59:9fff:fec7:1112/64 scope link valid_lft forever preferred_lft forever   Delete the IP address after the duplicate IP address is removed.\nncn-w001# ip a d IP_ADDRESS/MASK dev bond0 For example:\nncn-w001# ip a d 10.1.1.1/16 dev bond0     (Not Recommended) Allow the duplicate IP address to exist.\nThis is not recommended because it is unstable and can make the work harder to correct down the line. The easiest way to deal with the duplicate is by adding another IP address, and then logging into the duplicate and nullifying it. This block will disable the safeguard for duplicate IP addresses.\nncn-w001# sed -i \u0026#39;^CHECK_DUPLICATE_IP=.*/CHECK_DUPLICATE_IP=\u0026#34;no\u0026#34;/\u0026#39; \\ /etc/sysconfig/network/config ncn-w001# wicked ifup INTERFACE_NAME     Notes   Running wicked ifreload on a worker node can have the side-effect of causing Slurm and UAI pods to lose their macvlan attachments. In this case, restarts of those services (in the Kubernetes user namespace) can be performed by executing the following command:\nncn-w# kubectl delete po -n user $(kubectl get po -n user | grep -v NAME | awk \u0026#39;\\{ print $1 }\u0026#39;)   "
},
{
	"uri": "/docs-csm/en-11/operations/node_management/troubleshoot_issues_with_redfish_endpoint_discovery/",
	"title": "Troubleshoot Issues With Redfish Endpoint Discovery",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Issues with Redfish Endpoint Discovery If a Redfish endpoint is in the HTTPsGetFailed status, the endpoint does not need to be fully rediscovered. The error indicates an issue in the inventory process done by the Hardware State Manager (HSM). Restart the inventory process to fix this issue.\nUpdate the HSM inventory to resolve issues with discovering Redfish endpoints.\nThe following is an example of the HSM error:\nncn-m001# cray hsm inventory redfishEndpoints describe x3000c0s15b0 Domain = \u0026#34; MACAddr = \u0026#34;b42e993b70ac\u0026#34; Enabled = true Hostname = \u0026#34;10.254.2.100\u0026#34; RediscoverOnUpdate = true FQDN = \u0026#34;10.254.2.100\u0026#34; User = \u0026#34;root\u0026#34; Password = \u0026#34; Type = \u0026#34;NodeBMC\u0026#34; ID = \u0026#34;x3000c0s15b0\u0026#34; [DiscoveryInfo] LastDiscoveryAttempt = \u0026#34;2019-11-18T21:34:29.990441Z\u0026#34; RedfishVersion = \u0026#34;1.1.0\u0026#34; LastDiscoveryStatus = \u0026#34;HTTPsGetFailed\u0026#34; Prerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system.  Procedure   Restart the HSM inventory process.\nncn-m001# cray hsm inventory discover create --xnames XNAME   Verify the Redfish endpoint has been rediscovered by HSM.\nncn-m001# cray hsm inventory redfishEndpoints describe XNAME Domain = \u0026#34; MACAddr = \u0026#34;b42e993b70ac\u0026#34; Enabled = true Hostname = \u0026#34;10.254.2.100\u0026#34; RediscoverOnUpdate = true FQDN = \u0026#34;10.254.2.100\u0026#34; User = \u0026#34;root\u0026#34; Password = \u0026#34; Type = \u0026#34;NodeBMC\u0026#34; ID = \u0026#34;x3000c0s15b0\u0026#34; [DiscoveryInfo] LastDiscoveryAttempt = \u0026#34;2019-11-18T21:34:29.990441Z\u0026#34; RedfishVersion = \u0026#34;1.1.0\u0026#34; LastDiscoveryStatus = \u0026#34;DiscoverOK\u0026#34; Troubleshooting can stop if the discovery status is DiscoverOK. If the component still fails to discover and Enabled = true, proceed to step 5.\n  Re-enable the RedfishEndpoint by setting both the Enabled and rediscoverOnUpdate fields to true.\nIf Enabled=false for the RedfishEndpoint, it may indicate a network or firmware issue with the BMC.\nncn-m001# cray hsm inventory redfishEndpoints update BMC_XNAME \\ --enabled true --rediscover-on-update true   Verify the Redfish endpoint has been rediscovered by HSM.\nRe-enabling the RedfishEndpoint will cause a rediscovery to start. Troubleshooting can stop if the discovery status is DiscoverOK.\nncn-m001# cray hsm inventory redfishEndpoints describe BMC_XNAME Troubleshooting: If discovery is still failing, use the curl command to manually contact the BMC via Redfish.\nncn-m001# curl -k -u USERNAME:PASSWORD https://BMC_XNAME/redfish/v1/ If there is no response to /redfish/v1/, the BMC is either not powered or there is a network issue.\n  Contact other Redfish URLs on the BMC.\nIf the response is one of the following, there is a firmware issue and a BMC restart or update may be needed:\n  An empty response:\nncn-m001# curl -ku USERNAME:PASSWORD \\ https://BMC_XNAME/redfish/v1/Systems/Node0 | jq . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 1330 100 1330 0 0 5708 0 --:--:-- --:--:-- --:--:-- 5708 {}   A garbled response:\nncn-m001# curl -ku USERNAME:PASSWORD \\ https://BMC_XNAME/redfish/v1/Managers/Self \u0026lt;pre style=\u0026#34;font-size:12px; font-family:monospace; color:#8B0000;\u0026#34;\u0026gt;[web.lua] Error in RequestHandler, thread: 0xb60670d8 is dead. ▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼ .\u0026amp;#47;redfish-handler.lua:0: attempt to index a nil value stack traceback: .\u0026amp;#47;turbo\u0026amp;#47;httpserver.lua:251: in function \u0026amp;lt;.\u0026amp;#47;turbo\u0026amp;#47;httpserver.lua:212\u0026amp;gt; [C]: in function \u0026amp;#39;xpcall\u0026amp;#39; .\u0026amp;#47;turbo\u0026amp;#47;iostream.lua:553: in function \u0026amp;lt;.\u0026amp;#47;turbo\u0026amp;#47;iostream.lua:544\u0026amp;gt; [C]: in function \u0026amp;#39;xpcall\u0026amp;#39; .\u0026amp;#47;turbo\u0026amp;#47;ioloop.lua:573: in function \u0026amp;lt;.\u0026amp;#47;turbo\u0026amp;#47;ioloop.lua:572\u0026amp;gt; ▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲\u0026lt;/pre\u0026gt;   The URLs listed for the Systems do not include /Systems/ in the URL:\nncn-m001# curl -ku USERNAME:PASSWORD \\ https://BMC_XNAME/redfish/v1/Systems | jq . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 421 100 421 0 0 1427 0 --:--:-- --:--:-- --:--:-- 1422 { \u0026#34;@odata.context\u0026#34;: \u0026#34;/redfish/v1/$metadata#ComputerSystemCollection.ComputerSystemCollection\u0026#34;, \u0026#34;@odata.etag\u0026#34;: \u0026#34;W/\\\u0026#34;1604517759\\\u0026#34;, \u0026#34;@odata.id\u0026#34;: \u0026#34;/redfish/v1/Systems\u0026#34;, \u0026#34;@odata.type\u0026#34;: \u0026#34;#ComputerSystemCollection.ComputerSystemCollection\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Collection of Computer Systems\u0026#34;, \u0026#34;Members\u0026#34;: [ { \u0026#34;@odata.id\u0026#34;: \u0026#34;/redfish/v1/Node0\u0026#34; }, { \u0026#34;@odata.id\u0026#34;: \u0026#34;/redfish/v1/Node1\u0026#34; } ], \u0026#34;Members@odata.count\u0026#34;: 2, \u0026#34;Name\u0026#34;: \u0026#34;Systems Collection\u0026#34; }   The URL for a system is missing the SystemID:\nncn-m001# curl -ku USERNAME:PASSWORD \\ https://BMC_XNAME/redfish/v1/Systems | jq . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 421 100 421 0 0 1427 0 --:--:-- --:--:-- --:--:-- 1422 { \u0026#34;@odata.context\u0026#34;: \u0026#34;/redfish/v1/$metadata#ComputerSystemCollection.ComputerSystemCollection\u0026#34;, \u0026#34;@odata.etag\u0026#34;: \u0026#34;W/\\\u0026#34;1604517759\\\u0026#34;, \u0026#34;@odata.id\u0026#34;: \u0026#34;/redfish/v1/Systems\u0026#34;, \u0026#34;@odata.type\u0026#34;: \u0026#34;#ComputerSystemCollection.ComputerSystemCollection\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Collection of Computer Systems\u0026#34;, \u0026#34;Members\u0026#34;: [ { \u0026#34;@odata.id\u0026#34;: \u0026#34;/redfish/v1/Systems/\u0026#34; } ], \u0026#34;Members@odata.count\u0026#34;: 1, \u0026#34;Name\u0026#34;: \u0026#34;Systems Collection\u0026#34; }   Troubleshooting: If there was no indication of a firmware issue, there may be an issue with Vault or the credentials stored in Vault.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/node_management/troubleshoot_loss_of_console_connections_and_logs_on_gigabyte_nodes/",
	"title": "Troubleshoot Loss Of Console Connections And Logs On Gigabyte Nodes",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Loss of Console Connections and Logs on Gigabyte Nodes Gigabyte console log information will no longer be collected, and if attempting to initiate a console session through the cray-conman pod, there will be an error reported. This error will occur every time the node is rebooted unless this workaround is applied.\nPrerequisites Console log information is no longer being collected for Gigabyte nodes or ConMan is reporting an error.\nProcedure   Use ipmitool to deactivate the current console connection.\nncn-m001# export USERNAME=root ncn-m001# export IPMI_PASSWORD=changeme ncn-m001# ipmitool -H xname -U $USERNAME -E sol deactivate   Retrieve the cray-conman pod ID.\nncn-m001# CONPOD=$(kubectl get pods -n services \\ -o wide|grep cray-conman|awk \u0026#39;{print $1}\u0026#39;) ncn-m001# echo $CONPOD cray-conman-77fdfc9f66-m2s9k   Log on to the pod.\nncn-m001# kubectl exec -it -n services $CONPOD /bin/bash   Initiate a console session to reconnect.\n[root@cray-conman-POD_ID app]# conman -j XNAME   "
},
{
	"uri": "/docs-csm/en-11/operations/node_management/update_compute_node_mellanox_hsn_nic_firmware/",
	"title": "Update Compute Node Mellanox HSN NIC Firmware",
	"tags": [],
	"description": "",
	"content": "Update Compute Node Mellanox HSN NIC Firmware This procedure updates liquid-cooled or standard rack compute node NIC mezzanine cards (NMC) firmware for Slingshot 10 Mellanox ConnectX-5 NICs. The deployed RPM on compute nodes contains the scripts and firmware images required to perform the firmware and configuration updates.\nAttention: The NIC firmware update is performed while the node is running the compute image (in-band). Use the CX-5 NIC firmware that is deployed with the compute node RPMs and not from some other repository.\nSee Update Firmware with FAS for information about automated firmware updates using Redfish.\nTime Required 2-5 minutes for a firmware update and 1-3 minutes for a configuration update.\nProcedure   SSH to the node as root.\n  Load the module.\nnid000001:~ # module load cray-shasta-mlnx-firmware nid000001:~ # module show cray-shasta-mlnx-firmware ------------------------------------------------------------------- /opt/cray/modulefiles/cray-shasta-mlnx-firmware/1.0.5: module-whatis \u0026#34;This module adds cray-shasta-mlnx-firmware v1.0.5 to the environment\u0026#34; prepend-path PATH /opt/cray/cray-shasta-mlnx-firmware/1.0.5/sbin -------------------------------------------------------------------   List the contents of the firmware directory.\nnid000001:~ # ls /opt/cray/cray-shasta-mlnx-firmware/1.0.5/share/firmware/* apply_mlnx_configs generate_mlnx_configs update_mlnx_firmware nid000001:~ # ls /opt/cray/cray-shasta-mlnx-firmware/1.0.5/share/firmware/ CRAY000000001/ MT_0000000011/ images/ nid000001:~ # ls /opt/cray/cray-shasta-mlnx-firmware/1.0.5/share/firmware/ CRAY000000001/ MT_0000000011/ images/ nid000001:~ # ls /opt/cray/cray-shasta-mlnx-firmware/1.0.5/share/firmware/* /opt/cray/cray-shasta-mlnx-firmware/1.0.5/share/firmware/CRAY000000001: config.xml fw-ConnectX5-rel-16_26_4012-Cray_Timms_mezz_100G_1P-UEFI-14.19.17-FlexBoot-3.5.805.bin /opt/cray/cray-shasta-mlnx-firmware/1.0.5/share/firmware/MT_0000000011: config.xml fw-ConnectX5-rel-16_26_4012-MCX515A-CCA_Ax-UEFI-14.19.17-FlexBoot-3.5.805.bin /opt/cray/cray-shasta-mlnx-firmware/1.0.5/share/firmware/images: CRAY000000001.bin MT_0000000011.bin   Update the firmware on the node.\nnid000001:~ # update_mlnx_firmware   Apply the configuration settings.\nnid000001:~ # apply_mlnx_configs   Determine the prepend pathname.\nnid000001:~ # module show cray-shasta-mlnx-firmware ------------------------------------------------------------------- /opt/cray/modulefiles/cray-shasta-mlnx-firmware/1.0.5: module-whatis \u0026#34;This module adds cray-shasta-mlnx-firmware v1.0.5 to the environment\u0026#34; prepend-path PATH /opt/cray/cray-shasta-mlnx-firmware/1.0.5/sbin -------------------------------------------------------------------   Log in to ncn-m001 and use pdsh to update the firmware.\nncn-m001# pdsh -w NODE_LIST /opt/cray/cray-shasta-mlnx-firmware/1.0.5/sbin/update_mlnx_firmware   Apply the configuration settings.\nncn-m001# pdsh -w NODE_LIST /opt/cray/cray-shasta-mlnx-firmware/1.0.5/sbin/apply_mlnx_configs   Use the Boot Orchestration Service (BOS) to reboot all the affected nodes.\nncn-m001# cray bos v1 session create --template-uuid SESSION_TEMPLATE \\ --operation reboot   "
},
{
	"uri": "/docs-csm/en-11/operations/node_management/reboot_ncns/",
	"title": "Reboot NCNs",
	"tags": [],
	"description": "",
	"content": "Reboot NCNs The following is a high-level overview of the non-compute node (NCN) reboot workflow:\n Run the NCN pre-reboot checks and procedures:  Ensure ncn-m001 is not running in \u0026ldquo;LiveCD\u0026rdquo; or install mode Check the metal.no-wipe settings for all NCNs Run all platform health checks, including checks on the Border Gateway Protocol (BGP) peering sessions Validate the current boot order   Run the rolling NCN reboot procedure:  Loop through reboots on storage nodes, worker nodes, and master nodes, where each boot consists of the following workflow: - Establish console session with node to reboot - Execute a Linux graceful shutdown or power off/on sequence to the node to allow it to boot up to completion - Execute NCN/platform health checks and do not go on to reboot the next NCN until health has been ensured on the most recently rebooted NCN - Disconnect console session with the node that was rebooted   Re-run all platform health checks, including checks on BGP peering sessions  The time duration for this procedure (if health checks are being executed in between each boot, as recommended) could take between two to four hours for a system with nine management nodes.\nThis same procedure can be used to reboot a single management node as outlined above. Be sure to carry out the NCN pre-reboot checks and procedures before and after rebooting the node. Execute the rolling NCN reboot procedure steps for the particular node type being rebooted.\nPrerequisites The kubectl command is installed.\nProcedure NCN Pre-Reboot Health Checks   Ensure that ncn-m001 is not running in \u0026ldquo;LiveCD\u0026rdquo; mode.\nThis mode should only be in effect during the initial product install. If the word \u0026ldquo;pit\u0026rdquo; is NOT in the hostname of ncn-m001, then it is not in the \u0026ldquo;LiveCD\u0026rdquo; mode.\nIf \u0026ldquo;pit\u0026rdquo; is in the hostname of ncn-m001, the system is not in normal operational mode and rebooting ncn-m001 may have unexpected results. This procedure assumes that the node is not running in the \u0026ldquo;LiveCD\u0026rdquo; mode that occurs during product install.\n  Check and set the metal.no-wipe setting on NCNs to ensure data on the node is preserved when rebooting.\nRefer to Check and Set the metal.no-wipe Setting on NCNs.\n  Run the platform health checks and analyze the results.\nRefer to the \u0026ldquo;Platform Health Checks\u0026rdquo; section in Validate CSM Health for an overview of the health checks.\n  Run the platform health scripts from a master or worker node.\nThe output of the following scripts will need to be referenced in the remaining sub-steps.\nncn-m001# /opt/cray/platform-utils/ncnHealthChecks.sh ncn-m001# /opt/cray/platform-utils/ncnPostgresHealthChecks.sh NOTE: If the ncnHealthChecks script output indicates any kube-multus-ds- pods are in a Terminating state, that can indicate a previous restart of these pods did not complete. In this case, it is safe to force delete these pods in order to let them properly restart by executing the kubectl delete po -n kube-system kube-multus-ds.. --force command. After executing this command, re-running the ncnHealthChecks script should indicate a new pod is in a Running state.\n  Check the status of the Kubernetes nodes.\nEnsure all Kubernetes nodes are in the Ready state.\nncn-m001# kubectl get nodes Troubleshooting: If the node that was rebooted is in a Not Ready state, run the following command to get more information.\nncn-m001# kubectl describe node NCN_HOSTNAME If that file is empty, run the following work-around to populate this file:\nncn-m001# cp /srv/cray/resources/common/containerd/00-multus.conf \\ /etc/cni/net.d/00-multus.conf ncn-m001# cat /etc/cni/net.d/00-multus.conf Verify the worker or master node is now in a Ready state:\nncn-m001# kubectl get nodes   Check the status of the Kubernetes pods.\nThe bottom of the output returned after running the /opt/cray/platform-utils/ncnHealthChecks.sh script will show a list of pods that may be in a bad state. The following command can also be used to look for any pods that are not in a Running or Completed state:\nncn-m001# kubectl get pods -o wide -A | grep -Ev \u0026#39;Running|Completed\u0026#39; It is important to pay attention to that list, but it is equally important to note what pods are in that list before and after node reboots to determine if the reboot caused any new issues.\nThere are pods that may normally be in an Error, Not Ready, or Init state, and this may not indicate any problems caused by the NCN reboots. Error states can indicate that a job pod ran and ended in an Error. That means that there may be a problem with that job, but does not necessarily indicate that there is an overall health issue with the system. The key takeaway (for health purposes) is understanding the statuses of pods prior to doing an action like rebooting all of the NCNs. Comparing the pod statuses in between each NCN reboot will give a sense of what is new or different with respect to health.\n  Verify Ceph health (the command mentioned below can be run on any master or storage node).\nThis output is included in the /opt/cray/platform-utils/ncnHealthChecks.sh script\nRun the following command during NCN reboots:\nncn-m001# watch -n 10 \u0026#39;ceph -s\u0026#39; This window can be kept up throughout the reboot process to ensure Ceph remains healthy and to watch if Ceph goes into a WARN state when rebooting storage node.\n  Check the status of the slurmctld and slurmdbd pods to determine if they are starting:\nncn-m001# kubectl describe pod -n user -lapp=slurmctld ncn-m001# kubectl describe pod -n user -lapp=slurmdbd Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedCreatePodSandBox 29m kubelet, ncn-w001 Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \u0026#34;314ca4285d0706ec3d76a9e953e412d4b0712da4d0cb8138162b53d807d07491\u0026#34;: Multus: Err in tearing down failed plugins: Multus: error in invoke Delegate add - \u0026#34;macvlan\u0026#34;: failed to allocate for range 0: no IP addresses available in range set: 10.252.2.4-10.252.2.4 Warning FailedCreatePodSandBox 29m kubelet, ncn-w001 Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox ... If the preceding error is displayed, then remove all files in the following directories on all worker nodes:\n /var/lib/cni/networks/macvlan-slurmctld-nmn-conf /var/lib/cni/networks/macvlan-slurmdbd-nmn-conf    Check that the BGP peering sessions are established.\nThis check will need to be run after all worker node have been rebooted. Ensure that the checks have been run to check BGP peering sessions on the spine switches (instructions will vary for Aruba and Mellanox switches)\nIf there are BGP Peering sessions that are not ESTABLISHED on either switch, refer to Check BGP Status and Reset Sessions.\n  Check for components that have failed status in CFS.\nIf there are any components with that status, this command will list them:\nncn-m001# cray cfs components list --status failed For any NCN components found, reset the error count to 0. Each component will also have to be disabled in CFS in order to not immediately trigger configuration. The components will be re-enabled when they reboot. NOTE: Be sure to replace the \u0026lt;xname\u0026gt; in the following command with the xname of the NCN component to be reset and disabled.\nncn-m001# cray cfs components update \u0026lt;xname\u0026gt; --error-count 0 --enabled false     NCN Rolling Reboot Before rebooting NCNs:\n Ensure pre-reboot checks have been completed, including checking the metal.no-wipe setting for each NCN. Do not proceed if any of the NCN metal.no-wipe settings are zero.  Utility Storage Nodes (Ceph)   Reboot each of the storage nodes (one at a time).\n  Establish a console session to each storage node.\nUse the Establish a Serial Connection to NCNs procedure referenced in step 4.\n  If booting from disk is desired then set the boot order.\n  Reboot the selected node.\nncn-s# shutdown -r now IMPORTANT: If the node does not shut down after 5 minutes, then proceed with the power reset below\nTo power off the node:\n  ncn-m001# export USERNAME=root ncn-m001# export IPMI_PASSWORD=changeme ncn-m001# ipmitool -U $USERNAME -E -H ${hostname}-mgmt -I lanplus power off ncn-m001# ipmitool -U $USERNAME -E -H ${hostname}-mgmt -I lanplus power status Ensure the power is reporting as off. This may take 5-10 seconds for this to update. Wait about 30 seconds after receiving the correct power status before issuing the next command.\n  To power back on the node:\n  ncn-m001# ipmitool -U $USERNAME -E -H ${hostname}-mgmt -I lanplus power on ncn-m001# ipmitool -U $USERNAME -E -H ${hostname}-mgmt -I lanplus power status   Ensure the power is reporting as on. This may take 5-10 seconds for this to update.\n  Watch on the console until the node has successfully booted and the login prompt is reached.\n  If desired verify method of boot is expected. If the /proc/cmdline begins with BOOT_IMAGE then this NCN booted from disk:\n  ncn# egrep -o \u0026#39;^(BOOT_IMAGE.+/kernel)\u0026#39; /proc/cmdline BOOT_IMAGE=(mduuid/a3899572a56f5fd88a0dec0e89fc12b4)/boot/grub2/../kernel  Retrieve the XNAME for the node being rebooted.\nThis xname is available on the node being rebooted in the following file:\nncn# ssh NODE cat /etc/cray/xname   Confirm what the Configuration Framework Service (CFS) configurationStatus is for the desiredConfig after rebooting the node.\nThe following command will indicate if a CFS job is currently in progress for this node. Replace the XNAME value in the following command with the xname of the node being rebooted.\nncn# cray cfs components describe XNAME --format json { \u0026#34;configurationStatus\u0026#34;: \u0026#34;configured\u0026#34;, \u0026#34;desiredConfig\u0026#34;: \u0026#34;ncn-personalization-full\u0026#34;, \u0026#34;enabled\u0026#34;: true, \u0026#34;errorCount\u0026#34;: 0, \u0026#34;id\u0026#34;: \u0026#34;x3000c0s7b0n0\u0026#34;, \u0026#34;retryPolicy\u0026#34;: 3, If the configurationStatus is pending, wait for the job to finish before continuing. If the configurationStatus is failed, this means the failed CFS job configurationStatus should be addressed now for this node. If the configurationStatus is unconfigured and the NCN personalization procedure has not been done as part of an install yet, this can be ignored.\nIf configurationStatus is failed, See Troubleshoot Ansible Play Failures in CFS Sessions for how to analyze the pod logs from cray-cfs to determine why the configuration may not have completed.\n  Run the platform health checks from the Validate CSM Health procedure.\nTroubleshooting: If the slurmctld and slurmdbd pods do not start after powering back up the node, check for the following error:\nncn-m001# kubectl describe pod -n user -lapp=slurmctld Warning FailedCreatePodSandBox 27m kubelet, ncn-w001 Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \u0026#34;82c575cc978db00643b1bf84a4773c064c08dcb93dbd9741ba2e581bc7c5d545\u0026#34;: Multus: Err in tearing down failed plugins: Multus: error in invoke Delegate add - \u0026#34;macvlan\u0026#34;: failed to allocate for range 0: no IP addresses available in range set: 10.252.2.4-10.252.2.4 ncn-m001# kubectl describe pod -n user -lapp=slurmdbd Warning FailedCreatePodSandBox 29m kubelet, ncn-w001 Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \u0026#34;314ca4285d0706ec3d76a9e953e412d4b0712da4d0cb8138162b53d807d07491\u0026#34;: Multus: Err in tearing down failed plugins: Multus: error in invoke Delegate add - \u0026#34;macvlan\u0026#34;: failed to allocate for range 0: no IP addresses available in range set: 10.252.2.4-10.252.2.4 Remove the following files on every worker node to resolve the failure:\n /var/lib/cni/networks/macvlan-slurmctld-nmn-conf /var/lib/cni/networks/macvlan-slurmdbd-nmn-conf    Disconnect from the console.\n  Repeat all of the sub-steps above for the remaining storage nodes, going from the highest to lowest number until all storage nodes have successfully rebooted.\n  Important: Ensure ceph -s shows that Ceph is healthy (HEALTH_OK) BEFORE MOVING ON to reboot the next storage node. Once Ceph has recovered the downed mon, it may take a several minutes for Ceph to resolve clock skew.\n  NCN Worker Nodes   Reboot each of the worker nodes (one at a time).\nNOTE: You are doing a single worker at a time, so please keep track of what ncn-w0xx you are on for these steps.\n  Establish a console session to the worker node you are rebooting.\nIMPORTANT: If the ConMan console pod is on the node being rebooted you will need to re-establish your session after the Cordon/Drain in step 2\nSee Establish a Serial Connection to NCNs for more information.\n  Failover any postgres leader that is running on the worker node you are rebooting.\nncn-m# /usr/share/doc/csm/upgrade/1.0/scripts/k8s/failover-leader.sh \u0026lt;node to be rebooted\u0026gt;   Cordon and Drain the node\nncn-m# kubectl drain --ignore-daemonsets=true --delete-local-data=true \u0026lt;node to be rebooted\u0026gt; You may run into pods that cannot be gracefully evicted because of Pod Disruption Budgets (PDB), for example:\nncn-m# error when evicting pod \u0026#34;\u0026lt;pod\u0026gt;\u0026#34; (will retry after 5s): Cannot evict pod as it would violate the pod\u0026#39;s disruption budget. In this case, there are some options. First, if the service is scalable, you can increase the scale to start up another pod on another node, and then the drain will be able to delete it. However, it will probably be necessary to force the deletion of the pod:\nncn-m# kubectl delete pod [-n \u0026lt;namespace\u0026gt;] --force --grace-period=0 \u0026lt;pod\u0026gt; This will delete the offending pod, and Kubernetes should schedule a replacement on another node. You can then rerun the kubectl drain command, and it should report that the node is drained\nncn-m# kubectl drain --ignore-daemonsets=true --delete-local-data=true \u0026lt;node to be rebooted\u0026gt;   If booting from disk is desired then set the boot order.\n  Reboot the selected node.\nncn-w# shutdown -r now IMPORTANT: If the node does not shut down after 5 minutes, then proceed with the power reset below\nTo power off the node:\n  ncn-m001# export USERNAME=root ncn-m001# export IPMI_PASSWORD=changeme ncn-m001# ipmitool -U $USERNAME -E -H ${hostname}-mgmt -I lanplus power off ncn-m001# ipmitool -U $USERNAME -E -H ${hostname}-mgmt -I lanplus power status Ensure the power is reporting as off. This may take 5-10 seconds for this to update. Wait about 30 seconds after receiving the correct power status before issuing the next command.\n  To power back on the node:\n  ncn-m001# ipmitool -U $USERNAME -E -H ${hostname}-mgmt -I lanplus power on ncn-m001# ipmitool -U $USERNAME -E -H ${hostname}-mgmt -I lanplus power status   Ensure the power is reporting as on. This may take 5-10 seconds for this to update.\n  Watch on the console until the node has successfully booted and the login prompt is reached.\n  If desired verify method of boot is expected. If the /proc/cmdline begins with BOOT_IMAGE then this NCN booted from disk:\n  ncn# egrep -o \u0026#39;^(BOOT_IMAGE.+/kernel)\u0026#39; /proc/cmdline BOOT_IMAGE=(mduuid/a3899572a56f5fd88a0dec0e89fc12b4)/boot/grub2/../kernel  Retrieve the XNAME for the node being rebooted.\nThis xname is available on the node being rebooted in the following file:\nncn# ssh NODE cat /etc/cray/xname   Confirm what the Configuration Framework Service (CFS) configurationStatus is for the desiredConfig after rebooting the node.\nThe following command will indicate if a CFS job is currently in progress for this node. Replace the XNAME value in the following command with the xname of the node being rebooted.\nncn# cray cfs components describe XNAME --format json { \u0026#34;configurationStatus\u0026#34;: \u0026#34;configured\u0026#34;, \u0026#34;desiredConfig\u0026#34;: \u0026#34;ncn-personalization-full\u0026#34;, \u0026#34;enabled\u0026#34;: true, \u0026#34;errorCount\u0026#34;: 0, \u0026#34;id\u0026#34;: \u0026#34;x3000c0s7b0n0\u0026#34;, \u0026#34;retryPolicy\u0026#34;: 3, If the configurationStatus is pending, wait for the job to finish before continuing. If the configurationStatus is failed, this means the failed CFS job configurationStatus should be addressed now for this node. If the configurationStatus is unconfigured and the NCN personalization procedure has not been done as part of an install yet, this can be ignored.\nIf configurationStatus is failed, See Troubleshoot Ansible Play Failures in CFS Sessions for how to analyze the pod logs from cray-cfs to determine why the configuration may not have completed.\n  Uncordon the node\nncn-m# kubectl uncordon \u0026lt;node you just rebooted\u0026gt;   Verify pods are running on the rebooted node.\nWithin a minute or two, the following command should begin to show pods in a Running state (replace NCN in the command below with the name of the worker node):\nncn-m# kubectl get pods -o wide -A | grep \u0026lt;node to be rebooted\u0026gt;   Run the platform health checks from the Validate CSM Health procedure.\nVerify that the Check the Health of the Etcd Clusters in the Services Namespace check from the ncnHealthChecks.sh script returns a healthy report for all members of each etcd cluster.\nIf terminating pods are reported when checking the status of the Kubernetes pods, wait for all pods to recover before proceeding.\n  Disconnect from the console.\n  Repeat all of the sub-steps above for the remaining worker nodes, going from the highest to lowest number until all worker nodes have successfully rebooted.\n    Ensure that BGP sessions are reset so that all BGP peering sessions with the spine switches are in an ESTABLISHED state.\nSee Check BGP Status and Reset Sessions.\n  NCN Master Nodes   Reboot each of the master nodes (one at a time) starting with ncn-m003 then ncn-m001. There are special instructions for ncn-m001 below since its console connection is not managed by conman.\n  Establish a console session to the master node you are rebooting.\nSee step Establish a Serial Connection to NCNs for more information.\n  If booting from disk is desired then set the boot order.\n  Reboot the selected node.\nncn-m001# shutdown -r now IMPORTANT: If the node does not shut down after 5 minutes, then proceed with the power reset below\nTo power off the node:\nncn-m001# export USERNAME=root ncn-m001# export IPMI_PASSWORD=changeme ncn-m001# ipmitool -U $USERNAME -E -H ${hostname}-mgmt -I lanplus power off ncn-m001# ipmitool -U $USERNAME -E -H ${hostname}-mgmt -I lanplus power status Ensure the power is reporting as off. This may take 5-10 seconds for this to update. Wait about 30 seconds after receiving the correct power status before issuing the next command.\nTo power back on the node:\nncn-m001# ipmitool -U $USERNAME -E -H ${hostname}-mgmt -I lanplus power on ncn-m001# ipmitool -U $USERNAME -E -H ${hostname}-mgmt -I lanplus power status Ensure the power is reporting as on. This may take 5-10 seconds for this to update.\n  Watch on the console until the node has successfully booted and the login prompt is reached.\n  If desired verify method of boot is expected. If the /proc/cmdline begins with BOOT_IMAGE then this NCN booted from disk:\n  ncn# egrep -o \u0026#39;^(BOOT_IMAGE.+/kernel)\u0026#39; /proc/cmdline BOOT_IMAGE=(mduuid/a3899572a56f5fd88a0dec0e89fc12b4)/boot/grub2/../kernel  Retrieve the XNAME for the node being rebooted.\nThis xname is available on the node being rebooted in the following file:\nncn# ssh NODE cat /etc/cray/xname   Confirm what the Configuration Framework Service (CFS) configurationStatus is for the desiredConfig after rebooting the node.\nThe following command will indicate if a CFS job is currently in progress for this node. Replace the XNAME value in the following command with the xname of the node being rebooted.\nncn# cray cfs components describe XNAME --format json { \u0026#34;configurationStatus\u0026#34;: \u0026#34;configured\u0026#34;, \u0026#34;desiredConfig\u0026#34;: \u0026#34;ncn-personalization-full\u0026#34;, \u0026#34;enabled\u0026#34;: true, \u0026#34;errorCount\u0026#34;: 0, \u0026#34;id\u0026#34;: \u0026#34;x3000c0s7b0n0\u0026#34;, \u0026#34;retryPolicy\u0026#34;: 3, If the configurationStatus is pending, wait for the job to finish before continuing. If the configurationStatus is failed, this means the failed CFS job configurationStatus should be addressed now for this node. If the configurationStatus is unconfigured and the NCN personalization procedure has not been done as part of an install yet, this can be ignored.\nIf configurationStatus is failed, See Troubleshoot Ansible Play Failures in CFS Sessions for how to analyze the pod logs from cray-cfs to determine why the configuration may not have completed.\n  Run the platform health checks in Validate CSM Health.\n  Disconnect from the console.\n  Repeat all of the sub-steps above for the remaining master nodes (excluding ncn-m001), going from the highest to lowest number until all master nodes have successfully rebooted.\n    Reboot ncn-m001.\n  Determine the CAN IP address for one of the other NCNs in the system to establish an SSH session with that NCN.\n  Establish a console session to ncn-m001 from a remote system, as ncn-m001 is the NCN that has an externally facing IP address.\n  If booting from disk is desired then set the boot order.\n  Power cycle the node\nEnsure the expected results are returned from the power status check before rebooting:\nexternal# export USERNAME=root external# export IPMI_PASSWORD=changeme external# ipmitool -U $USERNAME -E -H ${hostname}-mgmt -I lanplus power status To power off the node:\nexternal# ipmitool -U $USERNAME -E -H ${hostname}-mgmt -I lanplus power off external# ipmitool -U $USERNAME -E -H ${hostname}-mgmt -I lanplus power status Ensure the power is reporting as off. This may take 5-10 seconds for this to update. Wait about 30 seconds after receiving the correct power status before issuing the next command.\nTo power back on the node:\nexternal# ipmitool -U $USERNAME -E -H ${hostname}-mgmt -I lanplus power on external# ipmitool -U $USERNAME -E -H ${hostname}-mgmt -I lanplus power status Ensure the power is reporting as on. This may take 5-10 seconds for this to update.\n  Watch on the console until the node has successfully booted and the login prompt is reached.\n  Retrieve the XNAME for the node being rebooted.\nThis xname is available on the node being rebooted in the following file:\nncn# ssh NODE cat /etc/cray/xname   Confirm what the Configuration Framework Service (CFS) configurationStatus is for the desiredConfig after rebooting the node.\nThe following command will indicate if a CFS job is currently in progress for this node. Replace the XNAME value in the following command with the xname of the node being rebooted.\nncn# cray cfs components describe XNAME --format json { \u0026#34;configurationStatus\u0026#34;: \u0026#34;configured\u0026#34;, \u0026#34;desiredConfig\u0026#34;: \u0026#34;ncn-personalization-full\u0026#34;, \u0026#34;enabled\u0026#34;: true, \u0026#34;errorCount\u0026#34;: 0, \u0026#34;id\u0026#34;: \u0026#34;x3000c0s7b0n0\u0026#34;, \u0026#34;retryPolicy\u0026#34;: 3, If the configurationStatus is pending, wait for the job to finish before continuing. If the configurationStatus is failed, this means the failed CFS job configurationStatus should be addressed now for this node. If the configurationStatus is unconfigured and the NCN personalization procedure has not been done as part of an install yet, this can be ignored.\nIf configurationStatus is failed, See Troubleshoot Ansible Play Failures in CFS Sessions for how to analyze the pod logs from cray-cfs to determine why the configuration may not have completed.\n  Run the platform health checks in Validate CSM Health.\n  Disconnect from the console.\n    Remove any dynamically assigned interface IPs that did not get released automatically by running the CASMINST-2015 script:\n  ncn-m001# /usr/share/doc/csm/scripts/CASMINST-2015.sh  Re-run the platform health checks and ensure that all BGP peering sessions are Established with both spine switches.\nSee Validate CSM Health for the platform health checks.\nSee Check BGP Status and Reset Sessions to check the BGP peering sessions.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/node_management/rebuild_ncns/",
	"title": "Rebuild NCNs",
	"tags": [],
	"description": "",
	"content": "Rebuild NCNs Rebuild a master, worker, or storage non-compute node (NCN). Use this procedure in the event that a node has a hardware failure, or some other issue with the node has occurred that warrants rebuilding the node.\nThe following is a high-level overview of the NCN rebuild workflow:\n Prepare Node  There is a different procedure for each type of node (worker, master, and storage).   Identify Node and Update Metadata  Same procedure for all node types.   Wipe Disks  Same for master and worker nodes, but different for storage nodes.   Power Cycle Node  Same procedure for all node types.   Rebuild Storage Node  Only needed for storage nodes   Validation  There is a different procedure for each type of node (worker, master, and storage).    Prerequisites The system is fully installed and has transitioned off of the LiveCD.\n For several of the commands in this section, you will need to have variables set with the name of the node being rebuilt and its xname.\nSet NODE to the hostname of the node being rebuilt (e.g. ncn-w001, ncn-w002, etc). Set XNAME to the xname of that node.\nncn# NODE=ncn-w00n ncn# XNAME=$(ssh $NODE cat /etc/cray/xname) ncn# echo $XNAME Procedure 1. Prepare Node Only follow the steps in the section for the node type that is being rebuilt:\n Worker node Master node Storage node  1.1. Prepare Worker Node Prepare a worker node before rebuilding it.\nSkip this section if rebuilding a master or storage node. Unless otherwise noted, these commands can be run on any node in the system.\n  Determine if the worker being rebuilt is running the cray-cps-cm-pm pod.\nIf the cray-cps-cm-pm pod is running, there will be an extra step to redeploy this pod after the node is rebuilt.\nncn# cray cps deployment list --format json | grep -C1 podname \u0026#34;node\u0026#34;: \u0026#34;ncn-w002\u0026#34;, \u0026#34;podname\u0026#34;: \u0026#34;cray-cps-cm-pm-j7td7\u0026#34; }, -- \u0026#34;node\u0026#34;: \u0026#34;ncn-w001\u0026#34;, \u0026#34;podname\u0026#34;: \u0026#34;cray-cps-cm-pm-lzbhm\u0026#34; }, -- \u0026#34;node\u0026#34;: \u0026#34;ncn-w003\u0026#34;, \u0026#34;podname\u0026#34;: \u0026#34;NA\u0026#34; }, -- \u0026#34;node\u0026#34;: \u0026#34;ncn-w004\u0026#34;, \u0026#34;podname\u0026#34;: \u0026#34;NA\u0026#34; }, -- \u0026#34;node\u0026#34;: \u0026#34;ncn-w005\u0026#34;, \u0026#34;podname\u0026#34;: \u0026#34;NA\u0026#34; } In this case, the ncn-w001 and ncn-w002 nodes have the pod.\nA 404 Not Found error is expected when the Content Projection Service (CPS) is not installed on the system. CPS is part of the COS product so if this worker node is being rebuilt before the COS product has been installed, CPS will not be installed yet.\n  Confirm what the Configuration Framework Service (CFS) configurationStatus is for the desiredConfig before shutting down the node.\nThe following command will indicate if a CFS job is currently in progress for this node. This command assumes you have set the variables from the prerequisites section.\nncn# cray cfs components describe $XNAME --format json { \u0026#34;configurationStatus\u0026#34;: \u0026#34;configured\u0026#34;, \u0026#34;desiredConfig\u0026#34;: \u0026#34;ncn-personalization-full\u0026#34;, \u0026#34;enabled\u0026#34;: true, \u0026#34;errorCount\u0026#34;: 0, \u0026#34;id\u0026#34;: \u0026#34;x3000c0s7b0n0\u0026#34;, \u0026#34;retryPolicy\u0026#34;: 3, If the configurationStatus is pending, wait for the job finish before rebooting this node. If the configurationStatus is failed, this means the failed CFS job configurationStatus preceded this worker rebuild, and that can be addressed independent of rebuilding this worker. If the configurationStatus is unconfigured and the NCN personalization procedure has not been done as part of an install yet, this can be ignored.\n  Drain the node to clear any pods running on the node.\nThe following command will both cordon and drain the node. If there are messages indicating that the pods cannot be evicted because of a pod distribution budget, note those pod names and manually delete them. This command assumes you have set the variables from the prerequisites section.\nncn# kubectl drain --ignore-daemonsets --delete-local-data $NODE You may run into pods that cannot be gracefully evicted due to Pod Disruption Budgets (PDB), for example:\nerror when evicting pod \u0026quot;\u0026lt;pod\u0026gt;\u0026quot; (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. In this case, there are some options. First, if the service is scalable, you can increase the scale to start up another pod on another node, and then the drain will be able to delete it. However, it will probably be necessary to force the deletion of the pod:\nncn# kubectl delete pod [-n \u0026lt;namespace\u0026gt;] --force --grace-period=0 \u0026lt;pod\u0026gt; This will delete the offending pod, and Kubernetes should schedule a replacement on another node. You can then rerun the kubectl drain command, and it should report that the node is drained\n  Remove the node from the cluster after the node is drained.\nThis command assumes you have set the variables from the prerequisites section.\nncn# kubectl delete node $NODE   1.2. Prepare Master Node Prepare a master node before rebuilding it.\nSkip this section if rebuilding a worker or storage node. The commands should be run on a master node that is not being rebuilt.\n  Confirm what the Configuration Framework Service (CFS) configurationStatus is for the desiredConfig before shutting down the node.\nThe following command will indicate if a CFS job is currently in progress for this node. This command assumes you have set the variables from the prerequisites section.\nncn-m# cray cfs components describe $XNAME --format json { \u0026#34;configurationStatus\u0026#34;: \u0026#34;configured\u0026#34;, \u0026#34;desiredConfig\u0026#34;: \u0026#34;ncn-personalization-full\u0026#34;, \u0026#34;enabled\u0026#34;: true, \u0026#34;errorCount\u0026#34;: 0, \u0026#34;id\u0026#34;: \u0026#34;x3000c0s7b0n0\u0026#34;, \u0026#34;retryPolicy\u0026#34;: 3, If the configurationStatus is pending, wait for the job finish before rebooting this node. If the configurationStatus is failed, this means the failed CFS job configurationStatus preceded this worker rebuild, and that can be addressed independent of rebuilding this worker. If the configurationStatus is unconfigured and the NCN personalization procedure has not been done as part of an install yet, this can be ignored.\n  Determine if the master node being rebuilt is the first master node.\nThe first master node is the node others contact to join the Kubernetes cluster. If this is the node being rebuilt, promote another master node to the initial node before proceeding.\nncn-m# craysys metadata get first-master-hostname ncn-m002 If the node returned is not the one being rebuilt, proceed to the step which stops etcd and skip the substeps here.\n  Reconfigure the Boot Script Service (BSS) to point to a new first master node.\nncn-mw# cray bss bootparameters list --name Global --format=json | jq \u0026#39;.[]\u0026#39; \u0026gt; Global.json   Edit the Global.json file and edit the indicated line.\nChange the first-master-hostname value to another node that will be promoted to the first master node. For example, if the first node is changing from ncn-m002 to ncn-m001, the line would be changed to the following:\n\u0026#34;first-master-hostname\u0026#34;: \u0026#34;ncn-m001\u0026#34;,   Get a token to interact with BSS via the REST API.\nncn# curl -i -s -k -H \u0026#34;Content-Type: application/json\u0026#34; \\  -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; \\  \u0026#34;https://api-gw-service-nmn.local/apis/bss/boot/v1/bootparameters\u0026#34; \\  -X PUT -d @./Global.json Ensure a good response, such as HTTP CODE 200, is returned in the curl output.\n  Configure the newly promoted first master node so it is able to have other nodes join the cluster.\nUse ssh to login to the newly-promoted master node chosen in the previous steps (ncn-m001 in this example), copy/paste the following script to a file, and then execute it.\n#!/bin/bash  source /srv/cray/scripts/metal/lib.sh export KUBERNETES_VERSION=\u0026#34;v$(cat /etc/cray/kubernetes/version)\u0026#34; echo $(kubeadm init phase upload-certs --upload-certs 2\u0026gt;\u0026amp;1 | tail -1) \u0026gt; /etc/cray/kubernetes/certificate-key export CERTIFICATE_KEY=$(cat /etc/cray/kubernetes/certificate-key) export MAX_PODS_PER_NODE=$(craysys metadata get kubernetes-max-pods-per-node) export PODS_CIDR=$(craysys metadata get kubernetes-pods-cidr) export SERVICES_CIDR=$(craysys metadata get kubernetes-services-cidr) envsubst \u0026lt; /srv/cray/resources/common/kubeadm.yaml \u0026gt; /etc/cray/kubernetes/kubeadm.yaml kubeadm token create --print-join-command \u0026gt; /etc/cray/kubernetes/join-command 2\u0026gt;/dev/null echo \u0026#34;$(cat /etc/cray/kubernetes/join-command)--control-plane --certificate-key $(cat /etc/cray/kubernetes/certificate-key)\u0026#34; \u0026gt; /etc/cray/kubernetes/join-command-control-plane mkdir -p /srv/cray/scripts/kubernetes cat \u0026gt; /srv/cray/scripts/kubernetes/token-certs-refresh.sh \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; #!/bin/bash if [[ \u0026#34;$1\u0026#34; != \u0026#34;skip-upload-certs\u0026#34; ]]; then kubeadm init phase upload-certs --upload-certs --config /etc/cray/kubernetes/kubeadm.yaml fi kubeadm token create --print-join-command \u0026gt; /etc/cray/kubernetes/join-command 2\u0026gt;/dev/null echo \u0026#34;$(cat /etc/cray/kubernetes/join-command) --control-plane --certificate-key $(cat /etc/cray/kubernetes/certificate-key)\u0026#34; \\ \u0026gt; /etc/cray/kubernetes/join-command-control-plane EOF chmod +x /srv/cray/scripts/kubernetes/token-certs-refresh.sh /srv/cray/scripts/kubernetes/token-certs-refresh.sh skip-upload-certs echo \u0026#34;0 */1 * * * root /srv/cray/scripts/kubernetes/token-certs-refresh.sh \u0026gt;\u0026gt; /var/log/cray/cron.log 2\u0026gt;\u0026amp;1\u0026#34; \u0026gt; /etc/cron.d/cray-k8s-token-certs-refresh   Determine the member ID of the master node being removed.\nRun the following command and find the line with the name of the master being removed. The member ID is the alphanumeric string in the first field of that line. The IP address is in the URL in the fourth field in the line. Note the member ID and IP address for use in subsequent steps.\nncn# etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt \\  --cert=/etc/kubernetes/pki/etcd/ca.crt \\  --key=/etc/kubernetes/pki/etcd/ca.key \\  --endpoints=localhost:2379 member list   Remove the master node from the etcd cluster backing Kubernetes.\nReplace the \u0026lt;MEMBER_ID\u0026gt; value with the value returned in the previous sub-step.\nncn# etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt \\  --cert=/etc/kubernetes/pki/etcd/ca.crt --key=/etc/kubernetes/pki/etcd/ca.key \\  --endpoints=localhost:2379 member remove \u0026lt;MEMBER_ID\u0026gt; \n    Stop the etcd service on the master node being removed.\nncn-m# systemctl stop etcd.service   Remove the node from the Kubernetes cluster.\nThis command should not be run on the node being deleted. This command assumes you have set the variables from the prerequisites section.\nncn# kubectl delete node $NODE   Add the node back into the etcd cluster so when it reboots it can rejoin.\nThe IP and hostname of the rebuilt node is needed for the following command. Replace the \u0026lt;IP_ADDRESS\u0026gt; address value with the IP address you noted in an earlier step from the etcdctl command.\nThis command assumes you have set the variables from the prerequisites section.\nncn# etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/ca.crt \\  --key=/etc/kubernetes/pki/etcd/ca.key --endpoints=localhost:2379 member add $NODE \\  --peer-urls=https://\u0026lt;IP_ADDRESS\u0026gt;:2380   1.3. Prepare Storage Node Prepare a storage node before rebuilding it.\nSkip this section if rebuilding a master or worker node.\n  Confirm what the Configuration Framework Service (CFS) configurationStatus is for the desiredConfig before shutting down the node.\nThe following command will indicate if a CFS job is currently in progress for this node. This command assumes you have set the variables from the prerequisites section.\nncn# cray cfs components describe $XNAME --format json { \u0026#34;configurationStatus\u0026#34;: \u0026#34;configured\u0026#34;, \u0026#34;desiredConfig\u0026#34;: \u0026#34;ncn-personalization-full\u0026#34;, \u0026#34;enabled\u0026#34;: true, \u0026#34;errorCount\u0026#34;: 0, \u0026#34;id\u0026#34;: \u0026#34;x3000c0s7b0n0\u0026#34;, \u0026#34;retryPolicy\u0026#34;: 3, If the configurationStatus is pending, wait for the job finish before rebooting this node. If the configurationStatus is failed, this means the failed CFS job configurationStatus preceded this worker rebuild, and that can be addressed independent of rebuilding this worker. If the configurationStatus is unconfigured and the NCN personalization procedure has not been done as part of an install yet, this can be ignored.\n  Check the status of Ceph.\nCheck the OSD status, weight, and location:\nncn-s# ceph osd tree ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 20.95917 root default -3 6.98639 host ncn-s001 2 ssd 1.74660 osd.2 up 1.00000 1.00000 5 ssd 1.74660 osd.5 up 1.00000 1.00000 8 ssd 1.74660 osd.8 up 1.00000 1.00000 11 ssd 1.74660 osd.11 up 1.00000 1.00000 -7 6.98639 host ncn-s002 0 ssd 1.74660 osd.0 up 1.00000 1.00000 4 ssd 1.74660 osd.4 up 1.00000 1.00000 7 ssd 1.74660 osd.7 up 1.00000 1.00000 10 ssd 1.74660 osd.10 up 1.00000 1.00000 -5 6.98639 host ncn-s003 1 ssd 1.74660 osd.1 down 0 1.00000 3 ssd 1.74660 osd.3 down 0 1.00000 6 ssd 1.74660 osd.6 down 0 1.00000 9 ssd 1.74660 osd.9 down 0 1.00000 Check the status of the Ceph cluster:\nncn-s# ceph -s cluster: id: 22d01fcd-a75b-4bfc-b286-2ed8645be2b5 health: HEALTH_WARN 4 osds down 1 host (8 osds) down Degraded data redundancy: 923/2768 objects degraded (33.345%), 94 pgs degraded 1/3 mons down, quorum ncn-s001,ncn-s002 services: mon: 3 daemons, quorum ncn-s001,ncn-s002 (age 43s), out of quorum: ncn-s003 mgr: ncn-s001(active, since 18h), standbys: ncn-s002 mds: cephfs:1 {0=ncn-s001=up:active} 1 up:standby osd: 16 osds: 8 up (since 34s), 12 in (since 34m) rgw: 2 daemons active (ncn-s001.rgw0, ncn-s002.rgw0) task status: scrub status: mds.ncn-s001: idle data: pools: 10 pools, 480 pgs objects: 923 objects, 29 KiB usage: 12 GiB used, 21 TiB / 21 TiB avail pgs: 923/2768 objects degraded (33.345%) 369 active+undersized 94 active+undersized+degraded 17 active+clean   If the node is a ceph-mon node, remove it from the mon map.\nSkip this step if the node is not a ceph-mon node.\nThe output in the previous sub-step indicated out of quorum: ncn-s003.\nncn-s# ceph mon dump dumped monmap epoch 5 epoch 5 fsid 22d01fcd-a75b-4bfc-b286-2ed8645be2b5 last_changed 2021-03-05 15:14:09.142113 created 2021-03-04 20:50:38.141908 min_mon_release 14 (nautilus) 0: [v2:10.252.1.9:3300/0,v1:10.252.1.9:6789/0] mon.ncn-s001 1: [v2:10.252.1.10:3300/0,v1:10.252.1.10:6789/0] mon.ncn-s002 2: [v2:10.252.1.11:3300/0,v1:10.252.1.11:6789/0] mon.ncn-s003 Remove the out of quorum node from the mon map. This command assumes you have set the variables from the prerequisites section.\nncn-s# cd /etc/ansible/ceph-ansible ncn-s# ceph mon rm $NODE removing mon.ncn-s003 at [v2:10.252.1.11:3300/0,v1:10.252.1.11:6789/0], there will be 2 monitors The Ceph cluster will now show as healthy. However, there will now only be two monitors, which is not an ideal situation because if there is another Ceph mon outage then the cluster will go read-only.\nncn-s# ceph -s cluster: id: 22d01fcd-a75b-4bfc-b286-2ed8645be2b5 health: HEALTH_WARN Degraded data redundancy: 588/2771 objects degraded (21.220%), 60 pgs degraded, 268 pgs undersized services: mon: 2 daemons, quorum ncn-s001,ncn-s002 (age 4m) mgr: ncn-s001(active, since 18h), standbys: ncn-s002 mds: cephfs:1 {0=ncn-s001=up:active} 1 up:standby osd: 12 osds: 8 up (since 9m), 8 in (since 42m); 148 remapped pgs rgw: 2 daemons active (ncn-s001.rgw0, ncn-s002.rgw0) task status: scrub status: mds.ncn-s001: idle data: pools: 10 pools, 480 pgs objects: 924 objects, 30 KiB usage: 8.0 GiB used, 14 TiB / 14 TiB avail pgs: 588/2771 objects degraded (21.220%) 307/2771 objects misplaced (11.079%) 208 active+undersized 138 active+clean+remapped 74 active+clean 60 active+undersized+degraded   Remove Ceph OSDs.\nThe ceph osd tree capture indicated that there are down OSDs on ncn-s003.\n-5 6.98639 host ncn-s003 1 ssd 1.74660 osd.1 down 0 1.00000 3 ssd 1.74660 osd.3 down 0 1.00000 6 ssd 1.74660 osd.6 down 0 1.00000 9 ssd 1.74660 osd.9 down 0 1.00000 Remove the OSDs to prevent the install from creating new OSDs on the drives, but there is still a reference to them in the crush map. It will time out trying to restart the old OSDs because of that reference.\nThis command assumes you have set the variables from the prerequisites section.\nncn-s# for osd in $(ceph osd ls-tree $NODE); do ceph osd destroy osd.$osd \\  --force; ceph osd purge osd.$osd --force; done destroyed osd.1 purged osd.1 destroyed osd.3 purged osd.3 destroyed osd.6 purged osd.6 destroyed osd.9 purged osd.9   Remove the node from the crush map.\nThis command assumes you have set the variables from the prerequisites section.\nncn-s# ceph osd crush rm $NODE   2. Identify Nodes and Update Metadata This section applies to all node types. The commands in this section assume you have set the variables from the prerequisites section.\n  Generate the Boot Script Service (BSS) boot parameters JSON file for modification and review.\nncn# cray bss bootparameters list --name $XNAME --format=json \u0026gt; ${XNAME}.json   \n  Inspect and modify the JSON file.\n  Remove the outer array brackets.\nDo this by removing the first and last line of the XNAME.json file, indicated with the [ and ] brackets.\n  Remove the leading whitespace on the new first and last lines.\nOn the new first and last lines of the file, removing all whitespace characters at the beginning of those lines. The first line should now just be a { character and the last line should now just be a } character.\n  Ensure the current boot parameters are appropriate for PXE booting.\nInspect the \u0026quot;params\u0026quot;: \u0026quot;kernel...\u0026quot; line. If the line begins with BOOT_IMAGE and/or does not contain metal.server, the following steps are needed:\n  Remove everything before kernel on the \u0026quot;params\u0026quot;: \u0026quot;kernel\u0026quot; line.\n  Re-run steps Retrieve the xname and Generate BSS JSON for another node/xname. Look for an example that does not contain BOOT_IMAGE.\nOnce an example is found, copy a portion of the params line for everything including and after biosdevname, and use that in the JSON file.\n  After copying the content after biosdevname, change the \u0026quot;hostname=\u0026lt;hostname\u0026gt;\u0026quot; to the correct host.\n    Set the kernel parameters to wipe the disk.\nLocate the portion of the line that contains \u0026quot;metal.no-wipe\u0026quot; and ensure it is set to zero \u0026quot;metal.no-wipe=0\u0026quot;.\n    Re-apply the boot parameters list for the node using the JSON file.\n  Get a token to interact with BSS using the REST API.\nncn# TOKEN=$(curl -s -k -S -d grant_type=client_credentials \\  -d client_id=admin-client -d client_secret=`kubectl get secrets admin-client-auth \\  -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\  https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token \\  | jq -r \u0026#39;.access_token\u0026#39;)   Do a PUT action for the new JSON file.\nncn# curl -i -s -k -H \u0026#34;Content-Type: application/json\u0026#34; -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; \\  \u0026#34;https://api-gw-service-nmn.local/apis/bss/boot/v1/bootparameters\u0026#34; -X PUT -d @./${XNAME}.json Ensure a good response (HTTP CODE 200) is returned in the output.\n    Verify the bss bootparameters list command returns the expected information.\n  Export the list from BSS to a file with a different name.\nncn# cray bss bootparameters list --name ${XNAME} --format=json \u0026gt; ${XNAME}.check.json   Compare the new JSON file with what was PUT to BSS.\nncn# diff ${XNAME}.json ${XNAME}.check.json The only difference between the files should be the square brackets that were removed from the file, and the whitespace changes on the first and last lines with curly braces. Expected output will look similar to:\n1,2c1 \u0026lt; [ \u0026lt; { --- \u0026gt; { 47,48c46 \u0026lt; } \u0026lt; ] --- \u0026gt; }     Watch the console for the node being rebuilt.\nLogin to a second session to use it to watch the console using the instructions in the following document: Log in to a Node Using ConMan. The first session will be needed to run the commands in the following Rebuild Node steps.\n  3. Wipe Disks Warning: This is the point of no return. Once the disks are wiped, the node must be rebuilt.\nAll commands in this section must be run on the node being rebuilt (unless otherwise indicated). These commands can be done from the ConMan console window.\nOnly follow the steps in the section for the node type that is being rebuilt:\n Wipe Disks: Master or Worker Node Wipe Disks: Storage Node  3.1. Wipe Disks: Master or Worker This section applies to master and worker nodes. Skip this section if rebuilding a storage node.\nncn-mw# mdisks=$(lsblk -l -o SIZE,NAME,TYPE,TRAN | grep -E \u0026#39;(sata|nvme|sas)\u0026#39; | sort -h | awk \u0026#39;{print \u0026#34;/dev/\u0026#34; $2}\u0026#39;) ncn-mw# wipefs --all --force $mdisks 3.2. Wipe Disks: Storage This section applies to storage nodes. Skip this section if rebuilding a master or worker node.\n  Delete CEPH Volumes\nncn-s# systemctl stop ceph-osd.target   Make sure the OSDs (if any) are not running after running the first command.\nncn-s# ls -1 /dev/sd* /dev/disk/by-label/* ncn-s# vgremove -f --select \u0026#39;vg_name=~ceph*\u0026#39;   Wipe the disks and RAIDs.\nncn# wipefs --all --force /dev/sd* /dev/disk/by-label/*   4. Power Cycle Node This section applies to all node types. The commands in this section assume you have set the variables from the prerequisites section.\n  Set the PXE boot option and power cycle the node. Run these commands from a node not being rebuilt.\n  Set the BMC variable to the hostname of the BMC of the node being rebuilt.\nThis command assumes you have set the variables from the prerequisites section.\nncn# BMC=${NODE}-mgmt   Export the root password of the BMC.\nncn# export IPMI_PASSWORD=changeme   Set the PXE/efiboot option.\nncn# ipmitool -I lanplus -U root -E -H $BMC chassis bootdev pxe options=efiboot   Power off the node.\nncn# ipmitool -I lanplus -U root -E -H $BMC chassis power off   Verify that the node is off.\nncn# ipmitool -I lanplus -U root -E -H $BMC chassis power status Ensure the power is reporting as off. This may take 5-10 seconds for this to update. Wait about 30 seconds after receiving the correct power status before issuing the next command.\n  Power on the node.\nncn# ipmitool -I lanplus -U root -E -H $BMC chassis power on   Verify that the node is on.\nEnsure the power is reporting as on. This may take 5-10 seconds for this to update.\nncn# ipmitool -I lanplus -U root -E -H $BMC chassis power status     Observe the boot.\nAfter a bit, the node should begin to boot. This can be viewed from the ConMan console window. Eventually, there will be a NBP file... message in the console output which indicates that the PXE boot has begun the TFTP download of the ipxe program. Later messages will appear as the Linux kernel loads and then the scripts in the initrd begin to run, including cloud-init.\nWait until cloud-init displays messages similar to these on the console to indicate that cloud-init has finished with the module called modules:final.\n[ 295.466827] cloud-init[9333]: Cloud-init v. 20.2-8.45.1 running 'modules:final' at Thu, 26 Aug 2021 15:23:20 +0000. Up 125.72 seconds. [ 295.467037] cloud-init[9333]: Cloud-init v. 20.2-8.45.1 finished at Thu, 26 Aug 2021 15:26:12 +0000. Datasource DataSourceNoCloudNet [seed=cmdline,http://10.92.100.81:8888/][dsmode=net]. Up 295.46 seconds Then press enter on the console to ensure that the the login prompt is displayed including the correct hostname of this node. Then exit the ConMan console (\u0026amp; then .), and then use ssh to log in to the node to complete the remaining validation steps.\nTroubleshooting: If the NBP file... output never appears, or something else goes wrong, go back to the steps for modifying XNAME.json file (see the step to inspect and modify the JSON file) and make sure these instructions were completed correctly.\nMaster nodes only: If cloud-init did not complete the newly-rebuilt node will need to have its etcd service definition manually updated. Reconfigure the etcd service, and restart the cloud init on the newly rebuilt master:\nncn-m# systemctl stop etcd.service; sed -i \u0026#39;s/new/existing/\u0026#39; \\  /etc/systemd/system/etcd.service /srv/cray/resources/common/etcd/etcd.service; \\  systemctl daemon-reload ; rm -rf /var/lib/etcd/member; \\  systemctl start etcd.service; /srv/cray/scripts/common/kubernetes-cloudinit.sh   Confirm vlan004 is up with the correct IP address on the rebuilt node.\nRun these commands on the rebuilt node.\n  Find the desired IP address.\nThis command assumes you have set the variables from the prerequisites section.\nncn# dig +short ${NODE}.hmn 10.254.1.16   Confirm the output from the dig command matches the interface.\nIf the IP addresses match, proceed to the next step. If they do not match, continue with the following sub-steps.\nncn# ip addr show vlan004 14: vlan004@bond0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether b8:59:9f:2b:2f:9e brd ff:ff:ff:ff:ff:ff inet 10.254.1.16/17 brd 10.254.127.255 scope global vlan004 valid_lft forever preferred_lft forever inet6 fe80::ba59:9fff:fe2b:2f9e/64 scope link valid_lft forever preferred_lft forever   Change the IP address for vlan004 if necessary.\nncn# vim /etc/sysconfig/network/ifcfg-vlan004 Set the IPADDR line to the correct IP address with a /17 mask. For example, if the correct IP address is 10.254.1.16, the line should be:\nIPADDR='10.254.1.16/17'   Restart the vlan004 network interface.\nncn# wicked ifreload vlan004   Confirm the output from the dig command matches the interface.\nncn# ip addr show vlan004       Confirm that vlan007 is up with the correct IP address on the rebuilt node.\nRun these commands on the rebuilt node.\n  Find the desired IP address.\nThis command assumes you have set the variables from the prerequisites section.\nncn# dig +short ${NODE}.can 10.103.8.11   Confirm the output from the dig command matches the interface.\nIf the IP addresses match, proceed to the next step. If they do not match, continue with the following sub-steps.\nncn# ip addr show vlan007 15: vlan007@bond0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether b8:59:9f:2b:2f:9e brd ff:ff:ff:ff:ff:ff inet 10.103.8.11/24 brd 10.103.8.255 scope global vlan007 valid_lft forever preferred_lft forever inet6 fe80::ba59:9fff:fe2b:2f9e/64 scope link valid_lft forever preferred_lft forever   Change the IP address for vlan007 if necessary.\nncn# vim /etc/sysconfig/network/ifcfg-vlan007 Set the IPADDR line to the correct IP address with a /24 mask. For example, if the correct IP address is 10.103.8.11, the line should be:\nIPADDR='10.103.8.11/24'   Restart the vlan007 network interface.\nncn# wicked ifreload vlan007   Confirm the output from the dig command matches the interface.\nncn# ip addr show vlan007       Set the wipe flag back so it will not wipe the disk when the node is rebooted.\n  Edit the XNAME.json file and set the metal.no-wipe=1 value.\n  Do a PUT action for the edited JSON file.\nThis command can be run from any node. This command assumes you have set the variables from the prerequisites section.\nncn# curl -i -s -k -H \u0026#34;Content-Type: application/json\u0026#34; \\  -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; \\  \u0026#34;https://api-gw-service-nmn.local/apis/bss/boot/v1/bootparameters\u0026#34; \\  -X PUT -d @./${XNAME}.json   The output from the ncnHealthChecks.sh script (run later in the \u0026ldquo;Validation\u0026rdquo; steps) can be used to verify the metal.no-wipe value on every NCN.\n  5. Rebuild Storage Node This section applies to storage nodes. Skip this section if rebuilding a master or worker node. All commands in this section must be run on any storage node that is already in the cluster and is not being rebuilt (unless otherwise indicated).\n  Use ssh to log in to the node where Ansible will run.\n  If rebuilding ncn-s001, log in to either ncn-s002 or ncn-s003.\n  If rebuilding any other storage node, log in to ncn-s001 and proceed to the next step.\n    Update the Ansible inventory.\n  Update the number of the last storage node.\nThere will be no output returned from the following commands. This step changes LASTNODE into the number of the last storage node. In this example, LASTNODE is changed to ncn-s003.\nncn-s# source /srv/cray/scripts/common/fix_ansible_inv.sh ncn-s# fix_inventory   Verify the Ansible inventory was changed.\nVerify that LASTNODE no longer exists in the inventory file (/etc/ansible/hosts).\n[all] ncn-s[001:LASTNODE].nmn [ceph_all] ncn-s[001:LASTNODE].nmn to:\n[all] ncn-s[001:003].nmn [ceph_all] ncn-s[001:003].nmn     Set the environment variable for the rados gateway vip.\nncn-s# cd /etc/ansible/group_vars ncn-s# export RGW_VIRTUAL_IP=$(craysys metadata get rgw-virtual-ip) ncn-s# echo $RGW_VIRTUAL_IP 10.252.1.3   Run the ceph-ansible playbook to reinstall the node and bring it back into the cluster.\nRun the following commands on the storage node being rebuilt.\nncn-s# cd /etc/ansible/ceph-ansible ncn-s# ansible-playbook /etc/ansible/ceph-ansible/site.yml   Open another SSH session to a storage node that is not currently being rebuilt, and then monitor the build.\nncn-s# watch ceph -s   Run the radosgw-sts-setup.yml Ansible play on the storage node being rebuilt.\nEnsure Ceph is healthy and the ceph-ansible playbook has finished before running the following Ansible play on the storage node being rebuilt.\nncn-s# ansible-playbook /etc/ansible/ceph-rgw-users/radosgw-sts-setup.yml On the node that has been rebuilt, verify the sts values are in the ceph.conf file.\nncn-s# grep sts /etc/ceph/ceph.conf rgw_s3_auth_use_sts = True rgw_sts_key = \u0026lt;REDACTED_KEY\u0026gt;   6. Validation As a result of rebuilding any NCN(s) remove any dynamically assigned interface IPs that did not get released automatically by running the CASMINST-2015 script:\nncn-m001# /usr/share/doc/csm/scripts/CASMINST-2015.sh Once that is done only follow the steps in the section for the node type that was rebuilt:\n Validate Worker Node Validate Master Node Validate Storage Node  6.1. Validate Worker Node Validate the worker node rebuilt successfully.\nSkip this section if a master or storage node was rebuilt.\n  Verify the new node is in the cluster.\nRun the following command from any master or worker node that is already in the cluster. It is helpful to run this command several times to watch for the newly rebuilt node to join the cluster. This should occur within 10 to 20 minutes.\nncn-mw# kubectl get nodes NAME STATUS ROLES AGE VERSION ncn-m001 Ready master 113m v1.18.6 ncn-m002 Ready master 113m v1.18.6 ncn-m003 Ready master 112m v1.18.6 ncn-w001 Ready \u0026lt;none\u0026gt; 112m v1.18.6 ncn-w002 Ready \u0026lt;none\u0026gt; 112m v1.18.6 ncn-w003 Ready \u0026lt;none\u0026gt; 112m v1.18.6   Ensure there is proper routing set up for liquid-cooled hardware.\n  Confirm /var/lib/containerd is on overlay on the node which was rebooted.\nRun the following command on the rebuilt node.\nncn-w# df -h /var/lib/containerd Filesystem Size Used Avail Use% Mounted on containerd_overlayfs 378G 245G 133G 65% /var/lib/containerd After several minutes of the node joining the cluster, pods should be in a Running state for the worker node.\n  Confirm the pods are beginning to get scheduled and reach a Running state on the worker node.\nRun this command on any master or worker node. This command assumes you have set the variables from the prerequisites section.\nncn# kubectl get po -A -o wide | grep $NODE   Confirm BGP is healthy.\nFollow the steps in the Check BGP Status and Reset Sessions to verify and fix BGP if needed.\n  Confirm what the Configuration Framework Service (CFS) configurationStatus is for the desiredConfig after rebooting the node.\nThe following command will indicate if a CFS job is currently in progress for this node. This command assumes you have set the variables from the prerequisites section.\nncn# cray cfs components describe $XNAME --format json { \u0026#34;configurationStatus\u0026#34;: \u0026#34;configured\u0026#34;, \u0026#34;desiredConfig\u0026#34;: \u0026#34;ncn-personalization-full\u0026#34;, \u0026#34;enabled\u0026#34;: true, \u0026#34;errorCount\u0026#34;: 0, \u0026#34;id\u0026#34;: \u0026#34;x3000c0s7b0n0\u0026#34;, \u0026#34;retryPolicy\u0026#34;: 3, If the configurationStatus is pending, wait for the job to finish before continuing. If the configurationStatus is failed, this means the failed CFS job configurationStatus should be addressed now for this node. If the configurationStatus is unconfigured and the NCN personalization procedure has not been done as part of an install yet, this can be ignored. If configurationStatus is failed, See Troubleshoot Ansible Play Failures in CFS Sessions for how to analyze the pod logs from cray-cfs to determine why the configuration may not have completed.\n  Redeploy the cray-cps-cm-pm pod.\nThis step is only required if the cray-cps-cm-pm pod was running on the node before it was rebuilt.\n The Content Projection Service (CPS) is part of the COS product so if this worker node is being rebuilt before the COS product has been installed, CPS will not be installed yet. This command can be run from any node.\n ncn# cray cps deployment update --nodes \u0026#34;ncn-w001,ncn-w002\u0026#34;   Collect data about the system management platform health (can be run from a master or worker NCN).\nncn-mw# /opt/cray/platform-utils/ncnHealthChecks.sh ncn-mw# /opt/cray/platform-utils/ncnPostgresHealthChecks.sh   6.2. Validate Master Node Validate the master node rebuilt successfully.\nSkip this section if a worker or storage node was rebuilt.\n  Verify the new node is in the cluster.\nRun the following command from any master or worker node that is already in the cluster. It is helpful to run this command several times to watch for the newly rebuilt node to join the cluster. This should occur within 10 to 20 minutes.\nncn-mw# kubectl get nodes NAME STATUS ROLES AGE VERSION ncn-m001 Ready master 113m v1.18.6 ncn-m002 Ready master 113m v1.18.6 ncn-m003 Ready master 112m v1.18.6 ncn-w001 Ready \u0026lt;none\u0026gt; 112m v1.18.6 ncn-w002 Ready \u0026lt;none\u0026gt; 112m v1.18.6 ncn-w003 Ready \u0026lt;none\u0026gt; 112m v1.18.6   Ensure there is proper routing set up for liquid-cooled hardware.\n  Confirm the sdc disk has the correct lvm on the rebuilt node.\nncn-m# lsblk | grep -A2 ^sdc sdc 8:32 0 447.1G 0 disk └─ETCDLVM 254:0 0 447.1G 0 crypt └─etcdvg0-ETCDK8S 254:1 0 32G 0 lvm /run/lib-etcd   Confirm etcd is running and shows the node as a member once again.\nThe newly built master node should be in the returned list.\nncn-m# etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/ca.crt \\  --key=/etc/kubernetes/pki/etcd/ca.key --endpoints=localhost:2379 member list   Confirm what the Configuration Framework Service (CFS) configurationStatus is for the desiredConfig after rebooting the node.\nThe following command will indicate if a CFS job is currently in progress for this node. This command assumes you have set the variables from the prerequisites section.\nncn# cray cfs components describe $XNAME --format json { \u0026#34;configurationStatus\u0026#34;: \u0026#34;configured\u0026#34;, \u0026#34;desiredConfig\u0026#34;: \u0026#34;ncn-personalization-full\u0026#34;, \u0026#34;enabled\u0026#34;: true, \u0026#34;errorCount\u0026#34;: 0, \u0026#34;id\u0026#34;: \u0026#34;x3000c0s7b0n0\u0026#34;, \u0026#34;retryPolicy\u0026#34;: 3, If the configurationStatus is pending, wait for the job to finish before continuing. If the configurationStatus is failed, this means the failed CFS job configurationStatus should be addressed now for this node. If the configurationStatus is unconfigured and the NCN personalization procedure has not been done as part of an install yet, this can be ignored. If configurationStatus is failed, See Troubleshoot Ansible Play Failures in CFS Sessions for how to analyze the pod logs from cray-cfs to determine why the configuration may not have completed.\n  Collect data about the system management platform health (can be run from a master or worker NCN).\nncn-mw# /opt/cray/platform-utils/ncnHealthChecks.sh ncn-mw# /opt/cray/platform-utils/ncnPostgresHealthChecks.sh   6.3. Validate Storage Node Validate the storage node rebuilt successfully.\nSkip this section if a master or worker node was rebuilt.\n  Verify there are 3 mons, 3 mds, 3 mgr processes, and rgw.s\nncn-m# ceph -s cluster: id: 22d01fcd-a75b-4bfc-b286-2ed8645be2b5 health: HEALTH_OK services: mon: 3 daemons, quorum ncn-s001,ncn-s002,ncn-s003 (age 4m) mgr: ncn-s001(active, since 19h), standbys: ncn-s002, ncn-s003 mds: cephfs:1 {0=ncn-s001=up:active} 2 up:standby osd: 12 osds: 12 up (since 2m), 12 in (since 2m) rgw: 3 daemons active (ncn-s001.rgw0, ncn-s002.rgw0, ncn-s003.rgw0) task status: scrub status: mds.ncn-s001: idle data: pools: 10 pools, 480 pgs objects: 926 objects, 31 KiB usage: 12 GiB used, 21 TiB / 21 TiB avail pgs: 480 active+clean   Verify the OSDs are back in the cluster.\nncn-m# ceph osd tree ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 20.95917 root default -3 6.98639 host ncn-s001 2 ssd 1.74660 osd.2 up 1.00000 1.00000 5 ssd 1.74660 osd.5 up 1.00000 1.00000 8 ssd 1.74660 osd.8 up 1.00000 1.00000 11 ssd 1.74660 osd.11 up 1.00000 1.00000 -7 6.98639 host ncn-s002 0 ssd 1.74660 osd.0 up 1.00000 1.00000 4 ssd 1.74660 osd.4 up 1.00000 1.00000 7 ssd 1.74660 osd.7 up 1.00000 1.00000 10 ssd 1.74660 osd.10 up 1.00000 1.00000 -5 6.98639 host ncn-s003 1 ssd 1.74660 osd.1 up 1.00000 1.00000 3 ssd 1.74660 osd.3 up 1.00000 1.00000 6 ssd 1.74660 osd.6 up 1.00000 1.00000 9 ssd 1.74660 osd.9 up 1.00000 1.00000   Verify the radosgw and haproxy are correct.\nThere will be an output (without an error) returned if radosgw and haproxy are correct.\nncn# curl -k https://rgw-vip.nmn \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt;\u0026lt;ListAllMyBucketsResult xmlns=\u0026#34;http://s3.amazonaws.com/doc/2006-03-01/ \u0026#34;\u0026gt;\u0026lt;Owner\u0026gt;\u0026lt;ID\u0026gt;anonymous\u0026lt;/ID\u0026gt;\u0026lt;DisplayName\u0026gt;\u0026lt;/DisplayName\u0026gt;\u0026lt;/Owner\u0026gt;\u0026lt;Buckets\u0026gt;\u0026lt;/Buckets\u0026gt;\u0026lt;/ListAllMyBucketsResult   Confirm what the Configuration Framework Service (CFS) configurationStatus is for the desiredConfig after rebooting the node.\nThe following command will indicate if a CFS job is currently in progress for this node. This command assumes you have set the variables from the prerequisites section.\nncn# cray cfs components describe $XNAME --format json { \u0026#34;configurationStatus\u0026#34;: \u0026#34;configured\u0026#34;, \u0026#34;desiredConfig\u0026#34;: \u0026#34;ncn-personalization-full\u0026#34;, \u0026#34;enabled\u0026#34;: true, \u0026#34;errorCount\u0026#34;: 0, \u0026#34;id\u0026#34;: \u0026#34;x3000c0s7b0n0\u0026#34;, \u0026#34;retryPolicy\u0026#34;: 3, If the configurationStatus is pending, wait for the job to finish before continuing. If the configurationStatus is failed, this means the failed CFS job configurationStatus should be addressed now for this node. If the configurationStatus is unconfigured and the NCN personalization procedure has not been done as part of an install yet, this can be ignored. If configurationStatus is failed, See Troubleshoot Ansible Play Failures in CFS Sessions for how to analyze the pod logs from cray-cfs to determine why the configuration may not have completed.\n  Collect data about the system management platform health (can be run from a master or worker NCN).\nncn-mw# /opt/cray/platform-utils/ncnHealthChecks.sh ncn-mw# /opt/cray/platform-utils/ncnPostgresHealthChecks.sh   "
},
{
	"uri": "/docs-csm/en-11/operations/node_management/replace_a_compute_blade/",
	"title": "Replace A Compute Blade",
	"tags": [],
	"description": "",
	"content": "Replace a Compute Blade Replace an HPE Cray EX liquid-cooled compute blade.\nShutdown software and power off the blade   Temporarily disable endpoint discovery service (MEDS) for the compute nodes(s) being replaced. This example disables MEDS for the compute node in cabinet 1000, chassis 3, slot 0 (x1000c3s0b0). If there is more than 1 node card, in the blade specify each node card (x1000c3s0b0,x1000c3s0b1).\nncn-m001# cray hsm inventory redfishEndpoints update --enabled false x1000c3s0b0   Verify that the workload manager (WLM) is not using the affected nodes.\n  Use Boot Orchestration Services (BOS) to shut down the affected nodes. Specify the appropriate BOS template for the node type.\nncn-m001# cray bos v1 session create --template-uuid BOS_TEMPLATE \\ --operation shutdown --limit x1000c3s0b0n0,x1000c3s0b0n1,x1000c3s0b1n0,x1000c3s0b1n1 Specify all the nodes in the blade using a comma separated list. This example shows the command to shut down an EX425 compute blade (Windom) in cabinet 1000, chassis 3, slot 5. This blade type includes two node cards, each with two logical nodes (4 processors).\n  Disable the chassis slot in the Hardware State Manager (HSM).\nThis example shows cabinet 1000, chassis 3, slot 0 (x1000c3s0).\nncn-m001# cray hsm state components enabled update --enabled false x1000c3s0 Disabling the slot prevents hms-discovery from attempting to automatically power on slots. If the slot automatically powers on after using CAPMC to power the slot off, then temporarily suspend the hms-discovery cron job in k8s:\n  Suspend the hms-discovery cron job to prevent slot power on.\nncn-m001# kubectl -n services patch cronjobs hms-discovery \\ -p \u0026#39;{\u0026#34;spec\u0026#34; : {\u0026#34;suspend\u0026#34; : true }}\u0026#39;   Verify that the hms-discovery cron job has stopped (ACTIVE column = 0).\nncn-m001# kubectl get cronjobs -n services hms-discovery NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE^M hms-discovery */3 * * * * True 0 117s 15d     Use CAPMC to power off slot 0 in chassis 3.\nncn-m001# cray capmc xname_off create --xnames x1000c3s0 \\ --recursive true --format json   Delete the HSM entries  Delete the node Ethernet interface MAC addresses and the Redfish endpoint from the Hardware State Manager (HSM).\nIMPORTANT: The HSM stores the node\u0026rsquo;s BMC NIC MAC addresses for the hardware management network and the node\u0026rsquo;s Ethernet NIC MAC addresses for the node management network. The MAC addresses for the node NICs must be updated in the DHCP/DNS configuration when a liquid-cooled blade is replaced. Their entries must be deleted from the HSM Ethernet interfaces table and be rediscovered. The BMC NIC MAC addresses for liquid-cooled blades are assigned algorithmically and should not be deleted from the HSM.\n  Delete the Node NIC MAC addresses from the HSM Ethernet interfaces table.\nQuery HSM to determine the Node NIC MAC addresses associated with the blade in cabinet 1000, chassis 3, slot 0, node card 0, node 0.\nncn-m001# cray hsm inventory ethernetInterfaces list \\ --component-id x1000c3s0b0n0 --format json [ { \u0026#34;ID\u0026#34;: \u0026#34;b42e99be1a2b\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Ethernet Interface Lan1\u0026#34;, \u0026#34;MACAddress\u0026#34;: \u0026#34;b4:2e:99:be:1a:2b\u0026#34;, \u0026#34;LastUpdate\u0026#34;: \u0026#34;2021-01-27T00:07:08.658927Z\u0026#34;, \u0026#34;ComponentID\u0026#34;: \u0026#34;x1000c3s0b0n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;IPAddresses\u0026#34;: [ { \u0026#34;IPAddress\u0026#34;: \u0026#34;10.252.1.26\u0026#34; } ] }, { \u0026#34;ID\u0026#34;: \u0026#34;b42e99be1a2c\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Ethernet Interface Lan2\u0026#34;, \u0026#34;MACAddress\u0026#34;: \u0026#34;b4:2e:99:be:1a:2c\u0026#34;, \u0026#34;LastUpdate\u0026#34;: \u0026#34;2021-01-26T22:43:10.593193Z\u0026#34;, \u0026#34;ComponentID\u0026#34;: \u0026#34;x1000c3s0b0n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;IPAddresses\u0026#34;: [] } ]  Delete each Node NIC MAC address the Hardware State Manager (HSM) Ethernet interfaces table.\nncn-m001# cray hsm inventory ethernetInterfaces delete b42e99be1a2b ncn-m001# cray hsm inventory ethernetInterfaces delete b42e99be1a2c   Delete the Redfish endpoint for the removed node.\n      Replace the blade hardware.\nReview the Remove a Compute Blade Using the Lift procedure in HPE Cray EX Hardware Replacement Procedures H-6173 for detailed instructions (https://internal.support.hpe.com/).\nCAUTION: Always power off the chassis slot or device before removal. The best practice is to unlatch and unseat the device while the coolant hoses are still connected, then disconnect the coolant hoses. If this is not possible, disconnect the coolant hoses, then quickly unlatch/unseat the device (within 10 seconds). Failure to do so may damage the equipment.\n  Power on and boot the compute nodes  Un-suspend the hms-discovery cronjob in k8s.\nncn-m001# kubectl -n services patch cronjobs hms-discovery -p \u0026#39;{\u0026#34;spec\u0026#34; : {\u0026#34;suspend\u0026#34; : false }}\u0026#39; ncn-m001# kubectl get cronjobs.batch -n services hms-discovery NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE hms-discovery */3 * * * * False 1 41s 33d ncn-m001# kubectl -n services logs hms-discovery-1600117560-5w95d hms-discovery | grep \u0026#34;Mountain discovery finished\u0026#34; | jq \u0026#39;.discoveredXnames\u0026#39; [ \u0026#34;x1000c3s0b0\u0026#34; ]   Enable MEDS for the compute node(s) in the blade.\nncn-m001# cray hsm inventory redfishEndpoints update --enabled true --rediscover-on-update true x1000c3s0b0   Wait for 3-5 minutes for the blade to power on and the node BMCs to be discovered.\n  Verify that the affected nodes are enabled in the HSM.\nncn-m001# cray hsm state components describe x1000c3s0b0n0 Type = \u0026#34;Node\u0026#34; Enabled = true State = \u0026#34;Off\u0026#34; . . .   To verify the BMC(s) has been discovered by the HSM.\nncn-m001# cray hsm inventory redfishEndpoints describe x1000c3s0b0 --format json { \u0026#34;ID\u0026#34;: \u0026#34;x1000c3s0b0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;NodeBMC\u0026#34;, \u0026#34;Hostname\u0026#34;: \u0026#34;x1000c3s0b0\u0026#34;, \u0026#34;Domain\u0026#34;: \u0026#34;, \u0026#34;FQDN\u0026#34;: \u0026#34;x1000c3s0b0\u0026#34;, \u0026#34;Enabled\u0026#34;: true, \u0026#34;UUID\u0026#34;: \u0026#34;e005dd6e-debf-0010-e803-b42e99be1a2d\u0026#34;, \u0026#34;User\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;, \u0026#34;MACAddr\u0026#34;: \u0026#34;b42e99be1a2d\u0026#34;, \u0026#34;RediscoverOnUpdate\u0026#34;: true, \u0026#34;DiscoveryInfo\u0026#34;: { \u0026#34;LastDiscoveryAttempt\u0026#34;: \u0026#34;2021-01-29T16:15:37.643327Z\u0026#34;, \u0026#34;LastDiscoveryStatus\u0026#34;: \u0026#34;DiscoverOK\u0026#34;, \u0026#34;RedfishVersion\u0026#34;: \u0026#34;1.7.0\u0026#34; } }  When LastDiscoveryStatus displays as DiscoverOK, the node BMC has been successfully discovered. If the last discovery state is DiscoveryStarted then the BMC is currently being inventoried by HSM. If the last discovery state is HTTPsGetFailed or ChildVerificationFailed, then an error has occurred during the discovery process.    Enable each node individually in the HSM database (in this example, the nodes are x1000c3s0b0n0-n3).\n  Optional: To force rediscovery of the components in the chassis (the example shows cabinet 1000, chassis 3).\nncn-m001# cray hsm inventory discover create --xnames x1000c3   Optional: Verify that discovery has completed (LastDiscoveryStatus = \u0026ldquo;DiscoverOK\u0026quot;).\nncn-m001# cray hsm inventory redfishEndpoints describe x1000c3 Type = \u0026#34;ChassisBMC\u0026#34; Domain = \u0026#34; MACAddr = \u0026#34;02:13:88:03:00:00\u0026#34; Enabled = true Hostname = \u0026#34;x1000c3\u0026#34; RediscoverOnUpdate = true FQDN = \u0026#34;x1000c3\u0026#34; User = \u0026#34;root\u0026#34; Password = \u0026#34; IPAddress = \u0026#34;10.104.0.76\u0026#34; ID = \u0026#34;x1000c3b0\u0026#34; [DiscoveryInfo] LastDiscoveryAttempt = \u0026#34;2020-09-03T19:03:47.989621Z\u0026#34; RedfishVersion = \u0026#34;1.2.0\u0026#34; LastDiscoveryStatus = \u0026#34;DiscoverOK\u0026#34;   Verify that the correct firmware versions for node BIOS, node controller (nC), NIC mezzanine card (NMC), GPUs, and so on.\n  Optional: If necessary, update the firmware. Review the Firmware Action Service (FAS) documentation.\nncn-m001# cray fas actions create CUSTOM_DEVICE_PARAMETERS.json   Update the System Layout Service (SLS).\n  Dump the existing SLS configuration.\nncn-m001# cray sls networks describe HSN --format=json \u0026gt; existingHSN.json   Copy existingHSN.json to a newHSN.json, edit newHSN.json with the changes, then run\nncn-m001# curl -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; https://API_SYSTEM/apis/sls/v1/networks/HSN -X PUT -d @newHSN.json     Reload DVS on NCNs.\n  Use boot orchestration to power on and boot the nodes.\nSpecify the appropriate BOS template for the node type.\nncn-m001# cray bos v1 session create --template-uuid BOS_TEMPLATE --operation reboot --limit x1000c3s0b0n0,x1000c3s0b0n1,x1000c3s0b1n0,x1000c3s0b1n1   "
},
{
	"uri": "/docs-csm/en-11/operations/node_management/reset_credentials_on_redfish_devices_for_reinstallation/",
	"title": "Reset Credentials On Redfish Devices",
	"tags": [],
	"description": "",
	"content": "Reset Credentials on Redfish Devices Before re-installing or upgrading the system the credentials need to be changed back to their defaults for any devices that had their credentials changed post-install. This is necessary for the installation process to properly discover and communicate with these devices.\nPrerequisites Administrative privileges are required.\nProcedure   Create a SCSD payload file with the default credentials for the Redfish devices that have been changed from the defaults.\nThe following example shows a payload file that will set the devices `x0c0s0b0` and `x0c0s1b0` back to the default Redfish credentials.\n{ \u0026#34;Force\u0026#34;: false, \u0026#34;Username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;\u0026lt;BMC root password\u0026gt;\u0026#34;, \u0026#34;Targets\u0026#34;: [ \u0026#34;x0c0s0b0\u0026#34;, \u0026#34;x0c0s1b0\u0026#34; ] }   Set credentials for multiple targets.\nncn-m001# cray scsd bmc globalcreds create PAYLOAD_FILE --format json { \u0026#34;Targets\u0026#34;: [ { \u0026#34;Xname\u0026#34;: \u0026#34;x0c0s0b0\u0026#34;, \u0026#34;StatusCode\u0026#34;: 200, \u0026#34;StatusMsg\u0026#34;: \u0026#34;OK\u0026#34; }, { \u0026#34;Xname\u0026#34;: \u0026#34;x0c0s1b0\u0026#34;, \u0026#34;StatusCode\u0026#34;: 200, \u0026#34;StatusMsg\u0026#34;: \u0026#34;OK\u0026#34; }, ] }   For more information about using the System Configuration Service, refer to System Configuration Service\n"
},
{
	"uri": "/docs-csm/en-11/operations/node_management/swap_a_compute_blade_with_a_different_system/",
	"title": "Swap A Compute Blade With A Different System",
	"tags": [],
	"description": "",
	"content": "Swap a compute blade with a different system Swap an HPE Cray EX liquid-cooled compute blade between two systems.\n  The two systems in this example are:\n Source system - Cray EX TDS cabinet x9000 with a healthy EX425 blade (Windom dual-injection) in chassis 3, slot 0 Destination system - Cray EX cabinet x1005 with a defective EX425 blade (Windom dual-injection) in chassis 3, slot 0    Substitute the correct xnames or other parameters in the command examples that follow.\n  All the nodes in the blade must be specified using a comma separated list. For example, EX425 compute blades include two node cards, each with two logical nodes (4 nodes).\n  Prerequisites   The Slingshot fabric must be configured with the desired topology for both blades\n  The System Layout Service (SLS) must have the desired HSN configuration\n  The blade that is removed from the source system must be installed in the empty slot left by the blade removed from destination system and visa-versa.\n  Check the status of the high-speed network (HSN) and record link status before the procedure.\n  Review the following command examples. The commands can be used to capture the required values from the HSM ethernetInterfaces table and write the values to a file. The file then can be used to automate subsequent commands in this procedure, for example:\nncn-m001:# mkdir blade_swap_scripts; cd blade_swap_scripts ncn-m001:# cat blade_query.sh #!/bin/bash BLADE=$1 OUTFILE=$2 BLADE_DOT=$BLADE. cray hsm inventory ethernetInterfaces list --format json | jq -c --arg BLADE \u0026#34;$BLADE_DOT\u0026#34; \u0026#39;map(select(.ComponentID|test($BLADE))) | map(select(.Description == \u0026#34;Node Maintenance Network\u0026#34;)) | .[] | {xname: .ComponentID, ID: .ID,MAC: .MACAddress, IP: .IPAddresses[0].IPAddress,Desc: .Description}\u0026#39; \u0026gt; $OUTFILE ncn-m001:# ./blade_query.sh x1000c0s1 x1000c0s1.json ncn-m001:# cat x1000c0s1.json {\u0026#34;xname\u0026#34;:\u0026#34;x9000c3s1b0n0\u0026#34;,\u0026#34;ID\u0026#34;:\u0026#34;0040a6836339\u0026#34;,\u0026#34;MAC\u0026#34;:\u0026#34;00:40:a6:83:63:39\u0026#34;,\u0026#34;IP\u0026#34;:\u0026#34;10.100.0.10\u0026#34;,\u0026#34;Desc\u0026#34;:\u0026#34;Node Maintenance Network\u0026#34;} {\u0026#34;xname\u0026#34;:\u0026#34;x9000c3s1b0n1\u0026#34;,\u0026#34;ID\u0026#34;:\u0026#34;0040a683633a\u0026#34;,\u0026#34;MAC\u0026#34;:\u0026#34;00:40:a6:83:63:3a\u0026#34;,\u0026#34;IP\u0026#34;:\u0026#34;10.100.0.98\u0026#34;,\u0026#34;Desc\u0026#34;:\u0026#34;Node Maintenance Network\u0026#34;} {\u0026#34;xname\u0026#34;:\u0026#34;x9000c3s1b1n0\u0026#34;,\u0026#34;ID\u0026#34;:\u0026#34;0040a68362e2\u0026#34;,\u0026#34;MAC\u0026#34;:\u0026#34;00:40:a6:83:62:e2\u0026#34;,\u0026#34;IP\u0026#34;:\u0026#34;10.100.0.123\u0026#34;,\u0026#34;Desc\u0026#34;:\u0026#34;Node Maintenance Network\u0026#34;} {\u0026#34;xname\u0026#34;:\u0026#34;x9000c3s1b1n1\u0026#34;,\u0026#34;ID\u0026#34;:\u0026#34;0040a68362e3\u0026#34;,\u0026#34;MAC\u0026#34;:\u0026#34;00:40:a6:83:62:e3\u0026#34;,\u0026#34;IP\u0026#34;:\u0026#34;10.100.0.122\u0026#34;,\u0026#34;Desc\u0026#34;:\u0026#34;Node Maintenance Network\u0026#34;} To delete anethernetInterfaces entry using curl:\nncn-m001:# for ID in $(cat x9000c3s1.json | jq -r \u0026#39;.ID\u0026#39;); do cray hsm inventory ethernetInterfaces delete $ID; done To insert an ethernetInterfaces entry using curl:\nncn-m001:# while read PAYLOAD ; do curl -H \u0026#34;Authorization: Bearer $MY_TOKEN\u0026#34; -L -X POST \u0026#39;https://api-gw-service-nmn.local/apis/smd/hsm/v1/Inventory/EthernetInterfaces\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; --data-raw \u0026#34;$(echo $PAYLOAD | jq -c \u0026#39;{ComponentID: .xname,Description: .Desc,MACAddress: .MAC,IPAddress: .IP}\u0026#39;)\u0026#34;;sleep 5; done \u0026lt; x9000c3s1.json   The blades must have the coolant drained and filled during the swap to minimize cross-contamination of cooling systems.\n Review procedures in HPE Cray EX Coolant Service Procedures H-6199 Review the HPE Cray EX Hand Pump User Guide H-6200    Prepare the source system blade for removal   Using the work load manager (WLM), drain running jobs from the affected nodes on the blade. Refer to the vendor documentation for the WLM for more information.\n  Use Boot Orchestration Services (BOS) to shut down the affected nodes in the source blade (in this example, x9000c3s0). Specify the appropriate xname and BOS template for the node type in the following command.\nncn-m001# BOS_TEMPLATE=cos-2.0.30-slurm-healthy-compute ncn-m001# cray bos session create --template-uuid $BOS_TEMPLATE --operation shutdown --limit x9000c3s0b0n0,x9000c3s0b0n1,x9000c3s0b1n0,x9000c3s0b1n1   Disable the Redfish endpoints for the nodes  Temporarily disable endpoint discovery service (MEDS) for each compute node. Disabling the slot prevents hms-discovery from automatically powering on the slot.\nncn-m001# cray hsm inventory redfishEndpoints update --enabled false x9000c3s0b0 ncn-m001# cray hsm inventory redfishEndpoints update --enabled false x9000c3s0b1   Clear the node controller settings  Remove the system specific settings from each node controller on the blade.\nncn-m001# curl -k -u root:PASSWORD -X POST -H \\ \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;ResetType\u0026#34;:\u0026#34;StatefulReset\u0026#34;}\u0026#39; \\ https://x9000c3s0b0/redfish/v1/Managers/BMC/Actions/Manager.Reset ncn-m001# curl -k -u root:PASSWORD -X POST -H \\ \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;ResetType\u0026#34;:\u0026#34;StatefulReset\u0026#34;}\u0026#39; \\ https://x9000c3s0b1/redfish/v1/Managers/BMC/Actions/Manager.Reset Use Ctrl-C to return to the prompt if command does not return.\n  Power off the chassis slot  Suspend the hms-discovery cron job.\nncn-m001# kubectl -n services patch cronjobs hms-discovery -p \u0026#39;{\u0026#34;spec\u0026#34; : {\u0026#34;suspend\u0026#34; : true }}\u0026#39;   Verify that the hms-discovery cron job has stopped (ACTIVE = 0 and SUSPEND = True).\nncn-m001# kubectl get cronjobs -n services hms-discovery NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE hms-discovery */3 * * * * True 0 117s 15d   Power off the chassis slot. This examples powers off slot 0, chassis 3, in cabinet 9000.\nncn-m001# cray capmc xname_off create --xnames x9000c3s0 --recursive true     Disable the chassis slot  Disable the chassis slot. This example disables slot 0, chassis 3, in cabinet 9000.\nncn-m001# cray hsm state components enabled update --enabled false x9000c3s0   Record MAC and IP addresses for nodes IMPORTANT: Record the node management network (NMN) MAC and IP addresses for each node in the blade (labeled Node Maintenance Network). To prevent disruption in the data virtualization service (DVS), these addresses must be maintained in the HSM when the blade is swapped and discovered.\nThe hardware management network MAC and IP addresses are assigned algorithmically and must not be deleted from the HSM.\n Query HSM to determine the ComponentID, MAC, and IP addresses for each node in the blade. The prerequisites show an example of how to gather HSM values and store them to a file.\nncn-m001# cray hsm inventory ethernetInterfaces list --component-id x9000c3s0b0n0 --format json [ { \u0026#34;ID\u0026#34;: \u0026#34;0040a6836339\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Node Maintenance Network\u0026#34;, \u0026#34;MACAddress\u0026#34;: \u0026#34;00:40:a6:83:63:39\u0026#34;, \u0026#34;LastUpdate\u0026#34;: \u0026#34;2021-04-09T21:51:04.662063Z\u0026#34;, \u0026#34;ComponentID\u0026#34;: \u0026#34;x9000c3s0b0n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;IPAddresses\u0026#34;: [ { \u0026#34;IPAddress\u0026#34;: \u0026#34;10.100.0.10\u0026#34; } ] } ]   Record the following values for the blade:\n`ComponentID: \u0026#34;x9000c3s0b0n0\u0026#34;` `MACAddress: \u0026#34;00:40:a6:83:63:39\u0026#34;` `IPAddress: \u0026#34;10.100.0.10\u0026#34;`   Repeat the command to record the ComponentID, MAC, and IP addresses for the Node Maintenance Network the other nodes in the blade.\n  Delete the NMN MAC and IP addresses each node in the blade from the HSM. Do not delete the MAC and IP addresses for the node BMC.\nncn-m001# cray hsm inventory ethernetInterfaces delete 0040a6836339   Repeat the preceding command for each node in the blade.\n  Delete the Redfish endpoints for each node.\nncn-m001# cray hsm inventory redfishEndpoints delete x9000c3s0b0 ncn-m001# cray hsm inventory redfishEndpoints delete x9000c3s0b1     Remove the blade Remove the blade from the source system.  Review the Remove a Compute Blade Using the Lift procedure in HPE Cray EX Hardware Replacement Procedures H-6173 for detailed instructions for replacing liquid-cooled blades (https://internal.support.hpe.com/).   Drain the coolant from the blade and fill with fresh coolant to minimize cross-contamination of cooling systems.  Review HPE Cray EX Coolant Service Procedures H-6199. If using the hand pump, review procedures in the HPE Cray EX Hand Pump User Guide H-6200 (https://internal.support.hpe.com/).   Install the blade from the source system in a storage rack or leave it on the cart.  Prepare the blade in the destination system for removal  Use WLM to drain jobs from the affected nodes on the blade.\n  Use BOS to shut down the affected nodes in the destination blade (in this example, x1005c3s0).\nncn-m001# BOS_TEMPLATE=cos-2.0.30-slurm-healthy-compute ncn-m001# cray bos session create --template-uuid $BOS_TEMPLATE --operation shutdown --limit x1005c3s0b0n0,x1005c3s0b0n1,x1005c3s0b1n0,x1005c3s0b1n1   Disable the Redfish endpoints for the nodes  When nodes are Off, temporarily disable endpoint discovery service (MEDS) for the compute nodes(s). Disabling chassis slot prevents hms-discovery from attempting to power them back on.\nncn-m001# cray hsm inventory redfishEndpoints update --enabled false x1005c3s0b0 ncn-m001# cray hsm inventory redfishEndpoints update --enabled false x1005c3s0b1   Clear the node controller settings  Remove system specific settings from each node controller on the blade.\nncn-m001# curl -k -u root:PASSWORD -X POST -H \u0026#39;Content-Type: application/json\u0026#39; -d \\ \u0026#39;{\u0026#34;ResetType\u0026#34;: \u0026#34;StatefulReset\u0026#34;}\u0026#39; \\ https://x1005c3s0b0/redfish/v1/Managers/BMC/Actions/Manager.Reset ncn-m001# curl -k -u root:PASSWORD -X POST -H \u0026#39;Content-Type: application/json\u0026#39; -d \\ \u0026#39;{\u0026#34;ResetType\u0026#34;: \u0026#34;StatefulReset\u0026#34;}\u0026#39; \\ https://x1005c3s0b1/redfish/v1/Managers/BMC/Actions/Manager.Reset   Power off the chassis slot  Suspend the hms-discovery cron job.\nncn-m001# kubectl -n services patch cronjobs hms-discovery \\ -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;suspend\u0026#34;: true }}\u0026#39;   Verify that the hms-discovery cron job has stopped (ACTIVE = 0 and SUSPEND = True).\nncn-m001# kubectl get cronjobs -n services hms-discovery NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE hms-discovery */3 * * * * True 0 128s 15d   Power off the chassis slot. This example powers off slot 0 in chassis 3 of cabinet 1005.\nncn-m001# cray capmc xname_off create --xnames x1005c3s0 --recursive true   Disable the chassis slot  Disable the chassis slot. This example disables slot 0, chassis 3, in cabinet 1005.\nncn-m001# cray hsm state components enabled update --enabled false x1005c3s0   Record the NIC MAC and IP addresses IMPORTANT: Record the ComponentID, MAC, and IP addresses for each node in the blade in the destination system. To prevent disruption in the data virtualization service (DVS), these addresses must be maintained in the HSM when the replacement blade is swapped and discovered.\nThe hardware management network NIC MAC addresses for liquid-cooled blades are assigned algorithmically and must not be deleted from the HSM.\n Query HSM to determine the ComponentID, MAC, and IP addresses associated with nodes in the destination blade. The prerequisites show an example of how to gather HSM values and store them to a file.\nncn-m001# cray hsm inventory ethernetInterfaces list \\ --component-id XNAME --format json [ { \u0026#34;ID\u0026#34;: \u0026#34;0040a6836399\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Node Maintenance Network\u0026#34;, \u0026#34;MACAddress\u0026#34;: \u0026#34;00:40:a6:83:63:99\u0026#34;, \u0026#34;LastUpdate\u0026#34;: \u0026#34;2021-04-09T21:51:04.662063Z\u0026#34;, \u0026#34;ComponentID\u0026#34;: \u0026#34;x1005c3s0b0n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;IPAddresses\u0026#34;: [ { \u0026#34;IPAddress\u0026#34;: \u0026#34;10.100.0.123\u0026#34; } ] } ]   Record the following Node Maintenance Network values for each node in the blade.\n`ComponentID: \u0026#34;x1005c3s0b0n0\u0026#34;` `MACAddress: \u0026#34;00:40:a6:83:63:99\u0026#34;` `IPAddress: \u0026#34;10.10.0.123\u0026#34;`   Delete the node NIC MAC and IP addresses from the HSM ethernetInterfaces table for each node.\nncn-m001# cray hsm inventory ethernetInterfaces delete 0040a6836399   Repeat the preceding command for each node in the blade.\n  Delete the Redfish endpoints for each node.\nncn-m001# cray hsm inventory redfishEndpoints delete x1005c3s0b0 ncn-m001# cray hsm inventory redfishEndpoints delete x1005c3s0b1   Swap the blade hardware  Remove the blade from destination system install it in a storage cart.\n  Install the blade from the source system into the destination system.\n  Bring up the blade in the destination system  Obtain an authentication token to access the API gateway. In the example below, replace myuser, mypass, and shasta in the cURL command with site-specific values. Note the value of access_token. Review Retrieve an Authentication Token for more information. The example is a script to secure a token and set it to the variable MY_TOKEN.\nncn-m001# MY_TOKEN=$(curl -s -d grant_type=password -d client_id=shasta -d \\ username=USERNAME -d password=PASSWORD \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39;) ncn-m001:# echo $MY_TOKEN eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJW . .   Kea automatically adds entries to the HSM ethernetInterfaces table when DHCP lease is provided (about every 5 minutes). To prevent from Kea from automatically adding MAC entries to the HSM ethernetInterfaces table, use the following commands:\n  Create an eth_interfaces file that contains the interface IDs for the Node Maintenance Network entries for the destination system. (When repeating this procedure for the source system, use the interface IDS for the source system.)\nncn-m001# cat eth_interfaces 0040a6836339 0040a683633a 0040a68362e2 0040a68362e3   Run the following commands in succession to remove the interfaces. Delete the cray-dhcp-kea pod to prevent the interfaces from being re-created.\nncn-m001# kubectl get pods -Ao wide | grep kea ncn-m001# kubectl delete -n services pod CRAY_DHCP_KEA_PODNAME ncn-m001# for ETH in $(cat eth_interfaces); do cray hsm inventory ethernetInterfaces delete $ETH --format json ; done     Add the MAC and IP addresses and also the Node Maintenance Network description to the interfaces. The ComponentID and IPAddress must be the values recorded from the destination blade and the MACAddress must be the value recorded from the source blade.\n`ComponentID: \u0026#34;x1005c3s0b0n0\u0026#34;` `MACAddress: \u0026#34;00:40:a6:83:63:99\u0026#34;` `IPAddress: \u0026#34;10.10.0.123\u0026#34;` ncn-m001# MAC=SOURCESYS_MAC_ADDRESS ncn-m001# IP_ADDRESS=DESTSYS_IP_ADDRESS ncn-m001# XNAME=DESTSYS_XNAME ncn-m001# curl -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -L -X POST \u0026#39;https://api-gw-service-nmn.local/apis/smd/hsm/v1/Inventory/EthernetInterfaces\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; --data-raw \u0026#39;{ \u0026#34;Description\u0026#34;: \u0026#34;Node Maintenance Network\u0026#34;, \u0026#34;MACAddress\u0026#34;: \u0026#34;$MAC\u0026#34;, \u0026#34;IPAddress\u0026#34;: \u0026#34;$IP_ADDRESS\u0026#34;, \u0026#34;ComponentID\u0026#34;: \u0026#34;$XNAME\u0026#34; }\u0026#39; Note: Kea may must be restarted when the curl command is issued.\nWhen repeating this procedure for the source system, ComponentID and IPAddress must be the values recorded from the source system, and the MACAddress must be the value recorded from the blade in the destination system.\nncn-m001# MAC=DESTSYS_MAC_ADDRESS ncn-m001# IP_ADDRESS=SOURCESYS_IP_ADDRESS ncn-m001# XNAME=SOURCESYS_XNAME To change or correct a curl command that has been entered, use a PATCH request, for example:\nncn-m001# curl -k -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; -L -X PATCH \\ \u0026#39;https://api-gw-service-nmn.local/apis/smd/hsm/v1/Inventory/EthernetInterfaces/0040a68350a4\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; --data-raw \u0026#39;{\u0026#34;MACAddress\u0026#34;:\u0026#34;xx:xx:xx:xx:xx:xx\u0026#34;,\u0026#34;IPAddress\u0026#34;:\u0026#34;10.xxx.xxx.xxx\u0026#34;,\u0026#34;ComponentID\u0026#34;:\u0026#34;XNAME\u0026#34;}\u0026#39;   Repeat the preceding command for each node in the blade.\n  Enable and power on the chassis slot  Enable the chassis slot. The example enables slot 0, chassis 3, in cabinet 1005.\nncn-m001# cray hsm state components enabled update --enabled true x1005c3s0   Power on the chassis slot. The example powers on slot 0, chassis 3, in cabinet 1005.\nncn-m001# cray capmc xname_on create --xnames x1005c3s0 --recursive true   Enable discovery  Verify the hms-discovery cronjob is not suspended in k8s (ACTIVE = 1 and SUSPEND = False).\nncn-m001# kubectl -n services patch cronjobs hms-discovery \\ -p \u0026#39;{\u0026#34;spec\u0026#34; : {\u0026#34;suspend\u0026#34; : false }}\u0026#39; ncn-m001# kubectl get cronjobs.batch -n services hms-discovery NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE hms-discovery */3 * * * * False 1 41s 33d ncn-m001# kubectl get pods -Ao wide | grep hms-discovery ncn-m001# kubectl -n services logs hms-discovery-1600117560-5w95d \\ hms-discovery | grep \u0026#34;Mountain discovery finished\u0026#34; | jq \u0026#39;.discoveredXnames\u0026#39; [ \u0026#34;x1005c3s0b0\u0026#34; ]   Wait for 3 minutes for the blade to power on and the node controllers (BMCs) to be discovered.\nncn-m001# sleep 180   Verify discovery has completed  To verify the BMC(s) have been discovered by the HSM, run this command for each BMC in the blade.\nncn-m001# cray hsm inventory redfishEndpoints describe XNAME --format json { \u0026#34;ID\u0026#34;: \u0026#34;x1005c3s0b0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;NodeBMC\u0026#34;, \u0026#34;Hostname\u0026#34;: \u0026#34;x1005c3s0b0\u0026#34;, \u0026#34;Domain\u0026#34;: \u0026#34;, \u0026#34;FQDN\u0026#34;: \u0026#34;x1005c3s0b0\u0026#34;, \u0026#34;Enabled\u0026#34;: true, \u0026#34;User\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;, \u0026#34;MACAddr\u0026#34;: \u0026#34;02:03:E8:00:31:00\u0026#34;, \u0026#34;RediscoverOnUpdate\u0026#34;: true, \u0026#34;DiscoveryInfo\u0026#34;: { \u0026#34;LastDiscoveryAttempt\u0026#34;: \u0026#34;2021-06-10T18:01:59.920850Z\u0026#34;, \u0026#34;LastDiscoveryStatus\u0026#34;: \u0026#34;DiscoverOK\u0026#34;, \u0026#34;RedfishVersion\u0026#34;: \u0026#34;1.2.0\u0026#34; } }  When LastDiscoveryStatus displays as DiscoverOK, the node BMC has been successfully discovered. If the last discovery state is DiscoveryStarted then the BMC is currently being inventoried by HSM. If the last discovery state is HTTPsGetFailed or ChildVerificationFailed, then an error has occurred during the discovery process.    Optional: To force rediscovery of the components in the chassis (the example shows cabinet 1005, chassis 3).\nncn-m001# cray hsm inventory discover create --xnames x1005c3b0   Optional: Verify that discovery has completed (LastDiscoveryStatus = \u0026ldquo;DiscoverOK\u0026quot;).\nncn-m001# cray hsm inventory redfishEndpoints describe x1005c3b0 Type = \u0026#34;ChassisBMC\u0026#34; Domain = \u0026#34; MACAddr = \u0026#34;02:03:ed:03:00:00\u0026#34; Enabled = true Hostname = \u0026#34;x1005c3\u0026#34; RediscoverOnUpdate = true FQDN = \u0026#34;x1005c3\u0026#34; User = \u0026#34;root\u0026#34; Password = \u0026#34; IPAddress = \u0026#34;10.104.0.76\u0026#34; ID = \u0026#34;x1005c3b0\u0026#34; [DiscoveryInfo] LastDiscoveryAttempt = \u0026#34;2020-09-03T19:03:47.989621Z\u0026#34; RedfishVersion = \u0026#34;1.2.0\u0026#34; LastDiscoveryStatus = \u0026#34;DiscoverOK\u0026#34;   Enable each node individually in the HSM database.\nncn-m001# cray hsm state components enabled update --enabled true x1005c3s0b0n0 ncn-m001# cray hsm state components enabled update --enabled true x1005c3s0b0n1 ncn-m001# cray hsm state components enabled update --enabled true x1005c3s0b1n0 ncn-m001# cray hsm state components enabled update --enabled true x1005c3s0b1n1   Verify that the nodes are enabled in the HSM. This command must be run for each node in the blade.\nncn-m001# cray hsm state components describe x1005c3s0b0n0 Type = \u0026#34;Node\u0026#34; Enabled = true State = \u0026#34;Off\u0026#34; . . .   Power on and boot the nodes  Use boot orchestration to power on and boot the nodes. Specify the appropriate BOS template for the node type.\nncn-m001# BOS_TEMPLATE=cos-2.0.30-slurm-healthy-compute ncn-m001# cray bos session create --template-uuid $BOS_TEMPLATE \\ --operation reboot --limit x1005c3s0b0n0,x1005c3s0b0n1,x1005c3s0b1n0,x1005c3s0b1n1   Check firmware  Verify that the correct firmware versions for node BIOS, node controller (nC), NIC mezzanine card (NMC), GPUs, and so on.\n  If necessary, update the firmware. Review the FAS Admin Procedures and Update Firmware with FAS procedure.\nncn-m001# cray fas actions create CUSTOM_DEVICE_PARAMETERS.json   Check DVS There should be a cray-cps pod (the broker), three cray-cps-etcd pods and their waiter, and at least one cray-cps-cm-pm pod. Usually there are two cray-cps-cm-pm pods, one on ncn-w002 and one on ncn-w003 and other worker nodes\n Check the cray-cps pods on worker nodes and verify they are Running.\n# kubectl get pods -Ao wide | grep cps services cray-cps-75cffc4b94-j9qzf 2/2 Running 0 42h 10.40.0.57 ncn-w001 services cray-cps-cm-pm-g6tjx 5/5 Running 21 41h 10.42.0.77 ncn-w003 services cray-cps-cm-pm-kss5k 5/5 Running 21 41h 10.39.0.80 ncn-w002 services cray-cps-etcd-knt45b8sjf 1/1 Running 0 42h 10.42.0.67 ncn-w003 services cray-cps-etcd-n76pmpbl5h 1/1 Running 0 42h 10.39.0.49 ncn-w002 services cray-cps-etcd-qwdn74rxmp 1/1 Running 0 42h 10.40.0.42 ncn-w001 services cray-cps-wait-for-etcd-jb95m 0/1 Completed   SSH to each worker node running CPS/DVS, and run dmesg -T to ensure that there are no recurring \u0026quot;DVS: merge_one\u0026quot;  error messages as shown. The error messages indicate that DVS is detecting an IP address change for one of the client nodes.\nncn-m001# dmesg -T | grep \u0026#34;DVS: merge_one\u0026#34; [Tue Jul 21 13:09:54 2020] DVS: merge_one#351: New node map entry does not match the existing entry [Tue Jul 21 13:09:54 2020] DVS: merge_one#353: nid: 8 -\u0026gt; 8 [Tue Jul 21 13:09:54 2020] DVS: merge_one#355: name: 'x3000c0s19b1n0' -\u0026gt; 'x3000c0s19b1n0' [Tue Jul 21 13:09:54 2020] DVS: merge_one#357: address: '10.252.0.26@tcp99' -\u0026gt; '10.252.0.33@tcp99' [Tue Jul 21 13:09:54 2020] DVS: merge_one#358: Ignoring.   Make sure the Configuration Framework Service (CFS) finished successfully. Review HPE Cray EX DVS Administration Guide 1.4.1 S-8004.\n  SSH to the node and check each DVS mount.\nnid001133:~ # mount | grep dvs | head -1 /var/lib/cps-local/0dbb42538e05485de6f433a28c19e200 on /var/opt/cray/gpu/nvidia-squashfs-21.3 type dvs (ro,relatime,blksize=524288,statsfile=/sys/kernel/debug/dvs/mounts/1/stats,attrcache_timeout=14400,cache,nodatasync,noclosesync,retry,failover,userenv,noclusterfs,killprocess,noatomic,nodeferopens,no_distribute_create_ops,no_ro_cache,loadbalance,maxnodes=1,nnodes=6,nomagic,hash_on_nid,hash=modulo,nodefile=/sys/kernel/debug/dvs/mounts/1/nodenames,nodename=x3000c0s6b0n0:x3000c0s5b0n0:x3000c0s4b0n0:x3000c0s9b0n0:x3000c0s8b0n0:x3000c0s7b0n0) nid001133:~ # ls /var/opt/cray/gpu/nvidia-squashfs-21.3 rootfs   Check the HSN for the affected nodes  Determine the pod name for the Slingshot fabric manager pod and check the status of the fabric.\nncn-m001# kubectl exec -it -n services \\ $(kubectl get pods --all-namespaces |grep slingshot | awk \u0026#39;{print $2}\u0026#39;) \\ -- fmn_status   Check DNS Check for duplicate IP entries in the State Management Database (SMD). Duplicate entries will cause DNS operations to fail.\nncn-m001:~ # ssh uan01 ssh: Could not resolve hostname uan01: Temporary failure in name resolution ncn-m001:~ # ssh x3000c0s14b0n0 ssh: Could not resolve hostname x3000c0s14b0n0: Temporary failure in name resolution ncn-m001:~ # ssh x1000c1s1b0n1 ssh: Could not resolve hostname x1000c1s1b0n1: Temporary failure in name resolution The Kea configuration error will display a message similar to the message below. This message indicates a duplicate IP address (10.100.0.105) in the SMD:\nConfig reload failed [{'result': 1, 'text': \u0026quot;Config reload failed: configuration error using file '/usr/local/kea/cray-dhcp-kea-dhcp4.conf': failed to add new host using the HW address '00:40:a6:83:50:a4 and DUID '(null)' to the IPv4 subnet id '0' for the address 10.100.0.105: There's already a reservation for this address\u0026quot;}] ncn-m001#  Use the following example curl command to check for active DHCP leases. If there are 0 DHCP leases, there is a configuration error.\nncn-m001# curl -s -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d \\ \u0026#39;{ \u0026#34;command\u0026#34;: \u0026#34;lease4-get-all\u0026#34;, \u0026#34;service\u0026#34;: [ \u0026#34;dhcp4\u0026#34; ] }\u0026#39; cray-dhcp-kea-api:8000 | jq [ { \u0026#34;arguments\u0026#34;: { \u0026#34;leases\u0026#34;: [] }, \u0026#34;result\u0026#34;: 3, \u0026#34;text\u0026#34;: \u0026#34;0 IPv4 lease(s) found.\u0026#34; } ] ncn-m001#   If there are duplicate entries in the SMD as a result of the swap procedure, (10.100.0.105 in this example), delete the duplicate entry.\n  Show the EthernetInterfaces for the duplicate IP address:\nncn-m001# curl -s http://cray-smd/hsm/v1/Inventory/EthernetInterfaces?IPAddress=10.100.0.105 | jq [ { \u0026#34;ID\u0026#34;: \u0026#34;0040a68350a4\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Node Maintenance Network\u0026#34;, \u0026#34;MACAddress\u0026#34;: \u0026#34;00:40:a6:83:50:a4\u0026#34;, \u0026#34;IPAddress\u0026#34;: \u0026#34;10.100.0.105\u0026#34;, \u0026#34;LastUpdate\u0026#34;: \u0026#34;2021-08-24T20:24:23.214023Z\u0026#34;, \u0026#34;ComponentID\u0026#34;: \u0026#34;x1000c7s7b0n1\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;Node\u0026#34; }, { \u0026#34;ID\u0026#34;: \u0026#34;0040a683639a\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Node Maintenance Network\u0026#34;, \u0026#34;MACAddress\u0026#34;: \u0026#34;00:40:a6:83:63:9a\u0026#34;, \u0026#34;IPAddress\u0026#34;: \u0026#34;10.100.0.105\u0026#34;, \u0026#34;LastUpdate\u0026#34;: \u0026#34;2021-08-27T19:15:53.697459Z\u0026#34;, \u0026#34;ComponentID\u0026#34;: \u0026#34;x1000c7s7b0n1\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;Node\u0026#34; } ]   Delete the older entry.\nncn-m001: curl -X DELETE -s -k http://cray-smd/hsm/v1/Inventory/EthernetInterfaces/0040a68350a4     Check DNS using dnslookup.\nncn-w001:~ # nslookup 10.252.1.29 29.1.252.10.in-addr.arpa\tname = uan01. 29.1.252.10.in-addr.arpa\tname = uan01.local. 29.1.252.10.in-addr.arpa\tname = x3000c0s14b0n0. 29.1.252.10.in-addr.arpa\tname = x3000c0s14b0n0.local. 29.1.252.10.in-addr.arpa\tname = uan01-nmn. 29.1.252.10.in-addr.arpa\tname = uan01-nmn.local. ncn-w001:~ # nslookup uan01 Server:\t10.92.100.225 Address:\t10.92.100.225#53 Name:\tuan01 Address: 10.252.1.29 ncn-w001:~ # nslookup x3000c0s14b0n0 Server:\t10.92.100.225 Address:\t10.92.100.225#53 Name:\tx3000c0s14b0n0 Address: 10.252.1.29   Check SSH.\nncn-m001:~ # ssh x3000c0s14b0n0 The authenticity of host \u0026#39;x3000c0s14b0n0 (10.252.1.29)\u0026#39; can\u0026#39;t be established. ECDSA key fingerprint is SHA256:wttHXF5CaJcQGPTIq4zWp0whx3JTwT/tpx1dJNyyXkA. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added \u0026#39;x3000c0s14b0n0\u0026#39; (ECDSA) to the list of known hosts. Last login: Tue Aug 31 10:45:49 2021 from 10.252.1.9   Bring up the blade in the source system To minimize cross-contamination of cooling systems, drain the coolant from the blade removed from destination system and fill with fresh coolant .   Review the HPE Cray EX Coolant Service Procedures H-6199. If using the hand pump, review procedures in the HPE Cray EX Hand Pump User Guide H-6200 (https://internal.support.hpe.com/).  Install the blade from the destination system into source system.   Review the Remove a Compute Blade Using the Lift procedure in HPE Cray EX Hardware Replacement Procedures H-6173 for detailed instructions for replacing liquid-cooled blades (https://internal.support.hpe.com/).  Repeat steps 26 through 50 to power on the nodes in the source system.  "
},
{
	"uri": "/docs-csm/en-11/operations/node_management/launch_a_virtual_kvm_on_intel_servers/",
	"title": "Launch A Virtual KVM On Intel Servers",
	"tags": [],
	"description": "",
	"content": "Launch a Virtual KVM on Intel Servers This procedure shows how to launch a virtual KVM to connect to an Intel server. The virtual KVM can be launched on any host that is on the same network as the server\u0026rsquo;s BMC. This method of connecting to a server is frequently used during system installation.\nPrerequisites  A laptop or workstation with a browser and access to the Internet The externally visible BMC IP address of the server\u0026rsquo;s integrated BMC  Procedure   Connect to the server\u0026rsquo;s BMC by entering the externally visible BMC IP address in the address bar of a web browser.\nThe login page appears.\nTroubleshooting:\n Java exception: If a Java exception occurs when trying to connect via SOL, see Change Java Security Settings. Unable to access BMC: If unable to access the node\u0026rsquo;s BMC and ConMan is being used, ConMan may be blocking that access. See ../conman/Troubleshoot ConMan Blocking Access to a Node BMC.    Log in to the BMC.\nOn the Intel Integrated BMC Web Console login page, enter the root user name and password for the BMC.\nThe Summary page appears.\n**Trouble?**If using the Firefox browser and it takes a long time to connect, look in the corner for \u0026ldquo;TLS handshake.\u0026rdquo; That is an indication of a TLS handshake issue, which can occur when the browser has too many self-signed matching certificates. To fix this, locate the cert8.db and cert9.db files on the local host and rename them to cert8.db.bak and cert9.db.bak, respectively.\n  Launch the remote console.\n  On the Remote Control tab, click KVM/Console Redirection.\n  Click the Launch Console button near the top of the KVM/Console Redirection page.\nThis launches a Java web application called iKVM Viewer.\n  The virtual KVM (iKVM Viewer) is ready to use. There is now a virtual iKVM session connected to the server that enables control via the web similar to standing directly in front of the physical KVM.\nTroubleshooting: If the interface appears to lock up while working in the BMC menus (often encountered when creating virtual drives), it may be necessary to reset the server using Power Control \u0026gt; Power Reset.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/node_management/move_a_standard_rack_node/",
	"title": "Move A Standard Rack Node",
	"tags": [],
	"description": "",
	"content": "Move a Standard Rack Node Update the location-based xname for a standard rack node within the system.\nPrerequisites   An authentication token has been retrieved.\nncn-m001# function get_token () { ADMIN_SECRET=$(kubectl get secrets admin-client-auth -ojsonpath='{.data.client-secret}' | base64 -d) curl -s -d grant_type=client_credentials -d client_id=admin-client -d client_secret=$ADMIN_SECRET \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token \\ | python -c 'import sys, json; print json.load(sys.stdin)[\u0026quot;access_token\u0026quot;]' }   The Cray command line interface (CLI) tool is initialized and configured on the system.\n  Procedure   Set variables for the new and old xname locations.\nThe NEWPORT variable is the xname of the port that the node BMC will be connected to after it is moved. The xname is typically something similar to x3003c0w41j42. The OLDENDPOINT variable is the xname of the BMC at its old location, for example, x3006c0r41b0.\nncn-m001# NEWPORT=x3003c0w41j42 ncn-m001# OLDENDPOINT=x3006c0r41b0   Generate and upload new management switch port information to the System Layout Service (SLS) and save it to a file.\nThis step may be skipped if this is a direct swap of nodes, where both the source and destination are already populated.\n  Query SLS to generate content for the new file.\nQuery the old port in SLS and replace the old xname (x3000c0w31j31 in this example) with the name of the current location of the hardware in the system.\nncn-m001# cray sls hardware describe x3000c0w31j31 --format json { \u0026#34;TypeString\u0026#34;: \u0026#34;MgmtSwitchConnector\u0026#34;, \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0w31\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_mgmt_switch_connector\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0w31j31\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NodeNics\u0026#34;: [ \u0026#34;x3000c0s24b0\u0026#34; ], \u0026#34;VendorName\u0026#34;: \u0026#34;ethernet1/1/31\u0026#34; } }   Create the new file with the updated location of the node.\nThe following is an example file. The Parent, Xname, NodeNics, and VendorName properties must be adjusted to match the new location of the node. The VendorName property may be obtained by logging into the switch that the node will be connected to.\nncn-m001# cat newport.json { \u0026#34;Parent\u0026#34;: \u0026#34;x3003c0w41\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3003c0w41j42\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;MgmtSwitchConnector\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_mgmt_switch_connector\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NodeNics\u0026#34;: [ \u0026#34;x3004c0r42b0\u0026#34; ], \u0026#34;VendorName\u0026#34;: \u0026#34;ethernet1/1/42\u0026#34; } }     Upload the updated node settings captured in the new JSON file.\nReplace the CUSTOM_FILE value in the following command with the name of the file created in the previous step.\nncn-m001# curl -i -X PUT -H \u0026#34;Authorization: Bearer $(get_token)\u0026#34; \\ https://api-gw-service-nmn.local/apis/sls/v1/hardware/$NEWPORT -d @CUSTOM_FILE   Delete the existing redfishEndpoint and ethernetInterfaces from the Hardware State Manager (HSM).\nncn-m001# cray hsm inventory redfishEndpoints delete $OLDENDPOINT message = \u0026#34;deleted 1 entry\u0026#34; code = 0 ncn-m001# for ID in $(cray hsm inventory ethernetInterfaces list \\ --format json | jq -r \u0026#34;.[] | select(.ComponentID==\\\u0026#34;$OLDENDPOINT\\\u0026#34;).ID\u0026#34;); \\ do cray hsm inventory ethernetInterfaces delete $ID; done message = \u0026#34;deleted 1 entry\u0026#34; code = 0   "
},
{
	"uri": "/docs-csm/en-11/operations/node_management/move_a_standard_rack_node_samerack_samehsnports/",
	"title": "Move A Standard Rack Node (same Rack/same HSN Ports)",
	"tags": [],
	"description": "",
	"content": "Move a Standard Rack Node (Same Rack/Same HSN Ports) This procedure move standard rack UAN or compute node to a different location and uses the same Slingshot switch ports and management network ports.\nUpdate the location-based xname for a standard rack node within the system.\nIf a node has an incorrect xname based on its physical location, then this procedure can utilized to correct the xname of the node without the need to physically moving the node.\nPrerequisites   An authentication token has been retrieved.\nncn-m001# function get_token () { curl -s -S -d grant_type=client_credentials \\  -d client_id=admin-client \\  -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\  https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39; }   The Cray command line interface (CLI) tool is initialized and configured on the system.\n  This procedure applies only to standard rack nodes. Liquid-cooled compute blades do not require the use of the MgmtSwitchConnector object in the System Layout Service (SLS) to perform discovery.\n  This procedure moves an application node or compute node to a different location in an HPE Cray standard rack.\n  The node must use the same Slingshot switch switch ports.\n  The node must use the same management network switch ports.\n  Limitations This procedure assumes there are no changes to the node high-speed network switch port or management network ports.\nProcedure This procedure works with both application and compute nodes. This example moves a compute node in rack 3000 at U17 to U27 in the same rack.\n  Shut down software and power off the node.\n  Disconnect the power cables, management network cables, and high-speed network (HSN) cables.\n If this procedure is being followed to correct a nodes xname, then this step can be skipped.\n   Move the node to the new location in the rack (U27), connect the management cables and HSN cables, but do not connect the power cables.\n If this procedure is being followed to correct a nodes xname, then this step can be skipped.\n   Update Node in the System Layout Service (SLS)  Setup environment variables for the original node and node BMC xnames:\nncn-m001# OLD_NODE_XNAME=x3000c0s17b1n0 ncn-m001# echo $OLD_NODE_XNAME x3000c0s17b1n0 ncn-m001# OLD_BMC_XNAME=$(echo $OLD_NODE_XNAME | egrep -o \u0026#39;x[0-9]+c[0-9]+s[0-9]+b[0-9]+\u0026#39;) ncn-m001# echo $OLD_BMC_XNAME x3000c0s17b1   Setup environment variables for the new node and node BMC xnames:\nncn-m001# NEW_NODE_XNAME=x3000c0s27b1n0 ncn-m001# echo $NEW_NODE_XNAME x3000c0s27b1n0 ncn-m001# NEW_BMC_XNAME=$(echo $NEW_NODE_XNAME | egrep -o \u0026#39;x[0-9]+c[0-9]+s[0-9]+b[0-9]+\u0026#39;) ncn-m001# echo $NEW_BMC_XNAME x3000c0s27b1   Update SLS with the node\u0026rsquo;s new xname.\n Get Node from SLS:  ncn-m001# cray sls hardware describe \u0026#34;$OLD_NODE_XNAME\u0026#34; --format json \u0026gt; sls_node.original.json Sample contents of sls_node.original.json\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0s17b1\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0s17b1n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;LastUpdated\u0026#34;: 1631829089, \u0026#34;LastUpdatedTime\u0026#34;: \u0026#34;2021-09-16 21:51:29.997834 +0000 +0000\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;Aliases\u0026#34;: [ \u0026#34;nid000001\u0026#34; ], \u0026#34;NID\u0026#34;: 1, \u0026#34;Role\u0026#34;: \u0026#34;Compute\u0026#34; } }  Update SLS Node object with new xnames:\nncn-m001# jq --arg NODE_XNAME \u0026#34;$NEW_NODE_XNAME\u0026#34; --arg BMC_XNAME \u0026#34;$NEW_BMC_XNAME\u0026#34; \\  \u0026#39;.Parent = $BMC_XNAME | .Xname = $NODE_XNAME\u0026#39; sls_node.original.json \\  \u0026gt; sls_node.json Expected content of sls_node.original.json:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0s19b1\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0s19b1n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;LastUpdated\u0026#34;: 1631829089, \u0026#34;LastUpdatedTime\u0026#34;: \u0026#34;2021-09-16 21:51:29.997834 +0000 +0000\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;Aliases\u0026#34;: [ \u0026#34;nid000001\u0026#34; ], \u0026#34;NID\u0026#34;: 1, \u0026#34;Role\u0026#34;: \u0026#34;Compute\u0026#34; } }  Only the fields Parent and Xname should have been updated.\n   Create new Node object in SLS:\nncn-m001# curl -i -X POST -H \u0026#34;Authorization: Bearer $(get_token)\u0026#34; \\  https://api-gw-service-nmn.local/apis/sls/v1/hardware -d @sls_node.json  Note: If a 503 is returned, verify that get_token function has been defined.\n Expected output:\nHTTP/2 200 content-type: application/json date: Mon, 18 Oct 2021 20:30:02 GMT content-length: 42 x-envoy-upstream-service-time: 71 server: istio-envoy {\u0026quot;code\u0026quot;:0,\u0026quot;message\u0026quot;:\u0026quot;inserted new entry\u0026quot;}   Delete old Node object from SLS:\nncn-m001# cray sls hardware delete $OLD_NODE_XNAME Expected output:\ncode = 0 message = \u0026quot;deleted entry and its descendants\u0026quot;     Update MgmtSwitchConnector in SLS with the node BMC\u0026rsquo;s new xname:\n  Get MgmtSwitchConnector object from SLS:\nncn-m001# cray sls search hardware list --node-nics \u0026#34;$OLD_BMC_XNAME\u0026#34; --format json \u0026gt; sls_MgmtSwitchConnector.original.json Sample contents of sls_MgmtSwitchConnector.original.json\n[ { \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0w22\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0w22j33\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_mgmt_switch_connector\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;MgmtSwitchConnector\u0026#34;, \u0026#34;LastUpdated\u0026#34;: 1631829089, \u0026#34;LastUpdatedTime\u0026#34;: \u0026#34;2021-09-16 21:51:29.997834 +0000 +0000\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NodeNics\u0026#34;: [ \u0026#34;x3000c0s17b1\u0026#34; ], \u0026#34;VendorName\u0026#34;: \u0026#34;ethernet1/1/33\u0026#34; } } ]   Update MgmtSwitchConnector object with the new node BMC xname:\nncn-m001# jq --arg BMC_XNAME \u0026#34;$NEW_BMC_XNAME\u0026#34; \\  \u0026#39;.[0] | .ExtraProperties.NodeNics = [ $BMC_XNAME ]\u0026#39; sls_MgmtSwitchConnector.original.json \\  \u0026gt; sls_MgmtSwitchConnector.json Expected content of sls_MgmtSwitchConnector.json:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0w22\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0w22j33\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_mgmt_switch_connector\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;MgmtSwitchConnector\u0026#34;, \u0026#34;LastUpdated\u0026#34;: 1631829089, \u0026#34;LastUpdatedTime\u0026#34;: \u0026#34;2021-09-16 21:51:29.997834 +0000 +0000\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NodeNics\u0026#34;: [ \u0026#34;x3000c0s27b1\u0026#34; ], \u0026#34;VendorName\u0026#34;: \u0026#34;ethernet1/1/33\u0026#34; } }  Only the NodeNics field should have been updated.\n   Determine the xname of the MgmtSwitchConnector:\nncn-m001# MGMT_SWITCH_CONNECTOR_XNAME=$(jq -r .Xname sls_MgmtSwitchConnector.json) ncn-m001# echo $MGMT_SWITCH_CONNECTOR_XNAME x3000c0w22j36   Update the MgmtSwitchConnector in SLS:\nncn-m001# curl -i -X PUT -H \u0026#34;Authorization: Bearer $(get_token)\u0026#34; \\  https://api-gw-service-nmn.local/apis/sls/v1/hardware/$MGMT_SWITCH_CONNECTOR_XNAME -d @sls_MgmtSwitchConnector.json Expected output:\nHTTP/2 200 content-type: application/json date: Mon, 18 Oct 2021 20:33:36 GMT content-length: 301 x-envoy-upstream-service-time: 8 server: istio-envoy {\u0026quot;Parent\u0026quot;:\u0026quot;x3000c0w22\u0026quot;,\u0026quot;Xname\u0026quot;:\u0026quot;x3000c0w22j36\u0026quot;,\u0026quot;Type\u0026quot;:\u0026quot;comptype_mgmt_switch_connector\u0026quot;,\u0026quot;Class\u0026quot;:\u0026quot;River\u0026quot;,\u0026quot;TypeString\u0026quot;:\u0026quot;MgmtSwitchConnector\u0026quot;,\u0026quot;LastUpdated\u0026quot;:1631829089,\u0026quot;LastUpdatedTime\u0026quot;:\u0026quot;2021-09-16 21:51:29.997834 +0000 +0000\u0026quot;,\u0026quot;ExtraProperties\u0026quot;:{\u0026quot;NodeNics\u0026quot;:[\u0026quot;x3000c0s21b4\u0026quot;],\u0026quot;VendorName\u0026quot;:\u0026quot;ethernet1/1/36\u0026quot;}}     Remove previously discovered node data from HSM  Remove previously discovered components from HSM:\nRemove Node component from HSM:\nncn-m001# cray hsm state components delete $OLD_NODE_XNAME Remove NodeBMC component from HSM:\nncn-m001# cray hsm state components delete $OLD_BMC_XNAME Remove NodeEnclosure component form HSM. The xname for a NodeEnclosure is similar to the node BMC xname, but the b is replaced with a e.\nncn-m001# OLD_NODE_ENCLOSURE_XNAME=x3000c0s17e0 ncn-m001# cray hsm state components delete $OLD_NODE_ENCLOSURE_XNAME   Delete the NodeBMC, Node NIC MAC addresses, and the Redfish endpoint for the U17 node from th HSM.\n  Delete the Node MAC addresses from the HSM.\nncn-m001# for ID in $(cray hsm inventory ethernetInterfaces list --component-id $OLD_NODE_XNAME --format json | jq -r .[].ID); do echo \u0026#34;Deleting MAC address: $ID\u0026#34; cray hsm inventory ethernetInterfaces delete $ID; done   Delete each NodeBMC MAC address from the Hardware State Manager (HSM) Ethernet interfaces table.\nncn-m001# for ID in $(cray hsm inventory ethernetInterfaces list --component-id $OLD_BMC_XNAME --format json | jq -r .[].ID); do echo \u0026#34;Deleting MAC address: $ID\u0026#34; cray hsm inventory ethernetInterfaces delete $ID; done   Delete the Redfish endpoint for the removed node.\nncn-m001# cray hsm inventory redfishEndpoints delete $OLD_BMC_XNAME     Connect the power cables to the node to power on the BMC.\n If this procedure is being followed to correct a nodes xname, then this step can be skipped.\n   Wait for 5 minutes for power on and the node BMCs to be discovered.\nncn-m001# sleep 300   Verify the node BMC has been discovered by the HSM.\nncn-m001# cray hsm inventory redfishEndpoints describe $NEW_BMC_XNAME --format json { \u0026#34;ID\u0026#34;: \u0026#34;x3000c0s27b1\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;NodeBMC\u0026#34;, \u0026#34;Hostname\u0026#34;: \u0026#34;x3000c0s27b1\u0026#34;, \u0026#34;Domain\u0026#34;: \u0026#34;, \u0026#34;FQDN\u0026#34;: \u0026#34;x3000c0s27b1\u0026#34;, \u0026#34;Enabled\u0026#34;: true, \u0026#34;UUID\u0026#34;: \u0026#34;e005dd6e-debf-0010-e803-b42e99be1a2d\u0026#34;, \u0026#34;User\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;, \u0026#34;MACAddr\u0026#34;: \u0026#34;b42e99be1a2d\u0026#34;, \u0026#34;RediscoverOnUpdate\u0026#34;: true, \u0026#34;DiscoveryInfo\u0026#34;: { \u0026#34;LastDiscoveryAttempt\u0026#34;: \u0026#34;2021-01-29T16:15:37.643327Z\u0026#34;, \u0026#34;LastDiscoveryStatus\u0026#34;: \u0026#34;DiscoverOK\u0026#34;, \u0026#34;RedfishVersion\u0026#34;: \u0026#34;1.7.0\u0026#34; } }  When LastDiscoveryStatus displays as DiscoverOK, the node BMC has been successfully discovered. If the last discovery state is DiscoveryStarted then the BMC is currently being inventoried by HSM. If the last discovery state is HTTPsGetFailed or ChildVerificationFailed then an error occurred during the discovery process.   For HTTPsGetFailed, verify that the BMC is pingable by its xname. If the xname of the BMC is not resolveable it, more time may be needed for DNS to update.\nIf hostname it does resolve, issue a discovery request to HSM:\nncn-m001# cray hsm inventory discover create --xnames $NEW_BMC_XNAME       Verify that the nodes are enabled in the HSM.\nncn-m001# cray hsm state components describe $NEW_NODE_XNAME Type = \u0026#34;Node\u0026#34; Enabled = true State = \u0026#34;Off\u0026#34; . . .   If necessary, enable each node individually in the HSM database (in this example, the nodes are x3000c0s27b[1-4]n0).\nncn-m001# cray hsm state components enabled update --enabled true x3000c0s27b1n0 ncn-m001# cray hsm state components enabled update --enabled true x3000c0s27b2n0 ncn-m001# cray hsm state components enabled update --enabled true x3000c0s27b3n0 ncn-m001# cray hsm state components enabled update --enabled true x3000c0s27b4n0   Use boot orchestration to power on and boot the nodes.\nSpecify the appropriate BOS template for the node type.\nncn-m001# cray bos v1 session create --template-uuid cle-VERSION \\ --operation reboot --limit x3000c0s27b1n0,x3000c0s27b2n1,x3000c0s27b3n0,x3000c0s27b4n0   "
},
{
	"uri": "/docs-csm/en-11/operations/node_management/node_management/",
	"title": "Node Management",
	"tags": [],
	"description": "",
	"content": "Node Management The HPE Cray EX system includes two types of nodes:\n Compute Nodes, where high performance computing applications are run, and node names in the form of nidXXXXXX Non-Compute Nodes (NCNs), which carry out system functions and come in three versions:  Master nodes, with names in the form of ncn-mXXX Worker nodes, with names in the form of ncn-wXXX Utility Storage nodes, with names in the form of ncn-sXXX    The HPE Cray EX system includes the following nodes:\n Nine or more non-compute nodes (NCNs) that host system services:  ncn-m001, ncn-m002, and ncn-m003 are configured as Kubernetes master nodes. ncn-w001, ncn-w002, and ncn-w003 are configured as Kubernetes worker nodes. Every system contains three or more worker nodes. ncn-s001, ncn-s002, and ncn-s003 for storage. Every system contains three or more utility storage node.   Four or more compute nodes, starting at nid000001.  "
},
{
	"uri": "/docs-csm/en-11/operations/node_management/node_management_workflows/",
	"title": "Node Management Workflows",
	"tags": [],
	"description": "",
	"content": "Node Management Workflows The following workflows are intended to be high-level overviews of node management tasks. These workflows depict how services interact with each other during node management and help to provide a quicker and deeper understanding of how the system functions.\nThe workflows and procedures in this section include:\n Add Nodes Remove Nodes Replace Nodes Move Nodes  Add Nodes  Add a Standard Rack Node  Use Cases: Administrator permanently adds select compute nodes to expand the system.\nComponents: This workflow is based on the interaction of the System Layout Service (SLS) with other hardware management services (HMS).\nMentioned in this workflow:\n System Layout Service (SLS) serves as a \u0026ldquo;single source of truth\u0026rdquo; for the system design. It details the physical locations of network hardware, compute nodes and cabinets. Further, it stores information about the network, such as which port on which switch should be connected to each compute node. Hardware State Manager (HSM) monitors and interrogates hardware components in an HPE Cray EX system, tracking hardware state and inventory information, and making it available via REST queries and message bus events when changes occur. HMS Notification Fanout Daemon (hmnfd) receives component state change notifications from the HSM. It fans notifications out to subscribers (typically compute nodes). Endpoint Discovery Service (REDS/MEDS) manages initial discovery, configuration, and geolocation of Redfish-enabled BMCs. It periodically makes Redfish requests to determine if hardware is present or missing. Heartbeat Tracker Service (hbtd) listens for heartbeats from components (mainly compute nodes). It tracks changes in heartbeats and conveys changes to HSM.  Workflow Overview: The following sequence of steps occur during this workflow.\n  Administrator updates SLS\nAdministrator creates a new hardware entry for the select xnames in SLS. Enter the node xnames in the SLS input file.\n  Administrator adds compute nodes\nThe Administrator physically adds select compute nodes and powers them on. Because the nodes are unknown, the DHCP and TFTP servers give it the special initialization ramdisk. The compute nodes performs local configuration.\nThe following steps (3-11) occur automatically as different APIs interact with each other.\n  Set BMC credentials\nThe compute node requests per-node BMC credentials. This message must include the MAC address of the BMC. A new set of credentials is generated by the discovery service.\nOnce the compute node is powered on, initialized, and discovered, REDS gets details about the new node like IP address, MAC address, sets the username and password for a BMC, state etc.\n  REDS/MEDS to SLS\nREDS/MEDS query SLS database for information about the new node.\nFor example: \u0026ldquo;What xname is connected to port XX on switch Y?\u0026rdquo;\n  SLS to REDS/MEDS\nSLS updates the discovery service with the new compute node and its xname.\nFor example: \u0026ldquo;XName x0c0\u0026hellip; is connected to port XX\u0026rdquo;.\n  REDS/MEDS to HSM\nDiscovery services update HSM about the new Redfish endpoint for the node. Details like xname and IP address of the new node are updated in HSM.\nFor example: \u0026ldquo;x0c0\u0026hellip; at IP address AAA.BBB.CCC.DDD\u0026rdquo;\n  HSM to SLS\nHSM queries SLS for NID and role assignments for the new node.\n  SLS to HSM\nHSM updates the Nodemap based on information received from SLS.\n  Node to Heartbeat Tracker Service\nThe Heartbeat Tracker Service receives heartbeats from the new compute node after the node is powered on.\n  Heartbeat Tracker Service to HSM\nThe Heartbeat Tracker Service report the heartbeat status to HSM.\n  HSM to HMNFD\nHSM sends the new compute node state information with State as ON to HMNFD. HMNFD fans out these notifications to the subscribing compute nodes.\n  Remove Nodes Use Cases: Administrator permanently removes select compute nodes to contract the system.\nComponents: This workflow is based on the interaction of the System Layout Service (SLS) with other hardware management services (HMS).\nMentioned in this workflow:\n System Layout Service (SLS) serves as a \u0026ldquo;single source of truth\u0026rdquo; for the system design. It details the physical locations of network hardware, compute nodes and cabinets. Further, it stores information about the network, such as which port on which switch should be connected to each compute node. Hardware State Manager (HSM) monitors and interrogates hardware components in an HPE Cray EX system, tracking hardware state and inventory information, and making it available via REST queries and message bus events when changes occur. HMS Notification Fanout Daemon (hmnfd) receives component state change notifications from the HSM. It fans notifications out to subscribers (typically compute nodes). Endpoint Discovery Service (REDS/MEDS) manages initial discovery, configuration, and geolocation of Redfish-enabled BMCs. It periodically makes Redfish requests to determine if hardware is present or missing. Heartbeat Tracker Service (hbtd) listens for heartbeats from components (mainly compute nodes). It tracks changes in heartbeats and conveys changes to HSM.  Workflow Overview: The following sequence of steps occur during this workflow.\n  Administrator updates SLS\nAdministrator deletes the node entries with the specific xname from SLS. Note that if deleting a parent object, then the children are also deleted from SLS. If the child object happens to be a parent, then the deletion can cascade down levels. If deleting a child object, it does not affect the parent.\n  Administrator physically removes the compute nodes\nThe Administrator powers off and physically removes the compute nodes.\nThe following steps (3-9) occur automatically as different APIs interact with each other.\n  No heartbeats\nThe Heartbeat Tracker Service stops receiving heartbeats and marks the nodes status as standby and then off as per Redfish event.\nStandby status implies that the node is no longer ready and presumed dead. It typically means that the heartbeat is lost. Off status implies that the location is not populated with a component.\n  Heartbeat Tracker Service to HSM\nThe Heartbeat Tracker Service reports the heartbeat status to HSM.\n  REDS/MEDS detect no BMC\nThe discovery service detects that the BMC is not there.\n  REDS/MEDS to SLS\nREDS/MEDS query SLS database for information about the missing BMCs.\n  SLS to REDS/MEDS\nSLS updates the discovery service that the BMC was removed.\n  REDS/MEDS to HSM\nDiscovery services update HSM that the BMC Redfish endpoints for the nodes were removed. HSM marks the state of BMCs and the nodes as empty.\nEmpty state implies that the location is not populated with a component.\n  HSM to HMNFD\nHSM sends the compute node state information with State as empty to HMNFD. HMNFD fans out this notification to the subscribing compute nodes.\n  Replace Nodes  Replace a Compute Blade Swap a Compute Blade with a Different System  Move Nodes  Move a Standard Rack Node Move a Standard Rack Node (Same HSN Ports)  "
},
{
	"uri": "/docs-csm/en-11/operations/node_management/dump_a_non-compute_node/",
	"title": "Dump A Non-compute Node",
	"tags": [],
	"description": "",
	"content": "Dump a Non-Compute Node Trigger an NCN memory dump and send the dump for analysis. This procedure is helpful for debugging NCN crashes.\nPrerequisites A non-compute node (NCN) has crashed or an admin has triggered a node crash.\nProcedure   Force a dump on an NCN.\nncn-m001# echo c \u0026gt; /proc/sysrq-trigger   Wait for the node to reboot.\nThe NCN dump is stored in /var/crash is on local disk after the node is rebooted.\n  Collect the dump data using the System Dump Utility (SDU).\nRefer to the \u0026ldquo;Run a Triage Collection with SDU\u0026rdquo; procedure in the SDU product stream documentation for more information about collecting dump data.\nThe --start_time command option can be customized. For example, \u0026ldquo;-1 day\u0026rdquo;, \u0026ldquo;-2 hours\u0026rdquo;, or a date/time string can be used. For more information on the SDU command options, use the sdu --help command.\nncn-m001# sdu --scenario triage --start_time DATE_OR_TIME_STRING --end_time DATE_OR_TIME_STRING \\ --plugin ncn.gather.nodes --plugin ncn.gather.kernel.dumps Refer to the https://documentation.suse.com/ for more information on memory dumps or crash dumps.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/node_management/enable_nodes/",
	"title": "Enable Nodes",
	"tags": [],
	"description": "",
	"content": "Enable Nodes Use the Hardware State Manager (HSM) Cray CLI commands to enable nodes on the system.\nEnabling nodes that are available provides an accurate system configuration and node map.\nPrerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system.  Procedure   Enable individual nodes with HSM.\nncn-m001# cray hsm state components enabled update --enabled true XNAME   Verify the desired nodes are enabled.\nncn-m001# cray hsm state components describe XNAME Type = \u0026#34;Node\u0026#34; Enabled = true State = \u0026#34;On\u0026#34; NID = 1003 Flag = \u0026#34;OK\u0026#34; Role = \u0026#34;Compute\u0026#34; NetType = \u0026#34;Sling\u0026#34; Arch = \u0026#34;X86\u0026#34; ID = \u0026#34;x5000c1s0b1n1\u0026#34;   "
},
{
	"uri": "/docs-csm/en-11/operations/node_management/enable_passwordless_connections_to_liquid_cooled_node_bmcs/",
	"title": "Enable Passwordless Connections To Liquid Cooled Node BMCs",
	"tags": [],
	"description": "",
	"content": "Enable Passwordless Connections to Liquid Cooled Node BMCs Set the passwordless SSH keys for the root account and/or console of all liquid-cooled Baseboard Management Controllers (BMCs) on the system. This procedure will not work on BMCs for air-cooled hardware.\nWarning: If admin uses SCSD to update the SSHConsoleKey value outside of ConMan, it will disrupt the ConMan connection to the console and collection of console logs. Refer to ConMan for more information.\nSetting up SSH keys enables administrators to view recent console messages and interact with the console device for nodes.\nPrerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system. This procedure requires administrative privileges.  Procedure   Save the public SSH key for the root user.\nncn-w001# export SSH_PUBLIC_KEY=$(cat /root/.ssh/id_rsa.pub | sed \u0026#39;s/[[:space:]]*$//\u0026#39;)   Enable passwordless SSH to the root user of the BMCs.\nSkip this step if passwordless SSH to the root user is not desired.\nncn-w001# export SCSD_SSH_KEY=$SSH_PUBLIC_KEY   Enable passwordless SSH to the consoles on the BMCs.\nSkip this step if passwordless SSH to the consoles is not desired.\nncn-w001# export SCSD_SSH_CONSOLE_KEY=$SSH_PUBLIC_KEY   Generate a System Configuration Service configuration via the scsd tool.\nThe admin must be authenticated to the Cray CLI before proceeding.\nncn-w001# cat \u0026gt; scsd_cfg.json \u0026lt;\u0026lt;DATA { \u0026#34;Force\u0026#34;:false, \u0026#34;Targets\u0026#34;: $(cray hsm inventory redfishEndpoints list --format=json | jq \u0026#39;[.RedfishEndpoints[] | .ID]\u0026#39; | sed \u0026#39;s/^/ /\u0026#39;), \u0026#34;Params\u0026#34;:{ \u0026#34;SSHKey\u0026#34;:\u0026#34;$(echo $SCSD_SSH_KEY)\u0026#34;, \u0026#34;SSHConsoleKey\u0026#34;:\u0026#34;$(echo $SCSD_SSH_CONSOLE_KEY)\u0026#34; } } DATA   Inspect the generated scsd_cfg.json file.\nEnsure the following are true before running the command below:\n The xname list looks valid/appropriate The SSHKey and SSHConsoleKey settings match the desired public key  ncn-w001# cray scsd bmc loadcfg create scsd\\_cfg.json Check the output to verify all hardware has been set with the correct keys. Passwordless SSH to the root user and/or the consoles should now function as expected.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/node_management/find_node_type_and_manufacturer/",
	"title": "Find Node Type And Manufacturer",
	"tags": [],
	"description": "",
	"content": "Find Node Type and Manufacturer There are three different vendors providing nodes for air-cooled cabinets, which are Gigabyte, Intel, and HPE. The Hardware State Manager (HSM) contains the information required to determine which type of air-cooled node is installed. The endpoint returned in the HSM command can be used to determine the manufacturer.\nHPE nodes contain the /redfish/v1/Systems/1 endpoint:\nncn-m001# cray hsm inventory componentEndpoints describe XNAME | jq '.RedfishURL' \u0026quot;x3000c0s18b0/redfish/v1/Systems/1\u0026quot; Gigabyte nodes contain the /redfish/v1/Systems/Self endpoint:\nncn-m001# cray hsm inventory componentEndpoints describe XNAME | jq '.RedfishURL' \u0026quot;x3000c0s7b0/redfish/v1/Systems/Self\u0026quot; Intel nodes contain the /redfish/v1/Systems/SERIAL_NUMBER endpoint:\nncn-m001# cray hsm inventory componentEndpoints describe XNAME | jq '.RedfishURL' \u0026quot;x3000c0s15b0/redfish/v1/Systems/BQWT92000021\u0026quot; "
},
{
	"uri": "/docs-csm/en-11/operations/node_management/launch_a_virtual_kvm_on_gigabyte_servers/",
	"title": "Launch A Virtual KVM On Gigabyte Servers",
	"tags": [],
	"description": "",
	"content": "Launch a Virtual KVM on Gigabyte Servers This procedure shows how to launch a virtual KVM to connect to Gigabyte server. The virtual KVM can be launched on any host that is on the same network as the server\u0026rsquo;s BMC. This method of connecting to a server is frequently used during system installation.\nPrerequisites  A laptop or workstation with a browser and access to the Internet The externally visible BMC IP address of the server\u0026rsquo;s integrated BMC  Procedure   Connect to the server\u0026rsquo;s BMC by entering the externally visible BMC IP address in the address bar of a web browser.\nThe login page appears.\n  Log in to the BMC.\nOn the American Megatrends BMC Firmware Information login page, enter the root user name and password for the BMC.\nThe Dashboard page appears.\n  Launch the remote console.\n  Click on the Remote Control tab.\n  Launch the desired KVM viewer.\n H5Viewer is compatible with most browsers. JViewer is an option when HTML5 is not available. Serial Over LAN should be used as a last resort option.    The virtual KVM is ready to use. There is now a virtual KVM session connected to the server that enables control via the web similar to standing directly in front of the physical KVM.\nTroubleshooting: If the interface appears to lock up while working in the BMC menus (often encountered when creating virtual drives), it may be necessary to reset the server using Power Control \u0026gt; Power Reset.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/node_management/check_the_bmc_failover_mode/",
	"title": "Check The BMC Failover Mode",
	"tags": [],
	"description": "",
	"content": "Check the BMC Failover Mode Gigabyte BMCs must have their failover mode disabled to prevent incorrect network assignment.\nIf Gigabyte BMC failover mode is not disabled, some BMCs may receive incorrect IP addresses. Specifically, a BMC may request an IP address on the wrong subnet and be unable to re-acquire a new IP address on the correct subnet. If this occurs, administrators should ensure that the impacted BMC has its failover feature disabled.\nProcedure   Check the failover setting on a Gigabyte BMC.\nFor example:\nncn-m001# export USERNAME=root ncn-m001# export IPMI_PASSWORD=changeme ncn-m001# ipmitool -I lanplus -U $USERNAME -E -H 172.30.52.247 raw 0x0c 0x02 0x01 210 0 0 11 00 00 The output can be interpreted as follows:\n 11 00 01 - failover mode is enabled. 11 00 00 – failover mode is disabled (this is the desired state). Please note that if a Gigabyte BMC is reset to defaults for any reason, or upgraded, the failover mode will need to be disabled again to switch the BMC to manual mode as the default setting is for failover mode to be enabled.    "
},
{
	"uri": "/docs-csm/en-11/operations/node_management/clear_space_in_root_file_system_on_worker_nodes/",
	"title": "Clear Space In Root File System On Worker Nodes",
	"tags": [],
	"description": "",
	"content": "Clear Space in Root File System on Worker Nodes The disk space on an NCN worker node can fill up if any services are consuming a large portion of the root file system on the node. This procedure shows how to safely clear some space on worker nodes to return them to an appropriate storage threshold.\nPrerequisites An NCN worker node has a full disk.\nProcedure   Check to see if Docker is running.\nncn-w001# syctemctl status docker ● docker.service - Docker Application Container Engine Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor pres\u0026gt; Active: **active** (running) since Wed 2020-06-10 11:03:49 CDT; 2 months 2 days \u0026gt; Docs: http://docs.docker.com Main PID: 3062 (dockerd) Tasks: 145 CGroup: /system.slice/docker.service ├─3062 /usr/bin/dockerd --add-runtime oci=/usr/sbin/docker-runc ├─3248 docker-containerd --config /var/run/docker/containerd/contain\u0026gt; ├─5557 /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port \u0026gt; └─5576 docker-containerd-shim -namespace moby -workdir /var/lib/dock\u0026gt; ... If Docker is active, proceed to the next step to check its usage.\n  View the file space usage for Docker.\nncn-w001# du -sh /var/lib/docker 178G /var/lib/docker If the output indicates usage is over 100GB, proceed to the next step to prune Docker.\n  Prune the Docker images.\nThe until=24 option in the command below preserves data less than one day old.\nncn-w001# docker image prune -a --filter until=24h Check the usage again with the du -sh /var/lib/docker command.\n  Prune the Docker volumes.\nThe until=24 option in the command below preserves data less than one day old.\nncn-w001# docker volume prune -a --filter until=24h Check the usage again with the du -sh /var/lib/docker command.\n  Check the usage of /var/log/cray.\nAnother potentially large consumer of space is /var/log/cray when certain debug flags are enabled.\nncn-w001# du -sh /var/log/cray 76M /var/log/cray If the usage is over 20GB, examine the logging and determine if any of the older log information needs to be kept. Candidates for clean up include old imfile-state files, as well as old forwarding-queue files. Reduce the quantity of any additional logging as soon as possible to prevent the disk from filling up again.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/node_management/configuration_of_ncn_bonding/",
	"title": "Configuration Of NCN Bonding",
	"tags": [],
	"description": "",
	"content": "Configuration of NCN Bonding Non-compute nodes (NCNs) have network interface controllers (NICs) connected to the management network that are configured in a redundant manner via Link Aggregation Control Protocol (LACP) link aggregation. The link aggregation configuration can be modified by editing and applying various configuration files either through Ansible or the interfaces directly.\nThe bond configuration exists across three files on an NCN. These files may vary depending on the NCN in use:\n  ifcfg-bond0\nPhysical configuration and member interfaces.\n  ifroute-bond0\nRouting, which is critical for NCN PXE to work.\n  ifrule-bond0\nRouting table selecting, which is critical for NCN PXE to work.\n  The following is an example of ifcfg-bond0:\nncn-w001# cat /etc/sysconfig/network/ifcfg-bond0 BONDING_MASTER=\u0026#39;yes\u0026#39; BONDING_MODULE_OPTS=\u0026#39;mode=802.3ad miimon=100 lacp_rate=fast xmit_hash_policy=layer2+3\u0026#39; BONDING_SLAVE0=\u0026#39;p1p1\u0026#39; BONDING_SLAVE1=\u0026#39;p1p2\u0026#39; MTU=\u0026#39;9238\u0026#39; STARTMODE=\u0026#39;auto\u0026#39; BOOTPROTO=\u0026#39;static\u0026#39; PREFIXLEN=\u0026#39;16\u0026#39; IPADDR=\u0026#39;10.1.1.1/16\u0026#39; ZONE=\u0026#39;Do not assign ZONE\u0026#39; The bond is configured with modules that can be changed at the network administrators discretion and coordination.\nIt may be useful to only adjust the XMIT value to layer2; the current setting is chosen as a default to match existing settings for compute nodes from the previous release. This can be weighed out if problems arise across the NCNs over the bond and dual-spine or Multi-Chassis Link Aggregation (MLAG).\nWicked NetworkManager Wicked is the SUSE NetworkManager and Daemon wrapper for handling interfaces' processes and applying their configuration. See the SUSE Wicked external documentation for more information.\nFor administrators familiar with the more common Linux distribution, Ubuntu, it has an analogue to Wicked called NetPlan. The benefit is that it removes tedious, low-level configurations. However, Wicked and NetPlan each have their own web of configuration. The examples below are useful ways Wicked can be used to debug and triage interfaces.\nTo view a system wide interface network configuration:\nncn-w001# wicked ifstatus all Use the following command to view information about a specific interface. In this example, vlan007 is used.\nncn-w001# wicked ifstatus --verbose vlan007 vlan007 up link: #4603, state up, mtu 1500 type: vlan bond0[7], hwaddr b8:59:9f:c7:11:12 control: none config: compat:suse:/etc/sysconfig/network/ifcfg-vlan007, uuid: 5cce4d33-8d99-50a2-b6c0-b4b3d101c557 leases: ipv4 static granted addr: ipv6 fe80::ba59:9fff:fec7:1112/64 scope link addr: ipv4 10.102.3.4/24 brd 10.102.3.4 scope universe label vlan007 [static] route: ipv4 0.0.0.0/0 via 10.102.3.20 dev vlan007 type unicast table 3 scope universe protocol boot route: ipv4 10.102.3.0/24 type unicast table main scope link protocol kernel pref-src 10.102.3.4 route: ipv6 fe80::/64 type unicast table main scope universe protocol kernel priority 256 To view information about the bond:\nncn-w001# wicked ifstatus bond0 bond0 device-not-running link: #9, state up, mtu 9238 type: bond, mode ieee802-3ad, hwaddr b8:59:9f:4a:f6:30 config: compat:suse:/etc/sysconfig/network/ifcfg-bond0 leases: ipv4 static failed "
},
{
	"uri": "/docs-csm/en-11/operations/node_management/configure_ntp_on_ncns/",
	"title": "Configure NTP On NCNs",
	"tags": [],
	"description": "",
	"content": "Configure NTP on NCNs The management nodes serve Network Time Protocol (NTP) at stratum 3, and all management nodes peer with each other. Currently, the PIT node booted from the LiveCD does not run NTP, but the other nodes do when they are booted. NTP is currently allowed on the Node Management Network (NMN) and Hardware Management Network (HMN).\nThe NTP peers are set in the data.json file, which is normally created during an initial install. To configure NTP, edit this file, restart basecamp, and then reboot the nodes to apply the change.\nIf NTP is not configured in the data.json file, the NCNs will simply peer with themselves until an upstream NTP server is configured. The time on the NCNs may not match the current time at the site, but they will stay in sync with each other.\nTopics:  Change NTP Config Troubleshooting NTP  chrony Log Files Force a Time Sync   Customize NTP  Set A Local Timezone Configure NTP on PIT to Local Timezone Configure NCN Images to Use Local Timezone    Details Change NTP Config There are three different methods for configuring NTP, which are described below. The first option is the recommended method.\n  Edit /etc/chrony.d/cray.conf and restart chronyd on each node.\nncn# vi /etc/chrony.d/cray.conf ncn# systemctl restart chronyd   Edit the data.json file, restart basecamp, and run the NTP script on each node.\nncn-m001# vi data.json ncn-m001# systemctl restart basecamp ncn# /srv/cray/scripts/metal/set-ntp-config.sh   Edit the data.json file, restart basecamp, and restart nodes so cloud-init runs on boot.\nncn-m001# vi data.json ncn-m001# systemctl restart basecamp Reboot each node.\nncn# reboot Cloud-init caches data, so there could be inconsistent results with this method.\n  Troubleshooting NTP Verify NTP is configured correctly and troubleshoot any issues.\nThe chronyc command can be used to gather information on the state of NTP.\n  Check if a host is allowed to use NTP from HOST. This example sets HOST to 10.252.0.7\nncn# chronyc accheck 10.252.0.7 208 Access allowed   Check the system clock performance.\nncn# chronyc tracking Reference ID : 0AFC0104 (ncn-s003) Stratum : 4 Ref time (UTC) : Mon Nov 30 20:02:24 2020 System time : 0.000007622 seconds slow of NTP time Last offset : -0.000014609 seconds RMS offset : 0.000015776 seconds Frequency : 6.773 ppm fast Residual freq : -0.000 ppm Skew : 0.008 ppm Root delay : 0.000075896 seconds Root dispersion : 0.000484318 seconds Update interval : 513.7 seconds Leap status : Normal   View information on drift and offset\nncn# chronyc sourcestats 210 Number of sources = 8 Name/IP Address NP NR Span Frequency Freq Skew Offset Std Dev ============================================================================== ncn-w001 6 3 42m -0.029 0.126 +4104ns 28us ncn-w002 6 6 42m -0.028 0.030 +44us 7278ns ncn-w003 12 7 23m -0.059 0.023 -35us 8359ns ncn-s002 36 17 213m -0.001 0.010 +5794ns 54us ncn-s003 36 17 212m -0.000 0.007 -178ns 40us ncn-m001 0 0 0 +0.000 2000.000 +0ns 4000ms ncn-m002 28 15 192m -0.007 0.009 +9942ns 49us ncn-m003 24 15 197m -0.005 0.009 +9442ns 46us   View the NTP servers, pools, and peers.\nncn# chronyc sources 210 Number of sources = 8 MS Name/IP address Stratum Poll Reach LastRx Last sample =============================================================================== =? ncn-w001 4 9 377 435 +162us[ +164us] +/- 679us =? ncn-w002 4 9 377 505 +118us[ +120us] +/- 277us =? ncn-w003 4 7 377 82 +850ns[+2686ns] +/- 504us =? ncn-s002 4 9 377 542 -38us[ -36us] +/- 892us =* ncn-s003 3 9 377 19 +13us[ +15us] +/- 110us =? ncn-m001 0 9 0 - +0ns[ +0ns] +/- 0ns =? ncn-m002 4 8 377 161 -47us[ -45us] +/- 408us =? ncn-m003 4 8 377 215 -11us[-9109ns] +/- 446us   chrony Log Files The chrony logs are stored at /var/log/chrony/\nForce a Time Sync   If the time is out of sync, force a sync of NTP.\nIf Kubernetes or other services are already up, they do not always react well if there is a large time jump. Ideally, this action should be made as the node is booting.\nncn# chronyc burst 4/4   Wait about 15 seconds while NTP measurements are gathered\nncn# sleep 15   Jump the clock manually\nncn# chronyc makestep   Customize NTP Set A Local Timezone This procedure needs to be completed on the PIT node before the other management nodes are deployed.\nConfigure NTP on PIT to Local Timezone HPE Cray EX systems with CSM software have UTC as the default time zone. To change this, you will need to set an environment variable, as well as chroot into the node images and change some files there. You can find a list of timezones to use in the commands below by running timedatectl list-timezones.\n  Run the following commands, replacing them with your timezone as needed.\npit# export NEWTZ=America/Chicago pit# echo -e \u0026#34;\\nTZ=${NEWTZ}\u0026#34; \u0026gt;\u0026gt; /etc/environment pit# sed -i \u0026#34;s#^timedatectl set-timezone UTC#timedatectl set-timezone ${NEWTZ}#\u0026#34; /root/bin/configure-ntp.sh pit# sed -i \u0026#39;s/--utc/--localtime/\u0026#39; /root/bin/configure-ntp.sh pit# /root/bin/configure-ntp.sh   The configure-ntp.sh script should have the information for your local timezone in the output.\npit# /root/bin/configure-ntp.sh CURRENT TIME SETTINGS rtc: 2021-03-26 11:34:45.873331+00:00 sys: 2021-03-26 11:34:46.015647+0000 200 OK 200 OK NEW TIME SETTINGS rtc: 2021-03-26 06:35:16.576477-05:00 sys: 2021-03-26 06:35:17.004587-0500   Verify the new timezone setting by running timedatectl and hwclock --verbose.\npit# timedatectl Local time: Fri 2021-03-26 06:35:58 CDT Universal time: Fri 2021-03-26 11:35:58 UTC RTC time: Fri 2021-03-26 11:35:58 Time zone: America/Chicago (CDT, -0500) Network time on: no NTP synchronized: no RTC in local TZ: no pit# hwclock --verbose hwclock from util-linux 2.33.1 System Time: 1616758841.688220 Trying to open: /dev/rtc0 Using the rtc interface to the clock. Last drift adjustment done at 1616758836 seconds after 1969 Last calibration done at 1616758836 seconds after 1969 Hardware clock is on local time Assuming hardware clock is kept in local time. Waiting for clock tick... ...got clock tick Time read from Hardware Clock: 2021/03/26 06:40:42 Hw clock time : 2021/03/26 06:40:42 = 1616758842 seconds since 1969 Time since last adjustment is 6 seconds Calculated Hardware Clock drift is 0.000000 seconds 2021-03-26 06:40:41.685618-05:00   If the time is off and not accurate to your timezone, you will need to manually set the date and then run the NTP script again.\n# Set as close as possible to the real time pit# timedatectl set-time \u0026#34;2021-03-26 00:00:00\u0026#34; pit# /root/bin/configure-ntp.sh The PIT is now configured to your local timezone.\n  Configure NCN Images to Use Local Timezone You need to adjust the node images so that they also boot in the local timezone. This is accomplished by chrooting into the unsquashed images, making some modifications, and then squashing it back up and moving the new images into place.\n  Set some variables.\nThis example uses IMGTYPE=ceph for the utility storage nodes, but the same process should also be done with IMGTYPE=k8s for the Kubernetes master and worker nodes.\npit# export NEWTZ=America/Chicago pit# export IMGTYPE=ceph pit# export IMGDIR=/var/www/ephemeral/data/${IMGTYPE}   Go to the Ceph image directory and unsquash the image.\npit# cd ${IMGDIR} pit# unsquashfs *.squashfs   Start a chroot session inside the unsquashed image. Your prompt may change to reflect that you are now in the root directory of the image.\npit# chroot ./squashfs-root   Inside the chroot session, you will modify a few files by running the following commands, and then exit from the chroot session.\npit-chroot# echo TZ=${NEWTZ} \u0026gt;\u0026gt; /etc/environment pit-chroot# sed -i \u0026#34;s#^timedatectl set-timezone UTC#timedatectl set-timezone $NEWTZ#\u0026#34; /srv/cray/scripts/metal/set-ntp-config.sh pit-chroot# sed -i \u0026#39;s/--utc/--localtime/\u0026#39; /srv/cray/scripts/metal/set-ntp-config.sh pit-chroot# /srv/cray/scripts/common/create-kis-artifacts.sh pit-chroot# exit pit#   Back outside the chroot session, you will now back up the original images and copy the new ones into place.\npit# mkdir -v ${IMGDIR}/orig pit# mv -v *.kernel *.xz *.squashfs ${IMGDIR}/orig/ pit# cp -v squashfs-root/squashfs/* . pit# chmod -v 644 ${IMGDIR}/initrd.img.xz   Unmount the squashfs mount (which was mounted by the earlier unsquashfs command).\npit# umount -v ${IMGDIR}/squashfs-root/mnt/squashfs   Repeat all of the previous steps, with this change to the IMGTYPE variable.\nThis example uses IMGTYPE=k8s for the Kubernetes master and worker nodes.\npit# export IMGTYPE=k8s pit# export IMGDIR=/var/www/ephemeral/data/${IMGTYPE}   Go to the k8s image directory and unsquash the image.\npit# cd ${IMGDIR} pit# unsquashfs *.squashfs   Start a chroot session inside the unsquashed image. Your prompt may change to reflect that you are now in the root directory of the image.\npit# chroot ./squashfs-root   Inside the chroot session, you will modify a few files by running the following commands, and then exit from the chroot session.\npit-chroot# echo TZ=${NEWTZ} \u0026gt;\u0026gt; /etc/environment pit-chroot# sed -i \u0026#34;s#^timedatectl set-timezone UTC#timedatectl set-timezone $NEWTZ#\u0026#34; /srv/cray/scripts/metal/set-ntp-config.sh pit-chroot# sed -i \u0026#39;s/--utc/--localtime/\u0026#39; /srv/cray/scripts/metal/set-ntp-config.sh pit-chroot# /srv/cray/scripts/common/create-kis-artifacts.sh pit-chroot# exit pit#   Back outside the chroot session, you will now back up the original images and copy the new ones into place.\npit# mkdir -v ${IMGDIR}/orig pit# mv -v *.kernel *.xz *.squashfs ${IMGDIR}/orig/ pit# cp -v squashfs-root/squashfs/* . pit# chmod -v 644 ${IMGDIR}/initrd.img.xz   Unmount the squashfs mount (which was mounted by the earlier unsquashfs command)\npit# umount -v ${IMGDIR}/squashfs-root/mnt/squashfs   Now link the new images so that the NCNs will get them from the LiveCD node when they boot.\npit# set-sqfs-links.sh   Make a note that when performing the csi handoff of NCN boot artifacts in Redeploy PIT Node, you must be sure to specify these new images. Otherwise ncn-m001 will use the default timezone when it boots, and subsequent reboots of the other NCNs will also lose the customized timezone changes.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/node_management/disable_nodes/",
	"title": "Disable Nodes",
	"tags": [],
	"description": "",
	"content": "Disable Nodes Use the Hardware State Manager (HSM) Cray CLI commands to disable nodes on the system.\nDisabling nodes that are not configured correctly allows the system to successfully boot.\nPrerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system.  Procedure   Disable individual nodes with HSM.\nncn-m001# cray hsm state components enabled update --enabled false XNAME   Verify the desired nodes are disabled.\nncn-m001# cray hsm state components describe XNAME Type = \u0026#34;Node\u0026#34; Enabled = false State = \u0026#34;On\u0026#34; NID = 1003 Flag = \u0026#34;OK\u0026#34; Role = \u0026#34;Compute\u0026#34; NetType = \u0026#34;Sling\u0026#34; Arch = \u0026#34;X86\u0026#34; ID = \u0026#34;x5000c1s0b1n1\u0026#34;   After changing the state of nodes, be cautious when powering them on/off. The preferred method for safely powering them on/off is via the Boot Orchestration Service (BOS). The Cray Advanced Platform Monitoring and Control (CAPMC) service is used to directly control the power for nodes, regardless of the state in HSM. CAPMC does not check if a node is disabled in HSM.\n"
},
{
	"uri": "/docs-csm/en-11/operations/node_management/add_a_standard_rack_node/",
	"title": "Add A Standard Rack Node",
	"tags": [],
	"description": "",
	"content": "Add a Standard Rack Node These procedures are intended for trained technicians and support personnel only. Always follow ESD precautions when handling this equipment.\n  An authentication token has been retrieved.\nncn-m001# function get_token () { ADMIN_SECRET=$(kubectl get secrets admin-client-auth -ojsonpath='{.data.client-secret}' | base64 -d) curl -s -d grant_type=client_credentials -d client_id=admin-client -d client_secret=$ADMIN_SECRET https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | python -c 'import sys, json; print json.load(sys.stdin)[\u0026quot;access_token\u0026quot;]' }   The example is this procedure adds a User Access Node (UAN) or compute node to an HPE Cray standard rack system. This example adds a node to rack number 3000 at U27.\nProcedures for updating the Hardware State Manager (HSM) or System Layout Service (SLS) are similar when adding additional compute nodes or User Application Nodes (UANs). The contents of the node object in the SLS are slightly different for each node type.\nRefer to the OEM documentation for information about the node architecture, installation, and cabling.\nFor this procedure, a new object must be created in the SLS and modifications will be required to the Slingshot HSN topology.\nPrerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system.  Procedure   Create a new node object in SLS.\nNew node objects require the following information:\n  Parent: xname of the new node\u0026rsquo;s BMC\n  Xname: xname of the new node\n  Role: Either Compute or Application\n  Aliases: Array of aliases for the node, for compute nodes, this is in the form of nid0000\n  NID: The Node ID integer for the node, applies only to compute nodes.\n  SubRole: Such as UAN, Gateway, or other valid HSM SubRoles\n  If adding a compute node:\nncn-m001# curl -s -k -H \u0026quot;Authorization: Bearer ${TOKEN}\u0026quot; -X POST --data '{ \u0026quot;Parent\u0026quot;: \u0026quot;x3000c0s27b0\u0026quot;, \u0026quot;Xname\u0026quot;: \u0026quot;x3000c0s27b0n0\u0026quot;, \u0026quot;Type\u0026quot;: \u0026quot;comptype_node\u0026quot;, \u0026quot;Class\u0026quot;: \u0026quot;River\u0026quot;, \u0026quot;TypeString\u0026quot;: \u0026quot;Node\u0026quot;, \u0026quot;ExtraProperties\u0026quot;: { \u0026quot;Aliases\u0026quot;: [ \u0026quot;nid000001\u0026quot; ], \u0026quot;NID\u0026quot;: 1, \u0026quot;Role\u0026quot;: \u0026quot;Compute\u0026quot; } }' https://api-gw-service-nmn.local/apis/sls/v1/hardware | jq   If adding a UAN:\nncn-m001# curl -s -k -H \u0026quot;Authorization: Bearer ${TOKEN}\u0026quot; -X POST --data '{ \u0026quot;Parent\u0026quot;: \u0026quot;x3000c0s27b0\u0026quot;, \u0026quot;Xname\u0026quot;: \u0026quot;x3000c0s27b0n0\u0026quot;, \u0026quot;Type\u0026quot;: \u0026quot;comptype_node\u0026quot;, \u0026quot;Class\u0026quot;: \u0026quot;River\u0026quot;, \u0026quot;TypeString\u0026quot;: \u0026quot;Node\u0026quot;, \u0026quot;ExtraProperties\u0026quot;: { \u0026quot;Aliases\u0026quot;: [ \u0026quot;uan04\u0026quot; ], \u0026quot;Role\u0026quot;: \u0026quot;Application\u0026quot;, \u0026quot;SubRole\u0026quot;: \u0026quot;UAN\u0026quot; } }' https://api-gw-service-nmn.local/apis/sls/v1/hardware     Create a new MgmtSwitchConnector object in SLS.\nThe MgmtSwitchConnector connector is used by the hms-discovery job to determine which management switch port the node\u0026rsquo;s BMC is connected to. The SLS requires the following information:\n The management switch port that the new node\u0026rsquo;s BMC is connected to Xname: The xname for the MgmtSwitchConnector in the form of xXcCwWjJ  X is the rack number C is the chassis (standard racks are always chassis 0) W is the rack U position of the management network leaf switch J is the switch port number   NodeNics: The xname of the new node\u0026rsquo;s BMC. This field is an array in the payloads below, but should only contain 1 element.  VendorName: this field varies depending on the OEM for the management switch. For example, if the BMC is plugged into port 36 of the switch the following vendor names could apply: Aruba leaf switches use this format: 1/1/36 Dell leaf switches use this format: ethernet1/1/36    ncn-m001# curl -s -k -H \u0026quot;Authorization: Bearer ${TOKEN}\u0026quot; -X POST --data '{ \u0026quot;Parent\u0026quot;: \u0026quot;x3000c0w14\u0026quot;, \u0026quot;Xname\u0026quot;: \u0026quot;x3000c0w14j36\u0026quot;, \u0026quot;Type\u0026quot;: \u0026quot;comptype_mgmt_switch_connector\u0026quot;, \u0026quot;Class\u0026quot;: \u0026quot;River\u0026quot;, \u0026quot;TypeString\u0026quot;: \u0026quot;MgmtSwitchConnector\u0026quot;, \u0026quot;ExtraProperties\u0026quot;: { \u0026quot;NodeNics\u0026quot;: [ \u0026quot;x3000c0s27b0\u0026quot; ], \u0026quot;VendorName\u0026quot;: \u0026quot;1/1/36\u0026quot; } }' https://api-gw-service-nmn.local/apis/sls/v1/hardware | jq   Install the Node Hardware in the Rack  Install the new node hardware in the rack and connect power cables, HSN cables, and management network cables (if it has not already been installed).\nIf the node was added before modifying the SLS, then the node\u0026rsquo;s BMC should have been able to DHCP with Kea, and there will be an unknown MAC address in HSM Ethernet interfaces table.\nRefer to the OEM documentation for the node for information about the hardware installation and cabling.\n  Power on and Boot Compute Node  Power on the node to boot the BMC.\n  Wait for the hms-discovery cronjob to run, and for DNS to update.\nThe hms-discovery cronjob will attempt to identity Node and BMC MAC addresses from the HSM Ethernet interfaces table with the connection information present in SLS to correctly identity the new node.\n  After roughly 5-10 minutes the node\u0026rsquo;s BMC should be discovered by the HSM, and the node\u0026rsquo;s BMC can be resolved by using its xname in DNS.\nncn-m001# ping x3000c0s27b0   Verify that the nodes are enabled in the HSM.\nncn-m001# cray hsm state components describe x3000c0s27b0n0 Type = \u0026quot;Node\u0026quot; Enabled = **true** State = \u0026quot;Off\u0026quot; . . . To verify the node BMC has been discovered by the HSM.\nncn-m001# cray hsm inventory redfishEndpoints describe x3000c0s27b0 --format json { \u0026quot;ID\u0026quot;: \u0026quot;x3000c0s27b0\u0026quot;, \u0026quot;Type\u0026quot;: \u0026quot;NodeBMC\u0026quot;, \u0026quot;Hostname\u0026quot;: \u0026quot;x3000c0s27b0\u0026quot;, \u0026quot;Domain\u0026quot;: \u0026quot;, \u0026quot;FQDN\u0026quot;: \u0026quot;x3000c0s27b0\u0026quot;, \u0026quot;Enabled\u0026quot;: true, \u0026quot;UUID\u0026quot;: \u0026quot;e005dd6e-debf-0010-e803-b42e99be1a2d\u0026quot;, \u0026quot;User\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;Password\u0026quot;: \u0026quot;, \u0026quot;MACAddr\u0026quot;: \u0026quot;b42e99be1a2d\u0026quot;, \u0026quot;RediscoverOnUpdate\u0026quot;: true, \u0026quot;DiscoveryInfo\u0026quot;: { \u0026quot;LastDiscoveryAttempt\u0026quot;: \u0026quot;2021-01-29T16:15:37.643327Z\u0026quot;, \u0026quot;LastDiscoveryStatus\u0026quot;: \u0026quot;DiscoverOK\u0026quot;, \u0026quot;RedfishVersion\u0026quot;: \u0026quot;1.7.0\u0026quot; } }  When LastDiscoveryStatus displays as DiscoverOK, the node BMC has been successfully discovered. If the last discovery state is DiscoveryStarted then the BMC is currently being inventoried by HSM. If the last discovery state is HTTPsGetFailed or ChildVerificationFailed then an error occurred during the discovery process.    Enable each node individually in the HSM database (in this example, the nodes are x3000c0s27b1n0-n3).\nncn-m001# cray hsm state components enabled update --enabled true x3000c0s27b1n0 ncn-m001# cray hsm state components enabled update --enabled true x3000c0s27b0n1 ncn-m001# cray hsm state components enabled update --enabled true x3000c0s27b0n2 ncn-m001# cray hsm state components enabled update --enabled true x3000c0s27b0n3   To force rediscovery of the components in rack 3000 (standard racks are chassis 0).\nncn-m001# cray hsm inventory discover create --xnames x3000c0   Verify that discovery has completed.\nncn-m001# cray hsm inventory redfishEndpoints describe x3000c0 Type = \u0026quot;ChassisBMC\u0026quot; Domain = \u0026quot; MACAddr = \u0026quot;02:13:88:03:00:00\u0026quot; Enabled = true Hostname = \u0026quot;x3000c0\u0026quot; RediscoverOnUpdate = true FQDN = \u0026quot;x3000c0\u0026quot; User = \u0026quot;root\u0026quot; Password = \u0026quot; IPAddress = \u0026quot;10.104.0.76\u0026quot; ID = \u0026quot;x3000c0b0\u0026quot; [DiscoveryInfo] LastDiscoveryAttempt = \u0026quot;2020-09-03T19:03:47.989621Z\u0026quot; RedfishVersion = \u0026quot;1.2.0\u0026quot; LastDiscoveryStatus = **\u0026quot;DiscoverOK\u0026quot;**   Verify that the correct firmware versions for node BIOS, BMC, HSN NICs, GPUs, and so on.\n  If necessary, update the firmware.\nncn-m001# cray fas actions create CUSTOM_DEVICE_PARAMETERS.json See Update Firmware with FAS.\n  Use the Boot Orchestration Service (BOS) to power on and boot the nodes.\nUse the appropriate BOS template for the node type.\nncn-m001# cray bos v1 session create --template-uuid cle-VERSION \\ --operation reboot --limit x3000c0s27b0n0,x3000c0s27b0n1,x3000c0s27b0n2,x3000c0s27b00n3   Verify the chassis status LEDs indicate normal operation.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/node_management/change_interfaces_in_the_bond/",
	"title": "Change Interfaces In The Bond",
	"tags": [],
	"description": "",
	"content": "Change Interfaces in the Bond Configure the interfaces for bond0 on ncn-w001 and establish an \u0026ldquo;up\u0026rdquo; bond0 on all other NCNs.\nPrerequisites This procedure requires administrative privileges.\nProcedure   Customize the interfaces applied by Ansible for the bond configuration by editing the nics.yml file in the NCN group variables.\nncn-w001# vi /opt/cray/crayctl/files/group_vars/ncn/nics.yml ... # Interface configuration. # Whether to use the bond or not during NCN installs. ansible_hw_bond_enabled: yes # Use Jumboframes if networking is configured as such. ansible_hw_bond_jumboframes: yes ## Default: LACP LAGG on PCI-E NICs; or non-LAGG (single nic). ansible_hw_bond: unix_name: bond0 # 9238 is the Mellanox SN2100 Max per-port. mtu: \u0026#34;{{ ansible_hw_bond_jumboframes | ternary(9238, 1500) }}\u0026#34; members: - eth0 - eth1 - em1 - em2 module_opts: - \u0026#39;mode=802.3ad\u0026#39; # General default LACP --------------\u0026gt; : 802.3ad - \u0026#39;miimon=100\u0026#39; # General default link monitor time -\u0026gt; : 100ms - \u0026#39;lacp_rate=fast\u0026#39; # General default rate --------------\u0026gt; : fast - \u0026#39;xmit_hash_policy=layer2+3\u0026#39; # Enable IP Space --------\u0026gt; : layer2+3   Update the /opt/cray/crayctl/files/group_vars/ncn/platform.yml values for lan1 and lan3.\nncn-w001# vi /opt/cray/crayctl/files/group_vars/ncn/platform.yml The following is a summary of the bond:\n lan1 is the first member of the bond. lan3 is the second member of the bond. lan2 counter-intuitively has nothing to do with the bond. This is the external interface on ncn-w001 and the leaf interface for the other NCNs.    Run the baremetal-interface Ansible play so the changes can be picked up by the installer.\nStage 1 of the install will apply it to ncn-w001 (BIS), and the other NCNs will have it included in their AutoYaST files.\nncn-w001# ansible-playbook /opt/cray/crayctl/ansible_framework/main/baremetal-interface.yml   Restart the handlers for the interfaces so that the network manager reacquires bond members from a clean slate.\nThis is a quick and safe way to clear out many networking bugs in the field.\nncn-w001# systemctl restart wickedd-nanny   Verify the bond is correct.\nncn-w001# ansible ncn -m shell -a \u0026#39;ip a show bond0 \u0026amp;\u0026amp; \\ ip l show bond0 \u0026amp;\u0026amp; wicked ifstatus --verbose bond0\u0026#39;   "
},
{
	"uri": "/docs-csm/en-11/operations/node_management/change_java_security_settings/",
	"title": "Change Java Security Settings",
	"tags": [],
	"description": "",
	"content": "Change Java Security Settings If Java will not allow a connection to an Intel server via SOL or iKVM, change Java security settings to add an exception for the server\u0026rsquo;s BMC IP address.\nThe Intel servers ship with an insecure certificate, which causes an exception for Java when trying to connect via SOL or iKVM to these servers. The workaround is to add the server\u0026rsquo;s BMC IP address to the Exception Site List in the Java Control Panel of the machine attempting to connect to the Intel server.\nTo add an IP address to the Exception Site List:\nJava Control Panel \u0026gt; Security \u0026gt; Edit Site List\nThe following figures show examples of the Security tab of the Java Control Panel on several different operating systems.\nLinux Java Control Panel Security Tab MacOS Java Control Panel Security Tab Windows Java Control Panel Security Tab "
},
{
	"uri": "/docs-csm/en-11/operations/node_management/change_settings_for_hms_collector_polling_of_air_cooled_nodes/",
	"title": "Change Settings For Hms Collector Polling Of Air Cooled Nodes",
	"tags": [],
	"description": "",
	"content": "Change Settings for HMS Collector Polling of Air Cooled Nodes The cray-hms-hmcollector service polls all Air Cooled hardware to gather the necessary telemetry information for use by other services, such as the Cray Advanced Platform Monitoring and Control (CAPMC) service. This polling occurs every 10 seconds on a continual basis. Instabilities with the AMI Redfish implementation in the Gigabyte BMCs requires a less significant approach when gathering power and temperature telemetry data. If the BMCs are overloaded, they can become unresponsive, return incorrect data, or encounter other errors.\nAll of these issues prevent other services, such as CAPMC and the Firmware Action Service (FAS), from successfully acting on the BMCs. Recovery from this state requires a BMC reset and sometimes a hard power cycle by unplugging the server and plugging it back in.\nCollecting telemetry data while trying to boot Air Cooled compute nodes increases the burden on the BMCs and increases the likelihood of BMC issues. The most likely time to encounter BMCs in a bad state is when trying to boot Air Cooled compute nodes and User Access Nodes (UANs) using the Boot Orchestration Service, or when trying to do a firmware and/or BIOS update on the nodes. Check the service logs of CAPMC and FAS for error information returned from the BMCs.\nRecommendations for Polling The following are the best practices for using the HMS Collector polling:\n  Do not query the power state of Air Cooled nodes using CAPMC more than two or three times a minute.\n  This is done via the CAPMC get_xname_status command.\nncn-m001# cray capmc get_xname_status create --xnames LIST_OF_NODES     Polling of Air Cooled nodes should be disabled by default. Before nodes are booted, verify that cray-hms-hmcollector polling is disabled.\n  To check if polling is disabled:\nncn-m001# kubectl get deployments.apps -n services cray-hms-hmcollector -o json | jq \\ \u0026#39;.spec.template.spec.containers[].env[]|select(.name==\u0026#34;POLLING_ENABLED\u0026#34;)\u0026#39;   To disable polling if it is not already:\nncn-m001# kubectl edit deployment -n services cray-hms-hmcollector Change the value for the POLLING_ENABLED environment variable to false in the spec: section. Save and quit the editor for the changes to take effect. The cray-hms-hmcollector pod will automatically restart.\n    Only enable telemetry polling when needed, such as when running jobs.\nncn-m001# kubectl edit deployment -n services cray-hms-hmcollector Change the value for the POLLING_ENABLED environment variable to true in the spec: section. Save and quit the editor for the changes to take effect. The cray-hms-hmcollector pod will automatically restart.\n  If BMCs are encountering issues at a high rate, increase the polling interval. Do not set the polling interval to less than the default of 10s.\nncn-m001# kubectl edit deployment -n services cray-hms-hmcollector Change the value for the POLLING_INTERVAL environment variable to the selected rate in seconds. This value is located in the spec: section. Save and quit the editor for the changes to take effect. The cray-hms-hmcollector pod will automatically restart.\n  Reset BMCs in a Bad State Even with the polling recommendations above, it is still possible for the BMCs to end up in a bad state and will require a reset.\nTo restart the BMCs:\nncn-m001# export USERNAME=root ncn-m001# export IPMI_PASSWORD=changeme ncn-m001# ipmitool -H BMC_HOSTNAME -U $USERNAME -E -I lanplus mc reset cold If the reset does not recover the BMCs, shut down the nodes, unplug the servers and plug them back in. Use the following steps:\n  Shut down the nodes.\nFor each server with a BMC in a bad state:\nncn-m001# ipmitool -H BMC_HOSTNAME -U $USERNAME -E -I lanplus chassis power soft Wait 30 seconds after shutting down the nodes before proceeding.\n  Unplug the server Power Supply Units (PSUs) and wait 30 seconds.\n  Plug both server PSUs back in.\nWait a couple of minutes before proceeding.\n  Verify the BMCs are available again.\nncn-m001# ping -c 1 BMC_HOSTNAME   Check the power of the nodes.\nncn-m001# cray capmc get_xname_status create --xnames LIST_OF_NODES   After these steps, the nodes should be ready to be booted again with the Boot Orchestration Service (BOS).\n"
},
{
	"uri": "/docs-csm/en-11/operations/node_management/check_and_set_the_metalno-wipe_setting_on_ncns/",
	"title": "Check And Set The `metal.no-wipe` Setting On NCNs",
	"tags": [],
	"description": "",
	"content": "Check and Set the metal.no-wipe Setting on NCNs Configure the metal.no-wipe setting on non-compute nodes (NCNs) to preserve data on the nodes before doing an NCN reboot.\nRun the ./ncnGetXnames.shscript to view the metal.no-wipe settings for each NCN. The xname and metal.no-wipe settings are also dumped out when executing the /opt/cray/platform-utils/ncnHealthChecks.sh script.\nPrerequisites This procedure requires administrative privileges.\nProcedure   Change to the /opt/cray/platform-utils directory on any master or worker NCN.\nncn-m001# cd /opt/cray/platform-utils   Run the ./ncnGetXnames.sh script.\nThe output will include a listing of all of the NCNs, their xnames, and what the metal.no-wipe setting is for each.\nncn-m001# ./ncnGetXnames.sh ncn-m001: x3000c0s1b0n0 - metal.no-wipe=1 ncn-m002: x3000c0s2b0n0 - metal.no-wipe=1 ncn-m003: x3000c0s3b0n0 - metal.no-wipe=1 ncn-w001: x3000c0s4b0n0 - metal.no-wipe=1 ncn-w002: x3000c0s5b0n0 - metal.no-wipe=1 ncn-w003: x3000c0s6b0n0 - metal.no-wipe=1 ncn-s001: x3000c0s7b0n0 - metal.no-wipe=1 ncn-s002: x3000c0s8b0n0 - metal.no-wipe=1 ncn-s003: x3000c0s9b0n0 - metal.no-wipe=1 The metal.no-wipe setting must be set to 1 (metal.no-wipe=1) if doing a reboot of an NCN to preserve the current data on it. If it is not set to 1 when the NCN is rebooted, it will be completely wiped and will subsequently have to be rebuilt. If the metal.no-wipe status for one or more NCNs is not returned, re-run the ncnGetXnames.sh script.\n  Reset the metal.no-wipe settings for any NCN where it is set to 0.\nThis step can be skipped if the metal.no-wipe is already set to 1 for any NCNs being rebooted.\n  Generate a token from any master or worker NCN.\nncn-m001# export TOKEN=$(curl -k -s -S -d grant_type=client_credentials -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; \\ | base64 -d` https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token \\ | jq -r \u0026#39;.access_token\u0026#39;)   Update the metal.no-wipe settings.\nncn-m001# /tmp/csi handoff bss-update-param --set metal.no-wipe=1   Run the ./ncnGetXnames.sh script again to verify the no-wipe settings have been reset as expected.\nncn-m001# ./ncnGetXnames.sh ncn-m001: x3000c0s1b0n0 - metal.no-wipe=1 ncn-m002: x3000c0s2b0n0 - metal.no-wipe=1 ncn-m003: x3000c0s3b0n0 - metal.no-wipe=1 ncn-w001: x3000c0s4b0n0 - metal.no-wipe=1 ncn-w002: x3000c0s5b0n0 - metal.no-wipe=1 ncn-w003: x3000c0s6b0n0 - metal.no-wipe=1 ncn-s001: x3000c0s7b0n0 - metal.no-wipe=1 ncn-s002: x3000c0s8b0n0 - metal.no-wipe=1 ncn-s003: x3000c0s9b0n0 - metal.no-wipe=1     "
},
{
	"uri": "/docs-csm/en-11/operations/node_management/access_and_update_the_settings_for_replacement_ncns/",
	"title": "Access And Update Settings For Replacement NCNs",
	"tags": [],
	"description": "",
	"content": "Access and Update Settings for Replacement NCNs When a new NCN is added to the system as a hardware replacement, it might use the default credentials. Contact HPE Cray service to learn what these are.\nUse this procedure to verify that the default BMC credentials are set correctly after a replacement NCN is installed, cabled, and powered on.\nAll NCN BMCs must have credentials set up for ipmitool access.\nPrerequisites A new non-compute node (NCN) has been added to the system as a hardware replacement.\nProcedure   Determine if ipmitool access is configured for root on the BMC.\nncn# export USERNAME=root ncn# export IPMI_PASSWORD=changeme ncn# ipmitool -I lanplus -U $USERNAME -E -H NCN_NODE-mgmt power status Error: Unable to establish IPMI v2 / RMCP+ session   Connect to the BMC with the default login credentials. Contact service for the default credentials.\nncn# export USERNAME=defaultuser ncn# export IPMI_PASSWORD=defaultpassword ncn# ipmitool -I lanplus -U $USERNAME -E -H NCN_NODE-mgmt power status Chassis Power is on Troubleshooting: Follow the steps below if the credentials are not available:\n  Power cycle the replacement NCN.\n  Boot into Linux.\n  Use the factory reset command to regain access to the BMC login credentials.\nncn# ipmitool raw 0x32 0x66     Determine if the root user is configured.\nIn the example below, the root user does not exist yet.\nncn# ipmitool -I lanplus -U $USERNAME -E -H NCN_NODE-mgmt user list 1 ID Name\tCallin Link Auth IPMI Msg Channel Priv Limit 1 false false true ADMINISTRATOR 2 admin false false true ADMINISTRATOR 3 ADMIN false false true ADMINISTRATOR 4 true false false NO ACCESS 5 true false false NO ACCESS 6 true false false NO ACCESS 7 true false false NO ACCESS 8 true false false NO ACCESS 9 true false false NO ACCESS 10 true false false NO ACCESS 11 true false false NO ACCESS 12 true false false NO ACCESS 13 true false false NO ACCESS 14 true false false NO ACCESS 15 true false false NO ACCESS 16 true false false NO ACCESS   Add the new root user.\n  Enable the creation of new credentials.\nncn# ipmitool -I lanplus -U $USERNAME -E -H NCN_NODE-mgmt user enable 4   Set the new username to root.\nncn# ipmitool -I lanplus -U $USERNAME -E -H NCN_NODE-mgmt user set name 4 root   Set the new password.\nncn# ipmitool -I lanplus -U $USERNAME -E -H NCN_NODE-mgmt user set password 4 \u0026lt;BMC root password\u0026gt;   Grant user privileges to the new credentials.\nncn# ipmitool -I lanplus -U $USERNAME -E -H NCN_NODE-mgmt user priv 4 4 1   Enable messaging for the identified slot and set the privilege level for that slot when it is accessed over LAN.\nncn# ipmitool -I lanplus -U $USERNAME -E -H NCN_NODE-mgmt channel setaccess 1 4 callin=on ipmi=on link=on   Enable access to the serial over LAN (SOL) payload.\nncn# ipmitool -I lanplus -U $USERNAME -E -H NCN_NODE-mgmt sol payload enable 1 4     Verify the root credentials have been configured.\nncn# ipmitool -I lanplus -U $USERNAME -E -H NCN_NODE-mgmt user list 1 ID Name\tCallin Link Auth\tIPMI Msg Channel Priv Limit 1 false false true ADMINISTRATOR 2 admin false false true ADMINISTRATOR 3 ADMIN false false true ADMINISTRATOR 4 root true true true ADMINISTRATOR 5 true false false NO ACCESS 6 true false false NO ACCESS 7 true false false NO ACCESS 8 true false false NO ACCESS 9 true false false NO ACCESS 10 true false false NO ACCESS 11 true false false NO ACCESS 12 true false false NO ACCESS 13 true false false NO ACCESS 14 true false false NO ACCESS 15 true false false NO ACCESS 16 true false false NO ACCESS   Confirm the new credentials can be used with ipmitool.\nThe new credentials work if the command succeeds and generates output similar to the example below.\nncn# ipmitool -I lanplus -U root -E -H NCN_NODE-mgmt user list 1 ID Name\tCallin Link Auth\tIPMI Msg Channel Priv Limit 1 false false true ADMINISTRATOR 2 admin false false true ADMINISTRATOR 3 ADMIN false false true ADMINISTRATOR 4 root true true true ADMINISTRATOR 5 true false false NO ACCESS 6 true false false NO ACCESS 7 true false false NO ACCESS 8 true false false NO ACCESS 9 true false false NO ACCESS 10 true false false NO ACCESS 11 true false false NO ACCESS 12 true false false NO ACCESS 13 true false false NO ACCESS 14 true false false NO ACCESS 15 true false false NO ACCESS 16 true false false NO ACCESS   Verify the time is set correctly in the BIOS\nPlease refer to the \u0026ldquo;Ensure the current time is set in BIOS for all management NCNs\u0026rdquo; section in install/deploy_management_nodes.md\n  "
},
{
	"uri": "/docs-csm/en-11/operations/node_management/add_tls_certificates_to_bmcs/",
	"title": "Add TLS Certificates To BMCs",
	"tags": [],
	"description": "",
	"content": "Add TLS Certificates to BMCs Use the System Configuration Service (SCSD) tool to create TLS certificates and store them in Vault secure storage. Once certificates are created, they are placed on to the target BMCs.\nPrerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system.  Limitations TLS certificates can only be set for liquid-cooled BMCs. TLS certificate support for air-cooled BMCs is not supported in release 1.4.\nProcedure   Use SCSD to generate TLS certificates.\n  Create a cert_create.json JSON file containing all cabinet level certificate creation information.\n{ \u0026#34;Domain\u0026#34;: \u0026#34;Cabinet\u0026#34;, \u0026#34;DomainIDs\u0026#34;: [ \u0026#34;x0\u0026#34;, \u0026#34;x1\u0026#34;, \u0026#34;x2\u0026#34;, \u0026#34;x3\u0026#34;] }   Generate the TLS certificates.\nncn-m001# cray scsd bmc createcerts create --format json cert_create.json { \u0026#34;DomainIDs\u0026#34;: [ { \u0026#34;ID\u0026#34;: \u0026#34;x0\u0026#34;, \u0026#34;StatusCode\u0026#34;: 200, \u0026#34;StatusMsg\u0026#34;: \u0026#34;OK\u0026#34; }, { \u0026#34;ID\u0026#34;: \u0026#34;x1\u0026#34;, \u0026#34;StatusCode\u0026#34;: 200, \u0026#34;StatusMsg\u0026#34;: \u0026#34;OK\u0026#34; }, { \u0026#34;ID\u0026#34;: \u0026#34;x2\u0026#34;, \u0026#34;StatusCode\u0026#34;: 200, \u0026#34;StatusMsg\u0026#34;: \u0026#34;OK\u0026#34; }, { \u0026#34;ID\u0026#34;: \u0026#34;x3\u0026#34;, \u0026#34;StatusCode\u0026#34;: 200, \u0026#34;StatusMsg\u0026#34;: \u0026#34;OK\u0026#34; } ] }     Apply the TLS certificates to the target BMCs.\n  Create a new cert_set.json JSON file to specify the endpoints.\n{ \u0026#34;Force\u0026#34;: false, \u0026#34;CertDomain\u0026#34;: \u0026#34;Cabinet\u0026#34;, \u0026#34;Targets\u0026#34;: [ \u0026#34;x0c0s0b0\u0026#34;,\u0026#34;x0c0s1b0\u0026#34;,\u0026#34;x0c0s2b0\u0026#34;, \u0026#34;x0c0s3b0\u0026#34; ] }   Set the certificates on the target BMCs.\nncn-m001# cray scsd bmc setcerts create --format json cert_set.json { \u0026#34;Targets\u0026#34;: [ { \u0026#34;ID\u0026#34;: \u0026#34;x0c0s0b0\u0026#34;, \u0026#34;StatusCode\u0026#34;: 200, \u0026#34;StatusMsg\u0026#34;: \u0026#34;OK\u0026#34; }, { \u0026#34;ID\u0026#34;: \u0026#34;x0c0s1b0\u0026#34;, \u0026#34;StatusCode\u0026#34;: 200, \u0026#34;StatusMsg\u0026#34;: \u0026#34;OK\u0026#34; }, { \u0026#34;ID\u0026#34;: \u0026#34;x0c0s2b0\u0026#34;, \u0026#34;StatusCode\u0026#34;: 200, \u0026#34;StatusMsg\u0026#34;: \u0026#34;OK\u0026#34; }, { \u0026#34;ID\u0026#34;: \u0026#34;x0c0s3b0\u0026#34;, \u0026#34;StatusCode\u0026#34;: 200, \u0026#34;StatusMsg\u0026#34;: \u0026#34;OK\u0026#34; } ] }     Enable the CA_URI variable in all Hardware Management Services (HMS) that use Redfish.\nEach system\u0026rsquo;s customizations.yaml file needs an entry to specify the URI where the Certificate Authority (CA) bundle can be found.\nncn-m001# vi customizations.yaml ... spec: network: ... hms_ca_info: hms_svc_ca_uri: \u0026#34;/usr/local/cray-pki/certificate_authority.crt\u0026#34; ... services: ... cray-hms-reds: # hms_ca_uri: \u0026#34;vault://pki_common/ca_chain\u0026#34; # NOTE: this specifies the use of the Vault PKI directly hms_ca_uri: \u0026#34;{{ hms_ca_info.hms_svc_ca_uri}}\u0026#34; cray-hms-capmc: hms_ca_uri: \u0026#34;{{ hms_ca_info.hms_svc_ca_uri}}\u0026#34; cray-hms-meds: hms_ca_uri: \u0026#34;{{ hms_ca_info.hms_svc_ca_uri}}\u0026#34; cray-hms-hmcollector: hms_ca_uri: \u0026#34;{{ hms_ca_info.hms_svc_ca_uri}}\u0026#34; cray-hms-smd: hms_ca_uri: \u0026#34;{{ hms_ca_info.hms_svc_ca_uri }}\u0026#34; cray-hms-firmware-action: hms_ca_uri: \u0026#34;{{ hms_ca_info.hms_svc_ca_uri}}\u0026#34;   Deploy the services.\n  Edit the manifest.yaml file.\nThe modifications to the system\u0026rsquo;s customizations.yaml shown above will apply when CMS is installed or upgraded as a whole. When upgrading a single service, the manifest.yaml file must contain an override for ca_host_uri.\nncn-m001# vi manifest.yaml ## Example manifest for a single service upgrade --- schema: v2 name: example-manifest version: 1.0.0 failOnFirstError: True repositories: docker: dtr.dev.cray.com helm: helmrepo.dev.cray.com:8080 charts: # Will install/upgrade charts in the order below - name: \u0026#34;cray-hms-smd\u0026#34; namespace: \u0026#34;services\u0026#34; version: 1.4.4-20201104155929+70c870d overrides: - cray-service.imagesHost=\u0026#34;{repos[docker]}\u0026#34; values: hms_ca_uri: \u0026#34;/usr/local/cray-pki/certificate_authority.crt\u0026#34;   Edit the sysman.yaml file to retrieve the entries in the values: section.\nLocate the section for the target service in the sysman.yaml file and copy the information described in this step from the values: section. This content will be copied to the values: section in the manifest.yaml file in the next step.\nncn-m001# manifestgen -i /opt/cray/site-info/manifests/sysmgmt.yaml \\ -c /opt/cray/site-info/customizations.yaml \u0026gt; sysman.yaml ncn-m001# vi sysman.yaml ... - name: cray-hms-scsd namespace: services overrides: - cray-service.imagesHost=\u0026#34;{repos[docker]}\u0026#34; values: hms_ca_uri: /usr/local/cray-pki/certificate_authority.crt **\\#\\#\\#\\# only need to copy this line** ... The Mountain Endpoint Discovery Service (MEDS) and River Endpoint Discovery Service (REDS) have sealed secret information in the values: section that need to be copied as well. For example:\n... - name: cray-hms-reds namespace: services overrides: - cray-service.imagesHost=\u0026#34;{repos[docker]}\u0026#34; - imagesHost=\u0026#34;{repos[docker]}\u0026#34; values: cray-service: #### start copying from here sealedSecrets: - apiVersion: bitnami.com/v1alpha1 kind: SealedSecret metadata: annotations: sealedsecrets.bitnami.com/cluster-wide: \u0026#39;true\u0026#39; creationTimestamp: null name: cray-reds-credentials namespace: services spec: encryptedData: vault_redfish_defaults: AgBdzvLKM468cpWcrXxf8TcveJa4d0OWw1fJCxl138zDDCL1haLl1DY9cETQm73nPwgpKL8v3Tz+2qkpXR+HNomrjf XN+dauJA1lj1xTTKYwRRZdux0NlLuujxr9gjtChkT/CEvCA8gNDjA/O5/2RPaWizL5IGWXBLUhN/02KmNZozpfos3WhCewnhTJiEhGLoJ+ykl9oeMI3cf+W14dpZaU 0Tc5ZAIMfR+vrfTxIlxBClUhsa82Ot8RmtvQNacvGCWuuIRcUfZcCCMQzJCKWi75l0DtRu6VkhX1pnQq/mttGbWJkhveal/VJIEFm3eIOJzn6G1KyyTzU8tjRZHLey UTY61CrdbczDjfQ8v47T8v43G3bGUUsMcB8evNqAOvlG+DTy+WvcPnmLnVItJkQ/30m+xMIzWG0tLf/YIu2fA7u0i56hERVcg2dwC7HZUM7+GZbIsONtKthmna+EiT cewuuc/ftgRvxEGCS5lpTnOYhgYo/C0UNd7EmEOlzt+sWWQAocGKZemHiJVGU4HRSqyMJSk/mDTJlkN24EgfLj0k8VkrPPWFMT+hXi2YjLYrtkC9GtiDZ3tOkKPAxi yh6pR5unjhBv7LtXBbW3uD5xMvv34D0CKOcKWLeMZ97JH84Oroc2iUOP62MYVYfaA/BrPhhOS/TwhJ7SDU6q/a+Pn4I6OslrzYy8haGpiFx6lGhvpSg8F5ez1hYXB9 OmNS1UNdcX4qZpp2npOCKHpN8PeRhnD9cCC1+ObHCflMhjRiHHlQ9PdZi21DoWqvwluDVw92afPpHdVuuSJu8akEDigHUJe3ITnb4jnlQnHDe6TEZ7gyGjZqBMXzxE 7269k8DSUIEy1ofcGIBJBE5K9j+aUdlmQtiNBIh/jbV9x4y2PoA2Zyo7w3Foztn2Kw/jbXfA5b4Z47qWR57tMitv8ZwTk2m5aH+d9BHvnSVsouM/eThT8ptDJLO5gN HulXZoKYt5YMhdEY/I0lN/NIOmRX/HeYxPbjg1+dYhVSRzM= vault_switch_defaults: AgCV7pocyFV/BWZxqi9f3r4gUm7Csotf5e/X9iHo+U3Ctdkl4NW+iX1d8x+sG1UxjgSF7Vcis2y2JSbAgxz68LWBv/o tQrDOu3v6hbxS6mu+M19D6iL5EiMbMkHpWKaG2QtHjPWrw2ZBLb+oVYivJF5N83wb1uHnwnss5SpBZTXVYg8sd5viBwKnpacQrB6dcMilceJ1Ag9gGPacyz0+gMEOP tQZ2I4SFl82LkYdgWJyNqBvz3B8OA3SE7SBX3EKbUYUvdQ8QQptaz9l3gRVIRO8Z0I6HorYeOPzek0m6dDr6fHAAUJNrX2gcBQz/V/QvX1ngOpcpceGNumDwziwZb0 FmUQo8Tm1yrU6bcKWAch/FAv6M/HReE2eekOt41qd/dfWMs5EV5vUOauBfOdhirU1V8azlT+0HbuybWolcpTQV01t6kIUoQgyeLu5xGjV8lYfCov+FBSgYGBaQ2ZVb L2ERWfzHLHIjvZVh0Hm6UaUc1tMqCW1gIW3FIYVMxijYet39qa54L/ARaJz/tl2u0pBwHDiJ2iTR6Lb8YGP4vFrGH7T2I9oLX6uc/K0IRTo3i7fpVBcckrXWbhLMyA 87bCoRxotERjZTIafduGgDMzJ0vlNrUK+7GAX2e8lI1hpmqc1f0CBSn6yVFELRvXX2Zgnm4yqRJb5TO0zGeoVSQCFEnHw6SdtcWmEzCiyTCDvm4r7kxmR35E6XGUqr Qb6ypjQB70HkLFs1KucGHnOgzH3FkKka4ge1c20s8hEPdeSmLMEX8aNxNrAT6t9WznlndxZItZzlwuWrRnGSuC4oE57UBcpKZawHA6bc/nYzskW template: metadata: annotations: sealedsecrets.bitnami.com/cluster-wide: \u0026#39;true\u0026#39; creationTimestamp: null name: cray-reds-credentials namespace: services type: Opaque hms_ca_uri: /usr/local/cray-pki/certificate_authority.crt   Copy the information recorded in the previous step to the values: section in the manifest.yaml file.\nEnsure the proper indenting is preserved when copying information.\nncn-m001# vi manifest.yaml   Run loftsman to perform the upgrade.\nIf the image is not already in place, use docker to put it into place. The following example is for SMD:\nncn-m001# docker pull dtr.dev.cray.com/cray/cray-hms-smd:1.4.4-20201104155929_70c870d ncn-m001# docker tag dtr.dev.cray.com/cray/cray-hms-smd:1.4.4-20201104155929_70c870d registry.local/cray/cray-hms-smd:1.4.4-20201104155929_70c870d ncn-m001# docker push registry.local/cray/cray-hms-smd:1.4.4-20201104155929_70c870d Perform the upgrade:\nncn-m001# loftsman ship --shape --images-registry dtr.dev.cray.com \\ --charts-repo http://helmrepo.dev.cray.com:8080 --loftsman-images-registry dtr.dev.cray.com \\ --manifest-file-path ./manifest.yaml     At any point the TLS certs can be re-generated and replaced on Redfish BMCs. The CA trust bundle can also be modified at any time. When this is to be done, the following steps are needed:\n  Modify the CA trust bundle.\nOnce the CA trust bundle is modified, each service will automatically pick up the new CA bundle data. There is no manual step.\n  Regenerate the TLS cabinet-level certificates as done is the preceding step.\n  Place the TLS certificates onto the Redfish BMCs as in the preceding step.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/network/metallb_bgp/troubleshoot_services_without_an_allocated_ip_address/",
	"title": "Troubleshoot Services Without An Allocated Ip Address",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Services without an Allocated IP Address Check if a given service has an IP address allocated for it if the Kubernetes LoadBalancer services in the NMN, HMN, or CAN address pools are not accessible from outside the cluster.\nRegain access to Kubernetes LoadBalancer services from outside the cluster.\nPrerequisites This procedure requires administrative privileges.\nProcedure   Check the status of the services with the kubectl command to see the External-IP of the service.\nIf \u0026lt;pending\u0026gt; appears in this column, the service is having a problem getting an IP address assigned from MetalLB.\nncn-w001# kubectl get service -A | grep Load ims cray-ims-b9cdea70-223f-4968-a0f4-589518c89a80-service LoadBalancer 10.17.97.66 \u0026lt;pending\u0026gt; 22:32678/TCP 2d9h ims cray-ims-eca49ecd-5434-46b2-9a3c-f4f0467f8ecb-service LoadBalancer 10.18.171.14 \u0026lt;pending\u0026gt; 22:30821/TCP 2d5h istio-system istio-ingressgateway LoadBalancer 10.26.49.253 10.92.100.50 80:30517/TCP,443:30754/TCP 3d5h istio-system istio-ingressgateway-can LoadBalancer 10.28.192.172 \u0026lt;pending\u0026gt; 80:30708/TCP,443:31430/TCP 3d5h istio-system istio-ingressgateway-hmn LoadBalancer 10.17.46.139 10.94.100.1 80:32444/TCP 3d5h   Check that the address pool in the annotation for the service matches one of the address pools in the MetalLB ConfigMap.\nTo view information on the service:\nncn-w001# kubectl -n istio-system describe service istio-ingressgateway-can Name: istio-ingressgateway-can Namespace: istio-system Labels: app=istio-ingressgateway chart=cray-istio heritage=Tiller istio=ingressgateway release=cray-istio Annotations: external-dns.alpha.kubernetes.io/hostname: shasta.SYSTEM_DOMAIN_NAME,auth.SYSTEM_DOMAIN_NAME,s3.SYSTEM_DOMAIN_NAME ** metallb.universe.tf/address-pool: customer-access** Selector: app=istio-ingressgateway,istio=ingressgateway,release=cray-istio Type: LoadBalancer IP: 10.28.192.172 Port: http2 80/TCP TargetPort: 80/TCP NodePort: http2 30708/TCP Endpoints: 10.39.0.5:80 Port: https 443/TCP TargetPort: 443/TCP NodePort: https 31430/TCP Endpoints: 10.39.0.5:443 Session Affinity: None External Traffic Policy: Cluster Events: \u0026lt;none\u0026gt; Run the following command to view the ConfigMap. There is no customer-access address pool in the example below, indicated it has not been added yet. This is why the external IP address value is \u0026lt;pending\u0026gt;.\nncn-w001# kubectl -n metallb-system get cm config -o yaml apiVersion: v1 data: config: | **address-pools:** - name: node-management protocol: layer2 addresses: - 10.92.100.0/24 - name: hardware-management protocol: layer2 addresses: - 10.94.100.0/24 - name: high-speed protocol: layer2 addresses: - 169.0.100.16/28 kind: ConfigMap metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;data\u0026#34;:{\u0026#34;config\u0026#34;:\u0026#34;address-pools:\\n- name: node-management\\n protocol: layer2\\n addresses:\\n - 10.92.100.0/24\\n- name: hardware-management\\n protocol: layer2\\n addresses:\\n - 10.94.100.0/24\\n- name: high-speed\\n protocol: layer2\\n addresses:\\n - 169.0.100.16/28\\n\u0026#34;},\u0026#34;kind\u0026#34;:\u0026#34;ConfigMap\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;config\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;metallb-system\u0026#34;}} creationTimestamp: \u0026#34;2020-01-09T20:33:25Z\u0026#34; name: config namespace: metallb-system resourceVersion: \u0026#34;1645\u0026#34; selfLink: /api/v1/namespaces/metallb-system/configmaps/config uid: 49967541-331f-11ea-9421-b42e993a2608   "
},
{
	"uri": "/docs-csm/en-11/operations/network/metallb_bgp/update_bgp_neighbors/",
	"title": "Update BGP Neighbors",
	"tags": [],
	"description": "",
	"content": "Update BGP Neighbors This page will detail the manual procedure to configure and verify BGP neighbors on the management switches.\nYou will not have BGP peers until CSM install.sh has run. This is where MetalLB is deployed.\n How do I check the status of the BGP neighbors?  Log into the spine switches and run show bgp ipv4 unicast summary for Aruba/HPE switches and show ip bgp summary for Mellanox.   Are my Neighbors stuck in IDLE?  Running clear ip bgp all on the Mellanox and clear bgp * on the Arubas will restart the BGP process. This process may need to be done when a system is reinstalled or when a worker node is rebuilt. If you cannot get the neighbors out of IDLE, make sure that passive neighbors are configured. This is in the automated scripts and shown in the example below. Passive neighbors should only be configured on NCN neighbors.   The BGP neighbors will be the worker NCN IP addresses on the NMN (node management network) (VLAN002). If your system is using HPE/Aruba, one of the neighbors will be the other spine switch.  Generate MetalLB configmap  Depending on the network architecture of your system you may need to peer with switches other than the spines. CSI has a BGP peers argument that accepts \u0026lsquo;aggregation\u0026rsquo; as an option, if no option is defined it will default to the spines as being the MetalLB peers.  CSI CLI arguments with --bgp-peers aggregation\nlinux# export IPMI_PASSWORD=changeme linux# csi config init --bootstrap-ncn-bmc-user root --bootstrap-ncn-bmc-pass $IPMI_PASSWORD --ntp-pool cfntp-4-1.us.cray.com,cfntp-4-2.us.cray.com --can-external-dns 10.103.8.113 --can-gateway 10.103.8.1 --site-ip 172.30.56.2/24 --site-gw 172.30.48.1 --site-dns 172.30.84.40 --site-nic em1 --system-name odin --bgp-peers aggregation Automated Process For Mellanox there is a script mellanox_set_bgp_peers.py and for Aruba there is CANU (Cray Automated Network Utility). In order for these scripts to work the following commands will need to be applied on the switches.\nAruba   In order for the automated process to work, the following command will need to be run on the switches:\nsw-spine-001(config)# https-server rest access-mode read-write   Run CANU.\nCANU requires three parameters: the IP address of switch 1, the IP address of Switch 2, and the path to the directory containing the file sls_input_file.json.\nThe IP addresses in this example should be replaced by the IP addresses of the switches. Make sure the $SLS_PATH variable is set to the correct directory.\npit# SYSTEM_NAME=eniac pit# SLS_PATH=\u0026#34;/var/www/ephemeral/prep/${SYSTEM_NAME}/\u0026#34; pit# canu -s 1.5 config bgp --ips 10.252.0.2,10.252.0.3 --csi-folder \u0026#34;${SLS_PATH}\u0026#34;   Mellanox   In order for the automated process to work, the following commands will need to be run on the switches:\nsw-spine-001 [standalone: master] \u0026gt; enable sw-spine-001 [standalone: master] # configure terminal sw-spine-001 [standalone: master] (config) # json-gw enable   Run the BGP helper script.\nThe BGP helper script requires three parameters: the IP address of switch 1, the IP address of Switch 2, and the path to CSI-generated network files.\n The IP addresses used should be Node Management Network IP addresses (NMN). These IP addresses will be used for the BGP Router-ID. The path to the CSI-generated network files must include CAN.yaml, HMN.yaml, HMNLB.yaml, NMNLB.yaml, and NMN.yaml. The path must include the $SYSTEM_NAME.  The IP addresses in this example should be replaced by the IP addresses of the switches. Make sure the $CSI_PATH variable is set to the correct directory.\npit# SYSTEM_NAME=eniac pit# CSI_PATH=\u0026#34;/var/www/ephemeral/prep/${SYSTEM_NAME}/networks/\u0026#34; pit# /usr/local/bin/mellanox_set_bgp_peers.py 10.252.0.2 10.252.0.3 \u0026#34;${CSI_PATH}\u0026#34;   Verification After following the previous steps, you will need to verify the configuration and verify the BGP peers are ESTABLISHED. If it is early in the install process and the CSM services have not been deployed yet, there will not be speakers to peer with, so the peering sessions may not be ESTABLISHED yet. This is expected and not a problem.\nManual Process On the Aruba switches, the output of the show bgp ipv4 unicast summary command should look like the following if the MetalLB speaker pods are running. If it is early in the install process and the CSM services have not been deployed yet, you may see the neighbors in Idle, Active, or Connect state. This is expected and not a problem.\nsw-spine-001# show bgp ipv4 unicast summary VRF : default BGP Summary ----------- Local AS : 65533 BGP Router Identifier : 10.252.0.1 Peers : 4 Log Neighbor Changes : No Cfg. Hold Time : 180 Cfg. Keep Alive : 60 Neighbor Remote-AS MsgRcvd MsgSent Up/Down Time State AdminStatus 10.252.0.3 65533 31457 31474 00m:02w:04d Established Up 10.252.2.8 65533 54730 62906 00m:02w:04d Established Up 10.252.2.9 65533 54732 62927 00m:02w:04d Established Up 10.252.2.18 65533 54732 62911 00m:02w:04d Established Up On the Mellanox switches, first you must run the switch commands listed in the Automated section above. The output of the show ip bgp summary command should look like the following if the MetalLB speaker pods are running. If it is early in the install process and the CSM services have not been deployed yet, you may see the neighbors in IDLE, ACTIVE, or CONNECT state. This is expected and not a problem.\nsw-spine-001 [standalone: master] # show ip bgp summary VRF name : default BGP router identifier : 10.252.0.1 local AS number : 65533 BGP table version : 308 Main routing table version: 308 IPV4 Prefixes : 261 IPV6 Prefixes : 0 L2VPN EVPN Prefixes : 0 ------------------------------------------------------------------------------------------------------------------ Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd ------------------------------------------------------------------------------------------------------------------ 10.252.0.7 4 65533 37421 42948 308 0 0 12:23:16:07 ESTABLISHED/53 10.252.0.8 4 65533 37421 42920 308 0 0 12:23:16:07 ESTABLISHED/51 10.252.0.9 4 65533 37420 42962 308 0 0 12:23:16:07 ESTABLISHED/51  If the BGP neighbors are not in the ESTABLISHED state make sure the IP addresses are correct for the route-map and BGP configuration and the MetalLB speaker pods are running on all of the workers. If IP addresses are incorrect you will have to update the configuration to match the IP addresses, the configuration below will need to be edited. You can get the NCN IP addresses from the CSI-generated files (NMN.yaml, CAN.yaml, HMN.yaml), these IP addresses are also located in /etc/dnsmasq.d/statics.conf on the PIT node. pit# grep w00 /etc/dnsmasq.d/statics.conf | grep nmn host-record=ncn-w003,ncn-w003.nmn,10.252.1.13 host-record=ncn-w002,ncn-w002.nmn,10.252.1.14 host-record=ncn-w001,ncn-w001.nmn,10.252.1.15  The route-map configuration will require you to get the HMN, and CAN IP addresses as well. Note the Bond0 Mac0/Mac1 entry is for the NMN. pit# grep ncn-w /etc/dnsmasq.d/statics.conf | egrep \u0026#34;Bond0|HMN|CAN\u0026#34; | grep -v mgmt dhcp-host=50:6b:4b:08:d0:4a,10.252.1.13,ncn-w003,20m # Bond0 Mac0/Mac1 dhcp-host=50:6b:4b:08:d0:4a,10.254.1.20,ncn-w003,20m # HMN dhcp-host=50:6b:4b:08:d0:4a,10.102.4.12,ncn-w003,20m # CAN dhcp-host=98:03:9b:0f:39:4a,10.252.1.14,ncn-w002,20m # Bond0 Mac0/Mac1 dhcp-host=98:03:9b:0f:39:4a,10.254.1.22,ncn-w002,20m # HMN dhcp-host=98:03:9b:0f:39:4a,10.102.4.13,ncn-w002,20m # CAN dhcp-host=98:03:9b:bb:a9:94,10.252.1.15,ncn-w001,20m # Bond0 Mac0/Mac1 dhcp-host=98:03:9b:bb:a9:94,10.254.1.24,ncn-w001,20m # HMN dhcp-host=98:03:9b:bb:a9:94,10.102.4.14,ncn-w001,20m # CAN  The Aruba configuration will require you to set the other peering switch as a BGP neighbor, the Mellanox configuration does not require this. You will need to delete the previous route-map, and BGP configuration on both switches.  Aruba delete commands sw-spine-001# configure terminal sw-spine-001(config)# no router bgp 65533 This will delete all BGP configurations on this device. Continue (y/n)? y sw-spine-001(config)# no route-map ncn-w003 sw-spine-001(config)# no route-map ncn-w002 sw-spine-001(config)# no route-map ncn-w001 Aruba configuration example ip prefix-list pl-can seq 10 permit 193.167.208.0/25 ge 24 ip prefix-list pl-hmn seq 20 permit 10.94.100.0/24 ge 24 ip prefix-list pl-nmn seq 30 permit 10.92.100.0/24 ge 24 ip prefix-list tftp seq 10 permit 10.92.100.60/32 ge 32 le 32 route-map ncn-w001 permit seq 10 match ip address prefix-list tftp match ip next-hop 10.252.1.7 set local-preference 1000 route-map ncn-w001 permit seq 20 match ip address prefix-list tftp match ip next-hop 10.252.1.8 set local-preference 1100 route-map ncn-w001 permit seq 30ß match ip address prefix-list tftp match ip next-hop 10.252.1.9 set local-preference 1200 route-map ncn-w001 permit seq 40 match ip address prefix-list pl-can set ip next-hop 10.103.10.10 route-map ncn-w001 permit seq 50 match ip address prefix-list pl-hmn set ip next-hop 10.254.1.14 route-map ncn-w001 permit seq 60 match ip address prefix-list pl-nmn set ip next-hop 10.252.1.9 route-map ncn-w002 permit seq 10 match ip address prefix-list tftp match ip next-hop 10.252.1.7 set local-preference 1000 route-map ncn-w002 permit seq 20 match ip address prefix-list tftp match ip next-hop 10.252.1.8 set local-preference 1100 route-map ncn-w002 permit seq 30 match ip address prefix-list tftp match ip next-hop 10.252.1.9 set local-preference 1200 route-map ncn-w002 permit seq 40 match ip address prefix-list pl-can set ip next-hop 10.103.10.9 route-map ncn-w002 permit seq 50 match ip address prefix-list pl-hmn set ip next-hop 10.254.1.12 route-map ncn-w002 permit seq 60 match ip address prefix-list pl-nmn set ip next-hop 10.252.1.8 route-map ncn-w003 permit seq 10 match ip address prefix-list tftp match ip next-hop 10.252.1.7 set local-preference 1000 route-map ncn-w003 permit seq 20 match ip address prefix-list tftp match ip next-hop 10.252.1.8 set local-preference 1100 route-map ncn-w003 permit seq 30 match ip address prefix-list tftp match ip next-hop 10.252.1.9 set local-preference 1200 route-map ncn-w003 permit seq 40 match ip address prefix-list pl-can set ip next-hop 10.103.10.8 route-map ncn-w003 permit seq 50 match ip address prefix-list pl-hmn set ip next-hop 10.254.1.10 route-map ncn-w003 permit seq 60 match ip address prefix-list pl-nmn set ip next-hop 10.252.1.7 ! router ospfv3 1 area 0.0.0.0 router bgp 65533 distance bgp 20 70 bgp router-id 10.252.0.2 maximum-paths 8 neighbor 10.252.0.3 remote-as 65533 neighbor 10.252.1.7 remote-as 65533 neighbor 10.252.1.7 passive neighbor 10.252.1.8 remote-as 65533 neighbor 10.252.1.8 passive neighbor 10.252.1.9 remote-as 65533 neighbor 10.252.1.9 passive address-family ipv4 unicast neighbor 10.252.0.3 activate neighbor 10.252.1.7 activate neighbor 10.252.1.7 route-map ncn-w003 in neighbor 10.252.1.8 activate neighbor 10.252.1.8 route-map ncn-w002 in neighbor 10.252.1.9 activate neighbor 10.252.1.9 route-map ncn-w001 in exit-address-family Mellanox delete commands sw-spine-001 [standalone: master] # sw-spine-001 [standalone: master] # conf t sw-spine-001 [standalone: master] (config) # no router bgp 65533 sw-spine-001 [standalone: master] (config) # no route-map ncn-w001 sw-spine-001 [standalone: master] (config) # no route-map ncn-w002 sw-spine-001 [standalone: master] (config) # no route-map ncn-w003 Mellanox configuration example ## Route-maps configuration ## route-map rm-ncn-w001 permit 10 match ip address pl-nmn route-map rm-ncn-w001 permit 10 set ip next-hop 10.252.0.7 route-map rm-ncn-w001 permit 20 match ip address pl-hmn route-map rm-ncn-w001 permit 20 set ip next-hop 10.254.0.7 route-map rm-ncn-w001 permit 30 match ip address pl-can route-map rm-ncn-w001 permit 30 set ip next-hop 10.103.8.7 route-map rm-ncn-w002 permit 10 match ip address pl-nmn route-map rm-ncn-w002 permit 10 set ip next-hop 10.252.0.8 route-map rm-ncn-w002 permit 20 match ip address pl-hmn route-map rm-ncn-w002 permit 20 set ip next-hop 10.254.0.8 route-map rm-ncn-w002 permit 30 match ip address pl-can route-map rm-ncn-w002 permit 30 set ip next-hop 10.103.8.8 route-map rm-ncn-w003 permit 10 match ip address pl-nmn route-map rm-ncn-w003 permit 10 set ip next-hop 10.252.0.9 route-map rm-ncn-w003 permit 20 match ip address pl-hmn route-map rm-ncn-w003 permit 20 set ip next-hop 10.254.0.9 route-map rm-ncn-w003 permit 30 match ip address pl-can route-map rm-ncn-w003 permit 30 set ip next-hop 10.103.8.9 ## ## BGP configuration ## protocol bgp router bgp 65533 vrf default router bgp 65533 vrf default router-id 10.252.0.1 force router bgp 65533 vrf default maximum-paths ibgp 32 router bgp 65533 vrf default neighbor 10.252.0.7 remote-as 65533 router bgp 65533 vrf default neighbor 10.252.0.7 route-map ncn-w001 router bgp 65533 vrf default neighbor 10.252.0.7 transport connection-mode passive router bgp 65533 vrf default neighbor 10.252.0.8 remote-as 65533 router bgp 65533 vrf default neighbor 10.252.0.8 route-map ncn-w002 router bgp 65533 vrf default neighbor 10.252.0.8 transport connection-mode passive router bgp 65533 vrf default neighbor 10.252.0.9 remote-as 65533 router bgp 65533 vrf default neighbor 10.252.0.9 route-map ncn-w003 router bgp 65533 vrf default neighbor 10.252.0.9 transport connection-mode passive  Once the IP addresses are updated for the route-maps and BGP neighbors you may need to restart the BGP process on the switches, you do this by running clear ip bgp all on the Mellanox and clear bgp * on the Arubas. (This may need to be done multiple times for all the peers to come up) When worker nodes are reinstalled, the BGP process will need to be restarted. If the BGP peers are still not coming up you should check the metallb.yaml config file for errors. The MetalLB config file should point to the NMN IP addresses of the switches configured.  metallb.yaml configuration example The peer-address should be the IP address of the switch that you are doing BGP peering with.\n--- apiVersion: v1 kind: ConfigMap metadata: namespace: metallb-system name: config data: config: |peers: - peer-address: 10.252.0.2 peer-asn: 65533 my-asn: 65533 - peer-address: 10.252.0.3 peer-asn: 65533 my-asn: 65533 address-pools: - name: customer-access protocol: bgp addresses: - 10.102.9.112/28 - name: customer-access-dynamic protocol: bgp addresses: - 10.102.9.128/25 - name: hardware-management protocol: bgp addresses: - 10.94.100.0/24 - name: node-management protocol: bgp addresses: - 10.92.100.0/24 "
},
{
	"uri": "/docs-csm/en-11/operations/network/metallb_bgp/check_bgp_status_and_reset_sessions/",
	"title": "Check BGP Status And Reset Sessions",
	"tags": [],
	"description": "",
	"content": "Check BGP Status and Reset Sessions Check the Border Gateway Protocol (BGP) status on the Aruba and Mellanox switches and verify that all sessions are in an Established state. If the state of any session in the table is Idle, the BGP sessions needs to be reset.\nPrerequisites This procedure requires administrative privileges.\nProcedure MELLANOX   Verify that all BGP sessions are in an Established state for the Mellanox spine switches.\nSSH to each spine switch to check the status of all BGP sessions.\n  SSH to a spine switch.\nFor example:\nncn-m001# ssh admin@sw-spine-001.mtl   View the status of the BGP sessions.\nsw-spine-001 [standalone: master] \u0026gt; enable sw-spine-001 [standalone: master] # show ip bgp summary VRF name : default BGP router identifier : 10.252.0.2 local AS number : 65533 BGP table version : 50 Main routing table version: 50 IPV4 Prefixes : 68 IPV6 Prefixes : 0 L2VPN EVPN Prefixes : 0 ------------------------------------------------------------------------------------------------------------------ Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd ------------------------------------------------------------------------------------------------------------------ 10.252.1.10 4 65533 3144 3564 50 0 0 1:01:50:41 ESTABLISHED/13 10.252.1.11 4 65533 3144 3569 50 0 0 1:01:50:40 ESTABLISHED/14 10.252.1.12 4 65533 3145 3576 50 0 0 1:01:50:41 ESTABLISHED/14 10.252.1.13 4 65533 3144 3568 50 0 0 1:01:50:41 ESTABLISHED/13 10.252.1.14 4 65533 3145 3572 50 0 0 1:01:50:41 ESTABLISHED/14 If any of the sessions are in an Idle state, proceed to the next step.\n    Reset BGP to re-establish the sessions.\n\n  SSH to each spine switch.\nFor example:\nncn-m001# ssh admin@sw-spine-001.mtl   Verify BGP is enabled.\nsw-spine-001 [standalone: master] \u0026gt; show protocols | include bgp bgp: enabled   Clear the BGP sessions.\nsw-spine-001 [standalone: master] \u0026gt; enable sw-spine-001 [standalone: master] # clear ip bgp all   Check the status of the BGP sessions to see if they are now Established.\nIt may take a few minutes for sessions to become Established.\nsw-spine-001 [standalone: master] \u0026gt; enable sw-spine-001 [standalone: master] # show ip bgp summary VRF name : default BGP router identifier : 10.252.0.2 local AS number : 65533 BGP table version : 50 Main routing table version: 50 IPV4 Prefixes : 68 IPV6 Prefixes : 0 L2VPN EVPN Prefixes : 0 ------------------------------------------------------------------------------------------------------------------ Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd ------------------------------------------------------------------------------------------------------------------ 10.252.1.10 4 65533 3144 3564 50 0 0 1:01:50:41 ESTABLISHED/13 10.252.1.11 4 65533 3144 3569 50 0 0 1:01:50:40 ESTABLISHED/14 10.252.1.12 4 65533 3145 3576 50 0 0 1:01:50:41 ESTABLISHED/14 10.252.1.13 4 65533 3144 3568 50 0 0 1:01:50:41 ESTABLISHED/13 10.252.1.14 4 65533 3145 3572 50 0 0 1:01:50:41 ESTABLISHED/14   Once all sessions are in an Established state, BGP reset is complete for the Mellanox switches.\nTroubleshooting: If some sessions remain Idle, re-run the Mellanox reset steps to clear and re-check status. If some sessions still remain Idle, proceed to reapply the cray-metallb helm chart, along with the BGP reset, to force the speaker pods to re-establish sessions with the switch.\n  Aruba   Verify that all BGP sessions are in an Established state for the Aruba spine switches.\nSSH to each spine switch to check the status of all BGP sessions.\n  SSH to a spine switch.\nncn-m001# ssh admin@sw-spine-001.mtl   View the status of the BGP sessions.\nsw-spine-001# show bgp ipv4 unicast summary VRF : default BGP Summary ----------- Local AS : 65533 BGP Router Identifier : 10.252.0.2 Peers : 4 Log Neighbor Changes : No Cfg. Hold Time : 180 Cfg. Keep Alive : 60 Confederation Id : 0 Neighbor Remote-AS MsgRcvd MsgSent Up/Down Time State AdminStatus 10.252.0.3 65533 19704 19708 00m:01w:00d Established Up 10.252.1.10 65533 34455 39416 00m:01w:04d Established Up 10.252.1.11 65533 34458 39400 00m:01w:04d Established Up 10.252.1.12 65533 34448 39415 00m:01w:04d Established Up If any of the sessions are in an Idle state, proceed to the next step.\n    Reset BGP to re-establish the sessions.\n\n  SSH to each spine switch.\nFor example:\nncn-m001# ssh admin@sw-spine-001.mtl   Clear the BGP sessions.\nsw-spine-001# clear bgp *   Check the status of the BGP sessions.\nIt may take a few minutes for sessions to become Established.\nsw-spine-001# show bgp ipv4 unicast summary VRF : default BGP Summary ----------- Local AS : 65533 BGP Router Identifier : 10.252.0.2 Peers : 4 Log Neighbor Changes : No Cfg. Hold Time : 180 Cfg. Keep Alive : 60 Confederation Id : 0 Neighbor Remote-AS MsgRcvd MsgSent Up/Down Time State AdminStatus 10.252.0.3 65533 19704 19708 00m:01w:00d Established Up 10.252.1.10 65533 34455 39416 00m:01w:04d Established Up 10.252.1.11 65533 34458 39400 00m:01w:04d Established Up 10.252.1.12 65533 34448 39415 00m:01w:04d Established Up   Once all sessions are in an Established state, BGP reset is complete for the Aruba switches.\nTroubleshooting: If some sessions remain Idle, re-run the Aruba reset steps to clear and re-check status. If some sessions still remain Idle, proceed to the next step to reapply the cray-metallb helm chart, along with the BGP reset to force the speaker pods to re-establish sessions with the switch.\n  Re-apply the cray-metallb Helm Chart   Determine the cray-metallb chart version that is currently deployed.\nncn-m001# helm ls -A -a | grep cray-metallb cray-metallb metallb-system 1 2021-02-10 14:58:43.902752441 -0600 CST deployed cray-metallb-0.12.2 0.8.1   Create a manifest file that will be used to reapply the same chart version.\nncn-m001# cat \u0026lt;\u0026lt; EOF \u0026gt; ./metallb-manifest.yaml apiVersion: manifests/v1beta1 metadata: name: reapply-metallb spec: charts: - name: cray-metallb namespace: metallb-system values: imagesHost: dtr.dev.cray.com version: 0.12.2 EOF   Open SSH sessions to all spine switches.\n  Determine the CSM_RELEASE version that is currently running and set an environment variable.\nFor example:\nncn-m001# CSM_RELEASE=0.8.0   Mount the PITDATA so that helm charts are available for the re-install (it might already be mounted) and verify that the chart with the expected version exists.\nncn-m001# mkdir -pv /mnt/pitdata ncn-m001# mount -L PITDATA /mnt/pitdata ncn-m001# ls /mnt/pitdata/csm-${CSM_RELEASE}/helm/cray-metallb* /mnt/pitdata/csm-0.8.0/helm/cray-metallb-0.12.2.tgz   Uninstall the current cray-metallb chart.\nUntil the chart is reapplied, this will also effect unbound name resolution, and all BGP sessions will be Idle for all of the worker nodes.\nncn-m001# helm del cray-metallb -n metallb-system   Use the open SSH sessions to the switches to clear the BGP sessions based on the above Mellanox or Aruba procedures.\nRefer to substeps 1-3 for Mellanox.\nRefer to substeps 1-2 for Aruba.\n  Reapply the cray-metallb chart based on the CSM_RELEASE.\nncn-m001# loftsman ship --manifest-path ./metallb-manifest.yaml \\ --charts-path /mnt/pitdata/csm-${CSM_RELEASE}/helm   Check that the speaker pods are all running.\nThis may take a few minutes.\nncn-m001# kubectl get pods -n metallb-system NAME READY STATUS RESTARTS AGE cray-metallb-controller-6d545b5ccc-mm4qz 1/1 Running 0 79m cray-metallb-speaker-4nrzq 1/1 Running 0 76m cray-metallb-speaker-b5m2n 1/1 Running 0 79m cray-metallb-speaker-h7s7b 1/1 Running 0 79m   Use the open SSH sessions to the switches to check the status of the BGP sessions.\nRefer to substeps 1-3 for Mellanox.\nRefer to substeps 1-2 for Aruba.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/network/metallb_bgp/metallb_in_bgp-mode/",
	"title": "Metallb In BGP-mode",
	"tags": [],
	"description": "",
	"content": "MetalLB in BGP-Mode MetalLB is a component in Kubernetes that manages access to LoadBalancer services from outside the Kubernetes cluster. There are LoadBalancer services on the Node Management Network (NMN), Hardware Management Network (HMN), and Customer Access Network (CAN).\nMetalLB can run in either Layer2-mode or BGP-mode for each address pool it manages. BGP-mode is used for the NMN, HMN, and CAN. This enables true load balancing (Layer2-mode does failover, not load balancing) and allows for a more robust layer 3 configuration for these networks.\nIn BGP-mode, the MetalLB speakers will peer with the BGP router on the spine switches and advertise the service LoadBalancer IP addresses. The BGP routers will accept those advertised prefixes and add them to the route table. The spines are configured with Equal-Cost Multi-Path (ECMP), meaning that each of these BGP route prefixes will load balance to any of the workers that has advertised the prefix. This process allows clients outside the cluster with access to the NMN, HMN, or CAN to be able to route to these Kubernetes services.\nBGP peering is only between the MetalLB speakers and the spine switches. It does not do any peering beyond that.\nThe routes in the BGP route table will only be the IP addresses of the Kubernetes LoadBalancer services. This is the fifth column displayed in the output of the following command:\nncn-w001# kubectl get service -A | grep LoadBalancer For example:\nNAMESPACE NAME TYPE CLUSTER-IP **EXTERNAL-IP** PORT(S) AGE ceph-rgw cray-s3 LoadBalancer 10.31.54.80 10.102.10.129 8080:31003/TCP 36d ims cray-ims-40f523ac-9b99-4f76-bb37-df6eb62540c8-service LoadBalancer 10.21.156.88 10.102.10.134 22:31604/TCP 35d ims cray-ims-50287398-b877-4a2b-bf18-c3618583c66f-service LoadBalancer 10.29.254.221 10.102.10.167 22:30314/TCP 12d ims cray-ims-577dec6e-dbac-4363-a423-bf39ed9b9e32-service LoadBalancer 10.22.200.115 10.102.10.158 22:32672/TCP 15d ims cray-ims-5b05e86e-f65b-4a5f-b5eb-2c31f0458722-service LoadBalancer 10.25.162.244 10.102.10.160 22:32707/TCP 14d ims cray-ims-7ffaf10f-75ca-4ccb-b10d-1c7cd31b3d4b-service LoadBalancer 10.20.16.190 10.102.10.132 22:31934/TCP 35d ims cray-ims-b1cd0827-bb51-4bcd-ac25-f64d5f7d0c44-service LoadBalancer 10.26.69.180 10.102.10.131 22:31281/TCP 35d ims cray-ims-bd0698b4-a104-48eb-9714-5b5889ad7b52-service LoadBalancer 10.18.114.136 10.102.10.135 22:31701/TCP 35d MetalLB does not manage access to any of the NCNs, UANs, or compute nodes.\n"
},
{
	"uri": "/docs-csm/en-11/operations/network/metallb_bgp/metallb_in_bgp-mode_configuration/",
	"title": "Metallb In BGP-mode Configuration",
	"tags": [],
	"description": "",
	"content": "MetalLB in BGP-Mode Configuration MetalLB in BGP-mode provides a more robust configuration for the Node Management Network (NMN), Hardware Management Network (HMN), and Customer Access Network (CAN). This configuration is generated from the csi config init input values. BGP-mode is enabled by updating the protocols in these configuration files.\nMetalLB Peer Configuration The content for metallb_bgp_peers is generated by the csi config init command. In addition to the MetalLB configuration, there is configuration needed on the spine switches to set up the BGP router on these switches.\nThe MetalLB ConfigMap can also be viewed:\npeers: - peer-address: 10.252.0.1 peer-asn: 65533 my-asn: 65533 - peer-address: 10.252.0.3 peer-asn: 65533 my-asn: 65533 To retrieve data about BGP peers:\nncn-m001# kubectl get cm config -n metallb-system The speakers get their peering configuration from the MetalLB ConfigMap. This configuration specifies the IP address of the spine or aggregate switch, as well as the Autonomous System Number (ASN) for the speaker and the switch with which it is peering. This ASN should be the same for both.\nCAN Configuration This network has two MetalLB address pools, one for static IP allocation and the other for dynamic IP allocation. Static allocation guarantees the same IP allocation for services using this pool across deployment and installations. Dynamic allocations means that the allocated IP addresses will be in this pool, but may change depending on the timing and ordering of the IP allocation.\nView the address pool configurations in the MetalLB ConfigMap after localization occurs. The MetalLB ConfigMap should not be edited directly. The following is an example of the values in the ConfigMap:\n- name: can-dynamic-pool **protocol: bgp** addresses: **- 10.102.5.128/25** - name: can-static-pool **protocol: bgp** addresses: **- 10.102.5.112/28** The CAN configuration is set in the csi config init input:\nlinux# csi config init . . . --can-cidr 10.102.5.0/26 --can-gateway 10.102.5.27 --can-static-pool 10.102.5.28/30 --can-dynamic-pool 10.102.5.32/27 . . . "
},
{
	"uri": "/docs-csm/en-11/operations/network/metallb_bgp/troubleshoot_bgp_not_accepting_routes_from_metallb/",
	"title": "Troubleshoot BGP Not Accepting Routes From Metallb",
	"tags": [],
	"description": "",
	"content": "Troubleshoot BGP not Accepting Routes from MetalLB Check the number of routes that the Border Gateway Protocol (BGP) Router is accepting in the peering session. This procedure is useful if Kubernetes LoadBalancer services in the NMN, HMN, or CAN address pools are not accessible from outside the cluster.\nRegain access to Kubernetes LoadBalancer services from outside the cluster.\nPrerequisites This procedure requires administrative privileges.\nProcedure   Log into the spine or aggregate switch.\nIn this example, the Aruba or Mellanox spine or aggregate switch is accessed from ncn-m001. In this case, sw-spine-001.mtl is being accessed:\nncn-m001# ssh admin@sw-spine-001.mtl   Check the number of routes that the BGP Router is accepting in the peering session.\n  Mellanox:\nLook at the number under the State/Pfx column in the output. There should be a number that matches the number of unique LoadBalancer IP addresses configured in the cluster.\nsw-spine-001# show ip bgp summary VRF name : vrf-default BGP router identifier : 10.252.0.1 local AS number : 65533 BGP table version : 45 Main routing table version: 45 IPV4 Prefixes : 51 IPV6 Prefixes : 0 L2VPN EVPN Prefixes : 0 ------------------------------------------------------------------------------------------------------------------ Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd ------------------------------------------------------------------------------------------------------------------ 10.252.0.4 4 65533 2687 3072 45 0 0 0:22:14:03 ESTABLISHED/17 10.252.0.5 4 65533 2687 3070 45 0 0 0:22:14:03 ESTABLISHED/17 10.252.0.6 4 65533 2687 3067 45 0 0 0:22:14:03 ESTABLISHED/17 If there is a number smaller than expected, check the routes that have been accepted with the following command:\nsw-spine-001# show ip route bgp Flags: F: Failed to install in H/W B: BFD protected (static route) i: BFD session initializing (static route) x: protecting BFD session failed (static route) c: consistent hashing p: partial programming in H/W VRF Name default: ------------------------------------------------------------------------------------------------------ Destination Mask Flag Gateway Interface Source AD/M ------------------------------------------------------------------------------------------------------ 10.92.100.0 255.255.255.255 c 10.252.0.4 vlan2 bgp 200/0 c 10.252.0.5 vlan2 bgp 200/0 c 10.252.0.6 vlan2 bgp 200/0 10.92.100.1 255.255.255.255 c 10.252.0.4 vlan2 bgp 200/0 c 10.252.0.5 vlan2 bgp 200/0 c 10.252.0.6 vlan2 bgp 200/0 10.92.100.60 255.255.255.255 c 10.252.0.4 vlan2 bgp 200/0 c 10.252.0.5 vlan2 bgp 200/0 c 10.252.0.6 vlan2 bgp 200/0 10.92.100.71 255.255.255.255 c 10.252.0.4 vlan2 bgp 200/0 c 10.252.0.5 vlan2 bgp 200/0 c 10.252.0.6 vlan2 bgp 200/0 10.92.100.72 255.255.255.255 c 10.252.0.4 vlan2 bgp 200/0 c 10.252.0.5 vlan2 bgp 200/0 c 10.252.0.6 vlan2 bgp 200/0 10.92.100.75 255.255.255.255 c 10.252.0.4 vlan2 bgp 200/0 c 10.252.0.5 vlan2 bgp 200/0 c 10.252.0.6 vlan2 bgp 200/0 10.92.100.76 255.255.255.255 c 10.252.0.4 vlan2 bgp 200/0 c 10.252.0.5 vlan2 bgp 200/0 c 10.252.0.6 vlan2 bgp 200/0 10.94.100.0 255.255.255.255 c 10.254.0.4 vlan4 bgp 200/0 c 10.254.0.5 vlan4 bgp 200/0 c 10.254.0.6 vlan4 bgp 200/0 10.94.100.1 255.255.255.255 c 10.254.0.4 vlan4 bgp 200/0 c 10.254.0.5 vlan4 bgp 200/0 c 10.254.0.6 vlan4 bgp 200/0 10.94.100.2 255.255.255.255 c 10.254.0.4 vlan4 bgp 200/0 c 10.254.0.5 vlan4 bgp 200/0 c 10.254.0.6 vlan4 bgp 200/0 10.94.100.3 255.255.255.255 c 10.254.0.4 vlan4 bgp 200/0 c 10.254.0.5 vlan4 bgp 200/0 c 10.254.0.6 vlan4 bgp 200/0 10.102.3.112 255.255.255.255 c 10.102.3.4 vlan7 bgp 200/0 c 10.102.3.5 vlan7 bgp 200/0 c 10.102.3.6 vlan7 bgp 200/0 10.102.3.113 255.255.255.255 c 10.102.3.4 vlan7 bgp 200/0 c 10.102.3.5 vlan7 bgp 200/0 c 10.102.3.6 vlan7 bgp 200/0 10.102.3.128 255.255.255.255 c 10.102.3.4 vlan7 bgp 200/0 c 10.102.3.5 vlan7 bgp 200/0 c 10.102.3.6 vlan7 bgp 200/0 10.102.3.129 255.255.255.255 c 10.102.3.4 vlan7 bgp 200/0 c 10.102.3.5 vlan7 bgp 200/0 c 10.102.3.6 vlan7 bgp 200/0 10.102.3.130 255.255.255.255 c 10.102.3.4 vlan7 bgp 200/0 c 10.102.3.5 vlan7 bgp 200/0 c 10.102.3.6 vlan7 bgp 200/0 10.102.3.131 255.255.255.255 c 10.102.3.4 vlan7 bgp 200/0 c 10.102.3.5 vlan7 bgp 200/0 c 10.102.3.6 vlan7 bgp 200/0 If the expected routes are not present, check the route-map or prefix-list configuration on the spine switch.\n  Aruba:\nTo check the status for Aruba:\nsw-spine-001# show bgp ipv4 unicast summary VRF : default BGP Summary ----------- Local AS : 65533 BGP Router Identifier : 10.252.0.2 Peers : 4 Log Neighbor Changes : No Cfg. Hold Time : 180 Cfg. Keep Alive : 60 Confederation Id : 0 Neighbor Remote-AS MsgRcvd MsgSent Up/Down Time State AdminStatus 10.252.0.3 65533 1041 1037 15h:00m:52s Established Up 10.252.1.7 65533 1752 2003 14h:29m:26s Established Up 10.252.1.8 65533 1752 2002 14h:29m:21s Established Up 10.252.1.9 65533 1751 2005 14h:28m:43s Established Up To check the routes for Aruba:\nsw-spine-001# show ip route bgp 10.92.100.71/32, vrf default via 10.252.1.7, [200/0], bgp via 10.252.1.8, [200/0], bgp via 10.252.1.9, [200/0], bgp 10.92.100.81/32, vrf default via 10.252.1.7, [200/0], bgp via 10.252.1.8, [200/0], bgp via 10.252.1.9, [200/0], bgp 10.92.100.222/32, vrf default via 10.252.1.7, [200/0], bgp via 10.252.1.8, [200/0], bgp via 10.252.1.9, [200/0], bgp 10.92.100.225/32, vrf default via 10.252.1.7, [200/0], bgp via 10.252.1.8, [200/0], bgp via 10.252.1.9, [200/0], bgp 10.94.100.0/32, vrf default via 10.254.1.10, [200/0], bgp via 10.254.1.12, [200/0], bgp via 10.254.1.14, [200/0], bgp 10.94.100.71/32, vrf default via 10.254.1.10, [200/0], bgp via 10.254.1.12, [200/0], bgp -- MORE --, next page: Space, next line: Enter, quit: q There should be a route for each unique LoadBalancer IP addresses configured in the cluster.\n    "
},
{
	"uri": "/docs-csm/en-11/operations/network/management_network/update_management_network_firmware/",
	"title": "Update Management Network Firmware",
	"tags": [],
	"description": "",
	"content": "Update Management Network Firmware This page describes how to update firmware on the management network switches.\nRequirements Access to the switches from the LiveCD/ncn-m001.\nConfiguration All firmware will be located at /var/www/fw/network on the LiveCD. It should contain the following files:\nncn-m001-pit:/var/www/network/firmware # ls -lh total 2.7G -rw-rw-r--+ 1 root root 658353828 Jan 7 2021 ArubaOS-CX_6400-6300_10_06_0010.stable.swi -rw-rw-r--+ 1 root root 384156519 May 3 22:18 ArubaOS-CX_8320_10_06_0110.stable.swi -rw-rw-r--+ 1 root root 444610797 Jan 7 2021 ArubaOS-CX_8325_10_06_0010.stable.swi -rw-rw-r--+ 1 root root 431371873 Apr 30 20:55 ArubaOS-CX_8360_10_06_0110.stable.swi -rw-rw-r--+ 1 root root 763636433 Aug 26 2020 onyx-X86_64-3.9.1014.stable.img -rw-rw-r--+ 1 root root 604119040 Oct 28 2020 OS10_Enterprise_10.5.1.4.stable.tar Switch Firmware    Vendor Model Version     Aruba 6300 ArubaOS-CX_6400-6300_10.06.0010   Aruba 8320 ArubaOS-CX_8320_10.06.0010 or ArubaOS-CX_8320_10.06.0110   Aruba 8325 ArubaOS-CX_8325_10.06.0010   Aruba 8360 ArubaOS-CX_8360_10.06.0010 or ArubaOS-CX_8360_10.06.0110   Dell S3048-ON 10.5.1.4   Dell S4148F-ON 10.5.1.4   Dell S4148T-ON 10.5.1.4   Mellanox MSN2100 3.9.1014   Mellanox MSN2700 3.9.1014    Aruba Firmware Best Practices Aruba software version number explained:\nFor example: 10.06.0120\n  10\t= OS\n  06\t= Major branch (new features)\n  0120\t= CPE release (bug fixes)\n  It is considered to be a best practice to keep all Aruba CX platform devices running the same software version.\nAruba CX devices two software image banks, which means sw images can be pre-staged to the device without booting to the new image.\nIf upgrading to a new major branch, in Aruba identified by the second integer in the software image number.\nWhen upgrading past a major software release, for example, from 10.6 to 10.8 (and skipping 10.7), issue the allow-unsafe-upgrades command to allow any low level firmware/driver upgrades to complete. If going from the 10.6 branch to 10.7 branch, this step can be skipped as the low level firmware/driver upgrade would be automatically completed.\nsw-leaf-001# config sw-leaf-001(config)# allow-unsafe-updates 30 This command will enable non-failsafe updates of programmable devices for the next 30 minutes. First, wait for all line and fabric modules to reach the ready state, and then reboot the switch to begin applying any needed updates. Ensure that the switch will not lose power, be rebooted again, or have any modules removed until all updates have finished and all line and fabric modules have returned to the ready state.\nWARNING: Interrupting these updates may make the product unusable!\nContinue (y/n)? y Unsafe updates : allowed (less than 30 minute(s) remaining) VSX software upgrade command can automatically upgrade both of the peers in VSX topology by staging upgrade and automatically doing traffic shifting between peers to minimize impact to network. The following examples include the option for standalone and vsx-pair upgrade.\nAruba Firmware Update - Standalone SSH into the switch being upgraded.\nExample: the IP address 10.252.1.12 used is the liveCD.\nsw-leaf-001# copy sftp://root@10.252.1.12//var/www/ephemeral/data/network_images/ArubaOS-CX_6400-6300_10_06_0010.stable.swi primary sw-leaf-001# write mem Copying configuration: [Success] Once the upload is complete, check the images:\nsw-leaf-001# show image --------------------------------------------------------------------------- ArubaOS-CX Primary Image --------------------------------------------------------------------------- Version : FL.10.06.0010 Size : 643 MB Date : 2020-12-14 10:06:34 PST SHA-256 : 78dc27c5e521e92560a182ca44dc04b60d222b9609129c93c1e329940e1e11f9 After the firmware is uploaded, boot the switch to the correct image.\nsw-leaf-001# boot system primary Once the reboot is complete, check and make sure the firmware version is correct.\nsw-leaf-001# show version ----------------------------------------------------------------------------- ArubaOS-CX (c) Copyright 2017-2020 Hewlett Packard Enterprise Development LP ----------------------------------------------------------------------------- Version : FL.10.06.0010 Build Date : 2020-09-29 07:44:16 PDT Build ID : ArubaOS-CX:FL.10.06.0010:3cbfcce60961:202009291304 Build SHA : 3cbfcce609617b0cf84a6b941a2b36c43dfeb2cb Active Image : primary Service OS Version : FL.01.07.0002 BIOS Version : FL.01.0002 Aruba Firmware Update - VSX Software Upgrade   SSH into the Primary VSX member of the VSX-pair to upgrade.\nExample: the IP 10.252.1.12 used is the LiveCD.\n  Upload the firmware.\nsw-leaf-001# copy sftp://root@10.252.1.12//var/www/ephemeral/data/network_images/ArubaOS-CX_6400-6300_10_06_0120.stable.swi primary sw-leaf-001# write mem Copying configuration: [Success]   Once the upload is complete, check the images.\nsw-leaf-001# show image --------------------------------------------------------------------------- ArubaOS-CX Primary Image --------------------------------------------------------------------------- Version : FL.10.06.0120 Size : 643 MB Date : 2021-03-14 10:06:34 PST SHA-256 : 78dc27c5e521e92560a182ca44dc04b60d222b9609129c93c1e329940e1e11f9   After the firmware is uploaded, boot the switch to the correct image.\n  Example: the IP address 10.252.1.12 used is the liveCD.\nsw-leaf-001# copy sftp://root@10.252.1.12//var/www/ephemeral/data/network_images/ArubaOS-CX_6400-6300_10_06_0120.stable.swi primary sw-leaf-001# write mem Copying configuration: [Success] Once the upload is complete, check the images:\nsw-leaf-001# show image --------------------------------------------------------------------------- ArubaOS-CX Primary Image --------------------------------------------------------------------------- Version : FL.10.06.0120 Size : 643 MB Date : 2021-03-14 10:06:34 PST SHA-256 : 78dc27c5e521e92560a182ca44dc04b60d222b9609129c93c1e329940e1e11f9 After the firmware is uploaded, boot the switch to the correct image. When upgrading a VSX pair, use the VSX upgrade command to automatically upgrade both pairs.\nsw-leaf-001# vsx update-software boot-bank primary This will trigger the upgrade process on the VSX pair and it will start the dialogue explaining what will happen next, i.e. if any firmware/driver upgrades are needed (i.e. the unit would reboot twice if this was the case) and it will show you on the screen the current status of the upgrade process. in VSX upgrade process the secondary VSX member will always boot first.\nMellanox Firmware Update   SSH into the switch being upgraded.\n  Fetch the image from ncn-m001.\nsw-spine-001 [standalone: master] # image fetch http://10.252.1.4/fw/network/onyx-X86_64-3.9.1014.stable.img   Install the image.\nsw-spine-001 [standalone: master] # image install onyx-X86_64-3.9.1014.stable.img   Select the image to boot next.\nsw-spine-001 [standalone: master] # image boot next   Write memory and reload.\nsw-spine-001 [standalone: master] # write memory sw-spine-001 [standalone: master] # reload   Once the switch is available, verify the image is installed.\nsw-spine-001 [standalone: master] # show images Installed images: Partition 1: version: X86_64 3.9.0300 2020-02-26 19:25:24 x86_64 Partition 2: version: X86_64 3.9.1014 2020-08-05 18:06:58 x86_64 Last boot partition: 2 Next boot partition: 1 Images available to be installed: 1: Image : onyx-X86_64-3.9.1014.stable.img Version: X86_64 3.9.1014 2020-08-05 18:06:58 x86_64   Dell Firmware Update   SSH into the switch being upgraded.\n  Fetch the image from ncn-m001.\nsw-leaf-001# image install http://10.252.1.4/fw/network/OS10_Enterprise_10.5.1.4.stable.tar   Check the image upload status.\nsw-leaf-001# show image status Image Upgrade State: download ================================================== File Transfer State: download -------------------------------------------------- State Detail: In progress Task Start: 2021-02-08T21:24:14Z Task End: 0000-00-00T00:00:00Z Transfer Progress: 7 % Transfer Bytes: 40949640 bytes File Size: 604119040 bytes Transfer Rate: 869 kbps   Reboot after the image is uploaded.\nsw-leaf-001# write memory sw-leaf-001# reload   Once the switch is available, verify the image is installed.\nsw-leaf-001# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.4 Build Version: 10.5.1.4.249   "
},
{
	"uri": "/docs-csm/en-11/operations/network/management_network/management_network_access_port_configurations/",
	"title": "Management Network Access Port Configurations",
	"tags": [],
	"description": "",
	"content": "Management Network Access Port Configurations Requirements  Access to switches SHCD  Configuration   This configuration describes the edge port configuration. This configuration is in the NMN/HMN/Mountain-TDS Management Tab of the SHCD.\n  Typically, these are ports that are connected to iLOs (BMCs), gateway nodes, or compute nodes/CMM switches.\nsw-leaf-001(config)# interface 1/1/35 no shutdown no routing vlan access 4 spanning-tree bpdu-guard spanning-tree port-type admin-edge   This configuration describes the ports that go to the Node Management Network (NMN/VLAN2).\n  Identify these ports by referencing the NMN tab on the SHCD.\nsw-leaf-001(config)# interface 1/1/35 no shutdown no routing vlan access 2 spanning-tree bpdu-guard spanning-tree port-type admin-edge   Apollo Server Port Configuration This is for the Apollo XL645d only.\niLO BMC port:\nsw-leaf-001(config)# interface 1/1/46 no shutdown no routing vlan trunk native 1 vlan trunk allowed 4 spanning-tree bpdu-guard spanning-tree port-type admin-edge exit NMN port from OCP card:\ninterface 1/1/14 no shutdown no routing vlan access 2 spanning-tree bpdu-guard spanning-tree port-type admin-edge exit UAN Port Configuration  UANs have the same network connections as Shasta v1.3. One connection will go to a NMN(VLAN2) access port, this is where the UAN will pxe boot and communicate with internal systems (see SHCD for UAN cabling). ONE OF THESE PORTS IS SHUTDOWN. One Bond (two connections) will be going to the MLAG/VSX pair of switches. This will be a TRUNK port for the CAN connection.  Aruba UAN NMN Configuration:\nsw-spine-001 (config)# interface 1/1/16 no shutdown mtu 9198 no routing vlan access 2 spanning-tree bpdu-guard spanning-tree port-type admin-edge exit sw-spine-002 (config)# interface 1/1/16 shutdown mtu 9198 no routing vlan access 2 spanning-tree bpdu-guard spanning-tree port-type admin-edge exit Aruba UAN CAN Configuration:\nPort configuration is the same on both switches.\ninterface lag 17 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 7 lacp mode active lacp fallback exit Gigabyte/Intel NCN Worker Port Configuration The cabling guidelines for all servers can be found in Cable Management Network Servers.\nMellanox port configuration:\nsw-spine-002 [gamora-mlag-domain: master] # show run int ethernet 1/1 interface ethernet 1/1 speed 40G force interface ethernet 1/1 mtu 9216 force interface ethernet 1/1 mlag-channel-group 1 mode active Mellanox MLAG configuration:\nsw-spine-002 [gamora-mlag-domain: master] # show run int mlag-port-channel 1 interface mlag-port-channel 1 interface mlag-port-channel 1 mtu 9216 force interface mlag-port-channel 1 switchport mode hybrid interface mlag-port-channel 1 no shutdown interface mlag-port-channel 1 switchport hybrid allowed-vlan add 2 interface mlag-port-channel 1 switchport hybrid allowed-vlan add 4 interface mlag-port-channel 1 switchport hybrid allowed-vlan add 7 interface mlag-port-channel 1 switchport hybrid allowed-vlan add 10 HPE NCN Worker Port Configuration Aruba port configuration:\nsw-spine-001 \u0026amp; sw-spine-002 (config)# interface 1/1/7 shutdown mtu 9198 lag 4 exit Aruba LAG configuration:\nsw-spine-001 \u0026amp; sw-spine-002 (config)# interface lag 4 multi-chassis shutdown no routing vlan access 10 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge Gigabyte/Intel NCN Master port configuration Mellanox port configuration:\nsw-spine-002 [gamora-mlag-domain: master] # show run int ethernet 1/1 interface ethernet 1/1 speed 40G force interface ethernet 1/1 mtu 9216 force interface ethernet 1/1 mlag-channel-group 1 mode active Mellanox MLAG port configuration:\nsw-spine-002 [gamora-mlag-domain: master] # show run int mlag-port-channel 1 interface mlag-port-channel 1 interface mlag-port-channel 1 mtu 9216 force interface mlag-port-channel 1 switchport mode hybrid interface mlag-port-channel 1 no shutdown interface mlag-port-channel 1 switchport hybrid allowed-vlan add 2 interface mlag-port-channel 1 switchport hybrid allowed-vlan add 4 interface mlag-port-channel 1 switchport hybrid allowed-vlan add 7 interface mlag-port-channel 1 switchport hybrid allowed-vlan add 10 HPE NCN Master Port Configuration Aruba port configuration:\nsw-spine02# show run int 1/1/7 interface 1/1/7 no shutdown mtu 9198 lag 4 exit Aruba LAG configuration:\nsw-spine02# show run int lag 4 interface lag 4 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7 lacp mode active lacp fallback exit Gigabyte/Intel NCN Storage Port Configuration Mellanox port configuration:\nsw-spine-002 [gamora-mlag-domain: master] # show run int ethernet 1/7 interface ethernet 1/7 speed 40G force interface ethernet 1/7 mtu 9216 force interface ethernet 1/7 mlag-channel-group 7 mode active Mellanox MLAG port configuration:\nsw-spine-002 [gamora-mlag-domain: master] # show run int mlag-port-channel 7 interface mlag-port-channel 7 interface mlag-port-channel 7 mtu 9216 force interface mlag-port-channel 7 switchport mode hybrid interface mlag-port-channel 7 no shutdown interface mlag-port-channel 7 switchport hybrid allowed-vlan add 2 interface mlag-port-channel 7 switchport hybrid allowed-vlan add 4 interface mlag-port-channel 7 switchport hybrid allowed-vlan add 7 interface mlag-port-channel 7 switchport hybrid allowed-vlan add HPE NCN Storage Port Configuration Aruba port configuration:\nsw-spine02# show run int 1/1/7 interface 1/1/7 no shutdown mtu 9198 lag 4 exit Aruba LAG configuration:\nsw-spine02# show run int lag 4 interface lag 4 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7 lacp mode active lacp fallback exit Aruba Storage port configuration (future use): These will be configured, but the ports will be shut down until needed.\nsw-spine02# show run int 1/1/7 interface 1/1/7 shutdown mtu 9198 lag 4 exit Aruba LAG configuration:\nsw-spine02# show run int lag 4 interface lag 4 multi-chassis shutdown no routing vlan access 10 lacp mode active lacp fallback exit CMM Port Configuration  This requires updated CMM firmware (version 1.4.20). A static LAG will be configured on the CDU switches. The CDU switches have two cables (10Gb RJ45) connecting to each CMM. This configuration offers increased throughput and redundancy.  Aruba Aruba CDU switch configuration. This configuration is identical across CDU VSX pairs. The VLANs used here are generated from CSI.\nsw-cdu-001 \u0026amp; sw-cdu-002 (config)# interface lag 1 multi-chassis static no shutdown description CMM_CAB_9000 no routing vlan trunk native 2000 vlan trunk allowed 2000,3000,4091 sw-cdu-001 \u0026amp; sw-cdu-002 (config)# interface 1/1/1 no shutdown mtu 9198 lag 1 exit Dell Dell CDU switch configuration. This configuration is identical across CDU VLT pairs. The VLANs used here are generated from CSI.\nsw-cdu-001 \u0026amp; sw-cdu-002 (config)# interface port-channel1 description CMM_CAB_1000 no shutdown switchport mode trunk switchport access vlan 2000 switchport trunk allowed vlan 3000 mtu 9216 vlt-port-channel 1 sw-cdu-001 \u0026amp; sw-cdu-002 (config)# interface ethernet1/1/1 description CMM_CAB_1000 no shutdown channel-group 1 mode on no switchport mtu 9216 flowcontrol receive on flowcontrol transmit on CEC Port Configuration The VLAN used here is generated from CSI. It is the HMN_MTN VLAN that is assigned to that cabinet.\nDell interface ethernet1/1/50 description CEC_CAB_1003_alt no shutdown switchport access vlan 3003 flowcontrol receive off flowcontrol transmit off spanning-tree bpduguard enable spanning-tree port type edge Aruba sw-cdu-001 \u0026amp; sw-cdu-002 (config)# interface 1/1/1 no shutdown mtu 9198 description cec1 no routing vlan access 3000 spanning-tree bpdu-guard spanning-tree port-type admin-edge "
},
{
	"uri": "/docs-csm/en-11/operations/network/management_network/management_network_acl_configuration/",
	"title": "Management Network ACL Configuration",
	"tags": [],
	"description": "",
	"content": "Management Network ACL Configuration This page describes the purpose of the ACLs and how they are configured.\nRequirements  Access to the switches  Aruba Configuration These ACLs are designed to block traffic from the Node Management Network (NMN) to and from the Hardware Management Network (HMN).\nThese need to be set where the Layer3 interface is located, this will most likely be a VSX pair of switches. These ACLs are required on both switches in the pair.\n  Create the access list.\naccess-list ip nmn-hmn 10 deny any 10.252.0.0/255.255.128.0 10.254.0.0/255.255.128.0 20 deny any 10.252.0.0/255.255.128.0 10.104.0.0/255.252.0.0 30 deny any 10.254.0.0/255.255.128.0 10.252.0.0/255.255.128.0 40 deny any 10.254.0.0/255.255.128.0 10.100.0.0/255.252.0.0 50 deny any 10.100.0.0/255.252.0.0 10.254.0.0/255.255.128.0 60 deny any 10.100.0.0/255.252.0.0 10.104.0.0/255.252.0.0 70 deny any 10.104.0.0/255.252.0.0 10.252.0.0/255.255.128.0 80 deny any 10.104.0.0/255.252.0.0 10.100.0.0/255.252.0.0 90 permit any any any Once it is created, it needs to be applied to a VLAN.\n  Apply ACL to a VLANs:\nsw-24g03(config)# vlan 2 sw-s24g03(config-vlan-2)# apply access-list ip nmn-hmn in sw-s24g03(config-vlan-2)# apply access-list ip nmn-hmn out sw-24g03(config)# vlan 4 sw-s24g03(config-vlan-4)# apply access-list ip nmn-hmn in sw-s24g03(config-vlan-4)# apply access-list ip nmn-hmn out If on an Aruba CDU switch, apply the same access-list to the 2xxx and 3xxx VLANs (MTN VLANs).\n  Mellanox Configuration Create the nmn-hmn access-list and apply it to vlan 2 and vlan 4.\nsw-spine-001\u0026gt; enable sw-spine-001# configure terminal sw-spine-001(config) # ipv4 access-list nmn-hmn sw-spine-001(config ipv4 access-list nmn-hmn) # bind-point rif sw-spine-001(config ipv4 access-list nmn-hmn) # seq-number 10 deny ip 10.252.0.0 mask 255.255.128.0 10.254.0.0 mask 255.255.128.0 sw-spine-001(config ipv4 access-list nmn-hmn) # seq-number 20 deny ip 10.252.0.0 mask 255.255.128.0 10.104.0.0 mask 255.252.0.0 sw-spine-001(config ipv4 access-list nmn-hmn) # seq-number 30 deny ip 10.254.0.0 mask 255.255.128.0 10.252.0.0 mask 255.255.128.0 sw-spine-001(config ipv4 access-list nmn-hmn) # seq-number 40 deny ip 10.254.0.0 mask 255.255.128.0 10.100.0.0 mask 255.252.0.0 sw-spine-001(config ipv4 access-list nmn-hmn) # seq-number 50 deny ip 10.100.0.0 mask 255.252.0.0 10.254.0.0 mask 255.255.128.0 sw-spine-001(config ipv4 access-list nmn-hmn) # seq-number 60 deny ip 10.100.0.0 mask 255.252.0.0 10.104.0.0 mask 255.252.0.0 sw-spine-001(config ipv4 access-list nmn-hmn) # seq-number 70 deny ip 10.104.0.0 mask 255.252.0.0 10.252.0.0 mask 255.255.128.0 sw-spine-001(config ipv4 access-list nmn-hmn) # seq-number 80 deny ip 10.104.0.0 mask 255.252.0.0 10.100.0.0 mask 255.252.0.0 sw-spine-001(config ipv4 access-list nmn-hmn) # seq-number 90 permit ip any any sw-spine-001(config ipv4 access-list nmn-hmn) # exit sw-spine-001(config) # interface vlan 2 ipv4 port access-group nmn-hmn sw-spine-001(config) # interface vlan 4 ipv4 port access-group nmn-hmn sw-spine-001(config) # exit sw-spine-001# write memory Dell Configuration Create the access list then apply it to all the vlan interfaces. In the example below, only the NMN VLAN is shown. This will need to go on all liquid-cooled and air-cooled networks.\nip access-list nmn-hmn seq 10 deny ip 10.252.0.0/17 10.254.0.0/17 seq 20 deny ip 10.252.0.0/17 10.104.0.0/14 seq 30 deny ip 10.254.0.0/17 10.252.0.0/17 seq 40 deny ip 10.254.0.0/17 10.100.0.0/14 seq 50 deny ip 10.100.0.0/14 10.254.0.0/17 seq 60 deny ip 10.100.0.0/14 10.104.0.0/14 seq 70 deny ip 10.104.0.0/14 10.252.0.0/17 seq 80 deny ip 10.104.0.0/14 10.100.0.0/14 seq 90 permit ip any any interface vlan2 ip access-group nmn-hmn in ip access-group nmn-hmn out "
},
{
	"uri": "/docs-csm/en-11/operations/network/management_network/management_network_can_setup/",
	"title": "Management Network CAN Setup",
	"tags": [],
	"description": "",
	"content": "Management Network CAN Setup Access from the customer site to the system over shared networks is known as the Customer Access Network (CAN).\nRequirements  Access to switches SHCD  Configuration The CAN configuration is highly dependent on customer requirements and may not meet the specifications below.\nTo access the HPE Cray EX nodes and services from the customer network, there is minimal configuration needed on the spine switch and the customer switch connected upstream from the spine switch to allow the customer_access_network subnet to be routed to the HPE Cray EX system.\nThe customer\u0026rsquo;s switch must be connected to the spine switches with a p2p subnet for each switch. In the example below, these two p2p subnets are 10.11.15.148/30 and 10.101.15.152/30. The subnets used are up to the customer.\nThere are two routes configured on the customer\u0026rsquo;s switch to route traffic for the CAN subnet to each of the spine switches. ECMP will load balance the traffic across each of the switches when both links are up and only use the active link when one of the links goes down.\nThe CAN is connected between each spine switch and the NCNs through vlan 7 running over the physical connections between the spines and the port on the NCN. This is the same physical connection used for the NMN and HMN on the NCNs.\nThe two physical connections between the NCN and spines is MLAG\u0026rsquo;ed. MAGP/VSX is used to provide a single virtual router gateway that can be used as the default route on each of the NCNs.\nThis is an example of the p2p configuration on the spine switches. The IP address should be replaced with the IP address chosen by the customer matching the customer\u0026rsquo;s switch configuration.\n  Mellanox:\ninterface ethernet 1/11 speed auto force interface ethernet 1/11 description to-can interface ethernet 1/11 no switchport force interface ethernet 1/11 ip address 10.101.15.150/30 primary   Aruba:\ninterface 1/1/36 no shutdown description to-can ip address 10.101.15.150/30 exit   There must then be two routes on the customer\u0026rsquo;s switch directing traffic for the customer_access_network subnet to the endpoint on the spine switch.\nThe following is an example of the route configuration on the customer switch. These addresses/subnets are generated from CSI and can be found in CAN.yaml.\nExample Snippet from CAN.yaml.\n- full_name: CAN Bootstrap DHCP Subnet cidr: ip: 10.101.8.0 mask: - 255 - 255 - 255 - 0 Customer switch example configuration:\nip route vrf default 10.101.8.0/24 10.101.15.150 ip route vrf default 10.101.8.0/24 10.101.15.154 Going the other direction, there must be a default route on each spine switch directing traffic not matching other routes to the endpoint on the customer\u0026rsquo;s switch.\nThis is an example of the route configuration on sw-spine-001.\n  Mellanox:\nip route vrf default 0.0.0.0/0 10.101.15.149   Aruba:\nip route 0.0.0.0/0 10.101.15.149   The spine switch must also have the customer_access_gateway IP address assigned to the vlan 7 interface on the switch. This provides a gateway for the default route on the NCNs and UANs, as well as a direct route to the customer_access_network from the spine switch.\n  Mellanox:\ninterface vlan 7 ip address 10.101.8.2/26 primary   Aruba:\nsw-spine-002(config)# int vlan 7 sw-spine-002(config-if-vlan)# ip address 10.102.11.3/24   Verification of CAN Configuration After completing this configuration, the administrator will be able to ping and log in to all of the NCNs at the external CAN IP address from a device on the customer network.\nexternal\u0026gt; ping 10.101.8.6 PING 10.101.8.6 (10.101.8.6): 56 data bytes 64 bytes from 10.101.8.6: icmp_seq=0 ttl=58 time=61.445 ms 64 bytes from 10.101.8.6: icmp_seq=1 ttl=58 time=70.263 ms 64 bytes from 10.101.8.6: icmp_seq=2 ttl=58 time=59.270 ms ^C --- 10.101.8.6 ping statistics --- 3 packets transmitted, 3 packets received, 0.0% packet loss round-trip min/avg/max/stddev = 59.270/63.659/70.263/4.753 ms external\u0026gt; ssh root@10.101.8.6 The authenticity of host '10.101.8.6 (10.101.8.6)' can't be established. ECDSA key fingerprint is SHA256:jnMGZnMcdPQ9QleyJADbI9AQAvo4DfGz0SOYbe3lraI. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added '10.101.8.6' (ECDSA) to the list of known hosts. Password: ncn-w001# "
},
{
	"uri": "/docs-csm/en-11/operations/network/management_network/management_network_flow_control_settings/",
	"title": "Management Network Flow Control Settings",
	"tags": [],
	"description": "",
	"content": "Management Network Flow Control Settings This page is designed to go over all the flow control settings for Dell/Mellanox systems.\nLeaf Switch Node Connections For the node connections to a leaf switch, disable the transmit flowcontrol and enable receive flowcontrol with the following commands:\nNOTE: If using a TDS system involving a Hill cabinet, make sure to confirm that no CMM nor CEC components are connected to any leaf switches in your system. If these components are connected to the leaf, confirm to which ports they are connected, and modify the commands below to avoid modifying the flowcontrol settings of those ports.\nsw-leaf-001# configure terminal sw-leaf-001(config)# interface range ethernet 1/1/1-1/1/48 sw-leaf-001(conf-range-eth1/1/1-1/1/48)# flowcontrol receive on sw-leaf-001(conf-range-eth1/1/1-1/1/48)# flowcontrol transmit off sw-leaf-001(conf-range-eth1/1/1-1/1/48)# end sw-leaf-001# write memory Switch-to-Switch Connections for MLAG, VLT, or VSX Flowcontrol is supposed to be enabled on the IPL for Mellanox MLAG. Flowcontrol settings are defined elsewhere for the links between redundant Dell, Mellanox, or Aruba switches. This includes IPL and keepalive interfaces. This document is not relevant for those links.\nThe configuration below is supposed to be on for Mellanox MLAG:\n dcb priority-flow-control enable force interface port-channel 100 dcb priority-flow-control mode on force Switch-to-Switch Connections Disable flowcontrol in both directions for all switch-to-switch connections: spine-to-leaf; spine-to-CDU; spine-to-aggregate; and aggregate-to-leaf (It is unlikely that any system has every type of connection).\nHow to identify which ports are part of a switch-to-switch connection will be covered first, and then the commands to make the changes. The commands for each switch group are provided separately, but it is strongly recommended to make the configuration changes for each end of the connection in short order; for example, for a spine-leaf connection, do not make the changes on the spine side if changes cannot also be made to the leaf switch end of the connection within a couple minutes. Repeat the above commands for each leaf switch in the system. These changes can be performed before the switch-to-switch connections, or concurrent with those changes.\nRecommended order of flow control changes for switch-to-switch connections:\n Make change on sw-spine-001 side of leaf/aggregate/CDU ISL connections. Make change on sw-spine-002 side of leaf/aggregate/CDU ISL connections. Make change on leaf/aggregate/CDU side of spine ISL connections. Make change on aggregate01/CDU01 side of VLT ISL connections. Make change on aggregate02/CDU02 side of VLT ISL connections. Repeat Steps 4 and 5 for each aggregate/CDU switch pair. Make change on aggregate side of leaf ISL connections. Make change on leaf side of aggregate ISL connections.  Identify Switch-to-Switch Connections Leaf Switches Our standard for the configuration uses \u0026lsquo;port-channel 100\u0026rsquo; for the connection to the spine or aggregate switch. To get what ports are part of this composite interface, use the following command:\nsw-leaf-001# show interface port-channel 100 summary LAG Mode Status Uptime Ports 100 L2-HYBRID up 2 weeks 5 days 01:2 Eth 1/1/51 (Up) Eth 1/1/52 (Up) The physical ports in this example are \u0026lsquo;1/1/51\u0026rsquo; and \u0026lsquo;1/1/52\u0026rsquo;. Record this information for each leaf switch.\nCDU Switches In order to get the ports involved in the connection to the spine switches, use the command shared for the leaf switch above.\nThe ports connecting the pair of CDU switches together is also required. The best way to determine the ports involved is to run the following command:\nsw-cdu-001# show running-configuration | grep discovery discovery-interface ethernet1/1/25,1/1/29 The ports 1/1/25 and 1/1/29 in this example are being used as connections between the CDU switches. As with the connection to the spine, record the ports involved.\nNOTE: It is very important that the flowcontrol settings for the CMM and CEC devices connected to the CDU switches NOT be modified.\nAggregate Switches On large air-cooled systems, aggregate switches are situated between the leaf and spine switches. In general, it is expected that every port that is up on these switches to either be a connection to the spine (as \u0026lsquo;port-channel 100\u0026rsquo;), a connection to a leaf, or a connection to its peer aggregate switch. To see which ports are currently up, run the following command:\nsw-10g01# show interface status -------------------------------------------------------------------------------------------------- Port Description Status Speed Duplex Mode Vlan Tagged-Vlans -------------------------------------------------------------------------------------------------- Eth 1/1/1 LEAF_CONN_1 up 10G full - Eth 1/1/2 LEAF_CONN_2 up 10G full - Eth 1/1/3 LEAF_CONN_3 up 10G full - Eth 1/1/4 LEAF_CONN_4 up 10G full - Eth 1/1/5 LEAF_CONN_5 up 10G full - Eth 1/1/6 LEAF_CONN_6 up 10G full - Eth 1/1/7 LEAF_CONN_7 up 10G full - Eth 1/1/8 LEAF_CONN_8 up 10G full - Eth 1/1/9 LEAF_CONN_9 up 10G full - Eth 1/1/10 down 0 full A 1 - Eth 1/1/11 down 0 full A 1 - Eth 1/1/12 down 0 full A 1 - Eth 1/1/13 down 0 full A 1 - Eth 1/1/14 down 0 full A 1 - Eth 1/1/15 down 0 full A 1 - Eth 1/1/16 down 0 full A 1 - Eth 1/1/17 down 0 full A 1 - Eth 1/1/18 down 0 full A 1 - Eth 1/1/19 down 0 full A 1 - Eth 1/1/20 down 0 full A 1 - Eth 1/1/21 down 0 full A 1 - Eth 1/1/22 down 0 full A 1 - Eth 1/1/23 down 0 full A 1 - Eth 1/1/24 down 0 full A 1 - Eth 1/1/25 up 100G full - Eth 1/1/26 down 0 full A 1 - Eth 1/1/27 up 40G full - Eth 1/1/28 up 40G full - Eth 1/1/29 up 100G full - Eth 1/1/30 down 0 full A 1 - Eth 1/1/31 down 0 full A 1 - Eth 1/1/32 down 0 full A 1 - Eth 1/1/33 down 0 full A 1 - Eth 1/1/34 down 0 full A 1 - Eth 1/1/35 down 0 full A 1 - Eth 1/1/36 down 0 full A 1 - Eth 1/1/37 down 0 full A 1 - Eth 1/1/38 down 0 full A 1 - Eth 1/1/39 down 0 full A 1 - Eth 1/1/40 down 0 full A 1 - Eth 1/1/41 down 0 full A 1 - Eth 1/1/42 down 0 full A 1 - Eth 1/1/43 down 0 full A 1 - Eth 1/1/44 down 0 full A 1 - Eth 1/1/45 down 0 full A 1 - Eth 1/1/46 down 0 full A 1 - Eth 1/1/47 down 0 full A 1 - Eth 1/1/48 down 0 full A 1 - Eth 1/1/49 down 0 full A 1 - Eth 1/1/50 down 0 full A 1 - Eth 1/1/51 down 0 full A 1 - Eth 1/1/52 down 0 full A 1 - Eth 1/1/53 down 0 full A 1 - Eth 1/1/54 down 0 full A 1 - -------------------------------------------------------------------------------------------------- From the output in this example, ports 1/1/1 through 1/1/9, 1/1/25, and 1/1/27 through 1/1/29 are up. Record this information.\nSpine Switches The convenient way to identify the ports involved with connections to other switches is to look at the output from show interface status.\nsw-spine-001 [standalone: master] # show interfaces status -------------------------------------------------------------------------------------------------------------------------------------------------------- Port Operational state Admin Speed MTU Description -------------------------------------------------------------------------------------------------------------------------------------------------------- mgmt1 Down Enabled UNKNOWN 1500 - mgmt0 Down Enabled UNKNOWN 1500 - Po100 Up Enabled 9216 mlag-isl Mpo1 Up Enabled 9216 - Mpo2 Up Enabled 9216 - Mpo3 Up Enabled 9216 - Mpo4 Up Enabled 9216 - Mpo5 Up Enabled 9216 - Mpo6 Up Enabled 9216 - Mpo7 Up Enabled 9216 - Mpo8 Up Enabled 9216 - Mpo9 Up Enabled 9216 - Mpo10 Up Enabled 9216 - Mpo11 Up Enabled 9216 - Mpo17 Up Enabled 9216 - Mpo113 Up Enabled 9216 - Mpo151 Up Enabled 9216 - Mpo152 Up Enabled 9216 - Eth1/1 (Mpo1) Up Enabled 40G 9216 - Eth1/2 (Mpo2) Up Enabled 40G 9216 - Eth1/3 (Mpo3) Up Enabled 40G 9216 - Eth1/4 (Mpo4) Up Enabled 40G 9216 - Eth1/5 (Mpo5) Up Enabled 40G 9216 - Eth1/6 (Mpo6) Up Enabled 40G 9216 - Eth1/7 (Mpo7) Up Enabled 40G 9216 - Eth1/8 (Mpo8) Up Enabled 40G 9216 - Eth1/9 (Mpo9) Up Enabled 40G 9216 - Eth1/10 (Mpo10) Up Enabled 40G 9216 - Eth1/11 (Mpo11) Up Enabled 40G 9216 - Eth1/12 (Po100) Up Enabled 40G 9216 sw-spine-002-1/12 Eth1/13 (Mpo113) Up Enabled 40G 9216 - Eth1/14 (Mpo113) Up Enabled 40G 9216 - Eth1/15/1 (Mpo151) Up Enabled 10G 9216 - Eth1/15/2 (Mpo152) Up Enabled 10G 9216 - Eth1/15/3 Down Enabled Unknown 1500 - Eth1/15/4 Down Enabled Unknown 1500 - Eth1/17 (Mpo17) Up Enabled 40G 9216 - Eth1/18 (Po100) Up Enabled 40G 9216 sw-spine-002-1/18 Eth1/19 Up Enabled 10G 1500 - Eth1/20 Down Enabled Unknown 1500 - Eth1/21 Down Enabled Unknown 1500 - Eth1/22 Down Enabled Unknown 1500 - Eth1/23 Down Enabled Unknown 1500 - Eth1/24 Down Enabled Unknown 1500 - Eth1/25 Down Enabled Unknown 1500 - Eth1/26 Down Enabled Unknown 1500 - Eth1/27 Down Enabled Unknown 1500 - Eth1/28 Down Enabled Unknown 1500 - Eth1/29 Down Enabled Unknown 1500 - Eth1/30 Down Enabled Unknown 1500 - Eth1/31 Down Enabled Unknown 1500 - Eth1/32 Down Enabled Unknown 1500 The links between the 2 spines should be port-channel 100 (Po100). The mlag-port-channel interfaces which are connections to leaf, aggregate, or CDU switches would be Mpo interfaces with indices greater than 100. So here, Mpo1-Mpo11 and Mpo17 are connections to NCNs, whereas Mpo113, Mpo151, and Mpo152 are connections to other switches. When identifying the port-channel and mlag-port-channel devices, look for the Eth rows, which have one of these labels in parentheses next to it. In the example above, these are:\n Eth1/12 Eth1/13 Eth1/14 Eth1/15/1 Eth1/15/2 Eth1/18  Record these ports AND the port-channel and mlag-port-channel interfaces.\nSpine Switch \u0026lsquo;flowcontrol\u0026rsquo; Configuration Change On the Mellanox spine switches, modify the flowcontrol settings on the port-channel, mlag-port-channel, and Ethernet interfaces. The general form looks similar to the following:\nsw-spine-001 [standalone: master] # configure terminal sw-spine-001 [standalone: master] (config) # interface port-channel \u0026lt;index\u0026gt; flowcontrol receive off force sw-spine-001 [standalone: master] (config) # interface port-channel \u0026lt;index\u0026gt; flowcontrol send off force sw-spine-001 [standalone: master] (config) # interface mlag-port-channel \u0026lt;index\u0026gt; flowcontrol receive off force sw-spine-001 [standalone: master] (config) # interface mlag-port-channel \u0026lt;index\u0026gt; flowcontrol send off force sw-spine-001 [standalone: master] (config) # interface ethernet \u0026lt;port\u0026gt; flowcontrol receive off force sw-spine-001 [standalone: master] (config) # interface ethernet \u0026lt;port\u0026gt; flowcontrol send off force sw-spine-001 [standalone: master] (config) # exit sw-spine-001 [standalone: master] # write memory \u0026ldquo;index\u0026rdquo; is the number after \u0026ldquo;Po\u0026rdquo; or \u0026ldquo;Mpo\u0026rdquo;, so \u0026ldquo;113\u0026rdquo; or \u0026ldquo;151\u0026rdquo;. \u0026ldquo;\u0026rdquo; would be the value after \u0026ldquo;Eth\u0026rdquo;, so \u0026ldquo;1/14\u0026rdquo; or \u0026ldquo;1/15/2\u0026rdquo;. Make sure to run the flowcontrol commands for each mlag-port-channel and Ethernet port.\nLeaf, CDU, and Aggregate Switch \u0026lsquo;flowcontrol\u0026rsquo; Configuration Change On the Dell switches, only modify the Ethernet interface configurations. The general form looks similar to the following:\nsw-leaf-001# configure terminal sw-leaf-001(config)# interface ethernet \u0026lt;port\u0026gt; sw-leaf-001(conf-if-eth\u0026lt;port\u0026gt;)# flowcontrol receive off sw-leaf-001(conf-if-eth\u0026lt;port\u0026gt;)# flowcontrol transmit off sw-leaf-001(conf-if-eth\u0026lt;port\u0026gt;)# end sw-leaf-001# write memory Do this for each port. Alternatively, set it up to do multiple ports as one command. For instance, the common leaf switch would have ports 51 and 52 as connections to the spine. So in that case, these commands would work:\nsw-leaf-001# configure terminal sw-leaf-001(config)# interface range ethernet 1/1/51-1/1/52 sw-leaf-001(conf-if-eth1/1/51-1/1/52)# flowcontrol receive off sw-leaf-001(conf-if-eth1/1/51-1/1/52)# flowcontrol transmit off sw-leaf-001(conf-if-eth1/1/51-1/1/52)# end sw-leaf-001# write memory Alternatively, a typical CDU switch would have ports 27 and 28 as uplinks to the spine, with ports 25 and 29 as connections to the peer CDU switch. So in that case, use the following commands:\nsw-cdu-001# configure terminal sw-cdu-001(config)# interface range ethernet 1/1/25,1/1/27-1/1/29 sw-cdu-001(conf-range-eth1/1/25,1/1/27-1/1/29)# flowcontrol receive off sw-cdu-001(conf-range-eth1/1/25,1/1/27-1/1/29)# flowcontrol transmit off sw-cdu-001(conf-range-eth1/1/25,1/1/27-1/1/29)# end sw-cdu-001# write memory Disable iSCSI on Dell Switches (Leaf, CDU, and Aggregate) The final configuration change needed on the Dell switches is to disable iSCSI in the configuration. This change ensures that all of the flowcontrol changes made above will persist through a reboot of the switch.\nRun the following commands on all Dell switches in the system:\nsw-leaf-001# configure terminal sw-leaf-001(config)# no iscsi enable sw-leaf-001(config)# exit sw-leaf-001# write memory "
},
{
	"uri": "/docs-csm/en-11/operations/network/management_network/management_network_switch_rename/",
	"title": "Management Network Switch Rename",
	"tags": [],
	"description": "",
	"content": "Management Network Switch Rename Any system moving from Shasta v1.3 to Shasta v1.4 software needs to adjust the hostnames and IP addresses for all switches to match the new standard. There is now a virtual IP address ending in .1 which is used by spine switches. In Shasta v1.3, the first spine switch used the .1 address. In Shasta v1.4, the ordering of the switches has changed with spine switches being grouped first. The hostname for switches has changed from two digits to a dash and then 3 digits. All IPv4 data for the switches and switch naming comes from Cray Site Init (CSI).\nCurrently, the ordering of the switches has changed with spine switches being grouped first. The hostname for switches has changed from two digits to a dash followed by three digits. All IPv4 data for the switches and switch naming comes from Cray Site Init (CSI).\nFrom v1.3, this example system had these IP addresses and hostnames on the Hardware Management Network (HMN). Similar names and IP address numbers for the Node Management Network (NMN) and Customer Access Network (CAN) as well. Also note that some systems may not have an agg or aggregation set of switches. This is ok. Simply follow the directions in this section, always paying attention to the CSI specified IPv4 addresses, and skipping the aggregation switch directions.\n10.1.0.1 sw-spine01 10.1.0.2 sw-leaf01 10.1.0.3 sw-spine02 10.1.0.4 sw-leaf02 10.1.0.5 sw-agg01 10.1.0.6 sw-agg02 10.1.0.7 sw-cdu01 10.1.0.8 sw-cdu02 The desired settings for the HMN will be similar to the following:\n10.1.0.2 sw-spine-001 10.1.0.3 sw-spine-002 10.1.0.4 sw-agg-001 10.1.0.5 sw-agg-002 10.1.0.6 sw-leaf-001 10.1.0.7 sw-leaf-002 10.1.0.8 sw-cdu-001 10.1.0.9 sw-cdu-002 The system in this example needs to do the renames in the following order:\n CDU switches: 8 to 9, and 7 to 8 Leaf switches: 4 to 7, and 2 to 6 Aggregation switches: 6 to 5, and 5 to 4 Spine switches: 3 to 3, and 1 to 2  These have IP address changes and name changes because there are now three digits instead of two for the switch hostname.\n  Check switch IP addresses, names, and component names in /var/www/ephemeral/prep/${SYSTEM_NAME}/networks when booted from the LiveCD on ncn-m001.\npit# export SYSTEM_NAME=eniac pit# cd /var/www/ephemeral/prep/${SYSTEM_NAME}/networks Excerpt from NMN.yaml:\npit# vi NMN.yaml ip_reservations: - ip_address: 10.252.0.2 name: sw-spine-001 comment: x3000c0h33s1 aliases: [] - ip_address: 10.252.0.3 name: sw-spine-002 comment: x3000c0h34s1 aliases: [] - ip_address: 10.252.0.4 name: sw-cdu-001 comment: d0w1 aliases: [] - ip_address: 10.252.0.5 name: sw-cdu-002 comment: d0w2 aliases: [] - ip_address: 10.252.0.6 name: sw-leaf-001 comment: x3000c0w38 aliases: [] - ip_address: 10.252.0.7 name: sw-leaf-002 comment: x3000c0w36 aliases: [] Excerpt from HMN.yaml:\npit# vi HMN.yaml ip_reservations: - ip_address: 10.254.0.2 name: sw-spine-001 comment: x3000c0h33s1 aliases: [] - ip_address: 10.254.0.3 name: sw-spine-002 comment: x3000c0h34s1 aliases: [] - ip_address: 10.254.0.4 name: sw-cdu-001 comment: d0w1 aliases: [] - ip_address: 10.254.0.5 name: sw-cdu-002 comment: d0w2 aliases: [] - ip_address: 10.254.0.6 name: sw-leaf-001 comment: x3000c0w38 aliases: [] - ip_address: 10.254.0.7 name: sw-leaf-002 comment: x3000c0w36 aliases: [] Excerpt from MTL.yaml:\npit# vi MTL.yaml ip_reservations: - ip_address: 10.1.0.2 name: sw-spine-001 comment: x3000c0h33s1 aliases: [] - ip_address: 10.1.0.3 name: sw-spine-002 comment: x3000c0h34s1 aliases: [] - ip_address: 10.1.0.4 name: sw-cdu-001 comment: d0w1 aliases: [] - ip_address: 10.1.0.5 name: sw-cdu-002 comment: d0w2 aliases: [] - ip_address: 10.1.0.6 name: sw-leaf-001 comment: x3000c0w38 aliases: [] - ip_address: 10.1.0.7 name: sw-leaf-002 comment: x3000c0w36 aliases: [] Excerpt from CAN.yaml:\nThe following example includes two spine switches. Systems running a previous release would have had these as ending in .1 and in .3. Note these switches are not named \u0026ldquo;spine\u0026rdquo; or \u0026ldquo;agg\u0026rdquo; because the SHCD may specify differing exit points, but with either option the IPv4 address is specified.\npit# vi CAN.yaml ip_reservations: - ip_address: 10.103.8.2 name: can-switch-1 comment: \u0026quot; aliases: [] - ip_address: 10.103.8.3 name: can-switch-2 comment: \u0026quot; aliases: []   Save the running-config from all switches before starting.\nNOTE: Mellanox switches require the enable command before doing show running-config.\npit# ssh admin@10.1.0.1 switch# show running-config switch# exit Save this information in a text file for later evaluation and comparison after all changes have been made.\npit# vi before.sw-spine01.txt Repeat this for all of the switches. The example system has switches up to 10.1.0.8.\n  Start moves with the highest numbered switch. I\nIn this case, that is sw-cdu02. If a switch is in a pair, start with the second half of the pair. For instance, switch 2 of 2.\nMove sw-cdu02 to sw-cdu-002 and increase IP addresses as specified in CSI output. It is a Dell switch.\nNOTE: Several addresses can be changed in a single session, but not the one used to connect. This first connection will skip vlan 1 and change all of the other vlans (vlan 2 and vlan 4 on a CDU switch).\npit# ssh admin@10.1.0.6 sw-cdu02# configure terminal sw-cdu02(config)# hostname sw-cdu-002 sw-cdu-002(config)# interface vlan2 sw-cdu-002(conf-if-vl-2)# ip address 10.252.0.7/17 sw-cdu-002(conf-if-vl-2)# interface vlan4 sw-cdu-002(conf-if-vl-4)# ip address 10.254.0.7/17 sw-cdu-002(conf-if-vl-4)# router ospf 1 sw-cdu-002(config-router-ospf-1)# router-id 10.252.0.7 sw-cdu-002(config-router-ospf-1)# exit sw-cdu-002(config)# exit sw-cdu-002# write memory Log out of the switch and return using the new IP address for vlan 2 so that vlan 1 can be corrected.\nsw-cdu-002# exit pit# ssh admin@10.252.0.7 sw-cdu-002# configure terminal sw-cdu-002(config)# interface vlan1 sw-cdu-002(conf-if-vl-1)# ip address 10.1.0.7/16 sw-cdu-002(conf-if-vl-1)# exit sw-cdu-002(config)# exit sw-cdu-002# write memory sw-cdu-002# exit pit#   Move sw-cdu01 to sw-cdu-001 and increase IP addresses as specified in CSI output. It is a Dell switch.\nNOTE: Several addresses can be changed in a single session, but not the one used to connect. This first connection will skip vlan 1 and change all of the other vlans (vlan 2 and vlan 4 on a CDU switch).\npit# ssh admin@10.1.0.5 sw-cdu01# configure terminal sw-cdu01(config)# hostname sw-cdu-001 sw-cdu-001(config)# interface vlan2 sw-cdu-001(conf-if-vl-2)# ip address 10.252.0.6/17 sw-cdu-001(conf-if-vl-2)# interface vlan4 sw-cdu-001(conf-if-vl-4)# ip address 10.254.0.6/17 sw-cdu-001(conf-if-vl-4)# router ospf 1 sw-cdu-001(config-router-ospf-1)# router-id 10.252.0.6 sw-cdu-001(config-router-ospf-1)# exit sw-cdu-001(config)# exit sw-cdu-001# write memory Log out of the switch and return using the new IP address for vlan 2 so that vlan 1 can be corrected.\nsw-cdu-001# exit pit# ssh admin@10.252.0.6 sw-cdu-001# configure terminal sw-cdu-001(config)# interface vlan1 sw-cdu-001(conf-if-vl-1)# ip address 10.1.0.6/16 sw-cdu-001(conf-if-vl-1)# exit sw-cdu-001(config)# exit sw-cdu-001# write memory sw-cdu-001# exit pit#   Move sw-leaf02 to sw-leaf-002 and increase IP addresses as specified in CSI output. It is a Dell switch.\nNOTE: Several addresses can be changed in a single session, but not the one used to connect. This first connection will skip vlan 1 and change all of the others (vlan 2, vlan 4, vlan 7, and vlan 10) on a leaf switch.\npit# ssh admin@10.1.0.4 sw-leaf02# configure terminal sw-leaf02(config)# hostname sw-leaf-002 sw-leaf-002(config)# interface vlan2 sw-leaf-002(conf-if-vl-2)# ip address 10.252.0.5/17 sw-leaf-002(conf-if-vl-2)# interface vlan4 sw-leaf-002(conf-if-vl-4)# ip address 10.254.0.5/17 sw-leaf-002(conf-if-vl-4)# interface vlan7 sw-leaf-002(conf-if-vl-7)# ip address 10.103.8.5/24 sw-leaf-002(conf-if-vl-7)# interface vlan10 sw-leaf-002(conf-if-vl-10)# ip address 10.11.0.5/16 sw-leaf-002(conf-if-vl-10)# router ospf 1 sw-leaf-002(config-router-ospf-1)# router-id 10.252.0.5 sw-leaf-002(config-router-ospf-1)# exit sw-leaf-002(config)# exit sw-leaf-002# write memory Log out of the switch and return using the new IP address for vlan 2 so that vlan 1 can be corrected.\nsw-leaf-002# exit pit# ssh admin@10.252.0.5 sw-leaf-002# configure terminal sw-leaf-002(config)# interface vlan1 sw-leaf-002(conf-if-vl-1)# ip address 10.1.0.5/16 sw-leaf-002(conf-if-vl-1)# exit sw-leaf-002(config)# exit sw-leaf-002# write memory sw-leaf-002# exit pit#   Move sw-leaf01 to sw-leaf-001 and increase IP addresses as specified in CSI output. It is a Dell switch.\nNOTE: Several addresses can be changed in a single session, but not the one used to connect. This first connection will skip vlan 1 and change all of the others (vlan 2, vlan 4, vlan 7, and vlan 10) on a leaf switch.\npit# ssh admin@10.1.0.2 sw-leaf01# configure terminal sw-leaf01(config)# hostname sw-leaf-001 sw-leaf-001(config)# interface vlan2 sw-leaf-001(conf-if-vl-2)# ip address 10.252.0.4/17 sw-leaf-001(conf-if-vl-2)# interface vlan4 sw-leaf-001(conf-if-vl-4)# ip address 10.254.0.4/17 sw-leaf-001(conf-if-vl-4)# interface vlan7 sw-leaf-001(config-router-ospf-1)# router-id 10.252.0.4 sw-leaf-001(config-router-ospf-1)# exit sw-leaf-001(config)# exit sw-leaf-001# write memory Log out of the switch and return using the new IP address for vlan 2 so that vlan 1 can be corrected.\nsw-leaf-001# exit pit# ssh admin@10.252.0.4 sw-leaf-001# configure terminal sw-leaf-001(config)# interface vlan1 sw-leaf-001(conf-if-vl-1)# ip address 10.1.0.4/16 sw-leaf-001(conf-if-vl-1)# exit sw-leaf-001(config)# exit sw-leaf-001# write memory sw-leaf-001# exit pit#   Move sw-spine02 to sw-spine-002. It already has the .3 IP address so does not need to change. It is a Mellanox switch.\nNOTE: You can change many addresses in a single session, but not the one you used to connect. This first connection will skip vlan 1 and change all of the others (vlan 2, vlan 4, vlan 7, and vlan 10) on a leaf switch.\nNOTE: The addresses for the spine switches in CAN.yaml shown above (ip_address: 10.103.8.2, name: can-switch-1) and (ip_address: 10.103.8.3, name: can-switch-2) were shown on the 10.103.8.0 subnet so the virtual-router address on interface vlan7 magp 7 should be 10.103.8.1.\npit# ssh admin@10.1.0.3 sw-spine02\u0026gt; enable sw-spine02# configure terminal sw-spine02(config)# hostname sw-spine-002 sw-spine02(config)# no protocol magp sw-spine-002(config)# interface vlan 1 ip address 10.1.0.3/16 primary sw-spine-002(config)# interface vlan 2 ip address 10.252.0.3/17 primary sw-spine-002(config)# interface vlan 4 ip address 10.254.0.3/17 primary sw-spine-002(config)# interface vlan 7 ip address 10.103.8.3/24 primary sw-spine-002(config)# interface vlan 10 ip address 10.11.0.3/16 primary sw-spine-002(config)# router bgp 65533 vrf default router-id 10.252.0.3 force sw-spine-002(config)# router ospf 1 vrf default router-id 10.252.0.3 sw-spine-002(config)# exit sw-spine-002# write memory Log out of the switch and return using the new IP address for vlan 2 so that vlan 1 can be corrected.\nsw-spine-002# exit pit# ssh admin@10.252.0.3 sw-spine-002 [standalone: master] \u0026gt; enable sw-spine-002 [standalone: master] # configure terminal sw-spine-002 [standalone: master] (config) # no protocol magp sw-spine-002 [standalone: master] (config) # protocol magp sw-spine-002 [standalone: master] (config) # interface vlan 1 sw-spine-002 [standalone: master] (config interface vlan 1) # no ip address sw-spine-002 [standalone: master] (config interface vlan 1) # ip address 10.1.0.3/16 primary sw-spine-002 [standalone: master] (config interface vlan 1) # exit sw-spine-002 [standalone: master] (config) # interface vlan 1 magp 1 sw-spine-002 [standalone: master] (config interface vlan 1 magp 1) # ip virtual-router address 10.1.0.1 sw-spine-002 [standalone: master] (config interface vlan 1 magp 1) # ip virtual-router mac-address 00:00:5E:00:01:01 sw-spine-002 [standalone: master] (config interface vlan 1 magp) # exit sw-spine-002 [standalone: master] (config) # interface vlan 2 magp 2 sw-spine-002 [standalone: master] (config interface vlan 2 magp 2) # ip virtual-router address 10.252.0.1 sw-spine-002 [standalone: master] (config interface vlan 2 magp 2) # ip virtual-router mac-address 00:00:5E:00:01:02 sw-spine-002 [standalone: master] (config interface vlan 2 magp 2) # exit sw-spine-002 [standalone: master] (config) # interface vlan 4 magp 4 sw-spine-002 [standalone: master] (config interface vlan 4 magp 4) # ip virtual-router address 10.254.0.1 sw-spine-002 [standalone: master] (config interface vlan 4 magp 4) # ip virtual-router mac-address 00:00:5E:00:01:04 sw-spine-002 [standalone: master] (config interface vlan 4 magp 4) # exit sw-spine-002 [standalone: master] (config) # interface vlan 7 magp 7 sw-spine-002 [standalone: master] (config interface vlan 7 magp 7) # ip virtual-router address 10.103.8.1 sw-spine-002 [standalone: master] (config interface vlan 7 magp 7) # ip virtual-router mac-address 00:00:5E:00:01:07 sw-spine-002 [standalone: master] (config interface vlan 7 magp 7) # exit sw-spine-002 [standalone: master] (config) # interface vlan 10 magp 10 sw-spine-002 [standalone: master] (config interface vlan 10 magp 10) # ip virtual-router address 10.11.0.1 sw-spine-002 [standalone: master] (config interface vlan 10 magp 10) # ip virtual-router mac-address 00:00:5E:00:01:10 sw-spine-002 [standalone: master] (config interface vlan 10 magp 10) # exit sw-spine-002 [standalone: master] (config) # exit sw-spine-002 [standalone: master] # write memory sw-spine-002 [standalone: master] # exit pit#   Move sw-spine01 to sw-spine-001 and increase IP addresses as specified in CSI output. It is a Mellanox switch.\nNOTE: Several addresses can be changed in a single session, but not the one used to connect. This first connection will skip vlan 1 and change all of the others (vlan 2, vlan 4, vlan 7, and vlan 10) on a leaf switch.\nNOTE: The addresses for the spine switches in CAN.yaml shown above (ip_address: 10.103.8.2, name: can-switch-1) and (ip_address: 10.103.8.3, name: can-switch-2) were shown on the 10.103.8.0 subnet so the virtual-router address on interface vlan7 magp 7 should be 10.103.8.1.\npit# ssh admin@10.1.0.1 sw-spine01\u0026gt; enable sw-spine01# configure terminal sw-spine01(config)# hostname sw-spine-001 sw-spine-001(config)# no protocol magp sw-spine-001(config)# interface vlan 2 sw-spine-001 [standalone: master] (config interface vlan 2) # no ip address sw-spine-001 [standalone: master] (config interface vlan 2) # ip address 10.252.0.2/17 primary sw-spine-001 [standalone: master] (config interface vlan 2) # exit sw-spine-001 [standalone: master] (config) # interface vlan 4 sw-spine-001 [standalone: master] (config interface vlan 4) # no ip address sw-spine-001 [standalone: master] (config interface vlan 4) # ip address 10.254.0.2/17 primary sw-spine-001 [standalone: master] (config interface vlan 4) # exit sw-spine-001 [standalone: master] (config) # interface vlan 7 sw-spine-001 [standalone: master] (config interface vlan 7) # no ip address sw-spine-001 [standalone: master] (config interface vlan 7) # ip address 10.103.8.2/24 primary sw-spine-001 [standalone: master] (config interface vlan 7) # exit sw-spine-001 [standalone: master] (config) # interface vlan 10 sw-spine-001 [standalone: master] (config interface vlan 10) # no ip address sw-spine-001 [standalone: master] (config interface vlan 10) # ip address 10.11.0.2/16 primary sw-spine-001 [standalone: master] (config interface vlan 10) # exit sw-spine-001 [standalone: master] (config) # router bgp 65533 vrf default router-id 10.252.0.2 force sw-spine-001 [standalone: master] (config) # router ospf 1 vrf default router-id 10.252.0.2 sw-spine-001 [standalone: master] (config) # exit sw-spine-001 [standalone: master] # write memory Log out of the switch and return using the new IP address for vlan 2 so that vlan 1 can be corrected.\nsw-spine-001# exit pit# ssh admin@10.252.0.2 sw-spine-001 [standalone: master] \u0026gt; enable sw-spine-001 [standalone: master] # configure terminal sw-spine-001 [standalone: master] (config) # no protocol magp sw-spine-001 [standalone: master] (config) # protocol magp sw-spine-001 [standalone: master] (config) # interface vlan 1 sw-spine-001 [standalone: master] (config interface vlan 1) # no ip address sw-spine-001 [standalone: master] (config interface vlan 1) # ip address 10.1.0.2/16 primary sw-spine-001 [standalone: master] (config interface vlan 1) # exit sw-spine-001 [standalone: master] (config) # interface vlan 1 magp 1 sw-spine-001 [standalone: master] (config interface vlan 1 magp 1) # ip virtual-router address 10.1.0.1 sw-spine-001 [standalone: master] (config interface vlan 1 magp 1) # ip virtual-router mac-address 00:00:5E:00:01:01 sw-spine-001 [standalone: master] (config interface vlan 1 magp 1) # exit sw-spine-001 [standalone: master] (config) # interface vlan 2 magp 2 sw-spine-001 [standalone: master] (config interface vlan 2 magp 2) # ip virtual-router address 10.252.0.1 sw-spine-001 [standalone: master] (config interface vlan 2 magp 2) # ip virtual-router mac-address 00:00:5E:00:01:02 sw-spine-001 [standalone: master] (config interface vlan 2 magp 2) # exit sw-spine-001 [standalone: master] (config) # interface vlan 4 magp 4 sw-spine-001 [standalone: master] (config interface vlan 4 magp 4) # ip virtual-router address 10.254.0.1 sw-spine-001 [standalone: master] (config interface vlan 4 magp 4) # ip virtual-router mac-address 00:00:5E:00:01:04 sw-spine-001 [standalone: master] (config interface vlan 4 magp 4) # exit sw-spine-001 [standalone: master] (config) # interface vlan 7 magp 7 sw-spine-001 [standalone: master] (config interface vlan 7 magp 7) # ip virtual-router address 10.103.8.1 sw-spine-001 [standalone: master] (config interface vlan 7 magp 7) # ip virtual-router mac-address 00:00:5E:00:01:07 sw-spine-001 [standalone: master] (config interface vlan 7 magp 7) # exit sw-spine-001 [standalone: master] (config) # interface vlan 10 magp 10 sw-spine-001 [standalone: master] (config interface vlan 10 magp 10) # ip virtual-router address 10.11.0.1 sw-spine-001 [standalone: master] (config interface vlan 10 magp 10) # ip virtual-router mac-address 00:00:5E:00:01:10 sw-spine-001 [standalone: master] (config interface vlan 10 magp 10) # exit sw-spine-001 [standalone: master] (config) # exit sw-spine-001 [standalone: master] # write memory sw-spine-001 [standalone: master] # exit pit#   Save the running-config from all switches after completion.\nNOTE: Mellanox switches require the enable command before doing show running-config.\npit# ssh admin@10.1.0.2 switch# show running-config switch# exit Save this information in a text file for comparison with the running-config saved before all changes were made.\npit# vi after.sw-spine01.txt Repeat this for all of the switches. The example system has switches up to 10.1.0.7.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/network/external_dns/ingress_routing/",
	"title": "Ingress Routing",
	"tags": [],
	"description": "",
	"content": "Ingress Routing Ingress routing to services via Istio\u0026rsquo;s ingress gateway is configured by VirtualService custom resource definitions (CRD). When using external hostnames, there needs to be a VirtualService CRD that matches the external hostname to the desired destination.\nFor example, the configuration below controls the ingress routing for prometheus.SYSTEM_DOMAIN_NAME:\nncn-w001# kubectl get vs -n sysmgmt-health cray-sysmgmt-health-prometheus NAME GATEWAYS HOSTS AGE cray-sysmgmt-health-prometheus [services/services-gateway] [prometheus.SYSTEM_DOMAIN_NAME] 22h ncn-w001# kubectl get vs -n sysmgmt-health cray-sysmgmt-health-prometheus -o yaml apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: creationTimestamp: \u0026#34;2020-07-09T17:49:07Z\u0026#34; generation: 1 labels: app: cray-sysmgmt-health-prometheus app.kubernetes.io/instance: cray-sysmgmt-health app.kubernetes.io/managed-by: Tiller app.kubernetes.io/name: cray-sysmgmt-health app.kubernetes.io/version: 8.15.4 helm.sh/chart: cray-sysmgmt-health-0.3.1 name: cray-sysmgmt-health-prometheus namespace: sysmgmt-health resourceVersion: \u0026#34;41620\u0026#34; selfLink: /apis/networking.istio.io/v1beta1/namespaces/sysmgmt-health/virtualservices/cray-sysmgmt-health-prometheus uid: d239dfcc-a827-4a51-9b73-6eccfb937088 spec: gateways: - services/services-gateway hosts: - prometheus.SYSTEM_DOMAIN_NAME http: - match: - authority: exact: prometheus.SYSTEM_DOMAIN_NAME route: - destination: host: cray-sysmgmt-health-promet-prometheus port: number: 9090 By matching the external hostname in the authority field, Istio\u0026rsquo;s ingress gateway is able to route incoming traffic from Keycloak Gatekeeper to the cray-sysmgmt-health-prometheus service in the sysmgmt-health namespace. Also, notice that the VirtualService for prometheus.SYSTEM_DOMAIN_NAME uses the existing services/services-gateway Gateway CRD and does not create a new one.\nSecure Ingress via Keycloak Gatekeeper Web apps intended to be accessed via the browser, such as Prometheus, Alertmanager, Grafana, Kiali, Jaeger, Kibana, Elasticsearch, should go through the Keycloak Gatekeeper reverse proxy. Browser sessions are automatically configured to use a JSON Web Token (JWT) for authorization to Istio\u0026rsquo;s ingress gateway, enabling a central enforcement point of Open Policy Agent (OPA) policies for system management traffic.\nThe Keycloak Gatekeeper will inject HTTP headers so that an upstream endpoint can identify the user and customize access as needed. To enable ingress via Keycloak Gatekeeper external hostnames, web apps need to be added to the keycloak_gatekeeper_proxied_hosts Ansible variable.\n"
},
{
	"uri": "/docs-csm/en-11/operations/network/external_dns/troubleshoot_systems_not_provisioned_with_external_ip_addresses/",
	"title": "Troubleshoot Connectivity To Services With External Ip Addresses",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Connectivity to Services with External IP addresses Systems that do not support CAN will not have services provisioned with external IP addresses on CAN. Kubernetes will report a \u0026lt;pending\u0026gt; status for the external IP address of the service experiencing connectivity issues.\nIf SSH access to a non-compute node (NCN) is available, it is possible to override resolution of external hostnames and forward local ports into the cluster for the cluster IP address of the corresponding service.\nWarning: This will bypass the Keycloak gatekeeper and Istio ingress gateway, which handle authentication and authorization.\nEnable systems without CAN to provision services with external hostnames.\nPrerequisites The Customer Access Network (CAN) is not supported on the system.\nProcedure   Search for the VirtualService object that corresponds to the desired service.\nThe command below will list all external hostnames.\nncn-w001# kubectl get vs -A | grep -v \u0026#39;[*]\u0026#39; NAMESPACE NAME GATEWAYS HOSTS AGE istio-system kiali [services/services-gateway] [kiali-istio.SYSTEM_DOMAIN_NAME] 2d16h istio-system prometheus [services/services-gateway] [prometheus-istio.SYSTEM_DOMAIN_NAME] 2d16h istio-system tracing [services/services-gateway] [jaeger-istio.SYSTEM_DOMAIN_NAME] 2d16h nexus nexus [services/services-gateway] [packages.local registry.local nexus.SYSTEM_DOMAIN_NAME] 2d16h services gitea-vcs-external [services/services-gateway] [vcs.SYSTEM_DOMAIN_NAME] 2d16h services sma-grafana [services-gateway] [sma-grafana.SYSTEM_DOMAIN_NAME] 2d16h services sma-kibana [services-gateway] [sma-kibana.SYSTEM_DOMAIN_NAME] 2d16h sysmgmt-health cray-sysmgmt-health-alertmanager [services/services-gateway] [alertmanager.SYSTEM_DOMAIN_NAME] 2d16h sysmgmt-health cray-sysmgmt-health-grafana [services/services-gateway] [grafana.SYSTEM_DOMAIN_NAME] 2d16h sysmgmt-health cray-sysmgmt-health-prometheus [services/services-gateway] [prometheus.SYSTEM_DOMAIN_NAME] 2d16h   Lookup the cluster IP and port for service.\nThe example below is for the cray-sysmgmt-health-promet-prometheus service.\nncn-w001# kubectl -n sysmgmt-health get service cray-sysmgmt-health-promet-prometheus NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE cray-sysmgmt-health-promet-prometheus ClusterIP 10.25.124.159 \u0026lt;none\u0026gt; 9090/TCP 23h   Setup port forwarding from a laptop or workstation to access the service.\nUse the cluster IP and port for the service obtained in the previous step. If the port is unprivileged, use the same port number on the local side.\nReplace the cluster IP, port, and system name values in the example below.\n$ ssh -L 9090:10.22.78.59:9090 root@SYSTEM_NCN_DOMAIN_NAME   Visit http://localhost:9090/ in a laptop or workstation browser.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/network/external_dns/troubleshoot_dns_configuration_issues/",
	"title": "Troubleshoot DNS Configuration Issues",
	"tags": [],
	"description": "",
	"content": "Troubleshoot DNS Configuration Issues Troubleshoot issues when DNS is not properly configured to delegate name resolution to the core DNS instance on a specific cluster. Although the CAN IP address may still be routable using the IP address directly, it may not work because Istio\u0026rsquo;s ingress gateway depends on the hostname (or SNI) to route traffic. For command line tools like cURL, using the \u0026ndash;resolve option to force correct resolution can be used to work around this issue.\nTo get names to resolve correctly in a browser, modifying /etc/hosts to map the external hostname to the appropriate CAN IP address may be necessary. In either case, knowing the correct CAN IP address is required to use the cURL --resolve option or to update /etc/hosts.\nAssuming CAN, BGP, MetalLB, and external DNS are properly configured on a system, name resolution requests can be sent directly to the desired DNS server.\nGain access to system services when external DNS is not configured properly.\nPrerequisites The Domain Name Service (DNS) is not configured properly.\nProcedure   View the DNS configuration on the system.\nncn-w001# kubectl -n services get svc cray-externaldns-coredns-udp NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE cray-externaldns-coredns-udp LoadBalancer 10.25.156.88 10.102.14.113 53:32674/UDP 45h   Confirm that DNS is configured properly.\nRun the following command from a laptop or workstation.\n$ dig SERVICE.SYSTEM_DOMAIN_NAME +short 10.102.14.131 If an IP address is returned, DNS is configured properly and the remaining steps in this procedure can be skipped. If an IP address is not returned, proceed to the next step.\n  Use the IP address to direct DNS requests directly to the cray-externaldns-coredns-udp service.\nReplace the example IP address (10.102.14.131) with the EXTERNAL-IP value returned in step 1. If an IP address is returned, it means upstream IT DNS is not configured correctly.\n$ dig SERVICE.SYSTEM_DOMAIN_NAME +short @10.102.14.113 10.102.14.131   Direct DNS requests to the cluster IP address from an NCN.\nReplace the example cluster IP address (10.25.156.88) with the CLUSTER-IP value returned in step 1. If an IP address is returned, external DNS is configured on the cluster and something is likely wrong with CAN/BGP.\nncn-w001# dig SERVICE.SYSTEM_DOMAIN_NAME +short @10.25.156.88 10.102.14.131   Access services in the event that external DNS is down, the backing etcd database is having issues, or something was configured incorrectly.\nSearch through Kubernetes service objects for external-dns.alpha.kubernetes.io/hostname annotations to find the corresponding external IP. The kubectl command makes it easy to generate an /etc/hosts compatible listing of IP addresses to hostnames using the go-template output format shown below.\nncn-m001# kubectl get svc --all-namespaces -o go-template --template \\ '{{ range .items }}{{ $lb := .status.loadBalancer }}{{ with .metadata.annotations }} {{ with (index . \u0026quot;external-dns.alpha.kubernetes.io/hostname\u0026quot;) }} {{ $hostnames := . }}{{ with $lb }}{{ range .ingress }} {{ printf \u0026quot;%s\\t%s\\n\u0026quot; .ip $hostnames }}{{ end }}{{ end }} {{ end }}{{ end }}{{ end }}' | sort -u | tr , ' ' 10.101.5.128 opa-gpm.SYSTEM_DOMAIN_NAME nexus.SYSTEM_DOMAIN_NAME grafana-istio.SYSTEM_DOMAIN_NAME jaeger-istio.SYSTEM_DOMAIN_NAME prometheus-istio.SYSTEM_DOMAIN_NAME kiali-istio.SYSTEM_DOMAIN_NAME prometheus.SYSTEM_DOMAIN_NAME alertmanager.SYSTEM_DOMAIN_NAME grafana.SYSTEM_DOMAIN_NAME vcs.SYSTEM_DOMAIN_NAME sma-grafana.SYSTEM_DOMAIN_NAME sma-kibana.SYSTEM_DOMAIN_NAME csms.SYSTEM_DOMAIN_NAME 10.101.5.129 api.SYSTEM_DOMAIN_NAME auth.SYSTEM_DOMAIN_NAME 10.101.5.130 s3.SYSTEM_DOMAIN_NAME 10.92.100.222 cray-dhcp-kea 10.92.100.225 cray-dns-unbound 10.94.100.222 cray-dhcp-kea 10.94.100.225 cray-dns-unbound   "
},
{
	"uri": "/docs-csm/en-11/operations/network/external_dns/update_the_can-external-dns_value_post-installation/",
	"title": "Update The CAN-external-DNS Value Post-installation",
	"tags": [],
	"description": "",
	"content": "Update the can-external-dns Value Post-Installation By default, the services/cray-externaldns-coredns-tcp and services/cray-externaldns-coredns-udp services both share the same Customer Access Network (CAN) external IP as defined by the can-external-dns value. This value is specified during the csi config init input.\nIt is expected to be in the static range reserved in MetalLB\u0026rsquo;s can-dynamic-pool subnet. Theoretically, this is the only CAN IP address that must be known external to the system so IT DNS can delegate the system-name.site-domain zone to services/cray-externaldns-coredns deployments.\nChanging it after install is relatively straightforward, and only requires the external IP address for services/cray-externaldns-coredns-tcp and services/cray-externaldns-coredns-udp services to be changed. This procedure will update the IP addresses that DNS queries.\nPrerequisites The system is installed.\nProcedure   Find the external IP address for the services/cray-externaldns-coredns-tcp and services/cray-externaldns-coredns-udp services.\nncn-w001# kubectl -n services get svc | grep cray-externaldns-coredns- cray-externaldns-coredns-tcp LoadBalancer 10.25.211.48 10.102.14.113 53:31111/TCP 2d2h cray-externaldns-coredns-udp LoadBalancer 10.25.156.88 10.102.14.113 53:32674/UDP 2d2h   Edit the services and change spec.loadBalancerIP to the desired CAN IP address.\n  Edit the cray-externaldns-coredns-tcp service.\nncn-w001# kubectl -n services edit svc cray-externaldns-coredns-tcp   Edit the cray-externaldns-coredns-udp service.\nncn-w001# kubectl -n services edit svc cray-externaldns-coredns-udp     "
},
{
	"uri": "/docs-csm/en-11/operations/network/external_dns/update_the_system-name_site-domain_value_post-installation/",
	"title": "Update The System-name.site-domain Value Post-installation",
	"tags": [],
	"description": "",
	"content": "Update the system-name.site-domain Value Post-Installation Update the domain name specified by the csi config init input. Updating the system-name.site-domain value without reinstalling the platform involves the following actions:\n Edit the Service object definitions and update all external-dns.alpha.kubernetes.io/hostname annotations. Edit the VirtualService (and possibly Gateway) object definitions and update spec.hosts and spec.http[].match[].authority settings.  Enables access to services that are accessible from the Customer Access Network (CAN) by updating the external domain for external hostnames.\nPrerequisites The system is installed.\nProcedure   Update the VirtualService object definitions for cray-sysmgmt-health-prometheus.\nncn-w001# kubectl get vs -n sysmgmt-health cray-sysmgmt-health-prometheus -o yaml apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: creationTimestamp: \u0026#34;2020-07-09T17:49:07Z\u0026#34; generation: 1 labels: app: cray-sysmgmt-health-prometheus app.kubernetes.io/instance: cray-sysmgmt-health app.kubernetes.io/managed-by: Tiller app.kubernetes.io/name: cray-sysmgmt-health app.kubernetes.io/version: 8.15.4 helm.sh/chart: cray-sysmgmt-health-0.3.1 name: cray-sysmgmt-health-prometheus namespace: sysmgmt-health resourceVersion: \u0026#34;41620\u0026#34; selfLink: /apis/networking.istio.io/v1beta1/namespaces/sysmgmt-health/virtualservices/cray-sysmgmt-health-prometheus uid: d239dfcc-a827-4a51-9b73-6eccfb937088 spec: gateways: - services/services-gateway hosts: - prometheus.vshasta.io http: - match: - authority: exact: prometheus.vshasta.io route: - destination: host: cray-sysmgmt-health-promet-prometheus port: number: 9090   Edit the CoreDNS configmap and update the domain name for any hostnames being managed by etcd.\nncn-w001# kubectl get configmap -n services cray-externaldns-coredns -o yaml apiVersion: v1 data: Corefile: |- .:53 { errors health log ready kubernetes cluster.local k8s_external internal.shasta prometheus 0.0.0.0:9153 etcd SYSTEM_DOMAIN_NAME { stubzones path /skydns endpoint http://cray-externaldns-etcd-client:2379 } } kind: ConfigMap metadata: creationTimestamp: \u0026#34;2020-01-11T17:35:21Z\u0026#34; labels: app.kubernetes.io/instance: cray-externaldns app.kubernetes.io/managed-by: Tiller app.kubernetes.io/name: coredns helm.sh/chart: coredns-1.5.6 name: cray-externaldns-coredns namespace: services resourceVersion: \u0026#34;4495\u0026#34; selfLink: /api/v1/namespaces/services/configmaps/cray-externaldns-coredns uid: bdaeb7ff-3498-11ea-8143-a4bf01581d70   Restart the cray-externaldns-coredns deployment.\n  Delete the current deployment.\nncn-w001# kubectl delete -f cray-externaldns-coredns.yaml   Restart the deployment.\nncn-w001# kubectl apply -f cray-externaldns-coredns.yaml     Update Keycloak for the gatekeeper client.\n  Log into Keycloak.\nThe URL for accessing keycloak is listed below. Replace the variables in the URL with the actual NCN\u0026rsquo;s DNS name.\nhttps://auth.SYSTEM_DOMAIN_NAME/keycloak   Navigate to the gatekeeper client.\n  Update the Valid Redirect URIs and Web Origins sections for the gatekeeper client.\n    Update the hostnames listed in the --self-signed-tls-hostname arguments in the cray-keycloak-gatekeeper-ingress deployment configuration, as well asspec.template.spec.containers.hostAliases[].hostnames[].\nncn-w001# kubectl get deployment -n services cray-keycloak-gatekeeper-ingress -o yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: annotations: deployment.kubernetes.io/revision: \u0026#34;1\u0026#34; creationTimestamp: \u0026#34;2020-01-11T17:41:05Z\u0026#34; generation: 1 labels: app.kubernetes.io/instance: cray-keycloak-gatekeeper app.kubernetes.io/managed-by: Tiller app.kubernetes.io/name: cray-keycloak-gatekeeper app.kubernetes.io/version: \u0026#34;1.0\u0026#34; helm.sh/chart: cray-keycloak-gatekeeper-0.1.2-shasta-1.1 name: cray-keycloak-gatekeeper-ingress namespace: services resourceVersion: \u0026#34;9024\u0026#34; selfLink: /apis/extensions/v1beta1/namespaces/services/deployments/cray-keycloak-gatekeeper-ingress uid: 8afd3780-3499-11ea-8143-a4bf01581d70 spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: cray-keycloak-gatekeeper-ingress strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: annotations: sidecar.istio.io/inject: \u0026#34;false\u0026#34; creationTimestamp: null labels: app: cray-keycloak-gatekeeper-ingress spec: containers: - args: - --enable-logging=true - --verbose - --skip-openid-provider-tls-verify - --skip-upstream-tls-verify - --enable-self-signed-tls - --self-signed-tls-hostnames - $(INSTANCE_IP) - --self-signed-tls-hostnames - prometheus.SYSTEM_DOMAIN_NAME - --self-signed-tls-hostnames - alertmanager.SYSTEM_DOMAIN_NAME - --self-signed-tls-hostnames - grafana.SYSTEM_DOMAIN_NAME - --self-signed-tls-hostnames - prometheus-istio.SYSTEM_DOMAIN_NAME - --self-signed-tls-hostnames - grafana-istio.SYSTEM_DOMAIN_NAME - --self-signed-tls-hostnames - kiali-istio.SYSTEM_DOMAIN_NAME - --self-signed-tls-hostnames - jaeger-istio.SYSTEM_DOMAIN_NAME - --self-signed-tls-hostnames - prometheus-kube.SYSTEM_DOMAIN_NAME - --self-signed-tls-hostnames - alertmanager-kube.SYSTEM_DOMAIN_NAME - --self-signed-tls-hostnames - grafana-kube.SYSTEM_DOMAIN_NAME - --self-signed-tls-hostnames - prometheus-ceph.SYSTEM_DOMAIN_NAME - --self-signed-tls-hostnames - vcs.SYSTEM_DOMAIN_NAME - --secure-cookie=false - --preserve-host - --resources - uri=/* env: - name: PROXY_LISTEN value: 0.0.0.0:443 - name: PROXY_DISCOVERY_URL valueFrom: secretKeyRef: key: discovery-url name: keycloak-gatekeeper-client - name: PROXY_CLIENT_ID valueFrom: secretKeyRef: key: client-id name: keycloak-gatekeeper-client - name: PROXY_CLIENT_SECRET valueFrom: secretKeyRef: key: client-secret name: keycloak-gatekeeper-client - name: PROXY_UPSTREAM_URL value: https://istio-ingressgateway.istio-system.svc.cluster.local/ - name: INSTANCE_IP valueFrom: fieldRef: apiVersion: v1 fieldPath: status.podIP - name: GODEBUG value: netdns=cgo image: registry.local/keycloak/keycloak-gatekeeper:latest imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 10 httpGet: path: /oauth/health port: 443 scheme: HTTPS initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 5 name: keycloak-gatekeeper ports: - containerPort: 443 name: https protocol: TCP readinessProbe: failureThreshold: 3 httpGet: path: /oauth/health port: 443 scheme: HTTPS periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst hostAliases: - hostnames: - auth.SYSTEM_DOMAIN_NAME ip: 10.92.100.50 restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 status: availableReplicas: 1 conditions: - lastTransitionTime: \u0026#34;2020-01-11T17:41:12Z\u0026#34; lastUpdateTime: \u0026#34;2020-01-11T17:41:12Z\u0026#34; message: Deployment has minimum availability. reason: MinimumReplicasAvailable status: \u0026#34;True\u0026#34; type: Available - lastTransitionTime: \u0026#34;2020-01-11T17:41:05Z\u0026#34; lastUpdateTime: \u0026#34;2020-01-11T17:41:12Z\u0026#34; message: ReplicaSet \u0026#34;cray-keycloak-gatekeeper-ingress-6c89dd9654\u0026#34; has successfully progressed. reason: NewReplicaSetAvailable status: \u0026#34;True\u0026#34; type: Progressing observedGeneration: 1 readyReplicas: 1 replicas: 1 updatedReplicas: 1   Restart the cray-keycloak-gatekeeper-ingress deployment.\n  Delete the current deployment.\nncn-w001# kubectl delete -f cray-keycloak-gatekeeper-ingress.yaml   Restart the deployment.\nncn-w001# kubectl apply -f cray-keycloak-gatekeeper-ingress.yaml     Generate and install new certificates for Istio.\nThe existing certificate has a SAN that includes *.{{shasta_domain}}.\nncn-w001# kubectl -n istio-system get secrets istio-ingressgateway-certs -o jsonpath=\u0026#39;{.data.tls\\.crt}\u0026#39;| \\ base64 -d | openssl x509 -text -noout Certificate: Data: Version: 3 (0x2) Serial Number: 1 (0x1) Signature Algorithm: sha256WithRSAEncryption Issuer: C = XX, ST = XX, L = XX, O = XX, OU = XX, CN = 07bc1749-f2e0-4e03-ac57-0150af38ace4 Validity Not Before: Jan 11 17:28:22 2020 GMT Not After : Jan 8 17:28:22 2030 GMT Subject: C = XX, ST = XX, O = XX, OU = XX, CN = sms Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (2048 bit) Modulus: 00:fe:90:1c:16:26:8c:d7:7f:9d:61:81:e7:97:70: 30:38:bf:c6:ea:e9:ed:4d:57:93:27:f1:c0:df:6e: 6a:51:f2:55:e1:c7:29:e4:c2:8f:b7:46:14:ba:34: 4a:7b:19:c0:a1:65:e4:ee:67:36:ad:91:d5:9c:08: fb:b7:53:31:72:4b:e6:7b:78:38:d0:02:7e:24:d7: a7:e6:0d:71:7e:94:66:a8:d4:94:87:bc:6a:8c:ea: 1f:9a:a0:89:27:63:de:33:a3:8f:0d:b6:54:f8:08: d8:4a:c1:a3:6a:f7:03:2a:a8:6c:a9:69:ba:ca:b5: 55:4d:06:fa:4a:34:74:e1:b2:8c:b6:a8:b2:06:33: ce:04:a7:cc:07:e5:a5:33:e1:4c:bf:88:19:66:ab: ee:a6:1c:69:3a:36:f0:13:59:78:e2:c6:04:37:44: 01:5e:72:81:86:39:f2:5b:99:22:9a:38:13:4b:fc: f3:9c:06:80:04:38:e3:7d:13:14:99:aa:7b:42:dd: c1:4c:bc:75:ba:5c:a1:6f:e9:9e:16:5b:b8:26:ff: 0d:e2:8b:65:83:e0:1c:e1:82:1e:8d:5a:d0:95:0b: f1:9a:25:3f:00:20:16:ab:e4:c8:b5:e8:e1:74:83: 75:22:23:df:d8:40:cb:1e:8c:be:8d:7e:fd:a9:36: f5:4b Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Basic Constraints: CA:FALSE X509v3 Subject Key Identifier: 2B:2C:75:24:55:37:7E:EE:D5:8B:EB:75:FC:6B:6C:7E:50:B3:40:E2 X509v3 Authority Key Identifier: keyid:E2:A3:D6:77:B9:BB:98:55:D6:15:00:2A:A7:6F:AA:E0:56:82:CE:D0 X509v3 Subject Alternative Name: DNS:ncn-m001.local, DNS:ncn-m002.local, DNS:ncn-m003.local, DNS:istio-ingressgateway.istio-system.svc.cluster.local, DNS:*.SYSTEM_DOMAIN_NAME, DNS:mgmt-plane-cmn.local, DNS:api-gw-service-nmn.local Signature Algorithm: sha256WithRSAEncryption 0c:e9:a5:9b:58:2f:b3:c8:4c:c8:20:4b:0f:7d:07:f0:4a:7b: f4:8f:31:de:27:1a:59:17:bc:d6:c3:a5:af:ff:8d:f5:a9:58: a2:47:ed:e1:8d:3e:48:30:8d:b1:2d:76:be:5e:35:25:34:57: 0d:93:bc:bb:db:ef:dc:9a:30:c6:8b:a7:7d:6e:fa:c8:64:c9: e8:fc:d7:39:30:d1:b2:d4:59:32:f2:c3:0b:17:8c:b0:58:2e: f0:1f:29:ac:65:2f:ad:ca:eb:25:18:15:e4:9b:18:c0:17:bb: 20:f0:2c:a6:1f:11:b1:b3:0c:1e:8d:ba:09:08:32:9e:16:cb: a7:e6:48:d9:fc:f2:e7:0c:82:20:88:a9:c4:7a:83:7a:b0:b5: 36:dd:4e:a0:ae:0b:c4:11:56:db:97:fd:47:c5:63:c8:8d:76: 91:47:d5:6b:08:4b:00:99:cc:4b:f6:d8:82:9c:dd:a8:37:73: 71:18:64:41:e7:8f:a8:26:ed:fe:4f:64:ce:50:76:3c:aa:ff: ae:cb:6e:d0:24:47:40:16:ab:29:6e:f2:50:9b:24:46:d0:1d: 58:41:32:eb:3c:aa:e4:4f:25:b1:f5:92:05:b4:1c:fa:14:2c: d9:b5:39:40:28:21:35:92:3d:79:96:96:22:f4:42:85:ef:63: bb:c0:10:4e   "
},
{
	"uri": "/docs-csm/en-11/operations/network/external_dns/add_ncns_and_uans_to_external_dns/",
	"title": "Add NCNs And UANs To External DNS",
	"tags": [],
	"description": "",
	"content": "Add NCNs and UANs to External DNS Edit the cray-externaldns-coredns ConfigMap to associate names with the Customer Access Network (CAN) IP addresses for non-compute nodes (NCNs) and User Access Nodes (UANs) in external DNS.\nThe cray-externaldns-coredns file contains the configuration files for external DNS' CoreDNS instance.\nPrerequisites This procedure requires administrative privileges.\nProcedure   View the existing cray-externaldns-coredns ConfigMap.\nncn-w001# kubectl -n services get configmap cray-externaldns-coredns -o jsonpath=\u0026#39;{.data}\u0026#39; map[Corefile:.:53 { errors health log ready kubernetes cluster.local k8s_external internal.shasta prometheus 0.0.0.0:9153 etcd SYSTEM_DOMAIN_NAME { stubzones path /skydns endpoint http://cray-externaldns-etcd-client:2379 } }]   Edit the cray-externaldns-coredns ConfigMap to add support for a static hosts file.\nncn-w001# kubectl -n services edit configmaps cray-externaldns-coredns Add a \u0026ldquo;hosts\u0026rdquo; file configuration (analogous to /etc/hosts) and update the Corefile configuration so CoreDNS uses it. For example:\ndata: Corefile: |- .:53 { errors health log ready kubernetes cluster.local k8s_external internal.shasta prometheus 0.0.0.0:9153 hosts /etc/coredns/hosts { fallthrough } etcd SYSTEM_DOMAIN_NAME { stubzones path /skydns endpoint http://cray-externaldns-etcd-client:2379 } } hosts: |- # IP FQDN short 10.99.1.51 dn07.SYSTEM_DOMAIN_NAME dn07   Restart the CoreDNS pods once the ConfigMap is updated.\nncn-w001# kubectl -n services rollout status deployment cray-externaldns-coredns Waiting for deployment \u0026#34;cray-externaldns-coredns\u0026#34; rollout to finish: 1 old replicas are pending termination... Waiting for deployment \u0026#34;cray-externaldns-coredns\u0026#34; rollout to finish: 1 old replicas are pending termination... Waiting for deployment \u0026#34;cray-externaldns-coredns\u0026#34; rollout to finish: 1 old replicas are pending termination... Waiting for deployment \u0026#34;cray-externaldns-coredns\u0026#34; rollout to finish: 1 of 2 updated replicas are available... deployment \u0026#34;cray-externaldns-coredns\u0026#34; successfully rolled out   "
},
{
	"uri": "/docs-csm/en-11/operations/network/external_dns/external_dns/",
	"title": "External DNS",
	"tags": [],
	"description": "",
	"content": "External DNS External DNS, along with the Customer Access Network (CAN), Border Gateway Protocol (BGP), and MetalLB, makes it simpler to access the HPE Cray EX API and system management services. Services are accessible directly from a laptop without needing to tunnel into a non-compute node (NCN) or override /etc/hosts settings. Some services may require a JSON Web Token (JWT) to access them, while others may require Keycloak to login using a DC LDAP password.\nThe following services are currently available:\n HPE Cray EX API (requires valid JWT) Keycloak Ceph RADOS gateway (requires valid JWT) Nexus System Management Health Prometheus (redirects to Keycloak for SSO) System Management Health Grafana (redirects to Keycloak for SSO) System Management Health Alertmanager (redirects to Keycloak for SSO) Kiali, for Istio service mesh visibility (redirects to Keycloak for SSO) Jaeger, for Istio tracing (redirects to Keycloak for SSO)  In general, external hostnames should resolve to a CAN external IP address for the following services:\n istio-system/istio-ingressgateway-can - Istio\u0026rsquo;s ingress gateway. services/cray-keycloak-gatekeeper-ingress - Keycloak Gatekeeper\u0026rsquo;s ingress reverse proxy that redirects browsers to Keycloak for log in, and then to Istio\u0026rsquo;s ingress gateway with a valid JWT for authorized access.  This can be verified using the dig command to resolve the external hostname and compare it with Kubernetes.\nWhat Happens if External DNS is not Used? Without forwarding to External DNS, administrators will not have the ability to use the externally exposed services, such as Prometheus, Grafana, the HPE Cray EX REST API, and more. See Externally Exposed Services for more information.\nAccessing most of these services by IP address will not work because the Ingress Gateway uses the name to direct requests to the appropriate service.\nDNS for HPE Cray EX Systems There is a separate set of DNS instances within HPE Cray EX that is used by the nodes and pods within the system for resolving names.\n  Unbound\nThe unbound DNS instance is used to resolve names for the physical equipment on the management networks within HPE Cray EX, such as NCNs, UANs, switches, compute nodes, and more. This instance is accessible only within the system.\n  K8s CoreDNS\nThere is a CoreDNS instance within Kubernetes that is used by Kubernetes pods to resolve names for internal pods and services. This instance is accessible only within the HPE Cray EX Kubernetes cluster.\n  Connect Customer DNS to External DNS The DNS instance at the customer site should use DNS forwarding to forward the subdomain specified by the system-name and site-domain values (combined to make the system-name.site-domain value) to the IP address specified by the can-external-dns value. These values are defined with the csi config init command. The specifics on how to do the forwarding configuration is dependent on the type of DNS used by the customer.\nThe External DNS instance currently does not support zone transfer.\n"
},
{
	"uri": "/docs-csm/en-11/operations/network/external_dns/external_dns_csi_config_init_input_values/",
	"title": "External DNS Csi Config Init Input Values",
	"tags": [],
	"description": "",
	"content": "External DNS csi config init Input Values External DNS requires the system-name, site-domain, and can-external-dns values that are defined with the csi config init command. These values are used to customize the External DNS configuration during installation.\nThe system-name and site-domain Values The system-name and site-domain values specified as part of the csi config init are used together in the system-name.site-domain format, creating the external domain for external hostnames for services accessible from the Customer Access Network (CAN). Changing this value requires updating all impacted external-dns.alpha.kubernetes.io/hostname annotations, VirtualService and possibly Gateway objects, the CoreDNS configmap, Keycloak settings for valid OAuth callback URLs, Keycloak gatekeeper configuration, and generating new certificates.\nWarning: Changing the system-name.site-domain value post-installation is not recommended because of the complexity of changes required.\nInput for csi config init:\n--system-name testsystem --site-domain example.com The can-external-dns Value The can-external-dns value is the IP address that DNS queries under the combined system-name.site-domain values need to be delegated.\nThis will be the shared IP address for services/cray-externaldns-coredns-tcp and services/cray-externaldns-coredns-udp services, which must be an IP address in the customer_access_static_metallb_address_pool subnet defined in the csi config init input. See Customer Access Network (CAN) for more information.\nChanging this value requires updating the loadBalancerIP value of the services/cray-externaldns-coredns-tcp and services/cray-externaldns-coredns-udp services.\nInput for csi config init:\n--can-external-dns 10.102.5.30 This input is the CAN IP address for resolution of system services.\n"
},
{
	"uri": "/docs-csm/en-11/operations/network/external_dns/external_dns_failing_to_discover_services_workaround/",
	"title": "External DNS Failing To Discover Services Workaround",
	"tags": [],
	"description": "",
	"content": "External DNS Failing to Discover Services Workaround Many external DNS issues can be worked around by directly connecting to the desired backend service. This can circumvent authentication and authorization protections, but it may be necessary to access specific services when mitigating critical issues.\nIstio\u0026rsquo;s ingress gateway uses Gateway and VirtualService objects to configure how traffic is routed to backend services. Currently, there is only one Gateway supporting the Customer Access Network (CAN), which is services/services-gateway. It is configured to support traffic for any host. Consequently, it is the VirtualService objects that ultimately control routing based on hostname.\nUse this procedure to resolve any external DNS routing issues with backend services.\nPrerequisites This procedure requires administrative privileges.\nProcedure   Search for the VirtualService object that corresponds to the desired service.\nThe command below will list all external hostnames.\nncn-w001# kubectl get vs -A | grep -v \u0026#39;[*]\u0026#39; NAMESPACE NAME GATEWAYS HOSTS AGE istio-system kiali [services/services-gateway] [kiali-istio.SYSTEM_DOMAIN_NAME] 2d16h istio-system prometheus [services/services-gateway] [prometheus-istio.SYSTEM_DOMAIN_NAME] 2d16h istio-system tracing [services/services-gateway] [jaeger-istio.SYSTEM_DOMAIN_NAME] 2d16h nexus nexus [services/services-gateway] [packages.local registry.local nexus.SYSTEM_DOMAIN_NAME] 2d16h services gitea-vcs-external [services/services-gateway] [vcs.SYSTEM_DOMAIN_NAME] 2d16h services sma-grafana [services-gateway] [sma-grafana.SYSTEM_DOMAIN_NAME] 2d16h services sma-kibana [services-gateway] [sma-kibana.SYSTEM_DOMAIN_NAME] 2d16h sysmgmt-health cray-sysmgmt-health-alertmanager [services/services-gateway] [alertmanager.SYSTEM_DOMAIN_NAME] 2d16h sysmgmt-health cray-sysmgmt-health-grafana [services/services-gateway] [grafana.SYSTEM_DOMAIN_NAME] 2d16h sysmgmt-health cray-sysmgmt-health-prometheus [services/services-gateway] [prometheus.SYSTEM_DOMAIN_NAME] 2d16h   Inspect the VirtualService object(s) to learn the destination service and port.\nUse the NAME value returned in the previous step. The following example is for the cray-sysmgmt-health-prometheus service.\nncn-w001# kubectl get vs -n sysmgmt-health cray-sysmgmt-health-prometheus -o yaml apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: creationTimestamp: \u0026#34;2020-07-09T17:49:07Z\u0026#34; generation: 1 labels: app: cray-sysmgmt-health-prometheus app.kubernetes.io/instance: cray-sysmgmt-health app.kubernetes.io/managed-by: Tiller app.kubernetes.io/name: cray-sysmgmt-health app.kubernetes.io/version: 8.15.4 helm.sh/chart: cray-sysmgmt-health-0.3.1 name: cray-sysmgmt-health-prometheus namespace: sysmgmt-health resourceVersion: \u0026#34;41620\u0026#34; selfLink: /apis/networking.istio.io/v1beta1/namespaces/sysmgmt-health/virtualservices/cray-sysmgmt-health-prometheus uid: d239dfcc-a827-4a51-9b73-6eccfb937088 spec: gateways: - services/services-gateway hosts: - prometheus.SYSTEM_DOMAIN_NAME http: - match: - authority: exact: prometheus.SYSTEM_DOMAIN_NAME route: - destination: host: cray-sysmgmt-health-promet-prometheus port: number: 9090 From the VirtualService data, it is straightforward to see how traffic will be routed. In this example, connections to prometheus.SYSTEM_DOMAIN_NAME will be routed to the cray-sysmgmt-health-prometheus service in the sysmgmt-health namespace on port 9090.\n  External DNS will now be connected to the back-end service.\n"
},
{
	"uri": "/docs-csm/en-11/operations/network/dns/enable_ncsd_on_uans/",
	"title": "Enable Ncsd On UANs",
	"tags": [],
	"description": "",
	"content": "Enable ncsd on UANs Configure User Access Nodes (UANs) to start the ncsd service at boot time.\nThe nscd service is not currently enabled by default and systemd does not start it at boot time. There are two ways to start nscd on UAN nodes: manually starting the service or enabling the service in the UAN image. While restarting nscd manually has to be performed each time the UAN is rebooted, enabling nscd in the image only has to be done once. Then all UANs that use the image will have nscd started automatically on boot.\nProcedure   Start ncsd manually on each UAN.\n  Log into a UAN.\n  Start ncsd using systemctl.\nuan# systemctl start nscd   Repeat the previous two substeps for every UAN.\n    Enable ncsd in the UAN image.\n  Determine the ID of the image used by the UAN. This ID can be found in the BOS session template used to boot the UAN:\n{ \u0026#34;boot_sets\u0026#34;: { \u0026#34;uan\u0026#34;: { \u0026#34;boot_ordinal\u0026#34;: 2, \u0026#34;kernel_parameters\u0026#34;: \u0026#34;console=ttyS0,115200 bad_page=panic crashkernel=340M hugepagelist=2m-2g intel_iommu=off intel_pstate=disable iommu=pt ip=nmn0:dhcp numa_interleave_omit=headless numa_zonelist_order=node oops=panic pageblock_order=14 pcie_ports=native printk.synchronous=y quiet rd.neednet=1 rd.retry=10 rd.shell turbo_boost_limit=999 ifmap=net2:nmn0,lan0:hsn0,lan1:hsn1 spire_join_token=${SPIRE_JOIN_TOKEN}\u0026#34;, \u0026#34;network\u0026#34;: \u0026#34;nmn\u0026#34;, \u0026#34;node_list\u0026#34;: [ LIST_OF_APPLICATION_NODES ], \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/IMS_IMAGE_ID/manifest.json\u0026#34;, # \u0026lt;-- image ID is here \u0026#34;rootfs_provider\u0026#34;: \u0026#34;cpss3\u0026#34;, \u0026#34;rootfs_provider_passthrough\u0026#34;: \u0026#34;dvs:api-gw-service-nmn.local:300:nmn0\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; } }, \u0026#34;cfs\u0026#34;: { \u0026#34;configuration\u0026#34;: \u0026#34;uan-config-PRODUCT_VERSION\u0026#34; }, \u0026#34;enable_cfs\u0026#34;: true, \u0026#34;name\u0026#34;: \u0026#34;uan-sessiontemplate-PRODUCT_VERSION\u0026#34; }   Use the procedure Customize an Image Root Using IMS to enable the nscd service in the image by running these two commands in the image chroot:\nsystemctl enable nscd.service /tmp/images.sh   Obtain the new resultant image ID from the previous step.\n  Update the UAN BOS session template with the new image ID. Refer to Create UAN Boot Images for instructions on updating the BOS session template.\n  Perform Boot UANs to reboot the UANs with the updated session template.\n    "
},
{
	"uri": "/docs-csm/en-11/operations/network/dns/manage_the_dns_unbound_resolver/",
	"title": "Manage The DNS Unbound Resolver",
	"tags": [],
	"description": "",
	"content": "Manage the DNS Unbound Resolver The unbound DNS instance is used to resolve names for the physical equipment on the management networks within the system, such as NCNs, UANs, switches, compute nodes, and more. This instance is accessible only within the HPE Cray EX system.\nCheck the Status of the cray-dns-unbound Pods Use the kubectl command to check the status of the pods:\nncn-w001# kubectl get -n services pods | grep unbound cray-dns-unbound-696c58647f-26k4c 2/2 Running 0 121m cray-dns-unbound-696c58647f-rv8h6 2/2 Running 0 121m cray-dns-unbound-coredns-q9lbg 0/2 Completed 0 121m cray-dns-unbound-manager-1596149400-5rqxd 0/2 Completed 0 20h cray-dns-unbound-manager-1596149400-8ppv4 0/2 Completed 0 20h cray-dns-unbound-manager-1596149400-cwksv 0/2 Completed 0 20h cray-dns-unbound-manager-1596149400-dtm9p 0/2 Completed 0 20h cray-dns-unbound-manager-1596149400-hckmp 0/2 Completed 0 20h cray-dns-unbound-manager-1596149400-t24w6 0/2 Completed 0 20h cray-dns-unbound-manager-1596149400-vzxnp 0/2 Completed 0 20h cray-dns-unbound-manager-1596222000-bcsk7 0/2 Completed 0 2m48s cray-dns-unbound-manager-1596222060-8pjx6 0/2 Completed 0 118s cray-dns-unbound-manager-1596222120-hrgbr 0/2 Completed 0 67s cray-dns-unbound-manager-1596222180-sf46q 1/2 NotReady 0 7s For more information about the pods displayed in the output above:\n cray-dns-unbound-xxx - These are the main unbound pods. cray-dns-unbound-manager-yyy - These are job pods that run periodically to update DNS from DHCP (Kea) and the SLS/SMD content for the Hardware State Manager (HSM). Pods will go into the Completed status, and then independently be reaped \u0026ldquo;later\u0026rdquo; by the Kubernetes job\u0026rsquo;s processes. cray-dns-unbound-coredns-zzz - This pod is run one time during installation of Unbound (Stage 4) and reconfigures CoreDNS/ExternalDNS to point to Unbound for all site/internet lookups.  The table below describes what the status of each pod means for the health of the cray-dns-unbound services and pods. The Init and NotReady states are not necessarily bad, but it means the pod is being started or is processing. The cray-dns-manager and cray-dns-coredns pods for cray-dns-unbound are job pods that run periodically.\n   Pod Healthy Status Error Status Other     cray-dns-unbound Running CrashBackOffLoop    cray-dns-coredns Completed CrashBackOffLoop InitNotReady   cray-dns-manager Completed CrashBackOffLoop InitNotReady    Unbound Logs Logs for the unbound Pods will show the status and health of actual DNS lookups. Any logs with ERROR or Exception are an indication that the Unbound service is not healthy.\nncn-w001# kubectl logs -n services -l app.kubernetes.io/instance=cray-dns-unbound -c unbound [1596224129] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224129] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224135] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224135] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224140] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224140] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224145] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224145] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224149] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224149] unbound[8:0] debug: using localzone health.check.unbound. transparent ...snip... [1597020669] unbound[8:0] error: error parsing local-data at 33 \u0026#39;69.0.254.10.in-addr.arpa. PTR .local\u0026#39;: Empty label [1597020669] unbound[8:0] error: Bad local-data RR 69.0.254.10.in-addr.arpa. PTR .local [1597020669] unbound[8:0] fatal error: Could not set up local zones Troubleshooting: If there are any errors in the Unbound logs:\n The \u0026ldquo;localzone health.check.unbound. transparent\u0026rdquo; log is not an issue. Typically, any error seen in Unbound, including the example above, falls under one of two categories:   A bad configuration can come from a misconfiguration in the Helm chart. Currently, only the site/external DNS lookup can be at fault.\nACTION: See the customization.yaml file and look at the system_to_site_lookup value(s). Ensure that the external lookup values are valid and working.\n  Bad data (as shown in the above example) comes only from the DNS Helper and can be seen in the manager logs.\nACTION: Review and troubleshoot the Manager Logs as shown below.\n    View Manager (DNS Helper) Logs Manager logs will show the status of the latest \u0026ldquo;true up\u0026rdquo; of DNS with respect to DHCP actual leases and SLS/SMD status. The following command shows the last four lines of the last Manager run, and can be adjusted as needed.\nncn-w001# kubectl logs -n services pod/$(kubectl get -n services pods \\ | grep unbound | tail -n 1 | cut -f 1 -d \u0026#39; \u0026#39;) -c manager | tail -n4 uid: bc1e8b7f-39e2-49e5-b586-2028953d2940 Comparing new and existing DNS records. No differences found. Skipping DNS update Any log with ERROR or Exception is an indication that DNS is not healthy. The above example includes one of two possible reports for a healthy manager run. The healthy states are described below, as long as the write to the ConfigMap has not failed:\n No differences found. Skipping DNS update Differences found. Writing new DNS records to our configmap.  Troubleshooting: The Manager runs periodically, about every minute in release v1.4. Check if this is a one-time occurrence or if it is a recurring issue.\n If the error shows in one Manager log, but not during the next one, this is likely a one-time failure. Check to see if the record exists in DNS, and if so, move on. If several or all Manager logs show errors, particularly the same error, this could be of several sources:   Bad network connections to DHCP and/or SLS/SMD.\nACTION: Capture as much log data as possible and contact customer support.\n  Bad data from DHCP and/or SLS/SMD.\nACTION: If connections to DHCP (Kea) are involved, refer to Troubleshoot DHCP Issues.\n    Restart Unbound If any errors discovered in the sections above have been deemed transient or have not been resolved, the Unbound pods can be restarted.\nUse the following command to restart the pods:\n  Restart Unbound\nncn-w001# kubectl -n services rollout restart deployment cray-dns-unbound   A rolling restart of the Unbound pods will occur, old pods will not be terminated and new pods will not be added to the load balancer until the new pods have successfully loaded the DNS records.\nClear Bad Data in the Unbound ConfigMap Unbound stores records it obtains from DHCP, SLS, and SMD via the Manager job in a ConfigMap. It is possible to clear this ConfigMap and allow the next Manager job to regenerate the content.\nThis is useful in the following cases:\n A transient failure in any Unbound process or required services has left the configuration data in a bad state. SLS and SMD data needed to be reset because of bad or incorrect data there. DHCP (Kea) has been restarted to clear errors.  The following clears the (DNS Helper) Manager generated data in the ConfigMap. This is generally safe as Unbound runtime data is held elsewhere.\nncn-w001# kubectl -n services patch configmaps cray-dns-unbound \\ --type merge -p \u0026#39;{\u0026#34;binaryData\u0026#34;:{\u0026#34;records.json.gz\u0026#34;:\u0026#34;H4sICLQ/Z2AAA3JlY29yZHMuanNvbgCLjuUCAETSaHADAAAA\u0026#34;}}\u0026#39; "
},
{
	"uri": "/docs-csm/en-11/operations/network/dns/powerdns_configuration/",
	"title": "PowerDNS Configuration",
	"tags": [],
	"description": "",
	"content": "PowerDNS Configuration External DNS PowerDNS replaces the CoreDNS server that earlier versions of CSM used to provide External DNS services.\nThe cray-dns-powerdns-can-tcp and cray-dns-powerdns-can-udp LoadBalancer resources are configured to service external DNS requests using the IP address specified by the CSI --can-external-dns command line argument.\nThe CSI --system-name and --site-domain command line arguments are combined to form the subdomain used for External DNS.\nSite setup In the following example, the IP address 10.101.8.113 is used for External DNS and the system has the subdomain wasp.dev.cray.com\nncn-m001:~ # kubectl -n services get service -l app.kubernetes.io/name=cray-dns-powerdns NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE cray-dns-powerdns-api ClusterIP 10.24.24.29 \u0026lt;none\u0026gt; 8081/TCP 21d cray-dns-powerdns-can-tcp LoadBalancer 10.27.91.157 10.101.8.113 53:30726/TCP 21d cray-dns-powerdns-can-udp LoadBalancer 10.17.232.118 10.101.8.113 53:30810/UDP 21d cray-dns-powerdns-hmn-tcp LoadBalancer 10.31.228.190 10.94.100.85 53:31080/TCP 21d cray-dns-powerdns-hmn-udp LoadBalancer 10.24.134.53 10.94.100.85 53:31338/UDP 21d cray-dns-powerdns-nmn-tcp LoadBalancer 10.22.159.196 10.92.100.85 53:31996/TCP 21d cray-dns-powerdns-nmn-udp LoadBalancer 10.17.203.241 10.92.100.85 53:31898/UDP 21d A system administrator would typically setup the subdomain wasp.dev.cray.com in their site DNS and create a record which points to the IP address 10.101.8.113, for example ins1.wasp.dev.cray.com.\nThe administrator would then delegate queries to wasp.dev.cray.com to ins1.wasp.dev.cray.com making it authoritative for that subdomain allowing CSM to respond to queries for services like prometheus.wasp.dev.cray.com\nThe specifics of how to configure to configuring DNS forwarding is dependent on the DNS server in use, please consult the documentation provided by the DNS server vendor for more information.\nAuthoritative Zone Transfer In addition to responding to external DNS queries, PowerDNS can support replication of domain information to secondary servers via AXFR (Authoritative Zone Transfer) queries.\nConfiguration parameters Zone transfer is configured via customizations.yaml parameters and can also be configured at install time via CSI command line arguments.\nParameter: spec.network.dns.primary_server_name\nCSI command line argument: --primary-server-name\nDefault value: primary\nDescription:\nThe name of the PowerDNS server, this is combined with the system domain information to create the NS record for zones, for example.\nwasp.dev.cray.com.\t1890\tIN\tNS\tprimary.wasp.dev.cray.com. This record will also point to the External DNS IP address\n$ dig +short primary.wasp.dev.cray.com 10.101.8.113 Parameter: spec.network.dns.secondary_servers\nCSI command line argument: --secondary-servers\nDefault value: \u0026quot;\nDescription:\nA comma separated list of DNS servers to notify in the format server name/ip address.\nexternaldns1.my.domain/1.1.1.1,externaldns2.my.domain/2.2.2.2 If the default value is used no servers to notify on zone update will be configured.\nParameter: spec.network.dns.notify_zones\nCSI command line argument: --notify-zones\nDefault value: \u0026quot;\nDescription:\nA comma separated list of zones to transfer.\nwasp.dev.cray.com,8.101.10.in-addr.arpa If the default value is used then PowerDNS will attempt to transfer all zones.\nExample configuration for BIND An example configuration demonstrating how to configure BIND as a secondary server for zone transfer.\nFor other DNS servers please consult the documentation provided by the DNS server vendor.\n// This is the primary configuration file for the BIND DNS server named. // // Please read /usr/share/doc/bind9/README.Debian.gz for information on the // structure of BIND configuration files in Debian, *BEFORE* you customize // this configuration file. // // If you are just adding zones, please do that in /etc/bind/named.conf.local include \u0026quot;/etc/bind/named.conf.options\u0026quot;; include \u0026quot;/etc/bind/named.conf.local\u0026quot;; include \u0026quot;/etc/bind/named.conf.default-zones\u0026quot;; include \u0026quot;/etc/bind/named.conf.log\u0026quot;; zone \u0026quot;wasp.dev.cray.com\u0026quot; { type slave; masters { 10.101.8.113; }; allow-notify { 10.101.8.8; 10.101.8.9; 10.101.8.10; }; file \u0026quot;/var/lib/bind/db.wasp.dev.cray.com\u0026quot;; }; zone \u0026quot;can.wasp.dev.cray.com\u0026quot; { type slave; masters { 10.101.8.113; }; allow-notify { 10.101.8.8; 10.101.8.9; 10.101.8.10; }; file \u0026quot;/var/lib/bind/db.can.wasp.dev.cray.com\u0026quot;; }; zone \u0026quot;8.101.10.in-addr.arpa\u0026quot; { type slave; masters { 10.101.8.113; }; allow-notify { 10.101.8.8; 10.101.8.9; 10.101.8.10; }; file \u0026quot;/var/lib/bind/db.8.101.10.in-addr.arpa\u0026quot;; }; masters should be set to the CAN IP address of the PowerDNS service. This is typically defined at install time by the --can-external-dns CSI option.\nallow-notify should contain the CAN IP addresses of all Kubernetes worker nodes.\nDNS Security Extensions and zone transfer Zone signing The CSM implementation of PowerDNS supports the DNS Security Extensions (DNSSEC) and the signing of zones with a user-supplied zone signing key.\nIf DNSSEC is to be used for zone transfer then the dnssec SealedSecret in customizations.yaml should be updated to include a base64 encoded version of the private key portion of the desired zone signing key.\nHere is an example of a zone signing key.\nncn-m001:~ # cat Kwasp.dev.cray.com.+013+63812.private Private-key-format: v1.3 Algorithm: 13 (ECDSAP256SHA256) PrivateKey: +WFrfooCjTtoRU5UfhrpuTL0IEm6hYc4YJ6u8CcYquo= Created: 20210817081902 Publish: 20210817081902 Activate: 20210817081902 Encode the key using the base64 utility.\nncn-m001:~ # base64 Kwasp.dev.cray.com.+013+63812.private UHJpdmF0ZS1rZXktZm9ybWF0OiB2MS4zCkFsZ29yaXRobTogMTMgKEVDRFNBUDI1NlNIQTI1NikK UHJpdmF0ZUtleTogK1dGcmZvb0NqVHRvUlU1VWZocnB1VEwwSUVtNmhZYzRZSjZ1OENjWXF1bz0K Q3JlYXRlZDogMjAyMTA4MTcwODE5MDIKUHVibGlzaDogMjAyMTA4MTcwODE5MDIKQWN0aXZhdGU6 IDIwMjEwODE3MDgxOTAyCg== Populate the generate block in customizations.yaml with the encoded key.\n IMPORTANT the name of the key in SealedSecret must match the name of the zone being secured, in the below example the zone name is wasp.dev.cray.com. If multiple zones are to be secured each zone should have its own entry even if the same key is used.\n spec: kubernetes: sealed_secrets: dnssec: generate: name: dnssec-keys data: - type: static_b64 args: name: wasp.dev.cray.com value: | UHJpdmF0ZS1rZXktZm9ybWF0OiB2MS4zCkFsZ29yaXRobTogMTMgKEVDRFNBUDI1NlNIQTI1NikK UHJpdmF0ZUtleTogK1dGcmZvb0NqVHRvUlU1VWZocnB1VEwwSUVtNmhZYzRZSjZ1OENjWXF1bz0K Q3JlYXRlZDogMjAyMTA4MTcwODE5MDIKUHVibGlzaDogMjAyMTA4MTcwODE5MDIKQWN0aXZhdGU6 IDIwMjEwODE3MDgxOTAyCg== Transaction signatures Transaction signatures (TSIG) provide a secure communication channel between a primary and secondary DNS server\nTo configure TSIG add the desired key to the dnssec generate block in customizations.yaml. At this time only a single transaction signing key is supported and that key is applied to all zones.\nspec: kubernetes: sealed_secrets: dnssec: generate: name: dnssec-keys data: - type: static_b64 args: name: wasp.dev.cray.com value: | UHJpdmF0ZS1rZXktZm9ybWF0OiB2MS4zCkFsZ29yaXRobTogMTMgKEVDRFNBUDI1NlNIQTI1NikK UHJpdmF0ZUtleTogK1dGcmZvb0NqVHRvUlU1VWZocnB1VEwwSUVtNmhZYzRZSjZ1OENjWXF1bz0K Q3JlYXRlZDogMjAyMTA4MTcwODE5MDIKUHVibGlzaDogMjAyMTA4MTcwODE5MDIKQWN0aXZhdGU6 IDIwMjEwODE3MDgxOTAyCg== - type: static args: name: wasp-key.tsig value: name: wasp-key algorithm: hmac-sha256 key: dnFC5euKixIKXAr6sZhI7kVQbQCXoDG5R5eHSYZiBxY=  IMPORTANT The key used for TSIG must have .tsig in the name and unlike the zone signing key it should not be base64 encoded.\n Example configuration for BIND An example configuration demonstrating how to extend the previous BIND configuration example and add the TSIG key.\nkey \u0026quot;wasp-key\u0026quot; { algorithm hmac-sha256; secret \u0026quot;dnFC5euKixIKXAr6sZhI7kVQbQCXoDG5R5eHSYZiBxY=\u0026quot;; }; # Primary server IP address (i.e., PowerDNS CAN ip) server 10.101.8.113 { keys { wasp-key; }; }; For other DNS servers please consult the documentation provided by the DNS server vendor.\n"
},
{
	"uri": "/docs-csm/en-11/operations/network/dns/troubleshoot_common_dns_issues/",
	"title": "Troubleshoot Common DNS Issues",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Common DNS Issues The Domain Name Service (DNS) is part of an integrated infrastructure set designed to provide dynamic host discovery, addressing, and naming. There are several different place to look for troubleshooting as DNS interacts with Dynamic Host Configuration Protocol (DHCP), the Hardware Management Service (HMS), the System Layout Service (SLS), and the State Manager Daemon (SMD).\nThe information below describes what to check when experiencing issues with DNS.\nTroubleshoot an Invalid Hostname It is important to verify if a hostname is correct. The values in the networks.yml or networks_derived.yml files are sometimes inaccurate.\nThe formats show below are valid hostnames:\n xnames  Node Management Network (NMN):  \u0026lt;xname\u0026gt; \u0026lt;xname\u0026gt;.local   Hardware Management Network (HMN):  \u0026lt;xname\u0026gt;-mgmt \u0026lt;xname\u0026gt;-mgmt.local     nid  \u0026lt;nid_number\u0026gt;-nmn \u0026lt;nid_number\u0026gt;-nmn.local    Additional steps are needed if a hostname or xname is either listed incorrectly or not listed at all in the networks.yml or networks_derived.yml files. The following actions need to be taken:\n Update the hostname in the Hardware State Manager (HSM). Re-run any Ansible plays that require the data in these files.  Check if a Host is in DNS Use the dig or nslookup commands directly against the Unbound resolver. A host is correctly in DNS if the response from the dig command includes the following:\n The ANSWER SECTION value exists with a valid hostname and IP address A QUERY value exists that has the status: NOERROR message  ncn-w001# dig HOSTNAME @10.92.100.225 ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.11.2 \u0026lt;\u0026lt;\u0026gt;\u0026gt; x3000c0r41b0 @10.92.100.225 ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 57196 ;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ;; QUESTION SECTION: ;x3000c0r41b0. IN A ;; ANSWER SECTION: x3000c0r41b0. 3600 IN A 10.254.127.200 ;; Query time: 0 msec ;; SERVER: 10.92.100.225#53(10.92.100.225) ;; WHEN: Fri Jul 17 18:49:48 UTC 2020 ;; MSG SIZE rcvd: 57 If either of the commands fail to meet the two conditions mentioned above, collect the logs to troubleshoot.\nIf there no record in the Unbound pod, that is also an indication that the host is not in DNS.\nncn-w001# kubectl describe -n services configmaps cray-dns-unbound | grep XNAME ... {\u0026#34;hostname\u0026#34;: \u0026#34;x1003c7s7b0\u0026#34;, \u0026#34;ip-address\u0026#34;: \u0026#34;10.104.12.191\u0026#34;} ... Check the cray-dns-unbound Logs for Errors Use the following command to check the logs. Any logs with a message saying ERROR or Exception are an indication that the Unbound service is not healthy.\nncn-w001# kubectl logs -n services -l \\ app.kubernetes.io/instance=cray-dns-unbound -c unbound [1596224129] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224129] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224135] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224135] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224140] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224140] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224145] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224145] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224149] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224149] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224128] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224128] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224134] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224134] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224138] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224138] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224144] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224144] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224148] unbound[8:0] debug: using localzone health.check.unbound. transparent [1596224148] unbound[8:0] debug: using localzone health.check.unbound. transparent To view the DNS Helper logs:\nncn-w001# kubectl logs -n services pod/$(kubectl get -n services pods | \\ grep unbound | tail -n 1 | cut -f 1 -d \u0026#39; \u0026#39;) -c manager | tail -n4 uid: bc1e8b7f-39e2-49e5-b586-2028953d2940 Comparing new and existing DNS records. No differences found. Skipping DNS update Verify that MetalLB/BGP Peering and Routes are Correct Log in to the spine switches and check that MetalLB is peering to the spines via BGP.\nCheck both spines if they are available and powered up. All worker nodes should be peered with the spine BGP.\nsw-spine-001 [standalone: master] # show ip bgp neighbors BGP neighbor: 10.252.0.4, remote AS: 65533, link: internal: Route-map (in/out) : rm-ncn-w001 BGP version : 4 Configured hold time in seconds : 180 keepalive interval in seconds (configured) : 60 keepalive interval in seconds (established with peer): 30 Minimum holdtime from neighbor in seconds : 90BGP neighbor: 10.252.0.5, remote AS: 65533, link: internal: Route-map (in/out) : rm-ncn-w002 BGP version : 4 Configured hold time in seconds : 180 keepalive interval in seconds (configured) : 60 keepalive interval in seconds (established with peer): 30 Minimum holdtime from neighbor in seconds : 90BGP neighbor: 10.252.0.6, remote AS: 65533, link: internal: Route-map (in/out) : rm-ncn-w003 BGP version : 4 Configured hold time in seconds : 180 keepalive interval in seconds (configured) : 60 keepalive interval in seconds (established with peer): 30 Minimum holdtime from neighbor in seconds : 90 Confirm that routes to Kea (10.92.100.222) via all the NCN worker nodes are available:\nsw-spine-001 [standalone: master] # show ip route 10.92.100.222 Flags: F: Failed to install in H/W B: BFD protected (static route) i: BFD session initializing (static route) x: protecting BFD session failed (static route) c: consistent hashing p: partial programming in H/W VRF Name default: ------------------------------------------------------------------------------------------------------ Destination Mask Flag Gateway Interface Source AD/M ------------------------------------------------------------------------------------------------------ default 0.0.0.0 c 10.102.255.9 eth1/16 static 1/1 10.92.100.222 255.255.255.255 c 10.252.0.4 vlan2 bgp 200/0 c 10.252.0.5 vlan2 bgp 200/0 c 10.252.0.6 vlan2 bgp 200/0 TCPDUMP Verify if the NCN is receiving DNS queries. On an NCN worker or manager with kubectl installed, run the following command:\nncn-w001# tcpdump -envli vlan002 port 53 The ping and SSH Commands Fail for Hosts in DNS If the IP address returned by the ping command is different than the IP address returned by the dig command, restart nscd on the impacted node. This is done with the following command:\nncn-w001# systemctl restart nscd.service Attempt to ping or SSH to the IP address that was experiencing issues after restarting nscd.\nCheck for Missing DHCP Leases Search for a DHCP lease by checking active leases for the service:\nncn-w001# curl -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X POST -H \\ \u0026#34;Content-Type: application/json\u0026#34; \\-d \u0026#39;{ \u0026#34;command\u0026#34;: \u0026#34;lease4-get-all\u0026#34;, \u0026#34;service\u0026#34;: \\ [ \u0026#34;dhcp4\u0026#34; ] }\u0026#39; https://api-gw-service-nmn.local/apis/dhcp-kea |jq For example:\nncn-w001# curl -s -k -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X \\ POST -H \u0026#34;Content-Type: application/json\u0026#34; \\-d \u0026#39;{ \u0026#34;command\u0026#34;: \u0026#34;lease4-get-all\u0026#34;, \u0026#34;service\u0026#34;: \\ [ \u0026#34;dhcp4\u0026#34; ] }\u0026#39; https://api-gw-service-nmn.local/apis/dhcp-kea \\ |jq|grep x3000c0s19b4 -A 6 -B 4 { \u0026#34;cltt\u0026#34;: 1597777241, \u0026#34;fqdn-fwd\u0026#34;: true, \u0026#34;fqdn-rev\u0026#34;: true, \u0026#34;hostname\u0026#34;: \u0026#34;x3000c0s19b4\u0026#34;, \u0026#34;hw-address\u0026#34;: \u0026#34;a4:bf:01:3e:d2:94\u0026#34;, \u0026#34;ip-address\u0026#34;: \u0026#34;10.254.127.205\u0026#34;, \u0026#34;state\u0026#34;: 0, \u0026#34;subnet-id\u0026#34;: 1, \u0026#34;valid-lft\u0026#34;: 300 } If there is not a DHCP lease found, then:\n Ensure the system is running and that its DHCP client is still sending requests. Reboot the system via Redfish/IPMI if required. See Troubleshoot DHCP Issues for more information.  "
},
{
	"uri": "/docs-csm/en-11/operations/network/dns/troubleshoot_powerdns/",
	"title": "Troubleshoot PowerDNS",
	"tags": [],
	"description": "",
	"content": "Troubleshoot PowerDNS List DNS Zone Contents The PowerDNS zone database is populated with data from two sources:\n The cray-powerdns-manager service creates the zones and DNS records based on data sourced from the System Layout Service (SLS) The external DNS records are populated by the cray-externaldns-external-dns service using data sourced from Kubernetes annotations and virtual service definitions  Use the cray-powerdns-visualizer command to view the zone structure that cray-powerdns-manager will create.\nncn-m001# kubectl -n services exec deployment/cray-powerdns-manager -c cray-powerdns-manager -- cray-powerdns-visualizer . ├── 252.10.in-addr.arpa. │ ├── [PTR] 5.252.10.in-addr.arpa. │ │ └── sw-leaf-002.nmn.wasp.dev.cray.com. │ ├── [PTR] 4.252.10.in-addr.arpa. │ │ └── sw-leaf-001.nmn.wasp.dev.cray.com. │ ├── [PTR] 3.252.10.in-addr.arpa. │ │ └── sw-spine-002.nmn.wasp.dev.cray.com. │ ├── [PTR] 6.2.252.10.in-addr.arpa. │ │ └── pbs_comm_service.nmn.wasp.dev.cray.com. │ ├── [PTR] 5.2.252.10.in-addr.arpa. │ │ └── pbs_service.nmn.wasp.dev.cray.com. │ ├── [PTR] 4.2.252.10.in-addr.arpa. │ │ └── slurmdbd_service.nmn.wasp.dev.cray.com. │ ├── [PTR] 3.2.252.10.in-addr.arpa. │ │ └── slurmctld_service.nmn.wasp.dev.cray.com. │ ├── [PTR] 2.2.252.10.in-addr.arpa. │ │ └── uai_macvlan_bridge.nmn.wasp.dev.cray.com. ... For more information on External DNS and troubleshooting steps, see the External DNS documentation.\nPowerDNS Logging When troubleshooting DNS problems, it may prove helpful to increase the level of logging from the default value of 3 (error).\n  Edit the cray-dns-powerdns ConfigMap.\nncn-m001# kubectl -n services edit cm cray-dns-powerdns   Set the loglevel parameter in pdns.conf to the desired setting.\npdns.conf: | config-dir=/etc/pdns include-dir=/etc/pdns/conf.d guardian=yes loglevel=3 setgid=pdns setuid=pdns socket-dir=/var/run version-string=anonymous   Restart the PowerDNS service.\nncn-m001# kubectl -n services rollout restart deployment cray-dns-powerdns deployment.apps/cray-dns-powerdns restarted   Refer to the external PowerDNS documentation for more information.\nVerify DNSSEC Operation Verify Zones are Being Signed with the Zone Signing Key Check that the required zone has a DNSKEY entry; this should match the public key portion of the zone signing key.\nncn-m001# kubectl -n services exec deployment/cray-dns-powerdns -c cray-dns-powerdns -- pdnsutil show-zone wasp.dev.cray.com This is a Master zone Last SOA serial number we notified: 2021090901 == 2021090901 (serial in the database) Zone has following allowed TSIG key(s): wasp-key Zone uses following TSIG key(s): wasp-key Metadata items: AXFR-MASTER-TSIG\twasp-key SOA-EDIT-API\tDEFAULT TSIG-ALLOW-AXFR\twasp-key Zone has NSEC semantics keys: ID = 1 (CSK), flags = 257, tag = 26690, algo = 13, bits = 256\tActive\tPublished ( ECDSAP256SHA256 ) CSK DNSKEY = wasp.dev.cray.com. IN DNSKEY 257 3 13 TAi+aXL+Z8ZSFHxz+iEWB3MEdi1JWgM/tb3Q1M76yVOq5Kaur9k+oIAHXvCSR19Iuu+0ZUAyLB0vKkhScJp3Tw== ; ( ECDSAP256SHA256 ) DS = wasp.dev.cray.com. IN DS 26690 13 1 8c926281afb822a2bea767f08c79b856a2427c26 ; ( SHA1 digest ) DS = wasp.dev.cray.com. IN DS 26690 13 2 2bfd71e5403f99d25496f5f7f352e71747bb72ee6eb240dcaf8b56b95d18ef6c ; ( SHA256 digest ) DS = wasp.dev.cray.com. IN DS 26690 13 4 df40f23a7ee051d7e3d40d4059640bda3558cd74a37110b25f7b8cf4e60506c77bf33a660400710d397df0a1cde26d70 ; ( SHA-384 digest ) If the DNSKEY record is incorrect, verify that the zone name is correct in the dnssec SealedSecret in customizations.yaml and that the desired zone signing key was used. Please see the PowerDNS Configuration Guide for more information.\nVerify TSIG Operation  IMPORTANT these examples are for informational purposes only. The use of the dig command -y option to present the key should be avoided in favour of the -k option with the secret in a file to avoid the key being displayed in ps command output or the shell history.\n   Determine the IP address of the external DNS service.\nncn-m001# kubectl -n services get service cray-dns-powerdns-can-tcp NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE cray-dns-powerdns-can-tcp LoadBalancer 10.27.91.157 10.101.8.113 53:30726/TCP 6d   Verify that an AXFR query to the external DNS service works when the correct TSIG key is presented.\n$ dig -t axfr wasp.dev.cray.com @10.101.8.113 -y \u0026quot;hmac-sha256:wasp-key:dnFC5euKixIKXAr6sZhI7kVQbQCXoDG5R5eHSYZiBxY=\u0026quot; +nocrypto | head ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.10.6 \u0026lt;\u0026lt;\u0026gt;\u0026gt; -t axfr wasp.dev.cray.com @10.101.8.113 -y hmac-sha256:wasp-key:dnFC5euKixIKXAr6sZhI7kVQbQCXoDG5R5eHSYZiBxY= +nocrypto ;; global options: +cmd wasp.dev.cray.com.\t3600\tIN\tSOA\ta.misconfigured.dns.server.invalid. hostmaster.wasp.dev.cray.com. 2021090901 10800 3600 604800 3600 wasp.dev.cray.com.\t3600\tIN\tRRSIG\tSOA 13 4 3600 20210930000000 20210909000000 26690 wasp.dev.cray.com. [omitted] wasp-key.\t0\tANY\tTSIG\thmac-sha256. 1632302505 300 32 XoySAOtCD52OzO/2MeFk0/x7MG6m93IxtWaNfhzaRkg= 44483 NOERROR 0 wasp.dev.cray.com.\t3600\tIN\tDNSKEY\t257 3 13 [key id = 26690] wasp.dev.cray.com.\t3600\tIN\tRRSIG\tDNSKEY 13 4 3600 20210930000000 20210909000000 26690 wasp.dev.cray.com. [omitted] sma-kibana.wasp.dev.cray.com. 300 IN\tA\t10.101.8.128 sma-kibana.wasp.dev.cray.com. 300 IN\tRRSIG\tA 13 5 300 20210930000000 20210909000000 26690 wasp.dev.cray.com. [omitted] When presented with an invalid key the transfer should fail.\n$ dig -t axfr wasp.dev.cray.com @10.101.8.113 -y \u0026quot;hmac-sha256:wasp-key:B7n/sK74pa7r0ygOZkKpW9mWkPjq8fV71j1SaTpzJMQ=\u0026quot; ;; Couldn't verify signature: expected a TSIG or SIG(0) ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.10.6 \u0026lt;\u0026lt;\u0026gt;\u0026gt; -t axfr wasp.dev.cray.com @10.101.8.113 -y hmac-sha256:wasp-key:B7n/sK74pa7r0ygOZkKpW9mWkPjq8fV71j1SaTpzJMQ= ;; global options: +cmd ; Transfer failed. The cray-dns-powerdns pod log will also indicate that the request failed.\nncn-m001# kubectl -n services logs cray-dns-powerdns-64fdf6597c-pqgdt -c cray-dns-powerdns --- Sep 22 09:31:17 Packet for 'wasp.dev.cray.com' denied: Signature with TSIG key 'wasp-key' failed to validate   "
},
{
	"uri": "/docs-csm/en-11/operations/network/dhcp/dhcp/",
	"title": "DHCP",
	"tags": [],
	"description": "",
	"content": "DHCP The Dynamic Host Configuration Protocol (DHCP) service on the HPE Cray EX system uses the Internet Systems Consortium (ISC) Kea tool. Kea provides more robust management capabilities for DHCP servers.\nFor more information: https://www.isc.org/kea/.\nThe following improvements to the DHCP service are included:\n Persistent and resilient data store for DHCP leases in Postgres API access to manage DHCP Scalable pod that uses metalLB instead of host networking Options for updates to HPE Cray EX management system IP addresses  DHCP Helper Workflow The DHCP-Helper uses the following workflow:\nThe order can vary in the workflow outlined below, and the services in this workflow can run asynchronously.\n Retrieve the network information.  Query the System Layout Service (SLS) Update Kea   Query Kea for active leases to check if SMD knows about the NIC.  Check MACs in KEA active lease against SMD known NIC MACs.   Query SMD for all known NICs and create DHCP leases.  Each NIC in SMD create DHCP reservation in Kea Create DHCP reservation in Kea Create hostname/MAC/IP address DHCP reservations Create hostname(alias)/MAC in DHCP reservation   Check to see if the NIC in SMD needs to have an IP added.  Update the IP address from Kea if NIC in SMD does not have an IP address set   Remove any potential race conditions between the DHCP reservation in Kea and the NIC information in SMD.  Compare information in NIC information in SMD and Kea DHCP reservation configurations Delete any active leases that did not match SMD    "
},
{
	"uri": "/docs-csm/en-11/operations/network/dns/dns/",
	"title": "Domain Name Service (DNS) Overview",
	"tags": [],
	"description": "",
	"content": "Domain Name Service (DNS) Overview DNS Architecture This diagram shows how the various components of the DNS infrastructure interact.\nDNS Components The DNS infrastructure is comprised of a number of components.\nUnbound (cray-dns-unbound) Unbound is a caching DNS resolver which is also used as the primary DNS server.\nThe DNS records served by Unbound include system component xnames, node hostnames, and service names and these records are read from the cray-dns-unbound ConfigMap which is populated by cray-dns-unbound-manager.\nThe DNS server functionality will be migrated to PowerDNS in a future release leaving Unbound acting purely as a caching DNS resolver.\nUnbound also forwards queries to PowerDNS or the site DNS server if the query cannot be answered by local data.\nUnbound Manager (cray-dns-unbound-manager) The cray-dns-unbound-manager cron job runs every three minutes and queries the System Layout Service, the Hardware State Manager, and the Kea DHCP server for new or changed hardware components and creates DNS records for these components in the cray-dns-unbound ConfigMap.\nThis job also initiates a rolling restart of Unbound if the cray-dns-unbound ConfigMap was modified.\nKubernetes DNS (coredns) Kubernetes creates DNS records for services and pods. A CoreDNS server running in the kube-system namespace is used for this purpose.\nThe CoreDNS service is also configured to forward DNS requests to Unbound in order to allow pods to resolve system hardware components and other services. This configuration is performed by the cray-dns-unbound-coredns job which is invoked whenever the cray-dns-unbound Helm chart is deployed or upgraded.\nSee the Kubernetes documentation for more information.\nExternalDNS (cray-externaldns-external-dns) ExternalDNS creates DNS records for services that are intended to be accessible via the Customer Access Network (CAN). For example grafana.wasp.dev.cray.com.\nKubernetes Services annotated with external-dns.alpha.kubernetes.io/hostname have DNS records created.\nStarting with CSM version 1.1 these DNS records are created in the PowerDNS server. Earlier versions of CSM used a dedicated CoreDNS server for ExternalDNS.\n Only DNS A records are created as ExternalDNS currently does not support the creation of the PTR records required for reverse lookup.\n PowerDNS (cray-dns-powerdns) PowerDNS is an authoritative DNS server which over the next few CSM releases will replace Unbound sa the primary DNS server within a CSM system.\nPowerDNS is able to respond to queries for services accessible via the CAN. records are externally accessible via the LoadBalancer IP address specified for the CSI --can-external-dns option.\nAs with earlier CSM releases it is possible to delegate to PowerDNS to resolve CAN services and it is also possible to configure zone transfer to sync the DNS records from PowerDNS to Site DNS.\nPowerDNS Manager (cray-powerdns-manager) The PowerDNS Manager serves a similar purpose to the Unbound Manager. It runs in the background and periodically queries the System Layout Service, the Hardware State Manager, and the Kea DHCP server for new or changed hardware components and creates DNS records for these components in PowerDNS.\nThe PowerDNS Manager also configures the PowerDNS server for zone transfer and DNSSEC if required.\nSite DNS This term is used to refer the external DNS server specified the CSI --site-dns option.\n"
},
{
	"uri": "/docs-csm/en-11/operations/network/customer_access_network/required_labels_if_can_is_not_configured/",
	"title": "Required Labels If CAN Is Not Configured",
	"tags": [],
	"description": "",
	"content": "Required Labels if CAN is Not Configured Some services on the system are required to access services outside of the HPE Cray EX system. If the Customer Access Network (CAN) is not configured on the system, these services will need to be pinned to ncn-m001 because that is the only node that has external access. See Customer Access Network (CAN) for more implications if CAN is not configured.\nThe label used for scheduling these services is no_external_access:\n If the no_external_access label is applied to a node with the value True, then any pods that require outside access will not be scheduled on that node. If the no_external_access label is applied to a node with the value False, or if the label does not exist on the node, then any pods that require outside access will be scheduled on that node.  Therefore, if CAN is not configured on the system, the label no_external_access=True must be applied to all NCN master and worker nodes other than ncn-m001.\nBefore Installation The label can be configured by setting the following values to True in the customizations.yaml file. This value needs to be set for each NCN master and worker node, excluding ncn-m001.\ncray-node-labels: nodeLabels: - ncn-m001:no_external_access=False - ncn-m002:no_external_access=True - ncn-m003:no_external_access=True - ncn-w001:no_external_access=False - ncn-w002:no_external_access=False - ncn-w003:no_external_access=False Post-Installation The label can be set by editing the Kubernetes ConfigMap by running the following command:\nncn-m001# kubectl edit cm -n services cray-node-labels Edit the following section as desired (save and close by hitting the ESC key and typing :wq):\nnode_labels: |2- - ncn-m001:no_external_access=False - ncn-m002:no_external_access=True - ncn-m003:no_external_access=True - ncn-w001:no_external_access=False - ncn-w002:no_external_access=False - ncn-w003:no_external_access=False To view the labels applied to each node:\nncn-m001# kubectl get nodes --show-labels "
},
{
	"uri": "/docs-csm/en-11/operations/network/customer_access_network/troubleshoot_can_issues/",
	"title": "Troubleshoot CAN Issues",
	"tags": [],
	"description": "",
	"content": "Troubleshoot CAN Issues Various connection points to check when using the CAN and how to fix any issues that arise.\nThe most frequent issue with the Customer Access Network (CAN) is trouble accessing IP addresses outside of the HPE Cray EX system from a node or pod inside the system.\nThe best way to resolve this issue is to try to ping an outside IP address from one of the NCNs other than ncn-m001, which has a direct connection that it can use instead of the Customer Access Network (CAN). The following are some things to check to make sure CAN is configured correctly:\nDoes the NCN have an IP Address Configured on the vlan007 Interface? Check the status of the vlan007 interface. Make sure it has an address specified.\nncn-w002# ip addr show vlan007 534: vlan007@bond0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 98:03:9b:b4:27:62 brd ff:ff:ff:ff:ff:ff inet 10.102.5.5/26 brd 10.101.8.255 scope global vlan007 valid_lft forever preferred_lft forever inet6 fe80::9a03:9bff:feb4:2762/64 scope link valid_lft forever preferred_lft forever If there is not an address specified, make sure the can- values have been defined in csi config init input.\nDoes the NCN have a Default Gateway Configured? Check the default route on an NCN other than ncn-m001. There should be a default route with a gateway matching the can-gateway value.\nncn-w002# ip route | grep default default via 10.102.5.27 dev vlan007 If there is not an address specified, make sure the can- values have been defined in csi config init input.\nCan the Node Reach the Default CAN Gateway? Check that the node can ping the default gateway shown in the default route.\nncn-w002# ping 10.102.5.27 PING 10.102.5.27 (10.102.5.27) 56(84) bytes of data. 64 bytes from 10.102.5.27: icmp_seq=1 ttl=64 time=0.148 ms 64 bytes from 10.102.5.27: icmp_seq=2 ttl=64 time=0.107 ms 64 bytes from 10.102.5.27: icmp_seq=3 ttl=64 time=0.133 ms 64 bytes from 10.102.5.27: icmp_seq=4 ttl=64 time=0.122 ms ^C --- 10.102.5.27 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3053ms rtt min/avg/max/mdev = 0.107/0.127/0.148/0.018 ms If the default gateway cannot be accessed, check the spine switch configuration.\nCan the Spines Reach Outside of the System? Check that each of the spines can ping an IP address outside of the HPE Cray EX system. This must be an IP address that is reachable from the network to which the CAN is connected. If there is only one spine being used on the system, only spine-001 needs to be checked.\nsw-spine-001 [standalone: master] # ping 8.8.8.8 PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data. 64 bytes from 8.8.8.8: icmp_seq=1 ttl=112 time=12.6 ms 64 bytes from 8.8.8.8: icmp_seq=2 ttl=112 time=12.5 ms 64 bytes from 8.8.8.8: icmp_seq=3 ttl=112 time=22.4 ms 64 bytes from 8.8.8.8: icmp_seq=4 ttl=112 time=12.5 ms ^C --- 8.8.8.8 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3004ms rtt min/avg/max/mdev = 12.501/15.022/22.440/4.285 ms If the outside IP address cannot be reached, check the spine switch configuration and the connection to the customer network.\nCan the Spines Reach the NCN? Check that each of the spines can ping one or more of the NCNs at its vlan007 IP address. If there is only one spine being used on the system, only spine-001 needs to be checked.\nsw-spine-001 [standalone: master] # ping 10.102.5.5 PING 10.102.5.5 (10.102.5.5) 56(84) bytes of data. 64 bytes from 10.102.5.5: icmp_seq=1 ttl=64 time=0.140 ms 64 bytes from 10.102.5.5: icmp_seq=2 ttl=64 time=0.134 ms 64 bytes from 10.102.5.5: icmp_seq=3 ttl=64 time=0.126 ms 64 bytes from 10.102.5.5: icmp_seq=4 ttl=64 time=0.178 ms ^C --- 10.102.5.5 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3058ms rtt min/avg/max/mdev = 0.126/0.144/0.178/0.023 ms If the NCN cannot be reached, check the spine switch configuration.\nCan a Device Outside the System Reach the CAN Gateway? Check that a device outside the HPE Cray EX system that is expected to have access to nodes and services on the CAN can ping the CAN gateway.\n$ ping 10.102.5.27 PING 10.102.5.27 (10.102.5.27): 56 data bytes 64 bytes from 10.102.5.27: icmp_seq=0 ttl=58 time=54.724 ms 64 bytes from 10.102.5.27: icmp_seq=1 ttl=58 time=65.902 ms 64 bytes from 10.102.5.27: icmp_seq=2 ttl=58 time=51.960 ms 64 bytes from 10.102.5.27: icmp_seq=3 ttl=58 time=55.032 ms 64 bytes from 10.102.5.27: icmp_seq=4 ttl=58 time=57.606 ms ^C --- 10.102.5.27 ping statistics --- 5 packets transmitted, 5 packets received, 0.0% packet loss round-trip min/avg/max/stddev = 51.960/57.045/65.902/4.776 ms If the CAN gateway cannot be reached from outside, check the spine switch configuration and the connection to the customer network.\n"
},
{
	"uri": "/docs-csm/en-11/operations/network/dhcp/troubleshoot_dhcp_issues/",
	"title": "Troubleshoot DHCP Issues",
	"tags": [],
	"description": "",
	"content": "Troubleshoot DHCP Issues There are several things to check for when troubleshooting issues with Dynamic Host Configuration Protocol (DHCP) servers.\nIncorrect DHCP IP Addresses One of the most common issues is when the DHCP IP addresses are not matching in the Domain Name Service (DNS).\nCheck to make sure cray-dhcp is not running in Kubernetes:\nncn-w001# kubectl get pods -A | grep cray-dhcp services cray-dhcp-5f8c8767db-hg6ch 1/1 Running 0 35d If the cray-dhcp pod is running, use the following command to shut down the pod:\nncn-w001# kubectl scale deploy cray-dhcp --replicas=0 If the IP addresses are still not lining up with DNS and cray-dhcp is confirmed not running, wait 800 seconds for DHCP leases to expire and renew.\nVerify the Status of the cray-dhcp-kea Pods and Services Check to see if the Kea DHCP services are running:\nncn-w001# kubectl get services -n services | grep kea cray-dhcp-kea-api ClusterIP 10.26.142.204 \u0026lt;none\u0026gt; 8000/TCP 5d23h cray-dhcp-kea-postgres ClusterIP 10.19.97.142 \u0026lt;none\u0026gt; 5432/TCP 5d23h cray-dhcp-kea-postgres-0 ClusterIP 10.30.214.27 \u0026lt;none\u0026gt; 5432/TCP 5d23h cray-dhcp-kea-postgres-1 ClusterIP 10.27.232.156 \u0026lt;none\u0026gt; 5432/TCP 5d23h cray-dhcp-kea-postgres-2 ClusterIP 10.22.242.251 \u0026lt;none\u0026gt; 5432/TCP 5d23h cray-dhcp-kea-postgres-config ClusterIP None \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 5d23h cray-dhcp-kea-postgres-repl ClusterIP 10.17.107.16 \u0026lt;none\u0026gt; 5432/TCP 5d23h cray-dhcp-kea-tcp-hmn LoadBalancer 10.24.79.120 10.94.100.222 67:32120/TCP 5d23h cray-dhcp-kea-tcp-nmn LoadBalancer 10.19.139.179 10.92.100.222 67:31652/TCP 5d23h cray-dhcp-kea-udp-hmn LoadBalancer 10.25.203.31 10.94.100.222 67:30840/UDP 5d23h cray-dhcp-kea-udp-nmn LoadBalancer 10.19.187.168 10.92.100.222 67:31904/UDP 5d23h If the services shown in the output above are not present, it could be an indication that something is not working correctly. To check to see if the Kea pods are running:\nncn-w001# kubectl get pods -n services -o wide | grep kea cray-dhcp-kea-788b4c899b-x6ltd 3/3 Running 0 36h 10.40.3.183 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-dhcp-kea-postgres-0 2/2 Running 0 5d23h 10.40.3.121 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-dhcp-kea-postgres-1 2/2 Running 0 5d23h 10.42.2.181 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-dhcp-kea-postgres-2 2/2 Running 0 5d23h 10.39.0.208 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; The pods should be in a Running state. The output above will also indicate which worker node the kea-dhcp pod is currently running on.\nTo restart the pods:\nncn-w001# kubectl delete pods -n services -l app.kubernetes.io/name=cray-dhcp-kea Use the command mentioned above to verify the pods are running again after restarting the pods.\nCheck the Current DHCP Leases Use the Kea API to retrieve data from the DHCP lease database. An authentication token will be needed to access the Kea API. To retrieve a token, run the following command from an NCN worker or manager:\nncn-w001# export TOKEN=$(curl -s -k -S -d grant_type=client_credentials \\ -d client_id=admin-client -d client_secret=`kubectl get secrets admin-client-auth \\ -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token \\ | jq -r \u0026#39;.access_token\u0026#39;) Once a token has been generated, the DHCP lease database can be viewed. The commands below are the most effective way to check the current DHCP leases:\n  View all leases:\nncn-w001# curl -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X POST -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;command\u0026#34;: \u0026#34;lease4-get-all\u0026#34;, \u0026#34;service\u0026#34;: [ \u0026#34;dhcp4\u0026#34; ] }\u0026#39; \\ https://api-gw-service-nmn.local/apis/dhcp-kea | jq   View the total amount of leases:\nncn-w001# curl -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X POST -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;command\u0026#34;: \u0026#34;lease4-get-all\u0026#34;, \u0026#34;service\u0026#34;: [ \u0026#34;dhcp4\u0026#34; ] }\u0026#39; \\ https://api-gw-service-nmn.local/apis/dhcp-kea | jq \u0026#39;.[].text\u0026#39;   Use an IP address to search for a hostname or MAC address:\nncn-w001# curl -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X POST -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;command\u0026#34;: \u0026#34;lease4-get\u0026#34;, \u0026#34;service\u0026#34;: [ \u0026#34;dhcp4\u0026#34; ], \u0026#34;arguments\u0026#34;: { \u0026#34;ip-address\u0026#34;: \u0026#34;x.x.x.x\u0026#34; } }\u0026#39; \\ https://api-gw-service-nmn.local/apis/dhcp-kea | jq   Use a MAC address to find a hostname or IP address:\nncn-w001# curl -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X POST -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;command\u0026#34;: \u0026#34;lease4-get-all\u0026#34;, \u0026#34;service\u0026#34;: [ \u0026#34;dhcp4\u0026#34; ] }\u0026#39; \\ https://api-gw-service-nmn.local/apis/dhcp-kea | jq \u0026#39;.[].arguments.leases[] | \\ select(.\u0026#34;hw-address\u0026#34;==\u0026#34;XX:XX:XX:XX:XX:5d\u0026#34;)\u0026#39;   Use a hostname to find a MAC address or IP address:\nncn-w001# curl -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; -X POST -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;command\u0026#34;: \u0026#34;lease4-get-all\u0026#34;, \u0026#34;service\u0026#34;: [ \u0026#34;dhcp4\u0026#34; ] }\u0026#39; \\ https://api-gw-service-nmn.local/apis/dhcp-kea | jq \u0026#39;.[].arguments.leases[] | \\ select(.\u0026#34;hostname\u0026#34;==\u0026#34;xNAME\u0026#34;)\u0026#39;   Check the Hardware State Manager (HSM) for Issues The HSM includes two important components:\n Systems Layout Service (SLS): This is the expected state of the system, as populated by the networks.yaml and other sources. State Manager Daemon (SMD): This is the discovered or active state of the system during runtime.  To view the information stored in SLS for a specific xname:\nncn-w001# cray sls hardware describe XNAME To view the information in SMD:\nncn-w001# cray hsm inventory ethernetInterfaces describe XNAME View the cray-dhcp-kea Logs The specific pod name is needed in order to check the logs for a pod. Run the command below to see the pod name:\nncn-w001# kubectl logs -n services -l \\ app.kubernetes.io/instance=cray-dhcp-kea -c cray-dhcp-kea 2020-08-03 21:47:50.580 INFO [kea-dhcp4.dhcpsrv/10] DHCPSRV_MEMFILE_LEASE_FILE_LOAD loading leases from file /cray-dhcp-kea-socket/dhcp4.leases 2020-08-03 21:47:50.580 INFO [kea-dhcp4.dhcpsrv/10] DHCPSRV_MEMFILE_LFC_SETUP setting up the Lease File Cleanup interval to 3600 sec 2020-08-03 21:47:50.580 WARN [kea-dhcp4.dhcpsrv/10] DHCPSRV_OPEN_SOCKET_FAIL failed to open socket: the interface eth0 has no usable IPv4 addresses configured 2020-08-03 21:47:50.580 WARN [kea-dhcp4.dhcpsrv/10] DHCPSRV_NO_SOCKETS_OPEN no interface configured to listen to DHCP traffic 2020-08-03 21:48:00.602 INFO [kea-dhcp4.commands/10] COMMAND_RECEIVED Received command \u0026#39;lease4-get-all\u0026#39; {\u0026#34;Dhcp4\u0026#34;: {\u0026#34;control-socket\u0026#34;: {\u0026#34;socket-name\u0026#34;: \u0026#34;/cray-dhcp-kea-socket/cray-dhcp-kea.socket\u0026#34;, \u0026#34;socket-type\u0026#34;: \u0026#34;unix\u0026#34;}, \u0026#34;hooks-libraries\u0026#34;: [{\u0026#34;library\u0026#34;: \u0026#34;/usr/local/lib/kea/hooks/libdhcp_lease_cmds.so\u0026#34;}, ...SNIP... waiting 10 seconds for any leases to be given out... [{\u0026#39;arguments\u0026#39;: {\u0026#39;leases\u0026#39;: []}, \u0026#39;result\u0026#39;: 3, \u0026#39;text\u0026#39;: \u0026#39;0 IPv4 lease(s) found.\u0026#39;}] 2020-08-03 21:48:22.734 INFO [kea-dhcp4.commands/10] COMMAND_RECEIVED Received command \u0026#39;config-get\u0026#39; To view the Kea logs:\nncn-w001# kubectl logs -n services -l app.kubernetes.io/instance=cray-dhcp-kea \\ -c cray-dhcp-kea | grep -i error TCPDUMP If a host is not getting an IP address, run a packet capture to see if DHCP traffic is being transmitted.\nncn-w001# tcpdump -w dhcp.pcap -envli vlan002 port 67 or port 68 This will make a .pcap file named dhcp in the current directory. It will collect all DHCP traffic on the specified port. In this example. it would be the DHCP traffic on interface vlan002 (10.252.0.0/17).\nTo view the DHCP traffic:\nncn-w001# tcpdump -r dhcp.pcap -v -n The output may be very long, so use any desired filters to narrow the results.\nTo do a tcpdump for a certain MAC address:\nncn-w001# tcpdump -i eth0 -vvv -s 1500 \u0026#39;((port 67 or port 68) and (udp[38:4] = 0x993b7030))\u0026#39; This example is using the MAC of b4:2e:99:3b:70:30. It will show the output on the terminal and will not save to a file.\nVerify that MetalLB/BGP Peering and Routes are Correct Log in to the spine switches and check that MetalLB is peering to the spines via BGP.\nCheck both spines if they are available and powered up. All worker nodes should be peered with the spine BGP.\nsw-spine-001 [standalone: master] # show ip bgp neighbors BGP neighbor: 10.252.0.4, remote AS: 65533, link: internal: Route-map (in/out) : rm-ncn-w001 BGP version : 4 Configured hold time in seconds : 180 keepalive interval in seconds (configured) : 60 keepalive interval in seconds (established with peer): 30 Minimum holdtime from neighbor in seconds : 90BGP neighbor: 10.252.0.5, remote AS: 65533, link: internal: Route-map (in/out) : rm-ncn-w002 BGP version : 4 Configured hold time in seconds : 180 keepalive interval in seconds (configured) : 60 keepalive interval in seconds (established with peer): 30 Minimum holdtime from neighbor in seconds : 90BGP neighbor: 10.252.0.6, remote AS: 65533, link: internal: Route-map (in/out) : rm-ncn-w003 BGP version : 4 Configured hold time in seconds : 180 keepalive interval in seconds (configured) : 60 keepalive interval in seconds (established with peer): 30 Minimum holdtime from neighbor in seconds : 90 Confirm that routes to Kea (10.92.100.222) via all the NCN worker nodes are available:\nsw-spine-001 [standalone: master] # show ip route 10.92.100.222 Flags: F: Failed to install in H/W B: BFD protected (static route) i: BFD session initializing (static route) x: protecting BFD session failed (static route) c: consistent hashing p: partial programming in H/W VRF Name default: ------------------------------------------------------------------------------------------------------ Destination Mask Flag Gateway Interface Source AD/M ------------------------------------------------------------------------------------------------------ default 0.0.0.0 c 10.102.255.9 eth1/16 static 1/1 10.92.100.222 255.255.255.255 c 10.252.0.4 vlan2 bgp 200/0 c 10.252.0.5 vlan2 bgp 200/0 c 10.252.0.6 vlan2 bgp 200/0 "
},
{
	"uri": "/docs-csm/en-11/operations/network/customer_access_network/can_with_dual-spine_configuration/",
	"title": "CAN With Dual-spine Configuration",
	"tags": [],
	"description": "",
	"content": "CAN with Dual-Spine Configuration The Customer Access Network (CAN) needs to be connected to both spines in a dual-spine configuration so that each spine can access the outside network. However, the NCNs should only have one default gateway. Therefore, the multi-active gateway protocol (MAGP) on the Mellanox spines can be used to create a virtual router gateway IP address that can direct to either of the spines, depending on the state of the spines. The Virtual Switching Extension (VSX) for Aruba spines serve the same purpose.\nFor more information:\n Mellanox: https://community.mellanox.com/s/article/howto-configure-magp-on-mellanox-switches Aruba: https://www.arubanetworks.com/techdocs/AOS-CX/10.04/HTML/5200-6728/index.html#book.html  The following is an example of the point-to-point configuration on the spine switches. The IP address should be replaced with the IP address chosen by the customer that matches the switch configuration.\n  Mellanox:\ninterface ethernet 1/11 speed auto force interface ethernet 1/11 description to-can interface ethernet 1/11 no switchport force interface ethernet 1/11 ip address 10.101.15.150/30 primary   Aruba:\ninterface 1/1/36 no shutdown description to-can ip address 10.101.15.150/30 exit   There must then be two routes on the customer\u0026rsquo;s switch directing traffic for the customer_access_network subnet to the endpoint on the spine switch. The following is an example of the route configuration on the customer switch. These addresses and subnets are generated from CSI and can be found in the CAN.yaml file.\n- full_name: CAN Bootstrap DHCP Subnet cidr: ip: 10.101.8.0 mask: - 255 - 255 - 255 - 0 There must be a default route on each spine switch that will direct traffic that does not match other routes to the endpoint on the customer switch. The following examples are for the route configuration on sw-spine-001.\n  Mellanox:\nip route vrf default 0.0.0.0/0 10.101.15.149   Aruba:\nip route 0.0.0.0/0 10.101.15.149   The spine switch must also have the customer_access_gateway IP address assigned to the vlan 7 interface on the switch. This provides a gateway for the default route on the NCNs and UANs, as well as a direct route to the customer_access_network from the spine switch. For example:\n  Mellanox:\ninterface vlan 7 ip address 10.101.8.2/26 primary   Aruba:\nsw-spine-002(config)# int vlan 7 sw-spine-002(config-if-vlan)# ip address 10.102.11.3/24   Distribution Spine/Switch Connection The connection between the distribution switch and the spines require two separate uplinks from the spine switch to the distribution switch. Two static routes need to be created on the distribution switch to route the CAN subnet to each of the spine switches. These routes will have equal cost (ECMP) to split the load across the two spines and provide redundancy if one of the spines should go down.\ninterface 1/1/41 no shutdown description WASP spine-001 1/11 ip address 10.101.15.149/30 interface 1/1/42 no shutdown description WASP spine-002 1/11 ip address 10.101.15.153/30 ip route 10.102.5/26 10.101.15.150 ip route 10.102.5/26 10.101.15.154 NCN/Spine Connection Each of the NCNs has a connection to each spine. These ports on the NCN will be configured as bonded. The spine ports on the other end of these connections will be configured as a multi-chassis link aggregation group (MLAG) port channel.\nThe NCN will also need to configure a default route pointing to a gateway IP address on the CAN. In a dual-spine configuration, this will continue to use the IP address specified in the can-gateway value. The main difference in the dual-spine configuration is that this IP address will now be a virtual IP address on the spine switches rather than the vlan 7 IP address. This virtual IP address is part of MAGP configuration. Each spine will have an IP address on its vlan 7 interface that is not the can-gateway IP address.\nThen the following MAGP configuration will be added (same configuration on each of the spines):\n virtual-router address = can-gateway virtual-router mac-address = 00:00:5E:00:01:07.  The MAC used here follows RFC 3768 Section 7.3 in the external documentation.    MAGP on the spine:\nprotocol magp interface vlan 7 magp 7 interface vlan 7 magp 7 ip virtual-router address 10.102.5.57 interface vlan 7 magp 7 ip virtual-router mac-address 00:00:5E:00:01:07 Vlan 7 on the spine:\ninterface vlan 7 ip address 10.102.5.1/26 primary In the example above, spine-001 is showed.\nDefault route on the NCN (configured by the can-network role):\nncn-m001# ip route default via 10.102.5.27 dev vlan007 "
},
{
	"uri": "/docs-csm/en-11/operations/network/customer_access_network/connect_to_the_can/",
	"title": "Connect To The CAN",
	"tags": [],
	"description": "",
	"content": "Connect to the CAN How to connect to the CAN physically and via layer 3.\nThere are multiple ways to connect to the Customer Access Network (CAN), both physically and via a layer 3 connection.\nPhysical Connection to the CAN The physical connection to the CAN is made via the load balancer or the spine switches. The uplink connection from the system to the customer network is achieved by using the highest numbered port(s). Customer can select a single uplink port or multiple uplink ports.\nIn the example below, a Mellanox SN2700 with a single uplink connection is being used. The cable would connect to port 32 as shown in the diagram below:\nLayer 3 Connection to the CAN The CAN clients in the system require a routing topology that is setup to route traffic to the customer network. This can be done in a variety of ways and will vary depending on the system setup and configuration. The different options for connecting to the CAN from layer 3 are described below.\nOption 1: Point-to-Point\nThis option provides a point-to-point routing topology between the customer switch and the HPE Cray EX TOR Spine Switch. See CAN with Dual-Spine Configuration for more information on using this topology for a dual-spine configuration.\nThe diagram below shows how the point-to-point routing topology works:\nOption 2: Single Gateway\nThe single gateway options requires the customer to provide an IP address that is on a /24 network. This IP address will act as the gateway for traffic bound to the HPE Cray EX CAN.\nFor example, a customer could use the 192.168.30.0/24 network to connect via the HPE Cray EX CAN uplink connection. The customer also needs to provide an IP address on this network, such as 192.168.30.253. This IP address will be assigned to the uplink port on the HPE Cray EX TOR Spine Switch.\nFor a dual-spine configuration, the admin would need to extended the customer network to both switches using one IP address for each switch. After extending the network, two equal routes need to be configured. The spine switches are configured to support multi-chassis link aggregation group (MLAG) from NCNs and UANs. These nodes are configured for bonding mode layer 2 and layer 3. See CAN with Dual-Spine Configuration for more information.\nThe diagram below shows how the connection is established:\nOption 3: Customer Specific\nThe system can be setup using other customer requirements. In order to do so, the following information will be needed:\n   Item Description     Customer network IP address Specifies an IP address on the customer network that is routable to the customer network gateway   CAN IP address space Specifies a dedicated network (typically /24) that is routable from the customer network to be used for the CAN   System name Specifies the system name to be used and configured   Domain namespace extension Specifies the domain extension to be used; for example, HPE Cray EX.customer.com   Customer network IP address for ncn-m001 Specifies an IP address for administrative access on the customer network for ncn-m001   (Optional) BMC customer network IP address for ncn-m001 Specifies an IP address for BMC access on the customer network for ncn-m001    "
},
{
	"uri": "/docs-csm/en-11/operations/network/customer_access_network/customer_access_network_can/",
	"title": "Customer Access Network",
	"tags": [],
	"description": "",
	"content": "Customer Access Network The Customer Access Network (CAN) provides access from outside the customer network to services, non-compute nodes (NCNs), and User Access Nodes (UANs) in the system. This allows for the following:\n Clients outside of the system:  Log in to each of the NCNs and UANs. Access web UIs within the system (e.g. Prometheus, Grafana, and more). Access the Rest APIs within the system. Access a DNS server within the system for resolution of names for the webUI and REST API services. Run Cray CLI commands from outside the system. Access the User Access Instances (UAI).   NCNs and UANs to access systems outside the cluster (e.g. LDAP, license servers, and more). Services within the cluster to access systems outside the cluster.  These nodes and services need an IP address that routes to the customer\u0026rsquo;s network in order to be accessed from outside the network.\nImplications if CAN is not Configured  No direct access to the NCNs other than ncn-m001. Will need to hop through ncn-m001 to get to the rest of the NCNs. No direct access to the UANs unless the UAN has a direct connection to the customer network. NCNs other than ncn-m001 do not have access to services outside of the system (e.g. LDAP, license servers, and more).  These nodes will not have an interface on any network with access outside of the HPE Cray EX system. These nodes will not have a default route. This includes access to any of the externally exposed services from these nodes.   Pods running on NCNs other than ncn-m001 will not have access to services outside of the system. No access to externally exposed services. See Externally Exposed Services for more information.  CAN Subnets IP addresses are allocated from a single IP subnet that is configured as the can-cidr value in the csi config init input. This subnet is further divided into three smaller subnets:\n Subnet for NCNs, UANs, and switches. Subnet for the MetalLB static pool (can-static-pool).  This is used for services that need to be pinned to the same IP address. For example, the external DNS service that needs to be configured in the upstream DNS server. This subnet currently needs only a few IP addresses.   Subnet for the MetalLB dynamic pool (can-dynamic-pool).  This is the largest subnet because it is used for most of the other services, which includes UAIs. These IP addresses can be allocated differently across deployments because these services are accessed by DNS name rather than by IP. The size of this subnet depends on how many UAI and Image Management Service (IMS) pods are needed at a given time because these IP addresses will come out of this subnet.    The minimum size for the CAN subnet is /26. The CAN /26 subnet allows for the following:\n  27 IP addresses for the NCNs, UANs, and Switches.\n  4 IP addresses for the CAN static service IP addresses.\n  32 IP addresses for the rest of the external CAN services.\nThe 32 service IP addresses will be used for the 5 standard customer-access service IP addresses and the remaining 27 IP addresses are for UAI and/or IMS services.\n  If there are more than 27 IP addresses needed for NCNs, UANs, and switches, and/or more than 32 IP addresses needed for the external CAN services, then the CAN subnet will need to be larger than a /26.\nCustomer Variables The following variables are defined in the csi config init input. This example uses values for the /26 layout described above. The can-gateway value should be an IP address at the end of the range for NCNs, UANs, and switches. For example, the IP address would be 10.102.5.27 using the figure shown above.\nlinux# csi config init . . . --can-cidr 10.102.5.0/26 --can-gateway 10.102.5.27 --can-static-pool 10.102.5.28/30 --can-dynamic-pool 10.102.5.32/27 --can-external-dns 10.102.5.29 --system-name testsystem --site-domain example.com . . . "
},
{
	"uri": "/docs-csm/en-11/operations/network/customer_access_network/externally_exposed_services/",
	"title": "Externally Exposed Services",
	"tags": [],
	"description": "",
	"content": "Externally Exposed Services The following services are exposed on the Customer Access Network (CAN). Each of these services requires an IP address on the CAN subnet so they are reachable on the CAN. This IP address is allocated by the MetalLB component.\nServices under Istio Ingress Gateway and Keycloak Gatekeeper Ingress share an ingress, so they all use the IP allocated to the Ingress.\nEach service is given a DNS name that is served by the External DNS service to make them resolvable from the site network. This makes it possible to access each of these services by name rather than finding the allocated IP. The DNS name is pre-pended to the system-name.site-domain specified during csi config init. For example, if the system is named TestSystem, and the site is example.com, the HPE Cray EX domain would be testsystem.example.com.\nSee External DNS for more information.\n   Service DNS Name Address Pool Requires CAN IP External Port Notes     Istio Ingress Gateway   customer-access Yes 80/443, 8081, 8888     HPE Cray EX REST API api    No Uses the IP ofIstio IngressGateway   Authentication auth    No Uses the IP ofIstio IngressGateway   S3 s3 customer-access Yes 8080     External DNS   customer-access Yes 53     Keycloak Gatekeeper Ingress   customer-access Yes 443     Sysmgmt-health Prometheus prometheus    No Uses the IP ofKeycloakGatekeeperIngress   Sysmgmt-health Alert Manager alertmanager    No Uses the IP ofKeycloakGatekeeperIngress   Sysmgmt-health Grafana grafana    No Uses the IP ofKeycloakGatekeeperIngress   Istio Prometheus prometheus-istio    No Uses the IP ofKeycloakGatekeeperIngress   Istio Kiali kiali-istio    No Uses the IP ofKeycloakGatekeeperIngress   Istio Jaeger jaeger-istio    No Uses the IP ofKeycloakGatekeeperIngress   VCS vcs    No Uses the IP ofKeycloakGatekeeperIngress   SMA Kibana sma-kibana    No Uses the IP ofKeycloakGatekeeperIngress   SMA Grafana sma-grafana    No Uses the IP ofKeycloakGatekeeperIngress   Nexus nexus    No Uses the IP ofKeycloakGatekeeperIngress   Rsyslog Aggregator rsyslog customer-access Yes 514/8514     UAI   customer-access Yes (multiple) 22 Can be several of these each with a unique ID   IMS \u0026lt;uid\u0026gt;.ims customer-access Yes (multiple) 22 Can be several of these each with a unique ID    "
},
{
	"uri": "/docs-csm/en-11/operations/network/network/",
	"title": "Network",
	"tags": [],
	"description": "",
	"content": "Network There are several different networks supported by the HPE Cray EX system. The following are the available internal and external networks, as well as the devices that connect to each network:\n Networks external to the system:  Customer Network (Data Center)  ncn-m001 BMC is connected by the customer network switch to the customer management network All NCNs (worker, master, and storage) are connected ClusterStor System Management Unit (SMU) interfaces User Access Nodes (UANs)     System networks:  Customer Network (Data Center)  ncn-m001 BMC is connected by the customer network switch to the customer management network ClusterStor SMU interfaces User Access Nodes (UANs)   Hardware Management Network (HMN)  BMCs for Admin tasks Power distribution units (PDU) Keyboard/video/mouse (KVM)   Node Management Network (NMN)  All NCNs and compute nodes User Access Nodes (UANs)   ClusterStor Management Network  ClusterStor controller management interfaces of all ClusterStor components (SMU, Metadata Management Unit (MMU), and Scalable Storage Unit (SSU))   High-Speed Network (HSN), which connects the following devices:  Kubernetes worker nodes UANs ClusterStor controller data interfaces of all ClusterStor components (SMU, MMU, and SSU) There must be at least two NCN\u0026rsquo;s whose BMCs are on the HMN. If these are not present, there cannot be multiple DVS servers that function correctly, which will have an effect on compute node root file system and PE scaling/performance/reliability.      During initial installation, several of those networks are created with default IP address ranges. See Default IP Address Ranges.\nA default configuration of Access Control Lists (ACL) is also set when the system is installed. The default configuration of ACLs between the NMN and HMN are described below:\nMountain NMN to HMN Ipv4 access-list deny-mtn-nmn-to-hmn bind point rif Ipv4 access-list deny-mtn-nmn-to-hmn seq-number 10 deny ip 10.100.0.0 mask 255.252.0.0 10.104.0.0 mask 255.252.0.0 Ipv4 access-list deny-mtn-nmn-to-hmn seq-number 20 deny ip 10.100.0.0 mask 255.252.0.0 10.254.0.0 mask 255.255.128.0 Ipv4 access-list deny-mtn-nmn-to-hmn seq-number 30 permit ip any any Interface vlan 2000 ipv4 port access-group deny-mtn-nmn-to-hmn Interface vlan 2001 ipv4 port access-group deny-mtn-nmn-to-hmn Interface vlan 2002 ipv4 port access-group deny-mtn-nmn-to-hmn Interface vlan 2003 ipv4 port access-group deny-mtn-nmn-to-hmn Mountain HMN to NMN Ipv4 access-list deny-mtn-hmn-to-nmn bind point rif Ipv4 access-list deny-mtn-hmn-to-nmn seq number 10 deny 10.104.0.0 mask 255.252.0.0 10.100.0.0 mask 255.252.0.0 Ipv4 access-list deny-mtn-hmn-to-nmn seq number 20 deny 10.104.0.0 mask 255.252.0.0 10.252.0.0 mask 255.255.128.0 Ipv4 access-list deny-mtn-hmn-to-nmn seq number 30 permit ip any any Interface vlan 3000 ipv4 port access-group deny-mtn-hmn-to-nmn Interface vlan 3001 ipv4 port access-group deny-mtn-hmn-to-nmn Interface vlan 3002 ipv4 port access-group deny-mtn-hmn-to-nmn Interface vlan 3003 ipv4 port access-group deny-mtn-hmn-to-nmn The network management system (NMS) data model and REST API enable customer sites to construct their own \u0026ldquo;networks\u0026rdquo; of nodes within the high-speed fabric, where a \u0026ldquo;network\u0026rdquo; is a collection of nodes that share a VLAN and an IP subnet.\nThe low-level network management components (switch, DHCP service, ARP service) of the NCNs and ClusterStor interfaces are configured to serve one particular network (the \u0026ldquo;supported network\u0026rdquo;) on the high-speed fabric. The supported network includes all of the compute nodes, thereby enabling those compute nodes to access the gateway, user access services, and ClusterStor devices. A site may create other networks as well, but it is only the supported network that is served by those devices.\n"
},
{
	"uri": "/docs-csm/en-11/operations/network/access_to_system_management_services/",
	"title": "Access To System Management Services",
	"tags": [],
	"description": "",
	"content": "Access to System Management Services The standard configuration for System Management Services (SMS) is the containerized REST micro-service with a public API. All of the micro-services provide an HTTP interface and are collectively exposed through a single gateway URL. The API gateway for the system is available at a well known URL based on the domain name of the system. It acts as a single HTTPS endpoint for terminating Transport Layer Security (TLS) using the configured certificate authority. All services and the API gateway are not dependent on any single node. This resilient arrangement ensures that services remain available during possible underlying hardware and network failures.\nAccess to individual APIs through the gateway is controlled by a policy-driven access control system. Administrators and users must retrieve a token for authentication before attempting to access APIs through the gateway and present a valid token with each API call. The authentication and authorization decisions are made at the gateway level which prevent unauthorized API calls from reaching the underlying micro-services. Refer to System Security and Authentication for more detail on the process of obtaining tokens and user management.\nReview the API documentation in the supplied container before attempting to use the API services. This container is generated with the release using the most current API descriptions in OpenAPI 2.0 format. Because this file serves as both an internal definition of the API contract and the external documentation of the API function, it is the most up-to-date reference available.\nThe API Gateway URL for accessing the APIs on a site-specific system is https://api.SYSTEM-NAME_DOMAIN-NAME/apis/.\nThe internal URL from a local console on any of the NCNs is https://api-gw-service-nmn.local/apis.\n"
},
{
	"uri": "/docs-csm/en-11/operations/network/connect_to_the_hpe_cray_ex_environment/",
	"title": "Connect To The HPE Cray Ex Environment",
	"tags": [],
	"description": "",
	"content": "Connect to the HPE Cray EX Environment The HPE Cray EX Management Network (SMNet) has multiple separate physical and logical links that are used to segregate traffic.\nThe diagram below shows the available connections from within the SMNet, as well as the connections to the customer network:\nThere are multiple ways to connect to the HPE Cray EX environment. The various methods are described in the following table:\n   Role Description     Administrative External customer network connection to the worker node\u0026rsquo;s hardware management and administrative port   Application node access External customer network connection to an Application Node   Customer Access Network (CAN) Customer connection to the CAN gateway to access the HPE Cray EX CAN    There are also several ways to physically connect to the nodes on the system. The following table describes the physical connections to the administrative and application nodes:\n   Role Description     Administrative node 1GB copper connection made from the customer network that provides administrative access and hardware management   Application node Different interconnect types are available from the customer network based on customer requirements    For more information on connecting to the CAN, see Connect to the CAN.\n"
},
{
	"uri": "/docs-csm/en-11/operations/network/default_ip_address_ranges/",
	"title": "Default Ip Address Ranges",
	"tags": [],
	"description": "",
	"content": "Default IP Address Ranges The initial installation of the system creates default networks with default settings and with no external exposure. These IP address default ranges ensure that no nodes in the system attempt to use the same IP address as a Kubernetes service or pod, which would result in undefined behavior that is extremely difficult to reproduce or debug.\nThe following table shows the default IP address ranges:\n   Network IP Address Range     Kubernetes service network 10.16.0.0/12   Kubernetes pod network 10.32.0.0/12   Install Network (MTL) 10.1.0.0/16   Node Management Network (NMN) 10.252.0.0/17   High Speed Network (HSN) 10.253.0.0/16   Hardware Management Network (HMN) 10.254.0.0/17   Mountain NMN (see note below table) 10.100.0.0/17   Mountain HMN (see note below table) 10.104.0.0/17   River NMN 10.106.0.0/17   River HMN 10.107.0.0/17   Load Balanced NMN    For the Mountain NMN:     Allocate a /22 from this range per liquid-cooled cabinet. For example, the following cabinets would be given the following IP addresses in the allocated ranges:\n cabinet 1 = 10.100.0.0/22 cabinet 2 = 10.100.4.0/22 cabinet 3 = 10.100.8.0/22 \u0026hellip;  For the Mountain HMN:\nAllocate a /22 from this range per liquid-cooled cabinet. For example, the following cabinets would be given the following IP addresses in the allocated ranges:\n cabinet 1 = 10.104.0.0/22 cabinet 2 = 10.104.4.0/22 cabinet 3 = 10.104.8.0/22 \u0026hellip;  The values in the table could be modified prior to install if there is a need to ensure that there are no conflicts with customer resources, such as LDAP or license servers. If a customer has more than one HPE Cray EX system, these values can be safely reused across them all.\nContact customer support for this site if it is required to change the IP address range for Kubernetes services or pods; for example, if the IP addresses within those ranges must be used for something else. The cluster must be fully reinstalled if either of those ranges are changed.\nCustomizable Network Values There are several network values and other pieces of system information that must be unique to the customer system.\n  IP address values and the network for ncn-m001 and the BMC on ncn-m001.\n  The main Customer Access Network (CAN) subnet and the two address pools mentioned below need to be part of the main subnet.\nFor more information on the CAN, see Customer Access Network.\n Subnet for the MetalLB static address pool (can-static-pool), which is used for services that need to be pinned to the same IP address, such as the system DNS service. Subnet for the MetalLB dynamic address pool (can-dynamic-pool), which is used for services such as User Access Instances (UAIs) that can be reached by DNS.    HPE Cray EX Domain: The value of the subdomain that is used to access externally exposed services.\nFor example, if the system is named TestSystem, and the site is example.com, the HPE Cray EX domain would be testsystem.example.com. Central DNS would need to be configured to delegate requests for addresses in this domain to the HPE Cray EX DNS IP address for resolution.\n  HPE Cray EX DNS IP: The IP address used for the HPE Cray EX DNS service. Central DNS delegates the resolution for addresses in the HPE Cray EX Domain to this server. The IP address will be in the can-static-pool subnet.\n  CAN gateway IP address: The IP address assigned to a specific port on the spine switch, which will act as the gateway between the CAN and the rest of the customer\u0026rsquo;s internal networks. This address would be the last-hop route to the CAN network.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/kubernetes/troubleshoot_postgres_database/",
	"title": "Troubleshoot Postgres Database",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Postgres Database General Postgres Troubleshooting Topics\n Is the Database Unavailable? Is the Database Disk Full? Is Replication Lagging? Is the Postgres status SyncFailed? Is a Cluster Member missing? Is the Postgres Leader missing?  The patronictl Tool The patronictl tool is used to call a REST API that interacts with Postgres databases. It handles a variety of tasks, such as listing cluster members and the replication status, configuring and restarting databases, and more.\nThe tool is installed in the database containers:\nncn-w001# kubectl exec -it -n services keycloak-postgres-0 -c postgres -- su postgres Use the following command for more information on the patronictl command:\npostgres@keycloak-postgres-0:~$ patronictl --help Is the Database Unavailable? If there are no endpoints for the main service, Patroni will mark the database as unavailable.\nThe following is an example for keycloak-postgres where no endpoints are listed, which means the database is unavailable.\nncn-w001# kubectl get endpoints keycloak-postgres -n services NAME ENDPOINTS AGE keycloak-postgres \u0026lt;none\u0026gt; 3d22h If the database is unavailable, check if the Disk Full is the cause of the issue. Otherwise, check the postgres-operator logs for errors.\nncn-w001# kubectl logs -l app.kubernetes.io/name=postgres-operator -n services Is the Database Disk Full? The following is an example for keycloak-postgres. One cluster member is failing to start because of a full pgdata disk. This was likely due to replication issues which caused the pg_wal files to grow.\nncn-w001# POSTGRESQL=keycloak-postgres ncn-w001# NAMESPACE=services ncn-w001# kubectl exec \u0026#34;${POSTGRESQL}-1\u0026#34; -c postgres -it -n ${NAMESPACE} -- patronictl list +-------------------+---------------------+------------+--------+--------------+----+-----------+ | Cluster | Member | Host | Role | State | TL | Lag in MB | +-------------------+---------------------+------------+--------+--------------+----+-----------+ | keycloak-postgres | keycloak-postgres-0 | 10.42.0.11 | | start failed | | unknown | | keycloak-postgres | keycloak-postgres-1 | 10.44.0.7 | | running | 4 | 0 | | keycloak-postgres | keycloak-postgres-2 | 10.36.0.40 | Leader | running | 4 | | +-------------------+---------------------+------------+--------+--------------+----+-----------+ ncn-w001# for i in {0..2}; do echo \u0026#34;${POSTGRESQL}-${i}:\u0026#34;; kubectl exec \u0026#34;${POSTGRESQL}-${i}\u0026#34; -n ${NAMESPACE} -c postgres -- df -h pgdata; done; keycloak-postgres-0: Filesystem Size Used Avail Use% Mounted on /dev/sde 976M 960M 0 100% /home/postgres/pgdata keycloak-postgres-1: Filesystem Size Used Avail Use% Mounted on /dev/rbd12 976M 152M 809M 16% /home/postgres/pgdata keycloak-postgres-2: Filesystem Size Used Avail Use% Mounted on /dev/rbd3 976M 136M 825M 15% /home/postgres/pgdata ncn-w001# kubectl logs \u0026#34;${POSTGRESQL}-0\u0026#34; -n ${NAMESPACE} -c postgres | grep FATAL 2021-07-14 17:52:48 UTC [30495]: [1-1] 60ef2470.771f 0 FATAL: could not write lock file \u0026#34;postmaster.pid\u0026#34;: No space left on device ncn-w001# kubectl exec \u0026#34;${POSTGRESQL}-0\u0026#34; -n ${NAMESPACE} -c postgres -it -- du -h --max-depth 1 /home/postgres/pgdata/pgroot/data/pg_wal To recover the cluster member that had failed to start because of disk pressure, attempt to reclaim some space on the pgdata disk.\nExec into that pod, copy the logs off (optional) and then clear the logs to recover some disk space. Then restart the Postgres cluster and postgres-operator.\nncn-w001# kubectl cp \u0026quot;${POSTGRESQL}-1\u0026quot;:/home/postgres/pgdata/pgroot/pg_log /tmp -c postgres -n ${NAMESPACE} ncn-w001# kubectl exec \u0026quot;${POSTGRESQL}-1\u0026quot; -n ${NAMESPACE} -c postgres -it -- bash root@cray-smd-postgres-1:/home/postgres# for i in {0..7}; do \u0026gt; /home/postgres/pgdata/pgroot/pg_log/postgresql-$i.csv; done ncn-w001# kubectl delete pod \u0026quot;${POSTGRESQL}-0\u0026quot; \u0026quot;${POSTGRESQL}-1\u0026quot; \u0026quot;${POSTGRESQL}-2\u0026quot; -n ${NAMESPACE} ncn-w001# kubectl delete pod -l app.kubernetes.io/name=postgres-operator -n services If disk issues persist or exist on multiple nodes and the above does not resolve the issue, see the Recover from Postgres WAL Event procedure.\nIs Replication Lagging? Postgres replication lag can be detected with Prometheus alerts and alert notifications (See Configure Prometheus Email Alert Notifications). If replication lag is not caught early, it can cause the disk mounted on /home/postgres/pgdata to fill up and the database to stop running. If this issue is caught before the database stops, it can be easily remediated using a partonictl command to reinitialize the lagging cluster member.\nCheck if Replication is Working When services have a Postgres cluster of pods, they need to be able to replicate data between them. When the pods are not able to replicate data, the database will become full. The patronictl list command will show the status of replication:\nThe following is an example where replication is working:\nncn-w001# kubectl exec keycloak-postgres-0 -c postgres -n services -it -- patronictl list +-------------------+---------------------+------------+--------+---------+----+-----------+ | Cluster | Member | Host | Role | State | TL | Lag in MB | +-------------------+---------------------+------------+--------+---------+----+-----------+ | keycloak-postgres | keycloak-postgres-0 | 10.40.0.23 | Leader | running | 1 | 0 | | keycloak-postgres | keycloak-postgres-1 | 10.42.0.25 | | running | 1 | 0 | | keycloak-postgres | keycloak-postgres-2 | 10.42.0.29 | | running | 1 | 0 | +-------------------+---------------------+------------+--------+---------+----+-----------+ The following is an example where replication is broken:\n+-------------------+---------------------+--------------+--------+----------+----+-----------+ | Cluster | Member | Host | Role | State | TL | Lag in MB | +-------------------+---------------------+--------------+--------+----------+----+-----------+ | keycloak-postgres | keycloak-postgres-0 | 10.42.10.22 | | starting | | unknown | | keycloak-postgres | keycloak-postgres-1 | 10.40.11.191 | Leader | running | 47 | 0 | | keycloak-postgres | keycloak-postgres-2 | 10.40.11.190 | | running | 14 | 608 | +-------------------+---------------------+--------------+--------+----------+----+-----------+ Recover Replication In the event that a state of broken Postgres replication persists and the space allocated for the WAL files fills up, the affected database will likely shut down and create a state where it can be very difficult to recover. This can impact the reliability of the related service and may require that it be redeployed with data repopulation procedures. If replication lag is caught and remediated before the database shuts down, replication can be recovered using patronictl reinit.\nA reinitialize will get the lagging replica member re-synced and replicating again. This should be done as soon as replication lag is detected. In the preceding example, keycloak-postgres-0 and keycloak-postgres-2 were not replicating properly (Lag\u0026gt;0 or unknown). To remediate, kubectl exec into the leader pod and use patronictl reinit \u0026lt;cluster\u0026gt; \u0026lt;lagging cluster member\u0026gt; to reinitialize the lagging member(s). For example:\nncn-w001# kubectl exec keycloak-postgres-1 -n services -it -- bash root@keycloak-postgres-1:/home/postgres# patronictl reinit keycloak-postgres keycloak-postgres-0 Are you sure you want to reinitialize members keycloak-postgres-0? [y/N]: y Failed: reinitialize for member keycloak-postgres-0, status code=503, (restarting after failure already in progress) Do you want to cancel it and reinitialize anyway? [y/N]: y Success: reinitialize for member keycloak-postgres-0 root@keycloak-postgres-1:/home/postgres# patronictl reinit keycloak-postgres keycloak-postgres-2 Are you sure you want to reinitialize members keycloak-postgres-2? [y/N]: y Failed: reinitialize for member keycloak-postgres-2, status code=503, (restarting after failure already in progress) Do you want to cancel it and reinitialize anyway? [y/N]: y Success: reinitialize for member keycloak-postgres-2 Verify that replication has recovered:\nncn-w001# kubectl exec keycloak-postgres-0 -c postgres -n services -it -- bash postgres@keycloak-postgres-2:~$ patronictl list +-------------------+---------------------+--------------+--------+---------+----+-----------+ | Cluster | Member | Host | Role | State | TL | Lag in MB | +-------------------+---------------------+--------------+--------+---------+----+-----------+ | keycloak-postgres | keycloak-postgres-0 | 10.42.10.22 | | running | 47 | 0 | | keycloak-postgres | keycloak-postgres-1 | 10.40.11.191 | Leader | running | 47 | 0 | | keycloak-postgres | keycloak-postgres-2 | 10.40.11.190 | | running | 47 | | +-------------------+---------------------+--------------+--------+---------+----+-----------+ Setup Alerts for Replication Lag Alerts exist in prometheus for the following:\n PostgresqlReplicationLagSMA PostgresqlReplicationLagServices PostgresqlFollowerReplicationLagSMA PostgresqlFollowerReplicationLagServices  When alert notifications are configured, replication issues can be detected quickly. If the replication issue persists such that the database becomes unavailable, recovery will likely be much more involved. Catching such issues as soon as possible is desired. See Configure Prometheus Email Alert Notifications.\nIs the Postgres status SyncFailed? Check all the postgresql resources Check for any postgresql resource that has a STATUS of SyncFailed. SyncFailed generally means that there is something between the postgres-operator and the Postgres cluster that is out of sync. This does not always mean that the cluster is unhealthy. To determine the underlying sync issue, check the postgres-operator logs for messages to further root cause the issue.\nOther STATUS values such as Updating are a non issue. It is expected that this will eventually change to Running or possibly SyncFailed if the postgres-operator encounters issues syncing updates to the postgresql cluster.\nncn-w001# kubectl get postgresql -A NAMESPACE NAME TEAM VERSION PODS VOLUME CPU-REQUEST MEMORY-REQUEST AGE STATUS services cray-console-data-postgres cray-console-data 11 3 2Gi 4h10m Running services cray-sls-postgres cray-sls 11 3 1Gi 4h12m SyncFailed services cray-smd-postgres cray-smd 11 3 30Gi 500m 8Gi 4h12m Updating services gitea-vcs-postgres gitea-vcs 11 3 50Gi 4h11m Running services keycloak-postgres keycloak 11 3 1Gi 4h13m Running spire spire-postgres spire 11 3 20Gi 1 4Gi 4h10m Running ncn-w001# kubectl get pods -l app.kubernetes.io/name=postgres-operator -n services NAME READY STATUS RESTARTS AGE cray-postgres-operator-6fffc48b4c-mqz7z 2/2 Running 0 5h26m ncn-w001# kubectl logs cray-postgres-operator-6fffc48b4c-mqz7z -n services -c postgres-operator | grep -i sync | grep -i msg Case 1 : msg=\u0026ldquo;could not sync cluster: could not sync persistent volumes: could not sync volumes: could not resize EBS volumes: some persistent volumes are not compatible with existing resizing providers\u0026rdquo; This generally means that the postgresql resource was updated to change the volume size from the Postgres operator\u0026rsquo;s perspective, but the additional step to resize the actual PVCs was not done so the operator and the Postgres cluster are not able to sync the resize change. The cluster is still healthy, but to complete the resize of the underlying Postgres PVCs, additional steps are needed.\nThe example below assumes that cray-smd-postgres is in SyncFailed and the volume size was recently increased to 100Gi (possibly by editing the volume size of postgresql cray-smd-postgres resource), but the pgdata-cray-smd-postgres PVC\u0026rsquo;s storage capacity was not updated to align with the change. To confirm this is the case:\nncn-w001# kubectl get postgresql cray-smd-postgres -n services -o jsonpath=\u0026#34;{.spec.volume.size}\u0026#34; 100Gi ncn-w001# kubectl get pvc -n services -l application=spilo,cluster-name=cray-smd-postgres NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE pgdata-cray-smd-postgres-0 Bound pvc-020cf339-e372-46ae-bc37-de2b55320e88 30Gi RWO k8s-block-replicated 70m pgdata-cray-smd-postgres-1 Bound pvc-3d42598a-188e-4301-a58e-0f0ce3944c89 30Gi RWO k8s-block-replicated 27m pgdata-cray-smd-postgres-2 Bound pvc-0d659080-7d39-409a-9ee5-1a1806971054 30Gi RWO k8s-block-replicated 27m To resolve this SyncFailed case, resize the pgdata PVCs for the selected Postgres cluster. Create the following function in the shell and execute the function by calling it with the appropriate arguments. For this example the pgdata-cray-smd-postgres PVCs will be resized to 100Gi to match that of the postgresql cray-smd-postgres volume size.\nfunction resize-postgresql-pvc { POSTGRESQL=$1 PGDATA=$2 NAMESPACE=$3 PGRESIZE=$4 # Check for required arguments if [ $# -ne 4 ]; then echo \u0026#34;Illegal number of parameters\u0026#34; exit 2 fi ## Check PGRESIZE matches current postgresql volume size postgresql_volume_size=$(kubectl get postgresql \u0026#34;${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; -o jsonpath=\u0026#34;{.spec.volume.size}\u0026#34;) if [ \u0026#34;${postgresql_volume_size}\u0026#34; != \u0026#34;${PGRESIZE}\u0026#34; ]; then echo \u0026#34;Invalid resize ${PGRESIZE}, expected ${postgresql_volume_size}\u0026#34; exit 2 fi ## Scale the postgres cluster to 1 member kubectl patch postgresql \u0026#34;${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34; : \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;:\u0026#34;/spec/numberOfInstances\u0026#34;, \u0026#34;value\u0026#34; : 1}]\u0026#39; while [ $(kubectl get pods -l \u0026#34;application=spilo,cluster-name=${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; | grep -v NAME | wc -l) != 1 ] ; do echo \u0026#34; waiting for pods to terminate\u0026#34;; sleep 2; done ## Delete the inactive PVCs, resize the active PVC and wait for the resize to complete kubectl delete pvc \u0026#34;${PGDATA}-1\u0026#34; \u0026#34;${PGDATA}-2\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; kubectl patch -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;resources\u0026#34;: {\u0026#34;requests\u0026#34;: {\u0026#34;storage\u0026#34;: \u0026#34;\u0026#39;${PGRESIZE}\u0026#39;\u0026#34;}}}}\u0026#39; \u0026#34;pvc/${PGDATA}-0\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; while [ -z \u0026#39;$(kubectl describe pvc \u0026#34;{PGDATA}-0\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; | grep FileSystemResizeSuccessful\u0026#39; ] ; do echo \u0026#34; waiting for PVC to resize\u0026#34;; sleep 2; done ## Scale the postgres cluster back to 3 members kubectl patch postgresql \u0026#34;${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34; : \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;:\u0026#34;/spec/numberOfInstances\u0026#34;, \u0026#34;value\u0026#34; : 3}]\u0026#39; while [ $(kubectl get pods -l \u0026#34;application=spilo,cluster-name=${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; | grep -v NAME | grep -c \u0026#34;Running\u0026#34;) != 3 ] ; do echo \u0026#34; waiting for pods to restart\u0026#34;; sleep 2; done } ncn-w001# resize-postgresql-pvc cray-smd-postgres pgdata-cray-smd-postgres services 100Gi In order to persist any Postgres PVC storage volume size changes, it is necessary that this change also be made to the customer-managed customizations.yaml file. See the Postgres PVC Resize information in the Post Install Customizations.\nCase 2: msg=\u0026ldquo;could not sync cluster: could not sync roles: could not init db connection: could not init db connection: still failing after 8 retries\u0026rdquo; This generally means that some state in the Postgres operator is out of sync with that of the postgresql cluster, resulting in database connection issues. To resolve this SyncFailed case, restarting the Postgres operator by deleting the pod may clear up the issue.\nncn-w001# kubectl delete pod -l app.kubernetes.io/name=postgres-operator -n services ## Wait for the `postgres-operator` to restart ncn-w001# kubectl get pods -l app.kubernetes.io/name=postgres-operator -n services NAME READY STATUS RESTARTS AGE cray-postgres-operator-6fffc48b4c-mqz7z 2/2 Running 0 6m If the database connection has been down for a long period of time and the SyncFailed persists after the above steps, a restart of the cluster and the postgres-operator may be needed for the service to reconnect to the Postgres cluster. For example, if the cray-gitea service is not able to connect to the Postgres database and the connection has been failing for many hours, restart the cluster and operator.\nncn-w001# CLIENT=gitea-vcs ncn-w001# POSTGRESQL=gitea-vcs-postgres ncn-w001# NAMESPACE=services ## Scale the service to 0 ncn-w001# kubectl scale deployment ${CLIENT} -n ${NAMESPACE} --replicas=0 ## Restart the Postgres cluster and the postgres-operator ncn-w001# kubectl delete pod \u0026#34;${POSTGRESQL}-0\u0026#34; \u0026#34;${POSTGRESQL}-1\u0026#34; \u0026#34;${POSTGRESQL}-2\u0026#34; -n ${NAMESPACE} ncn-w001# kubectl delete pods -n services -lapp.kubernetes.io/name=postgres-operator ncn-w001# while [ $(kubectl get postgresql ${POSTGRESQL} -n ${NAMESPACE} -o json | jq -r \u0026#39;.status.PostgresClusterStatus\u0026#39;) != \u0026#34;Running\u0026#34; ]; do echo \u0026#34;waiting for ${POSTGRESQL}to start running\u0026#34;; sleep 2; done ## Scale the service back to 1 (for different services this may be to 3) ncn-w001# kubectl scale deployment ${CLIENT} -n ${NAMESPACE} --replicas=1 Case 3: msg=\u0026ldquo;error while syncing cluster state: could not sync roles: could not init db connection: could not init db connection: pq: password authentication failed for user \u0026lt;username\u0026gt;\u0026rdquo; This generally means that the password for the given user is not the same as that specified in the Kubernetes secret. This can occur if the postgresql cluster was rebuilt and the data was restored, leaving the Kubernetes secrets out of sync with the Postgres cluster. To resolve this SyncFailed case, gather the username and password for the credential from Kubernetes, and update the database with these values. For example, if the user postgres is failing to authenticate between the cray-smd services and the cray-smd-postgres cluster, get the password for the postgres user from the Kubernetes secret and update the password in the database.\nncn-w001# CLIENT=cray-smd ncn-w001# POSTGRESQL=cray-smd-postgres ncn-w001# NAMESPACE=services ## Scale the service to 0 ncn-w001# kubectl scale deployment ${CLIENT} -n ${NAMESPACE} --replicas=0 ncn-w001# while [ $(kubectl get pods -n ${NAMESPACE} -l app.kubernetes.io/name=\u0026#34;${CLIENT}\u0026#34; | grep -v NAME | wc -l) != 0 ] ; do echo \u0026#34; waiting for pods to terminate\u0026#34;; sleep 2; done ## Determine what secrets are associated with the postgresql credentials ncn-w001# kubectl get secrets -n ${NAMESPACE} | grep \u0026#34;${POSTGRESQL}.credentials\u0026#34; services hmsdsuser.cray-smd-postgres.credentials Opaque 2 31m services postgres.cray-smd-postgres.credentials Opaque 2 31m services service-account.cray-smd-postgres.credentials Opaque 2 31m services standby.cray-smd-postgres.credentials Opaque 2 31m ## Gather the decoded username and password for the user that is failing to authenticate - for example postgres.cray-smd-postgres.credentials : ncn-w001# kubectl get secret postgres.cray-smd-postgres.credentials -n ${NAMESPACE} -ojsonpath=\u0026#39;{.data.username}\u0026#39; | base64 -d postgres ncn-w001# kubectl get secret postgres.cray-smd-postgres.credentials -n ${NAMESPACE} -ojsonpath=\u0026#39;{.data.password}\u0026#39;| base64 -d ABCXYZ ## Exec into the postgres leader, and update the username and password in the database ncn-w001# kubectl exec \u0026#34;${POSTGRESQL}-0\u0026#34; -n ${NAMESPACE} -c postgres -it -- patronictl list +-------------------+---------------------+------------+--------+---------+----+-----------+ | Cluster | Member | Host | Role | State | TL | Lag in MB | +-------------------+---------------------+------------+--------+---------+----+-----------+ | cray-smd-postgres | cray-smd-postgres-0 | 10.42.0.25 | Leader | running | 1 | | | cray-smd-postgres | cray-smd-postgres-1 | 10.44.0.34 | | running | | 0 | | cray-smd-postgres | cray-smd-postgres-2 | 10.36.0.44 | | running | | 0 | +-------------------+---------------------+------------+--------+---------+----+-----------+ ncn-w001# POSTGRES_LEADER=cray-smd-postgres-0 ncn-w001# kubectl exec ${POSTGRES_LEADER} -n ${NAMESPACE} -c postgres -it -- bash root@cray-smd-postgres-0:/home/postgres# /usr/bin/psql postgres postgres postgres=# ALTER USER postgres WITH PASSWORD \u0026#39;ABCXYZ\u0026#39;; ALTER ROLE postgres=# ## Restart the postgresql cluster ncn-w001# kubectl delete pod \u0026#34;${POSTGRESQL}-0\u0026#34; \u0026#34;${POSTGRESQL}-1\u0026#34; \u0026#34;${POSTGRESQL}-2\u0026#34; -n ${NAMESPACE} ncn-w001# while [ $(kubectl get postgresql ${POSTGRESQL} -n ${NAMESPACE} -o json | jq -r \u0026#39;.status.PostgresClusterStatus\u0026#39;) != \u0026#34;Running\u0026#34; ]; do echo \u0026#34;waiting for ${POSTGRESQL}to start running\u0026#34;; sleep 2; done ## Scale the service back to 3 ncn-w001# kubectl scale deployment ${CLIENT} -n ${NAMESPACE} --replicas=3 ncn-w001# while [ $(kubectl get pods -n ${NAMESPACE} -l app.kubernetes.io/name=\u0026#34;${CLIENT}\u0026#34; | grep -v NAME | wc -l) != 3 ] ; do echo \u0026#34; waiting for pods to start running\u0026#34;; sleep 2; done Is a Cluster Member missing? Most services expect to maintain a Postgres cluster consisting of three pods for resiliency (SMA is one exception where only two pods are expected to exist).\nDetermine if a cluster member is missing For a given Postgres cluster, check how many pods are running.\nncn-w001# POSTGRESQL=keycloak-postgres ncn-w001# NAMESPACE=services ncn-w001# kubectl get pods -A -l \u0026#34;application=spilo,cluster-name=${POSTGRESQL}\u0026#34; Recover from a missing member If the number of Postgres pods for the given cluster is more or less than expected, increase or decrease as needed. This example will patch the keycloak-postgres cluster resource so that three pods should be running.\nncn-w001# POSTGRESQL=keycloak-postgres ncn-w001# NAMESPACE=services ncn-w001# kubectl patch postgresql \u0026#34;${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34; : \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;:\u0026#34;/spec/numberOfInstances\u0026#34;, \u0026#34;value\u0026#34; : 3}]\u0026#39; Confirm the number of cluster members, otherwise known as pods, by checking the postgresql resource.\nncn-w001# kubectl get postgresql ${POSTGRESQL} -n ${NAMESPACE} NAME TEAM VERSION PODS VOLUME CPU-REQUEST MEMORY-REQUEST AGE STATUS keycloak-postgres keycloak 11 3 10Gi 29m Running If a pod is starting but remains in Pending, CrashLoopBackOff, ImagePullBackOff or other non Running states, describe the pod and/or get logs from the pod for further analysis. For example:\nncn-w001# kubectl get pods -A -l \u0026#34;application=spilo,cluster-name=${POSTGRESQL}\u0026#34; NAMESPACE NAME READY STATUS RESTARTS AGE services keycloak-postgres-0 0/3 Pending 0 36m services keycloak-postgres-1 3/3 Running 0 35m services keycloak-postgres-2 3/3 Running 0 34m ncn-w001# kubectl describe pod \u0026#34;${POSTGRESQL}-0\u0026#34; -n ${NAMESPACE} ncn-w001# kubectl logs \u0026#34;${POSTGRESQL}-0\u0026#34; -c postgres -n ${NAMESPACE} Is the Postgres Leader missing? If a Postgres cluster no longer has a leader, the database will need to be recovered.\nDetermine if the Postgres Leader is missing ncn-w001# POSTGRESQL=cray-smd-postgres ncn-w001# NAMESPACE=services ncn-w001# kubectl exec ${POSTGRESQL} -n ${NAMESPACE} -c postgres -- patronictl list +-------------------+---------------------+------------+------+--------------+----+-----------+ | Cluster | Member | Host | Role | State | TL | Lag in MB | +-------------------+---------------------+------------+------+--------------+----+-----------+ | cray-smd-postgres | cray-smd-postgres-0 | 10.42.0.25 | | running | | unknown | | cray-smd-postgres | cray-smd-postgres-1 | 10.44.0.34 | | start failed | | unknown | | cray-smd-postgres | cray-smd-postgres-2 | 10.36.0.44 | | start failed | | unknown | +-------------------+---------------------+------------+------+--------------+----+-----------+ Recover from a missing Postgres Leader See the Recover from Postgres WAL Event procedure.\n"
},
{
	"uri": "/docs-csm/en-11/operations/kubernetes/view_postgres_information_for_system_databases/",
	"title": "View Postgres Information For System Databases",
	"tags": [],
	"description": "",
	"content": "View Postgres Information for System Databases Postgres uses SQL language to store and manage databases on the system. This procedure describes how to view and obtain helpful information about system databases, as well as the types of data being stored.\nPrerequisites This procedure requires administrative privileges.\nProcedure   Log in to the Postgres container.\nncn-w001# kubectl -n services exec -it cray-smd-postgres-0 -- bash Defaulting container name to postgres. Use \u0026#39;kubectl describe pod/cray-smd-postgres-0 -n services\u0026#39; to see all of the containers in this pod. ____ _ _ / ___| _ __ (_) | ___ \\___ \\| \u0026#39;_ \\| | |/ _ \\  ___) | |_) | | | (_) | |____/| .__/|_|_|\\___/ |_| This container is managed by runit, when stopping/starting services use sv Examples: sv stop cron sv restart patroni Current status: (sv status /etc/service/*) run: /etc/service/cron: (pid 26) 487273s run: /etc/service/patroni: (pid 24) 487273s run: /etc/service/pgqd: (pid 25) 487273s   Log in as the postgres user.\nroot@cray-smd-postgres-0:/home/postgres# psql -U postgres psql (12.2 (Ubuntu 12.2-1.pgdg18.04+1), server 11.7 (Ubuntu 11.7-1.pgdg18.04+1)) Type \u0026#34;help\u0026#34; for help. postgres=#   List the existing databases.\npostgres=# \\l List of databases Name | Owner | Encoding | Collate | Ctype | Access privileges ------------+-----------------+----------+-------------+-------------+----------------------- hmsds | hmsdsuser | UTF8 | en_US.UTF-8 | en_US.UTF-8 | postgres | postgres | UTF8 | en_US.UTF-8 | en_US.UTF-8 | service_db | service_account | UTF8 | en_US.UTF-8 | en_US.UTF-8 | template0 | postgres | UTF8 | en_US.UTF-8 | en_US.UTF-8 | =c/postgres + | | | | | postgres=CTc/postgres template1 | postgres | UTF8 | en_US.UTF-8 | en_US.UTF-8 | =c/postgres + | | | | | postgres=CTc/postgres (5 rows)   Establish a connection to the desired database.\nIn the example below, the hmsds database is used.\npostgres=# \\c hmsds psql (12.2 (Ubuntu 12.2-1.pgdg18.04+1), server 11.7 (Ubuntu 11.7-1.pgdg18.04+1)) You are now connected to database \u0026#34;hmsds\u0026#34; as user \u0026#34;postgres\u0026#34;. hmsds=#   List the data types that are in the database being viewed.\nhmsds-# \\dt List of relations Schema | Name | Type | Owner --------+-------------------------+-------+----------- public | comp_endpoints | table | hmsdsuser public | comp_eth_interfaces | table | hmsdsuser public | component_group_members | table | hmsdsuser public | component_groups | table | hmsdsuser public | component_lock_members | table | hmsdsuser public | component_locks | table | hmsdsuser public | components | table | hmsdsuser public | discovery_status | table | hmsdsuser public | hwinv_by_fru | table | hmsdsuser public | hwinv_by_loc | table | hmsdsuser public | hwinv_hist | table | hmsdsuser public | job_state_rf_poll | table | hmsdsuser public | job_sync | table | hmsdsuser public | node_nid_mapping | table | hmsdsuser public | power_mapping | table | hmsdsuser public | rf_endpoints | table | hmsdsuser public | schema_migrations | table | hmsdsuser public | scn_subscriptions | table | hmsdsuser public | service_endpoints | table | hmsdsuser public | system | table | hmsdsuser (20 rows)   "
},
{
	"uri": "/docs-csm/en-11/operations/kubernetes/restore_an_etcd_cluster_from_a_backup/",
	"title": "Restore An Etcd Cluster From A Backup",
	"tags": [],
	"description": "",
	"content": "Restore an etcd Cluster from a Backup Use an existing backup of a healthy etcd cluster to restore an unhealthy cluster to a healthy state.\nThe commands in this procedure can be run on any master node (ncn-mXXX) or worker node (ncn-wXXX) on the system.\n NOTE\nEtcd Clusters can be restored using the automation script or the manual procedure below. The automation script follows the same steps as the manual procedure. If the automation script fails to get the date from backups, follow the manual procedure.\n Prerequisites A backup of a healthy etcd cluster has been created.\nRestore with Automation Script The automated script will restore the cluster from the most recent backup if it finds a backup created within the last 7 days. If it does not discover a backup within the last 7 days, it will ask the user if they would like to rebuild the cluster.\nncn-w001 # cd /opt/cray/platform-utils/etcd_restore_rebuild_util # rebuild/restore a single cluster ncn-w001:/opt/cray/platform-utils/etcd_restore_rebuild_util # ./etcd_restore_rebuild.sh -s cray-bos-etcd # rebuild/restore multiple clusters ncn-w001:/opt/cray/platform-utils/etcd_restore_rebuild_util # ./etcd_restore_rebuild.sh -m cray-bos-etcd,cray-uas-mgr-etcd # rebuild/restore all clusters ncn-w001:/opt/cray/platform-utils/etcd_restore_rebuild_util # ./etcd_restore_rebuild.sh -a An example using the automation script is below.\nncn-m001:/opt/cray/platform-utils/etcd_restore_rebuild_util # ./etcd_restore_rebuild.sh -s cray-externaldns-etcd The following etcd clusters will be restored/rebuilt: cray-externaldns-etcd You will be accepting responsibility for any missing data if there is a restore/rebuild over a running etcd k/v. HPE assumes no responsibility. Proceed restoring/rebuilding? (yes/no) yes Proceeding: restoring/rebuilding etcd clusters. ----- Restoring from cray-externaldns/etcd.backup_v8362_2021-08-18-20:00:09 etcdrestore.etcd.database.coreos.com/cray-externaldns-etcd created - 3/3 Running Successfully restored cray-externaldns-etcd etcdrestore.etcd.database.coreos.com \u0026quot;cray-externaldns-etcd\u0026quot; deleted Restore with Manual Procedure   List the backups for the desired etcd cluster.\nThe example below uses the Boot Orchestration Service (BOS).\nncn-w001# kubectl exec -it -n operators \\ $(kubectl get pod -n operators | grep etcd-backup-restore | head -1 | awk \u0026#39;{print $1}\u0026#39;) \\ -c boto3 -- list_backups cray-bos cray-bos/etcd.backup_v108497_2020-03-20-23:42:37 cray-bos/etcd.backup_v125815_2020-03-21-23:42:37 cray-bos/etcd.backup_v143095_2020-03-22-23:42:38 cray-bos/etcd.backup_v160489_2020-03-23-23:42:37 cray-bos/etcd.backup_v176621_2020-03-24-23:42:37 cray-bos/etcd.backup_v277935_2020-03-30-23:52:54 cray-bos/etcd.backup_v86767_2020-03-19-18:00:05   Restore the cluster using a backup.\nReplace etcd.backup_v277935_2020-03-30-23:52:54 in the command below with the name of the backup being used.\nncn-w001# kubectl exec -it -n operators \\ $(kubectl get pod -n operators | grep etcd-backup-restore | head -1 | awk \u0026#39;{print $1}\u0026#39;) \\ -c util -- restore_from_backup cray-bos etcd.backup_v277935_2020-03-30-23:52:54 etcdrestore.etcd.database.coreos.com/cray-bos-etcd created   Restart the pods for the etcd cluster.\n  Watch the pods come back online.\nThis may take a couple minutes.\nncn-w001# kubectl -n services get pod | grep SERVICE_NAME cray-bos-etcd-498jn7th6p 1/1 Running 0 4h1m cray-bos-etcd-dj7d894227 1/1 Running 0 3h59m cray-bos-etcd-tk4pr4kgqk 1/1 Running 0 4   Delete the EtcdRestore custom resource.\nThis step will make it possible for future restores to occur. Replace the etcdrestore.etcd.database.coreos.com/cray-bos-etcd value with the name returned in step 2.\nncn-w001# kubectl -n services delete etcdrestore.etcd.database.coreos.com/cray-bos-etcd etcdrestore.etcd.database.coreos.com \u0026#34;cray-bos-etcd\u0026#34; deleted     "
},
{
	"uri": "/docs-csm/en-11/operations/kubernetes/restore_bare-metal_etcd_clusters_from_an_s3_snapshot/",
	"title": "Restore Bare-metal Etcd Clusters From An S3 Snapshot",
	"tags": [],
	"description": "",
	"content": "Restore Bare-Metal etcd Clusters from an S3 Snapshot The etcd cluster that serves Kubernetes on master nodes is backed up every 10 minutes. These backups are pushed to Ceph Rados Gateway (S3).\nRestoring the etcd cluster from backup is only meant to be used in a catastrophic scenario, whereby the Kubernetes cluster and master nodes are being rebuilt. This procedure shows how to restore the bare-metal etcd cluster from an Simple Storage Service (S3) snapshot.\nThe etcd cluster needs to be restored from a backup when the Kubernetes cluster and master nodes are being rebuilt.\nPrerequisites The Kubernetes cluster on master nodes is being rebuilt.\nProcedure   Select a snapshot to restore a backup.\nThe following command lists the available backups. It must be run from the /opt/cray/platform-utils/s3 directory.\nncn# ./list-objects.py --bucket-name etcd-backup bare-metal/etcd-backup-2020-02-04-18-00-10.tar.gz bare-metal/etcd-backup-2020-02-04-18-10-06.tar.gz bare-metal/etcd-backup-2020-02-04-18-20-02.tar.gz bare-metal/etcd-backup-2020-02-04-18-30-10.tar.gz bare-metal/etcd-backup-2020-02-04-18-40-06.tar.gz bare-metal/etcd-backup-2020-02-04-18-50-03.tar.gz Note the file name for the desired snapshot/backup.\n  Download the snapshot and copy it to all NCN master nodes.\n  Retrieve the backup from S3 and uncompress it.\nncn# mkdir /tmp/etcd_restore ncn# cd /opt/cray/platform-utils/s3 ncn# ./download-file.py --bucket-name etcd-backup \\ --key-name bare-metal/etcd-backup-2020-02-04-18-50-03.tar.gz \\ --file-name /tmp/etcd_restore/etcd-backup-2020-02-04-18-50-03.tar.gz ncn# cd /tmp/etcd_restore ncn# gunzip etcd-backup-2020-02-04-18-50-03.tar.gz ncn# tar -xvf etcd-backup-2020-02-04-18-50-03.tar ncn# mv etcd-backup-2020-02-04-18-50-03/etcd-dump.bin /tmp   Push the file to the other NCN master nodes.\nncn# scp /tmp/etcd-dump.bin ncn-m002:/tmp ncn# scp /tmp/etcd-dump.bin ncn-m003:/tmp     Prepare to restore the member directory for ncn-m001.\n  Log in as root to ncn-m001.\n  Create a new temporary /tmp/etcd_restore directory.\nncn-m001# mkdir /tmp/etcd_restore   Change to the /tmp/etcd_restore directory.\nncn-m001# cd /tmp/etcd_restore   Retrieve the \u0026lsquo;initial-cluster\u0026rsquo; and \u0026lsquo;initial-advertise-peer-urls\u0026rsquo; values from the kubeadmcfg.yaml file.\nThe returned values will be used in the next step.\nncn-m001# grep -e initial-cluster: -e initial-advertise-peer-urls: \\ /etc/kubernetes/kubeadmcfg.yaml initial-cluster: ncn-m001=https://10.252.1.7:2380,ncn-m002=https://10.252.1.8:2380,ncn-m003=https://10.252.1.9:2380 initial-advertise-peer-urls: https://10.252.1.7:2380   Restore the member directory.\nncn-m001# ETCDCTL_API=3 etcdctl --cacert /etc/kubernetes/pki/etcd/ca.crt \\  --cert /etc/kubernetes/pki/etcd/server.crt \\  --key /etc/kubernetes/pki/etcd/server.key \\  --name ncn-m001 \\  --initial-cluster ncn-m001=https://10.252.1.7:2380,ncn-m002=https://10.252.1.8:2380,ncn-m003=https://10.252.1.9:2380 \\  --initial-cluster-token tkn \\  --initial-advertise-peer-urls https://10.252.1.7:2380 \\  snapshot restore /tmp/etcd-dump.bin     Prepare to restore the member directory for ncn-m002.\n  Log in as root to ncn-m002.\n  Create a new temporary /tmp/etcd_restore directory.\nncn-m002# mkdir /tmp/etcd_restore   Change to the /tmp/etcd_restore directory.\nncn-m002# cd /tmp/etcd_restore   Retrieve the \u0026lsquo;initial-cluster\u0026rsquo; and \u0026lsquo;initial-advertise-peer-urls\u0026rsquo; values from the kubeadmcfg.yaml file.\nThe returned values will be used in the next step.\nncn-m002# grep -e initial-cluster: -e initial-advertise-peer-urls: \\ /etc/kubernetes/kubeadmcfg.yaml initial-cluster: ncn-m001=https://10.252.1.7:2380,ncn-m002=https://10.252.1.8:2380,ncn-m003=https://10.252.1.9:2380 initial-advertise-peer-urls: https://10.252.1.8:2380   Restore the member directory.\nncn-m002# ETCDCTL_API=3 etcdctl --cacert /etc/kubernetes/pki/etcd/ca.crt \\ --cert /etc/kubernetes/pki/etcd/server.crt \\ --key /etc/kubernetes/pki/etcd/server.key \\ --name ncn-m002 \\ --initial-cluster ncn-m001=https://10.252.1.7:2380,ncn-m002=https://10.252.1.8:2380,ncn-m003=https://10.252.1.9:2380 \\ --initial-cluster-token tkn \\ --initial-advertise-peer-urls https://10.252.1.8:2380 \\ snapshot restore /tmp/etcd-dump.bin     Prepare to restore the member directory for ncn-m003.\n  Log in as root to ncn-m003.\n  Create a new temporary /tmp/etcd_restore directory.\nncn-m003# mkdir /tmp/etcd_restore   Change to the /tmp/etcd_restore directory.\nncn-m003# cd /tmp/etcd_restore   Retrieve the \u0026lsquo;initial-cluster\u0026rsquo; and \u0026lsquo;initial-advertise-peer-urls\u0026rsquo; values from the kubeadmcfg.yaml file.\nThe returned values will be used in the next step.\nncn-m003# grep -e initial-cluster: -e initial-advertise-peer-urls: \\ /etc/kubernetes/kubeadmcfg.yaml initial-cluster: ncn-m001=https://10.252.1.7:2380,ncn-m002=https://10.252.1.8:2380,ncn-m003=https://10.252.1.9:2380 initial-advertise-peer-urls: https://10.252.1.9:2380   Restore the member directory.\nncn-m003# ETCDCTL_API=3 etcdctl --cacert /etc/kubernetes/pki/etcd/ca.crt \\ --cert /etc/kubernetes/pki/etcd/server.crt \\ --key /etc/kubernetes/pki/etcd/server.key \\ --name ncn-m003 \\ --initial-cluster ncn-m001=https://10.252.1.7:2380,ncn-m002=https://10.252.1.8:2380,ncn-m003=https://10.252.1.9:2380 \\ --initial-cluster-token tkn \\ --initial-advertise-peer-urls https://10.252.1.9:2380 \\ snapshot restore /tmp/etcd-dump.bin     Stop the current running cluster.\nIf the cluster is currently running, run the following command on all three master nodes (ncn-m001, ncn-m002, ncn-m003).\n  Stop the cluster on ncn-m001.\nncn-m001# systemctl stop etcd   Stop the cluster on ncn-m002.\nncn-m002# systemctl stop etcd   Stop the cluster on ncn-m003.\nncn-m003# systemctl stop etcd     Start the restored cluster on each master node.\nRun the following commands on all three master nodes (ncn-m001, ncn-m002, ncn-m003) to start the restored cluster.\n  Start the cluster on ncn-m001.\nncn-m001# rm -rf /var/lib/etcd/member ncn-m001# mv ncn-m001.etcd/member/ /var/lib/etcd/ ncn-m001# systemctl start etcd   Start the cluster on ncn-m002.\nncn-m002# rm -rf /var/lib/etcd/member ncn-m002# mv ncn-m002.etcd/member/ /var/lib/etcd/ ncn-m002# systemctl start etcd   Start the cluster on ncn-m003.\nncn-m003# rm -rf /var/lib/etcd/member ncn-m003# mv ncn-m003.etcd/member/ /var/lib/etcd/ ncn-m003# systemctl start etcd     Confirm the membership of the cluster.\nncn-m001# ETCDCTL_API=3 etcdctl --cacert /etc/kubernetes/pki/etcd/ca.crt \\ --cert /etc/kubernetes/pki/etcd/server.crt \\ --key /etc/kubernetes/pki/etcd/server.key member list 448a8d056377359a, started, ncn-m001, https://10.252.1.7:2380, https://10.252.1.7:2379,https://127.0.0.1:2379 986f6ff2a30b01cb, started, ncn-m002, https://10.252.1.8:2380, https://10.252.1.8:2379,https://127.0.0.1:2379 d5a8e497e2788510, started, ncn-m003, https://10.252.1.9:2380, https://10.252.1.9:2379,https://127.0.0.1:2379   "
},
{
	"uri": "/docs-csm/en-11/operations/kubernetes/restore_postgres/",
	"title": "Restore Postgres",
	"tags": [],
	"description": "",
	"content": "Restore Postgres Below are the service specific steps required to restore data to a Postgres cluster.\nRestore Postgres Procedures by Service:\n Restore Postgres for Spire Restore Postgres for Keycloak Restore Postgres for HSM (Hardware State Manager) Restore Postgres for SLS (System Layout Service) Restore Postgres for VCS Restore Postgres for Capsules Services  \nRestore Postgres for Spire In the event that the spire Postgres cluster is in a state that the cluster must be rebuilt and the data restored, the following procedures are recommended. This assumes that a dump of the database exists.\n  Copy the database dump to an accessible location.\n  If a manual dump of the database was taken, check that the dump file exists in a location off the Postgres cluster. It will be needed in the steps below.\n  If the database is being automatically backed up, then the most recent version of the dump and the secrets should exist in the postgres-backup S3 bucket. These will be needed in the steps below. List the files in the postgres-backup S3 bucket and if the files exist, download the dump and secrets out of the S3 bucket. The python3 scripts below can be used to help list and download the files. Note that the .psql file contains the database dump and the .manifest file contains the secrets. The aws_access_key_id and aws_secret_access_key will need to be set based on the postgres-backup-s3-credentials secret.\n  ncn-w001# export S3_ACCESS_KEY=`kubectl get secrets postgres-backup-s3-credentials -ojsonpath=\u0026#39;{.data.access_key}\u0026#39; | base64 --decode` ncn-w001# export S3_SECRET_KEY=`kubectl get secrets postgres-backup-s3-credentials -ojsonpath=\u0026#39;{.data.secret_key}\u0026#39; | base64 --decode` list.py:\nimport io import boto3 import os # postgres-backup-s3-credentials are needed to list keys in the postgres-backup bucket s3_access_key = os.environ[\u0026#39;S3_ACCESS_KEY\u0026#39;] s3_secret_key = os.environ[\u0026#39;S3_SECRET_KEY\u0026#39;] s3 = boto3.resource( \u0026#39;s3\u0026#39;, endpoint_url=\u0026#39;http://rgw-vip.nmn\u0026#39;, aws_access_key_id=s3_access_key, aws_secret_access_key=s3_secret_key, verify=False) backup_bucket = s3.Bucket(\u0026#39;postgres-backup\u0026#39;) for file in backup_bucket.objects.filter(Prefix=\u0026#39;spire-postgres\u0026#39;): print(file.key) download.py:\nUpdate the script for the specific .manifest and .psql files you wish to download from S3.\nimport io import boto3 import os # postgres-backup-s3-credentials are needed to download from postgres-backup bucket s3_access_key = os.environ[\u0026#39;S3_ACCESS_KEY\u0026#39;] s3_secret_key = os.environ[\u0026#39;S3_SECRET_KEY\u0026#39;] s3_client = boto3.client( \u0026#39;s3\u0026#39;, endpoint_url=\u0026#39;http://rgw-vip.nmn\u0026#39;, aws_access_key_id=s3_access_key, aws_secret_access_key=s3_secret_key, verify=False) response = s3_client.download_file(\u0026#39;postgres-backup\u0026#39;, \u0026#39;spire-postgres-2021-07-21T19:03:18.manifest\u0026#39;, \u0026#39;spire-postgres-2021-07-21T19:03:18.manifest\u0026#39;) response = s3_client.download_file(\u0026#39;postgres-backup\u0026#39;, \u0026#39;spire-postgres-2021-07-21T19:03:18.psql\u0026#39;, \u0026#39;spire-postgres-2021-07-21T19:03:18.psql\u0026#39;)   Scale the spire service to 0.\nncn-w001# CLIENT=spire-server ncn-w001# NAMESPACE=spire ncn-w001# POSTGRESQL=spire-postgres ncn-w001# kubectl scale statefulset ${CLIENT} -n ${NAMESPACE} --replicas=0 # Wait for the pods to terminate ncn-w001# while [ $(kubectl get pods -n ${NAMESPACE} -l app.kubernetes.io/name=\u0026#34;${CLIENT}\u0026#34; | grep -v NAME | wc -l) != 0 ] ; do echo \u0026#34; waiting for pods to terminate\u0026#34;; sleep 2; done   Delete the spire Postgres cluster.\nncn-w001# kubectl get postgresql ${POSTGRESQL} -n ${NAMESPACE} -o json | jq \u0026#39;del(.spec.selector)\u0026#39; | jq \u0026#39;del(.spec.template.metadata.labels.\u0026#34;controller-uid\u0026#34;)\u0026#39; | jq \u0026#39;del(.status)\u0026#39; \u0026gt; postgres-cr.json ncn-w001# kubectl delete -f postgres-cr.json # Wait for the pods to terminate ncn-w001# while [ $(kubectl get pods -l \u0026#34;application=spilo,cluster-name=${POSTGRESQL}\u0026#34; -n ${NAMESPACE} | grep -v NAME | wc -l) != 0 ] ; do echo \u0026#34; waiting for pods to terminate\u0026#34;; sleep 2; done   Create a new single instance spire Postgres cluster.\nncn-w001# cp postgres-cr.json postgres-orig-cr.json ncn-w001# jq \u0026#39;.spec.numberOfInstances = 1\u0026#39; postgres-orig-cr.json \u0026gt; postgres-cr.json ncn-w001# kubectl create -f postgres-cr.json # Wait for the pod and Postgres cluster to start running ncn-w001# while [ $(kubectl get pods -l \u0026#34;application=spilo,cluster-name=${POSTGRESQL}\u0026#34; -n ${NAMESPACE} | grep -v NAME | wc -l) != 1 ] ; do echo \u0026#34; waiting for pod to start running\u0026#34;; sleep 2; done ncn-w001# while [ $(kubectl get postgresql \u0026#34;${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; -o json | jq -r \u0026#39;.status.PostgresClusterStatus\u0026#39;) != \u0026#34;Running\u0026#34; ] ; do echo \u0026#34; waiting for postgresql to start running\u0026#34;; sleep 2; done   Copy the database dump file to the Postgres member.\nncn-w001# DUMPFILE=spire-postgres-2021-07-21T19:03:18.psql ncn-w001# kubectl cp ./${DUMPFILE} \u0026#34;${POSTGRESQL}-0\u0026#34;:/home/postgres/${DUMPFILE} -c postgres -n ${NAMESPACE}   Restore the data.\nncn-w001# kubectl exec \u0026#34;${POSTGRESQL}-0\u0026#34; -c postgres -n ${NAMESPACE} -it -- psql -U postgres \u0026lt; ${DUMPFILE}   Either update or re-create the spire-postgres secrets.\n    Update the secrets in Postgres.\nIf a manual dump was done, and the secrets were not saved, then the secrets in the newly created Postgres cluster will need to be updated.\nBased off the four spire-postgres secrets, collect the password for each Postgres username: postgres, service_account, spire, and standby. Then kubectl exec into the Postgres pod and update the password for each user. For example:\nncn-w001# for secret in postgres.spire-postgres.credentials service-account.spire-postgres.credentials spire.spire-postgres.credentials standby.spire-postgres.credentials; do echo -n \u0026#34;secret ${secret}username \u0026amp; password: \u0026#34;; echo -n \u0026#34;`kubectl get secret ${secret}-n ${NAMESPACE}-ojsonpath=\u0026#39;{.data.username}\u0026#39; | base64 -d` \u0026#34;; echo `kubectl get secret ${secret} -n ${NAMESPACE} -ojsonpath=\u0026#39;{.data.password}\u0026#39;| base64 -d`; done secret postgres.spire-postgres.credentials username \u0026amp; password: postgres ABCXYZ secret service-account.spire-postgres.credentials username \u0026amp; password: service_account ABC123 secret spire.spire-postgres.credentials username \u0026amp; password: spire XYZ123 secret standby.spire-postgres.credentials username \u0026amp; password: standby 123456 ncn-w001# kubectl exec \u0026#34;${POSTGRESQL}-0\u0026#34; -n ${NAMESPACE} -c postgres -it -- bash root@spire-postgres-0:/home/postgres# /usr/bin/psql postgres postgres postgres=# ALTER USER postgres WITH PASSWORD \u0026#39;ABCXYZ\u0026#39;; ALTER ROLE postgres=# ALTER USER service-account WITH PASSWORD \u0026#39;ABC123\u0026#39;; ALTER ROLE postgres=#ALTER USER spire WITH PASSWORD \u0026#39;XYZ123\u0026#39;; ALTER ROLE postgres=#ALTER USER standby WITH PASSWORD \u0026#39;123456\u0026#39;; ALTER ROLE postgres=#   Re-create secrets in Kubernetes.\nIf the Postgres secrets were auto-backed up, then re-create the secrets in Kubernetes.\nDelete and re-create the four spire-postgres secrets using the manifest that was copied from S3 in step 1 above.\nncn-w001# MANIFEST=spire-postgres-2021-07-21T19:03:18.manifest ncn-w001# kubectl delete secret postgres.spire-postgres.credentials service-account.spire-postgres.credentials spire.spire-postgres.credentials standby.spire-postgres.credentials -n ${NAMESPACE} ncn-w001# kubectl apply -f ${MANIFEST}    Restart the Postgres cluster.\nncn-w001# kubectl delete pod -n ${NAMESPACE} \u0026#34;${POSTGRESQL}-0\u0026#34; # Wait for the postgresql pod to start ncn-w001# while [ $(kubectl get pods -l \u0026#34;application=spilo,cluster-name=${POSTGRESQL}\u0026#34; -n ${NAMESPACE} | grep -v NAME | wc -l) != 1 ] ; do echo \u0026#34; waiting for pods to start running\u0026#34;; sleep 2; done   Scale the Postgres cluster back to 3 instances.\nncn-w001# kubectl patch postgresql \u0026#34;${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34; : \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;:\u0026#34;/spec/numberOfInstances\u0026#34;, \u0026#34;value\u0026#34; : 3}]\u0026#39; # Wait for the postgresql cluster to start running ncn-w001# while [ $(kubectl get postgresql \u0026#34;${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; -o json | jq -r \u0026#39;.status.PostgresClusterStatus\u0026#39;) != \u0026#34;Running\u0026#34; ] ; do echo \u0026#34; waiting for postgresql to start running\u0026#34;; sleep 2; done   Scale the spire service back to 3 replicas.\nncn-w001# kubectl scale statefulset ${CLIENT} -n ${NAMESPACE} --replicas=3 # Wait for the spire pods to start ncn-w001# while [ $(kubectl get pods -n ${NAMESPACE} -l app.kubernetes.io/name=\u0026#34;${CLIENT}\u0026#34; | grep -v NAME | wc -l) != 3 ] ; do echo \u0026#34; waiting for pods to start\u0026#34;; sleep 2; done   Restart the spire-agent on all the nodes.\nncn-w001# pdsh -w ncn-m00[1-3] \u0026#39;systemctl restart spire-agent\u0026#39; ncn-w001# pdsh -w ncn-w00[1-3] \u0026#39;systemctl restart spire-agent\u0026#39; ncn-w001# pdsh -w ncn-s00[1-3] \u0026#39;systemctl restart spire-agent\u0026#39;   Verify the service is working. The following should return a token.\nncn-w001:# /usr/bin/heartbeat-spire-agent api fetch jwt -socketPath=/root/spire/agent.sock -audience test   \nRestore Postgres for Keycloak In the event that the keycloak Postgres cluster is in a state that the cluster must be rebuilt and the data restored, the following procedures are recommended. This assumes that a dump of the database exists.\n  Copy the database dump to an accessible location.\n  If a manual dump of the database was taken, check that the dump file exists in a location off the Postgres cluster. It will be needed in the steps below.\n  If the database is being automatically backed up, then the most recent version of the dump and the secrets should exist in the postgres-backup S3 bucket. These will be needed in the steps below. List the files in the postgres-backup S3 bucket and if the files exist, download the dump and secrets out of the S3 bucket. The python3 scripts below can be used to help list and download the files. Note that the .psql file contains the database dump and the .manifest file contains the secrets. The aws_access_key_id and aws_secret_access_key will need to be set based on the postgres-backup-s3-credentials secret.\n  ncn-w001# export S3_ACCESS_KEY=`kubectl get secrets postgres-backup-s3-credentials -ojsonpath=\u0026#39;{.data.access_key}\u0026#39; | base64 --decode` ncn-w001# export S3_SECRET_KEY=`kubectl get secrets postgres-backup-s3-credentials -ojsonpath=\u0026#39;{.data.secret_key}\u0026#39; | base64 --decode` list.py:\nimport io import boto3 import os # postgres-backup-s3-credentials are needed to list keys in the postgres-backup bucket s3_access_key = os.environ[\u0026#39;S3_ACCESS_KEY\u0026#39;] s3_secret_key = os.environ[\u0026#39;S3_SECRET_KEY\u0026#39;] s3 = boto3.resource( \u0026#39;s3\u0026#39;, endpoint_url=\u0026#39;http://rgw-vip.nmn\u0026#39;, aws_access_key_id=s3_access_key, aws_secret_access_key=s3_secret_key, verify=False) backup_bucket = s3.Bucket(\u0026#39;postgres-backup\u0026#39;) for file in backup_bucket.objects.filter(Prefix=\u0026#39;keycloak-postgres\u0026#39;): print(file.key) download.py:\nUpdate the script for the specific .manifest and .psql files you wish to download from S3.\nimport io import boto3 import os # postgres-backup-s3-credentials are needed to download from postgres-backup bucket s3_access_key = os.environ[\u0026#39;S3_ACCESS_KEY\u0026#39;] s3_secret_key = os.environ[\u0026#39;S3_SECRET_KEY\u0026#39;] s3_client = boto3.client( \u0026#39;s3\u0026#39;, endpoint_url=\u0026#39;http://rgw-vip.nmn\u0026#39;, aws_access_key_id=s3_access_key, aws_secret_access_key=s3_secret_key, verify=False) response = s3_client.download_file(\u0026#39;postgres-backup\u0026#39;, \u0026#39;keycloak-postgres-2021-07-29T17:56:07.manifest\u0026#39;, \u0026#39;keycloak-postgres-2021-07-29T17:56:07.manifest\u0026#39;) response = s3_client.download_file(\u0026#39;postgres-backup\u0026#39;, \u0026#39;keycloak-postgres-2021-07-29T17:56:07.psql\u0026#39;, \u0026#39;keycloak-postgres-2021-07-29T17:56:07.psql\u0026#39;)   Scale the keycloak service to 0.\nncn-w001# CLIENT=cray-keycloak ncn-w001# NAMESPACE=services ncn-w001# POSTGRESQL=keycloak-postgres ncn-w001# kubectl scale statefulset ${CLIENT} -n ${NAMESPACE} --replicas=0 # Wait for the pods to terminate ncn-w001# while [ $(kubectl get pods -n ${NAMESPACE} -l app.kubernetes.io/instance=\u0026#34;${CLIENT}\u0026#34; | grep -v NAME | wc -l) != 0 ] ; do echo \u0026#34; waiting for pods to terminate\u0026#34;; sleep 2; done   Delete the keycloak Postgres cluster.\nncn-w001# kubectl get postgresql ${POSTGRESQL} -n ${NAMESPACE} -o json | jq \u0026#39;del(.spec.selector)\u0026#39; | jq \u0026#39;del(.spec.template.metadata.labels.\u0026#34;controller-uid\u0026#34;)\u0026#39; | jq \u0026#39;del(.status)\u0026#39; \u0026gt; postgres-cr.json ncn-w001# kubectl delete -f postgres-cr.json # Wait for the pods to terminate ncn-w001# while [ $(kubectl get pods -l \u0026#34;application=spilo,cluster-name=${POSTGRESQL}\u0026#34; -n ${NAMESPACE} | grep -v NAME | wc -l) != 0 ] ; do echo \u0026#34; waiting for pods to terminate\u0026#34;; sleep 2; done   Create a new single instance keycloak Postgres cluster.\nncn-w001# cp postgres-cr.json postgres-orig-cr.json ncn-w001# jq \u0026#39;.spec.numberOfInstances = 1\u0026#39; postgres-orig-cr.json \u0026gt; postgres-cr.json ncn-w001# kubectl create -f postgres-cr.json # Wait for the pod and Postgres cluster to start running ncn-w001# while [ $(kubectl get pods -l \u0026#34;application=spilo,cluster-name=${POSTGRESQL}\u0026#34; -n ${NAMESPACE} | grep -v NAME | wc -l) != 1 ] ; do echo \u0026#34; waiting for pod to start running\u0026#34;; sleep 2; done ncn-w001# while [ $(kubectl get postgresql \u0026#34;${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; -o json | jq -r \u0026#39;.status.PostgresClusterStatus\u0026#39;) != \u0026#34;Running\u0026#34; ] ; do echo \u0026#34; waiting for postgresql to start running\u0026#34;; sleep 2; done   Copy the database dump file to the Postgres member.\nncn-w001# DUMPFILE=keycloak-postgres-2021-07-29T17:56:07.psql ncn-w001# kubectl cp ./${DUMPFILE} \u0026#34;${POSTGRESQL}-0\u0026#34;:/home/postgres/${DUMPFILE} -c postgres -n ${NAMESPACE}   Restore the data.\nncn-w001# kubectl exec \u0026#34;${POSTGRESQL}-0\u0026#34; -c postgres -n ${NAMESPACE} -it -- psql -U postgres \u0026lt; ${DUMPFILE}   Either update or re-create the keycloak-postgres secrets.\n    Update the secrets in Postgres.\nIf a manual dump was done, and the secrets were not saved, then the secrets in the newly created Postgres cluster will need to be updated.\nBased off the three keycloak-postgres secrets, collect the password for each Postgres username: postgres, service_account, and standby. Then kubectl exec into the Postgres pod and update the password for each user. For example:\nncn-w001# for secret in postgres.keycloak-postgres.credentials service-account.keycloak-postgres.credentials standby.keycloak-postgres.credentials; do echo -n \u0026#34;secret ${secret}username \u0026amp; password: \u0026#34;; echo -n \u0026#34;`kubectl get secret ${secret}-n ${NAMESPACE}-ojsonpath=\u0026#39;{.data.username}\u0026#39; | base64 -d` \u0026#34;; echo `kubectl get secret ${secret} -n ${NAMESPACE} -ojsonpath=\u0026#39;{.data.password}\u0026#39;| base64 -d`; done secret postgres.keycloak-postgres.credentials username \u0026amp; password: postgres ABCXYZ secret service-account.keycloak-postgres.credentials username \u0026amp; password: service_account ABC123 secret standby.keycloak-postgres.credentials username \u0026amp; password: standby 123456 ncn-w001# kubectl exec \u0026#34;${POSTGRESQL}-0\u0026#34; -n ${NAMESPACE} -c postgres -it -- bash root@keycloak-postgres-0:/home/postgres# /usr/bin/psql postgres postgres postgres=# ALTER USER postgres WITH PASSWORD \u0026#39;ABCXYZ\u0026#39;; ALTER ROLE postgres=# ALTER USER service-account WITH PASSWORD \u0026#39;ABC123\u0026#39;; ALTER ROLE postgres=#ALTER USER standby WITH PASSWORD \u0026#39;123456\u0026#39;; ALTER ROLE postgres=#   Re-create secrets in Kubernetes.\nIf the Postgres secrets were automatically backed up, then re-create the secrets in Kubernetes.\nDelete and re-create the three keycloak-postgres secrets using the manifest that was copied from S3 in step 1 above.\nncn-w001# MANIFEST=keycloak-postgres-2021-07-29T17:56:07.manifest ncn-w001# kubectl delete secret postgres.keycloak-postgres.credentials service-account.keycloak-postgres.credentials standby.keycloak-postgres.credentials -n ${NAMESPACE} ncn-w001# kubectl apply -f ${MANIFEST}    Restart the Postgres cluster.\nncn-w001# kubectl delete pod -n ${NAMESPACE} \u0026#34;${POSTGRESQL}-0\u0026#34; # Wait for the postgresql pod to start ncn-w001# while [ $(kubectl get pods -l \u0026#34;application=spilo,cluster-name=${POSTGRESQL}\u0026#34; -n ${NAMESPACE} | grep -v NAME | wc -l) != 1 ] ; do echo \u0026#34; waiting for pods to start running\u0026#34;; sleep 2; done   Scale the Postgres cluster back to 3 instances.\nncn-w001# kubectl patch postgresql \u0026#34;${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34; : \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;:\u0026#34;/spec/numberOfInstances\u0026#34;, \u0026#34;value\u0026#34; : 3}]\u0026#39; # Wait for the postgresql cluster to start running. This may take a few minutes to complete. ncn-w001# while [ $(kubectl get postgresql \u0026#34;${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; -o json | jq -r \u0026#39;.status.PostgresClusterStatus\u0026#39;) != \u0026#34;Running\u0026#34; ] ; do echo \u0026#34; waiting for postgresql to start running\u0026#34;; sleep 2; done   Scale the keycloak service back to 3 replicas.\nncn-w001# kubectl scale statefulset ${CLIENT} -n ${NAMESPACE} --replicas=3 # Wait for the keycloak pods to start ncn-w001# while [ $(kubectl get pods -n ${NAMESPACE} -l app.kubernetes.io/instance=\u0026#34;${CLIENT}\u0026#34; | grep -v NAME | wc -l) != 3 ] ; do echo \u0026#34; waiting for pods to start\u0026#34;; sleep 2; done Also check the status of the keycloak pods. If there are pods that do not show that both containers are ready (READY is 2/2), wait a few seconds and re-run the command until all containers are ready.\nncn-w001# kubectl get pods -n ${NAMESPACE} -l app.kubernetes.io/instance=\u0026#34;${CLIENT}\u0026#34; NAME READY STATUS RESTARTS AGE cray-keycloak-0 2/2 Running 0 35s cray-keycloak-1 2/2 Running 0 35s cray-keycloak-2 2/2 Running 0 35s   Re-run the keycloak-setup and keycloak-users-localize jobs, and restart Keycloak gatekeeper.\n    Run the keycloak-setup job to restore the Kubernetes client secrets:\nncn-w001# kubectl get job -n ${NAMESPACE} -l app.kubernetes.io/instance=cray-keycloak -o json \u0026gt; keycloak-setup.json ncn-w001# cat keycloak-setup.json | jq \u0026#39;.items[0]\u0026#39; | jq \u0026#39;del(.metadata.creationTimestamp)\u0026#39; | jq \u0026#39;del(.metadata.managedFields)\u0026#39; | jq \u0026#39;del(.metadata.resourceVersion)\u0026#39; | jq \u0026#39;del(.metadata.selfLink)\u0026#39; | jq \u0026#39;del(.metadata.uid)\u0026#39; | jq \u0026#39;del(.spec.selector)\u0026#39; | jq \u0026#39;del(.spec.template.metadata.labels)\u0026#39; | jq \u0026#39;del(.status)\u0026#39; | kubectl replace --force -f - Check the status of the keycloak-setup job. If the COMPLETIONS value is not 1/1, wait a few seconds and run the command again until the COMPLETIONS value is 1/1.\nncn-w001# kubectl get jobs -n ${NAMESPACE} -l app.kubernetes.io/instance=cray-keycloak NAME COMPLETIONS DURATION AGE keycloak-setup-2 1/1 59s 91s   Run the keycloak-users-localize job to restore the users and groups in S3 and the Kubernetes configmap:\nncn-w001# kubectl get job -n ${NAMESPACE} -l app.kubernetes.io/instance=cray-keycloak-users-localize -o json \u0026gt; cray-keycloak-users-localize.json ncn-w001# cat cray-keycloak-users-localize.json | jq \u0026#39;.items[0]\u0026#39; | jq \u0026#39;del(.metadata.creationTimestamp)\u0026#39; | jq \u0026#39;del(.metadata.managedFields)\u0026#39; | jq \u0026#39;del(.metadata.resourceVersion)\u0026#39; | jq \u0026#39;del(.metadata.selfLink)\u0026#39; | jq \u0026#39;del(.metadata.uid)\u0026#39; | jq \u0026#39;del(.spec.selector)\u0026#39; | jq \u0026#39;del(.spec.template.metadata.labels)\u0026#39; | jq \u0026#39;del(.status)\u0026#39; | kubectl replace --force -f -` Check the status of the cray-keycloak-users-localize job. If the COMPLETIONS value is not 1/1, wait a few seconds and run the command again until the COMPLETIONS value is 1/1.\nncn-w001# kubectl get jobs -n ${NAMESPACE} -l app.kubernetes.io/instance=cray-keycloak-users-localize NAME COMPLETIONS DURATION AGE keycloak-users-localize-2 1/1 45s 49s   Restart Keycloak gatekeeper:\nncn-w001# kubectl rollout restart -n ${NAMESPACE} deployment/cray-keycloak-gatekeeper-ingress    Verify the service is working. The following should return an access_token for an existing user. Replace the  and  as appropriate.\nncn-w001:# curl -s -k -d grant_type=password -d client_id=shasta -d username=\u0026lt;username\u0026gt; -d password=\u0026lt;password\u0026gt; https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token {\u0026#34;access_token\u0026#34;:\u0026#34;....   \nRestore Postgres for VCS In the event that the VCS Postgres cluster is in a state that the cluster must be rebuilt and the data restored, the following procedures are recommended. This assumes that a dump of the database exists, as well as a backup of the VCS PVC.\n  Copy the database dump to an accessible location.\n  If a manual dump of the database was taken, check that the dump file exists in a location off the Postgres cluster. It will be needed in the steps below.\n  If the database is being automatically backed up, then the most recent version of the dump and the secrets should exist in the postgres-backup S3 bucket. These will be needed in the steps below. List the files in the postgres-backup S3 bucket and if the files exist, download the dump and secrets out of the S3 bucket. The python3 scripts below can be used to help list and download the files. Note that the .psql file contains the database dump and the .manifest file contains the secrets. The aws_access_key_id and aws_secret_access_key will need to be set based on the postgres-backup-s3-credentials secret.\n  ncn-w001# export S3_ACCESS_KEY=`kubectl get secrets postgres-backup-s3-credentials -ojsonpath=\u0026#39;{.data.access_key}\u0026#39; | base64 --decode` ncn-w001# export S3_SECRET_KEY=`kubectl get secrets postgres-backup-s3-credentials -ojsonpath=\u0026#39;{.data.secret_key}\u0026#39; | base64 --decode` list.py:\nimport io import boto3 import os # postgres-backup-s3-credentials are needed to list keys in the postgres-backup bucket s3_access_key = os.environ[\u0026#39;S3_ACCESS_KEY\u0026#39;] s3_secret_key = os.environ[\u0026#39;S3_SECRET_KEY\u0026#39;] s3 = boto3.resource( \u0026#39;s3\u0026#39;, endpoint_url=\u0026#39;http://rgw-vip.nmn\u0026#39;, aws_access_key_id=s3_access_key, aws_secret_access_key=s3_secret_key, verify=False) backup_bucket = s3.Bucket(\u0026#39;postgres-backup\u0026#39;) for file in backup_bucket.objects.filter(Prefix=\u0026#39;vcs-postgres\u0026#39;): print(file.key) download.py:\nUpdate the script for the specific .manifest and .psql files you wish to download from S3.\nimport boto3 import os # postgres-backup-s3-credentials are needed to download from postgres-backup bucket s3_access_key = os.environ[\u0026#39;S3_ACCESS_KEY\u0026#39;] s3_secret_key = os.environ[\u0026#39;S3_SECRET_KEY\u0026#39;] s3_client = boto3.client( \u0026#39;s3\u0026#39;, endpoint_url=\u0026#39;http://rgw-vip.nmn\u0026#39;, aws_access_key_id=s3_access_key, aws_secret_access_key=s3_secret_key, verify=False) response = s3_client.download_file(\u0026#39;postgres-backup\u0026#39;, \u0026#39;vcs-postgres-2021-07-21T19:03:18.manifest\u0026#39;, \u0026#39;vcs-postgres-2021-07-21T19:03:18.manifest\u0026#39;) response = s3_client.download_file(\u0026#39;postgres-backup\u0026#39;, \u0026#39;vcs-postgres-2021-07-21T19:03:18.psql\u0026#39;, \u0026#39;vcs-postgres-2021-07-21T19:03:18.psql\u0026#39;)   Scale the VCS service to 0.\nncn-w001# SERVICE=gitea-vcs ncn-w001# SERVICELABEL=vcs ncn-w001# NAMESPACE=services ncn-w001# POSTGRESQL=gitea-vcs-postgres ncn-w001# kubectl scale deployment ${SERVICE} -n ${NAMESPACE} --replicas=0 # Wait for the pods to terminate ncn-w001# while [ $(kubectl get pods -n ${NAMESPACE} -l app.kubernetes.io/name=\u0026#34;${SERVICELABEL}\u0026#34; | grep -v NAME | wc -l) != 0 ] ; do echo \u0026#34; waiting for pods to terminate\u0026#34;; sleep 2; done   Delete the VCS Postgres cluster.\nncn-w001# kubectl get postgresql ${POSTGRESQL} -n ${NAMESPACE} -o json | jq \u0026#39;del(.spec.selector)\u0026#39; | jq \u0026#39;del(.spec.template.metadata.labels.\u0026#34;controller-uid\u0026#34;)\u0026#39; | jq \u0026#39;del(.status)\u0026#39; \u0026gt; postgres-cr.json ncn-w001# kubectl delete -f postgres-cr.json # Wait for the pods to terminate ncn-w001# while [ $(kubectl get pods -l \u0026#34;application=spilo,cluster-name=${POSTGRESQL}\u0026#34; -n ${NAMESPACE} | grep -v NAME | wc -l) != 0 ] ; do echo \u0026#34; waiting for pods to terminate\u0026#34;; sleep 2; done   Create a new single instance vcs Postgres cluster.\nncn-w001# cp postgres-cr.json postgres-orig-cr.json ncn-w001# jq \u0026#39;.spec.numberOfInstances = 1\u0026#39; postgres-orig-cr.json \u0026gt; postgres-cr.json ncn-w001# kubectl create -f postgres-cr.json # Wait for the pod and Postgres cluster to start running ncn-w001# while [ $(kubectl get pods -l \u0026#34;application=spilo,cluster-name=${POSTGRESQL}\u0026#34; -n ${NAMESPACE} | grep -v NAME | wc -l) != 1 ] ; do echo \u0026#34; waiting for pod to start running\u0026#34;; sleep 2; done ncn-w001# while [ $(kubectl get postgresql \u0026#34;${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; -o json | jq -r \u0026#39;.status.PostgresClusterStatus\u0026#39;) != \u0026#34;Running\u0026#34; ] ; do echo \u0026#34; waiting for postgresql to start running\u0026#34;; sleep 2; done   Copy the database dump file to the Postgres member.\nncn-w001# DUMPFILE=gitea-vcs-postgres-2021-07-21T19:03:18.sql ncn-w001# kubectl cp ./${DUMPFILE} \u0026#34;${SERVICE}-0\u0026#34;:/home/postgres/${DUMPFILE} -c postgres -n services   Restore the data.\nncn-w001# kubectl exec \u0026#34;${SERVICE}-0\u0026#34; -c postgres -n services -it -- psql -U postgres \u0026lt; ${DUMPFILE}   Either update or re-create the gitea-vcs-postgres secrets.\n    Update the secrets in Postgres.\nIf a manual dump was done, and the secrets were not saved, then the secrets in the newly created Postgres cluster will need to be updated.\nBased off the three gitea-vcs-postgres secrets, collect the password for each Postgres username: postgres, service_account, and standby. Then kubectl exec into the Postgres pod and update the password for each user. For example:\nncn-w001# for secret in postgres.gitea-vcs-postgres.credentials service-account.gitea-vcs-postgres.credentials gitea.gitea-vcs-postgres.credentials standby.gitea-vcs-postgres.credentials; do echo -n \u0026#34;secret ${secret}username \u0026amp; password: \u0026#34;; echo -n \u0026#34;`kubectl get secret ${secret}-n ${NAMESPACE}-ojsonpath=\u0026#39;{.data.username}\u0026#39; | base64 -d` \u0026#34;; echo `kubectl get secret ${secret} -n ${NAMESPACE} -ojsonpath=\u0026#39;{.data.password}\u0026#39;| base64 -d`; done secret postgres.gitea-vcs-postgres.credentials username \u0026amp; password: postgres ABCXYZ secret service-account.gitea-vcs-postgres.credentials username \u0026amp; password: service_account ABC123 secret gitea.gitea-vcs-postgres.credentials username \u0026amp; password: gitea XYZ123 secret standby.gitea-vcs-postgres.credentials username \u0026amp; password: standby 123456 ncn-w001# kubectl exec \u0026#34;${POSTGRESQL}-0\u0026#34; -n ${NAMESPACE} -c postgres -it -- bash root@gitea-vcs-postgres-0:/home/postgres# /usr/bin/psql postgres postgres postgres=# ALTER USER postgres WITH PASSWORD \u0026#39;ABCXYZ\u0026#39;; ALTER ROLE postgres=# ALTER USER service-account WITH PASSWORD \u0026#39;ABC123\u0026#39;; ALTER ROLE postgres=#ALTER USER gitea WITH PASSWORD \u0026#39;XYZ123\u0026#39;; ALTER ROLE postgres=#ALTER USER standby WITH PASSWORD \u0026#39;123456\u0026#39;; ALTER ROLE postgres=#   Re-create secrets in Kubernetes.\nIf the Postgres secrets were auto-backed up, then re-create the secrets in Kubernetes.\nDelete and re-create the four gitea-vcs-postgres secrets using the manifest that was copied from S3 in step 1 above.\nncn-w001# MANIFEST=gitea-vcs-postgres-2021-07-21T19:03:18.manifest ncn-w001# kubectl delete secret postgres.gitea-vcs-postgres.credentials service-account.gitea-vcs-postgres.credentials standby.gitea-vcs-postgres.credentials -n services ncn-w001# kubectl apply -f ${MANIFEST}    Restart the Postgres cluster.\nncn-w001# kubectl delete pod -n ${NAMESPACE} \u0026#34;${POSTGRESQL}-0\u0026#34; # Wait for the postgresql pod to start ncn-w001# while [ $(kubectl get pods -l \u0026#34;application=spilo,cluster-name=${POSTGRESQL}\u0026#34; -n ${NAMESPACE} | grep -v NAME | wc -l) != 1 ] ; do echo \u0026#34; waiting for pods to start running\u0026#34;; sleep 2; done   Scale the Postgres cluster back to 3 instances.\nncn-w001# kubectl patch postgresql \u0026#34;${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34; : \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;:\u0026#34;/spec/numberOfInstances\u0026#34;, \u0026#34;value\u0026#34; : 3}]\u0026#39; # Wait for the postgresql cluster to start running ncn-w001# while [ $(kubectl get postgresql \u0026#34;${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; -o json | jq -r \u0026#39;.status.PostgresClusterStatus\u0026#39;) != \u0026#34;Running\u0026#34; ] ; do echo \u0026#34; waiting for postgresql to start running\u0026#34;; sleep 2; done   Scale the gitea service back up.\nncn-w001# kubectl scale deployment ${SERVICE} -n ${NAMESPACE} --replicas=3 # Wait for the gitea pods to start ncn-w001# while [ $(kubectl get pods -n ${NAMESPACE} -l app.kubernetes.io/name=\u0026#34;${SERVICELABEL}\u0026#34; | grep -v NAME | wc -l) != 3 ] ; do echo \u0026#34; waiting for pods to start\u0026#34;; sleep 2; done   \nRestore Postgres for Capsules Capsules Warehouse Server In the event that the Capsules Warehouse Postgres cluster is in a state that the cluster must be rebuilt and the data restored, the following procedures are recommended. This assumes that a dump of the database exists.\n  Copy the database dump to an accessible location.\n  If a manual dump of the database was taken, check that the dump file exists in a location off the Postgres cluster. It will be needed in the steps below.\n  If the database is being automatically backed up, then the most recent version of the dump and the secrets should exist in the postgres-backup S3 bucket. These will be needed in the steps below. List the files in the postgres-backup S3 bucket and if the files exist, download the dump and secrets out of the S3 bucket. The python3 scripts below can be used to help list and download the files. Note that the .psql file contains the database dump and the .manifest file contains the secrets. The aws_access_key_id and aws_secret_access_key will need to be set based on the postgres-backup-s3-credentials secret.\n  ncn-w001# export S3_ACCESS_KEY=`kubectl get secrets postgres-backup-s3-credentials -ojsonpath=\u0026#39;{.data.access_key}\u0026#39; | base64 --decode` ncn-w001# export S3_SECRET_KEY=`kubectl get secrets postgres-backup-s3-credentials -ojsonpath=\u0026#39;{.data.secret_key}\u0026#39; | base64 --decode` list.py:\nimport io import boto3 import os # postgres-backup-s3-credentials are needed to list keys in the postgres-backup bucket s3_access_key = os.environ[\u0026#39;S3_ACCESS_KEY\u0026#39;] s3_secret_key = os.environ[\u0026#39;S3_SECRET_KEY\u0026#39;] s3 = boto3.resource( \u0026#39;s3\u0026#39;, endpoint_url=\u0026#39;http://rgw-vip.nmn\u0026#39;, aws_access_key_id=s3_access_key, aws_secret_access_key=s3_secret_key, verify=False) backup_bucket = s3.Bucket(\u0026#39;postgres-backup\u0026#39;) for file in backup_bucket.objects.filter(Prefix=\u0026#39;capsules-warehouse-server-postgres\u0026#39;): print(file.key) download.py:\nUpdate the script for the specific .manifest and .psql files you wish to download from S3.\nimport io import boto3 import os # postgres-backup-s3-credentials are needed to download from postgres-backup bucket s3_access_key = os.environ[\u0026#39;S3_ACCESS_KEY\u0026#39;] s3_secret_key = os.environ[\u0026#39;S3_SECRET_KEY\u0026#39;] s3_client = boto3.client( \u0026#39;s3\u0026#39;, endpoint_url=\u0026#39;http://rgw-vip.nmn\u0026#39;, aws_access_key_id=s3_access_key, aws_secret_access_key=s3_secret_key, verify=False) response = s3_client.download_file(\u0026#39;postgres-backup\u0026#39;, \u0026#39;capsules-warehouse-server-postgres-2021-07-21T19:03:18.manifest\u0026#39;, \u0026#39;capsules-warehouse-server-postgres-2021-07-21T19:03:18.manifest\u0026#39;) response = s3_client.download_file(\u0026#39;postgres-backup\u0026#39;, \u0026#39;capsules-warehouse-server-postgres-2021-07-21T19:03:18.psql\u0026#39;, \u0026#39;capsules-warehouse-server-postgres-2021-07-21T19:03:18.psql\u0026#39;)   Scale the capsules-warehouse-server service to 0.\nncn-w001# CLIENT=capsules-warehouse-server ncn-w001# NAMESPACE=services ncn-w001# POSTGRESQL=capsules-warehouse-server-postgres ncn-w001# kubectl scale -n ${NAMESPACE} --replicas=0 deployment/${CLIENT} # Wait for the pods to terminate ncn-w001# while [ $(kubectl get pods -n ${NAMESPACE} -l app.kubernetes.io/name=\u0026#34;${CLIENT}\u0026#34; | grep -v NAME | wc -l) != 0 ] ; do echo \u0026#34; waiting for pods to terminate\u0026#34;; sleep 2; done   Delete the capsules-warehouse-server Postgres cluster.\nncn-w001# kubectl get postgresql ${POSTGRESQL} -n ${NAMESPACE} -o json | jq \u0026#39;del(.spec.selector)\u0026#39; | jq \u0026#39;del(.spec.template.metadata.labels.\u0026#34;controller-uid\u0026#34;)\u0026#39; | jq \u0026#39;del(.status)\u0026#39; \u0026gt; postgres-cr.json ncn-w001# kubectl delete -f postgres-cr.json # Wait for the pods to terminate ncn-w001# while [ $(kubectl get pods -l \u0026#34;application=spilo,cluster-name=${POSTGRESQL}\u0026#34; -n ${NAMESPACE} | grep -v NAME | wc -l) != 0 ] ; do echo \u0026#34; waiting for pods to terminate\u0026#34;; sleep 2; done   Create a new single instance capsules-warehouse-server Postgres cluster.\nncn-w001# cp postgres-cr.json postgres-orig-cr.json ncn-w001# jq \u0026#39;.spec.numberOfInstances = 1\u0026#39; postgres-orig-cr.json \u0026gt; postgres-cr.json ncn-w001# kubectl create -f postgres-cr.json # Wait for the pod and Postgres cluster to start running ncn-w001# while [ $(kubectl get pods -l \u0026#34;application=spilo,cluster-name=${POSTGRESQL}\u0026#34; -n ${NAMESPACE} | grep -v NAME | wc -l) != 1 ] ; do echo \u0026#34; waiting for pod to start running\u0026#34;; sleep 2; done ncn-w001# while [ $(kubectl get postgresql \u0026#34;${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; -o json | jq -r \u0026#39;.status.PostgresClusterStatus\u0026#39;) != \u0026#34;Running\u0026#34; ] ; do echo \u0026#34; waiting for postgresql to start running\u0026#34;; sleep 2; done   Copy the database dump file to the Postgres member.\nncn-w001# DUMPFILE=capsules-warehouse-server-postgres-2021-07-21T19:03:18.psql ncn-w001# kubectl cp ./${DUMPFILE} \u0026#34;${POSTGRESQL}-0\u0026#34;:/home/postgres/${DUMPFILE} -c postgres -n ${NAMESPACE}   Restore the data.\nncn-w001# kubectl exec \u0026#34;${POSTGRESQL}-0\u0026#34; -c postgres -n ${NAMESPACE} -it -- psql -U postgres \u0026lt; ${DUMPFILE}   Either update or re-create the capsules-warehouse-server-postgres secrets.\n    Update the secrets in Postgres.\nIf a manual dump was done, and the secrets were not saved, then the secrets in the newly created Postgres cluster will need to be updated.\nBased off the four capsules-warehouse-server-postgres secrets, collect the password for each Postgres username: postgres, service_account, and standby. Then kubectl exec into the Postgres pod and update the password for each user. For example:\nncn-w001# for secret in postgres.capsules-warehouse-server-postgres.credentials service-account.capsules-warehouse-server-postgres.credentials standby.capsules-warehouse-server-postgres.credentials; do echo -n \u0026#34;secret ${secret}username \u0026amp; password: \u0026#34;; echo -n \u0026#34;`kubectl get secret ${secret}-n ${NAMESPACE}-ojsonpath=\u0026#39;{.data.username}\u0026#39; | base64 -d` \u0026#34;; echo `kubectl get secret ${secret} -n ${NAMESPACE} -ojsonpath=\u0026#39;{.data.password}\u0026#39;| base64 -d`; done secret postgres.capsules-warehouse-server-postgres.credentials username \u0026amp; password: postgres ABCXYZ secret service-account.capsules-warehouse-server-postgres.credentials username \u0026amp; password: service_account ABC123 secret standby.capsules-warehouse-server-postgres.credentials username \u0026amp; password: standby 123456 ncn-w001# kubectl exec \u0026#34;${POSTGRESQL}-0\u0026#34; -n ${NAMESPACE} -c postgres -it -- bash root@capsules-warehouse-server-postgres-0:/home/postgres# /usr/bin/psql postgres postgres postgres=# ALTER USER postgres WITH PASSWORD \u0026#39;ABCXYZ\u0026#39;; ALTER ROLE postgres=# ALTER USER service_account WITH PASSWORD \u0026#39;ABC123\u0026#39;; ALTER ROLE postgres=#ALTER USER standby WITH PASSWORD \u0026#39;123456\u0026#39;; ALTER ROLE postgres=#   Re-create secrets in Kubernetes.\nIf the Postgres secrets were auto-backed up, then re-create the secrets in Kubernetes.\nDelete and re-create the three capsules-warehouse-server-postgres secrets using the manifest that was copied from S3 in step 1 above.\nncn-w001# MANIFEST=capsules-warehouse-server-postgres-2021-07-21T19:03:18.manifest ncn-w001# kubectl delete secret postgres.capsules-warehouse-server-postgres.credentials service-account.capsules-warehouse-server-postgres.credentials standby.capsules-warehouse-server-postgres.credentials -n ${NAMESPACE} ncn-w001# kubectl apply -f ${MANIFEST}    Restart the Postgres cluster.\nncn-w001# kubectl delete pod -n ${NAMESPACE} \u0026#34;${POSTGRESQL}-0\u0026#34; # Wait for the postgresql pod to start ncn-w001# while [ $(kubectl get pods -l \u0026#34;application=spilo,cluster-name=${POSTGRESQL}\u0026#34; -n ${NAMESPACE} | grep -v NAME | wc -l) != 1 ] ; do echo \u0026#34; waiting for pods to start running\u0026#34;; sleep 2; done   Scale the Postgres cluster back to 3 instances.\nncn-w001# kubectl patch postgresql \u0026#34;${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34; : \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;:\u0026#34;/spec/numberOfInstances\u0026#34;, \u0026#34;value\u0026#34; : 3}]\u0026#39; # Wait for the postgresql cluster to start running ncn-w001# while [ $(kubectl get postgresql \u0026#34;${POSTGRESQL}\u0026#34; -n \u0026#34;${NAMESPACE}\u0026#34; -o json | jq -r \u0026#39;.status.PostgresClusterStatus\u0026#39;) != \u0026#34;Running\u0026#34; ] ; do echo \u0026#34; waiting for postgresql to start running\u0026#34;; sleep 2; done   Scale the capsules-warehouse-server service back to 3 replicas.\nncn-w001# kubectl scale -n ${NAMESPACE} --replicas=3 deployment/${CLIENT} # Wait for the capsules-warehouse-server pods to start ncn-w001# while [ $(kubectl get pods -n ${NAMESPACE} -l app.kubernetes.io/name=\u0026#34;${CLIENT}\u0026#34; | grep -v NAME | wc -l) != 3 ] ; do echo \u0026#34; waiting for pods to start\u0026#34;; sleep 2; done Also check the status of the capsules-warehouse-server pods. If there are pods that do not show that both containers are ready (READY is 2/2), wait a few seconds and re-run the command until all containers are ready.\nncn-w001# kubectl get pods -n ${NAMESPACE} -l app.kubernetes.io/instance=\u0026#34;${CLIENT}\u0026#34; NAME READY STATUS RESTARTS AGE capsules-warehouse-server-0 2/2 Running 0 35s capsules-warehouse-server-1 2/2 Running 0 35s capsules-warehouse-server-2 2/2 Running 0 35s   Verify Capsules services are accessible and contain the expected data. You may need to configure your default warehouse and default warehouse user as well as login though the keycloak service depending on where you login from. It is recommended to use a UAN.\nncn-w001# capsule list 2 Capsules found: someusername/a-preexisting-capsule someusername/another-preexisting-capsule   Capsules Dispatch Server The Capsules Dispatch Server can be restored in the same manner as the warehouse server by substituting the keyword warehouse with dispatch; however, the dispatch server maintains temporary information for running Capsules Environments. Therefore, restoring data to this service is not necessary. Using the analytics docs, you can instead cleanup existing jobs and skip this step.\n"
},
{
	"uri": "/docs-csm/en-11/operations/kubernetes/retrieve_cluster_health_information_using_kubernetes/",
	"title": "Retrieve Cluster Health Information Using Kubernetes",
	"tags": [],
	"description": "",
	"content": "Retrieve Cluster Health Information Using Kubernetes The kubectl CLI commands can be used to retrieve information about the Kubernetes cluster components.\nRetrieve Node Status ncn# kubectl get nodes NAME STATUS ROLES AGE VERSION ncn-m001 Ready master 19d v1.14.3 ncn-m002 Ready master 19d v1.14.3 ncn-m003 Ready master 19d v1.14.3 ncn-w001 Ready \u0026lt;none\u0026gt; 19d v1.14.3 ncn-w002 Ready \u0026lt;none\u0026gt; 19d v1.14.3 ncn-w003 Ready \u0026lt;none\u0026gt; 19d v1.14.3 Retrieve Pod Status ncn# kubectl get pods options NAME READY STATUS RESTARTS AGE api-gateway-6fffd4d854-btp4v 1/1 Running 2 11d api-gateway-6fffd4d854-l7z8m 1/1 Running 1 11d api-gateway-6fffd4d854-tqx8v 1/1 Running 1 11d api-gateway-database-8555685975-xw5gm 1/1 Running 1 11d cray-ars-6466bcf77d-s99rq 1/1 Running 1 11d cray-bss-7589d7459f-ttwf4 1/1 Running 1 11d cray-capmc-6bf5855b78-np72d 1/1 Running 1 11d cray-datastore-768576677d-gpnl7 1/1 Running 1 11d cray-dhcp-5fccf85696-wrqxb 1/1 Running 1 11d cray-tftp-885cc65c4-568jz 2/2 Running 2 11d kvstore-1-56fdb6574c-ts79v 1/1 Running 1 11d kvstore-2-694bc7567b-k99tl 1/1 Running 1 11d kvstore-3-5b658b9bd9-n9dgt 1/1 Running 1 11d Retrieve Information about Individual Pods ncn# kubectl describe pod POD_NAME -n NAMESPACE_NAME Retrieve a List of Healthy Pods ncn# kubectl get pods -A | grep -e \u0026#39;Completed|Running\u0026#39; ceph-cephfs cephfs-provisioner-74599ccfcd-bzf5b 1/1 Running 3 2d11h ceph-rbd rbd-provisioner-76c464c567-jvvk6 1/1 Running 3 2d11h cert-manager cray-certmanager-cainjector-8487d996d7-phbfr 1/1 Running 0 2d11h cert-manager cray-certmanager-cert-manager-d5fb67664-v9sgf 1/1 Running 0 2d11h cert-manager cray-certmanager-webhook-6b58c6bb79-dtpl7 1/1 Running 0 2d11h default kube-keepalived-vip-mgmt-plane-nmn-local-pkbxj 1/1 Running 0 2d11h default kube-keepalived-vip-mgmt-plane-nmn-local-v8w6k 1/1 Running 0 2d11h default kube-keepalived-vip-mgmt-plane-nmn-local-zzc4n 1/1 Running 1 2d11h istio-system grafana-ff8b4b964-grg8m 1/1 Running 0 2d11h istio-system istio-citadel-699fc7bcf-rn99p 1/1 Running 0 2d11h istio-system istio-ingressgateway-59bf97fbc5-h6x4w 1/1 Running 0 2d11h istio-system istio-ingressgateway-hmn-5d7777c75-2c5nz 1/1 Running 0 2d11h istio-system istio-ingressgateway-hmn-5d7777c75-dh4fl 1/1 Running 0 2d11h istio-system istio-init-crd-10-1.2.10-wgg99 0/1 Completed 0 2d11h istio-system istio-init-crd-11-1.2.10-dcp4h 0/1 Completed 0 2d11h Retrieve a List of Unhealthy Pods ncn# kubectl get pods -A | grep -e \u0026#39;Creating|ImagePull|Error|Init|Crash\u0026#39; services cray-conman-68fffd8d9d-8h6zb 1/2 CrashLoopBackOff 694 2d10h services cray-crus-549cb9cb5d-7gv6n 0/4 Init:0/2 0 2d10h services cray-hms-badger-api-69bd7bc5b6-2v5sf 0/2 Init:0/2 0 2d10h services cray-hms-badger-wait-for-postgres-8q94h 1/3 ImagePullBackOff 0 2d10h services cray-hms-pmdbd-6b7c9fc5d-fpcwp 1/2 CrashLoopBackOff 694 2d10h services cray-hms-rts-68f48765b-j2jcp 0/3 Init:0/2 0 2d10h services cray-hms-rts-init-n6df4 0/2 Init:0/2 0 2d10h services cray-meds-76f5d9848-f9rsn 1/2 CreateContainerConfigError 0 2d10h services cray-reds-77b9575457-qx7m5 0/2 Init:2/4 0 2d10h Retrieve Service Status ncn# kubectl get services -n NAMESPACE_NAME NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE cray-ars ClusterIP 10.96.216.213 \u0026lt;none\u0026gt; 80/TCP 5d cray-capmc ClusterIP 10.99.76.144 \u0026lt;none\u0026gt; 27777/TCP 5d cray-datastore ClusterIP 10.111.46.185 \u0026lt;none\u0026gt; 80/TCP 5d cray-tftp-service NodePort 10.103.83.144 \u0026lt;none\u0026gt; 69:69/UDP 5d kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 5d kvstore-1 ClusterIP 10.110.91.156 \u0026lt;none\u0026gt; 2181/TCP,2888/TCP,3888/TCP 5d kvstore-2 ClusterIP 10.106.148.164 \u0026lt;none\u0026gt; 2181/TCP,2888/TCP,3888/TCP 5d kvstore-3 ClusterIP 10.108.198.177 \u0026lt;none\u0026gt; 2181/TCP,2888/TCP,3888/TCP 5d Retrieve Status of Pods in a Specific Namespace ncn# kubectl get pods -n NAMESPACE_NAME NAME READY STATUS RESTARTS AGE kube-apiserver-sms-01 1/1 Running 2 11d kube-apiserver-sms-02 1/1 Running 1 11d kube-apiserver-sms-03 1/1 Running 1 11d kube-controller-manager-sms-01 1/1 Running 1 11d kube-controller-manager-sms-02 1/1 Running 1 11d kube-controller-manager-sms-03 1/1 Running 1 11d kube-dns-86f4d74b45-7g544 3/3 Running 3 11d kube-proxy-4b2cb 1/1 Running 1 11d kube-proxy-b9jgw 1/1 Running 1 11d kube-proxy-xzld7 1/1 Running 1 11d kube-scheduler-sms-01 1/1 Running 1 11d kube-scheduler-sms-02 1/1 Running 1 11d kube-scheduler-sms-03 1/1 Running 1 11d weave-net-mg2kp 2/2 Running 7 11d weave-net-qfd69 2/2 Running 13 11d weave-net-xnlm9 2/2 Running 30 11d Retrieve Pod Logs ncn# kubectl logs -n NAMESPACE_NAME POD_NAME "
},
{
	"uri": "/docs-csm/en-11/operations/kubernetes/sealed_secrets_procedures/",
	"title": "Sealed Secrets Procedures",
	"tags": [],
	"description": "",
	"content": "Sealed Secrets Procedures Tracked Sealed Secrets Tracked sealed secrets are regenerated every time secrets are seeded (see the use of utils/secrets-seed-customizations.sh above).\nTo view currently tracked sealed secrets:\nlinux# yq read /mnt/pitdata/${CSM_RELEASE}/shasta-cfg/customizations.yaml spec.kubernetes.tracked_sealed_secrets Expected output looks similar to the following:\n- cray_reds_credentials - cray_meds_credentials - cray_hms_rts_credentials In order to prevent tracked sealed secrets from being regenerated, they MUST BE REMOVED from the spec.kubernetes.tracked_sealed_secrets list in /mnt/pitdata/${CSM_RELEASE}/shasta-cfg/customizations.yaml prior to executing the Generate Sealed Secrets section of Prepare Site Init.\nTo retain the REDS/MEDS/RTS credentials, run:\nlinux# yq delete -i /mnt/pitdata/${CSM_RELEASE}/shasta-cfg/customizations.yaml spec.kubernetes.tracked_sealed_secrets.cray_reds_credentials linux# yq delete -i /mnt/pitdata/${CSM_RELEASE}/shasta-cfg/customizations.yaml spec.kubernetes.tracked_sealed_secrets.cray_meds_credentials linux# yq delete -i /mnt/pitdata/${CSM_RELEASE}/shasta-cfg/customizations.yaml spec.kubernetes.tracked_sealed_secrets.cray_hms_rts_credentials Decrypt Sealed Secrets for Review For administrators that would like to decrypt and review previously encrypted sealed secrets, use the secrets-decrypt.sh utility in SHASTA-CFG.\nSyntax: secret-decrypt.sh SEALED-SECRET-NAME SEALED-SECRET-PRIVATE-KEY CUSTOMIZATIONS-YAML\nlinux:/mnt/pitdata/prep/site-init# ./utils/secrets-decrypt.sh cray_meds_credentials ./certs/sealed_secrets.key ./customizations.yaml | jq .data.vault_redfish_defaults | sed -e \u0026#39;s/\u0026#34;//g\u0026#39; | base64 -d; echo Expected output looks similar to the following:\n{\u0026#34;Username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;...\u0026#34;} "
},
{
	"uri": "/docs-csm/en-11/operations/kubernetes/rebalance_healthy_etcd_clusters/",
	"title": "Rebalance Healthy Etcd Clusters",
	"tags": [],
	"description": "",
	"content": "Rebalance Healthy etcd Clusters Rebalance the etcd clusters. The clusters need to be in a healthy state, and there needs to be the same number of pods running on each worker node for the etcd clusters to be balanced.\nRestoring the balance of etcd clusters will help with the storage of Kubernetes cluster data.\nPrerequisites  etcd clusters are in a healthy state. etcd clusters do not have the same number of pods on each worker node.  Procedure   Check to see if clusters have two or more pods on the same worker node.\nThe following is an example of an unhealthy cluster. Two of the pods are on ncn-w001, and only one pod is on ncn-w003.\nncn-w001# kubectl get pods -o wide -A -l app=etcd NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES services cray-bos-etcd-cqjr66ldlr 1/1 Running 0 5d10h 10.39.1.55 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; services cray-bos-etcd-hsb2zfzxqv 1/1 Running 0 5d10h 10.36.0.13 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; services cray-bos-etcd-v9sfkxcpzc 1/1 Running 0 3d 10.39.2.58 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   Confirm the clusters are healthy.\nRefer to Check the Health and Balance of etcd Clusters.\n  Delete one of the pods that is on the same node as another in the cluster.\nncn-w001# kubectl -A delete pod POD_NAME   Check the health of the pods.\nRefer to Check the Health and Balance of etcd Clusters.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/kubernetes/rebuild_unhealthy_etcd_clusters/",
	"title": "Rebuild Unhealthy Etcd Clusters",
	"tags": [],
	"description": "",
	"content": "Rebuild Unhealthy etcd Clusters Rebuild any cluster that does not have healthy pods by deleting and redeploying unhealthy pods. This procedure includes examples for rebuilding etcd clusters in the services namespace. This procedure must be used for each unhealthy cluster, not just the services used in the following examples.\nThis process also applies when etcd is not visible when running the kubectl get pods command.\nA special use case is also included for the Content Projection Service (CPS) as the process for rebuilding the cluster is slightly different.\n NOTE\nEtcd Clusters can be rebuilt using the automation script or the manual procedure below. The automation script follows the same steps as the manual procedure. If the automation script fails at any step, continue rebuilding the cluster using the manual procedure.\n Prerequisites An etcd cluster has pods that are not healthy, or the etcd cluster has no pods. See Check the Health and Balance of etcd Clusters for more information.\nAutomation Script for Clusters in the Services Namespace The automated script will restore the cluster from a backup if it finds a backup created within the last 7 days. If it does not discover a backup within the last 7 days, it will ask the user if they would like to rebuild the cluster.\nncn-w001 # cd /opt/cray/platform-utils/etcd_restore_rebuild_util # rebuild/restore a single cluster ncn-w001:/opt/cray/platform-utils/etcd_restore_rebuild_util # ./etcd_restore_rebuild.sh -s cray-bos-etcd # rebuild/restore multiple clusters ncn-w001:/opt/cray/platform-utils/etcd_restore_rebuild_util # ./etcd_restore_rebuild.sh -m cray-bos-etcd,cray-uas-mgr-etcd # rebuild/restore all clusters ncn-w001:/opt/cray/platform-utils/etcd_restore_rebuild_util # ./etcd_restore_rebuild.sh -a An example using the automation script is below.\nncn-w001:/opt/cray/platform-utils/etcd_restore_rebuild_util # ./etcd_restore_rebuild.sh -s cray-uas-mgr-etcd The following etcd clusters will be restored/rebuilt: cray-uas-mgr-etcd You will be accepting responsibility for any missing data if there is a restore/rebuild over a running etcd k/v. HPE assumes no responsibility. Proceed restoring/rebuilding? (yes/no) yes Proceeding: restoring/rebuilding etcd clusters. The following etcd clusters did not have backups so they will need to be rebuilt: cray-uas-mgr-etcd Would you like to proceed rebuilding all of these etcd clusters? (yes/no) yes ----- Rebuilding cray-uas-mgr-etcd ----- Deployment and etcd cluster objects captured in yaml file yaml files edited deployment.apps \u0026quot;cray-uas-mgr\u0026quot; deleted etcdcluster.etcd.database.coreos.com \u0026quot;cray-uas-mgr-etcd\u0026quot; deleted Waiting for pods to terminate. etcdcluster.etcd.database.coreos.com/cray-uas-mgr-etcd created Waiting for pods to be 'Running'. - 0/3 Running - 1/3 Running - 2/3 Running - 3/3 Running error: unable to upgrade connection: container not found (\u0026quot;etcd\u0026quot;) cray-uas-mgr-etcd-42qj56htp5 - Could not reach endpoint. 1/5 Attempts. Will try again in 15 seconds. error: unable to upgrade connection: container not found (\u0026quot;etcd\u0026quot;) cray-uas-mgr-etcd-42qj56htp5 - Could not reach endpoint. 2/5 Attempts. Will try again in 15 seconds. cray-uas-mgr-etcd-42qj56htp5 - Endpoint reached successfully cray-uas-mgr-etcd-6rpprkwbpp - Endpoint reached successfully cray-uas-mgr-etcd-nzbzl7k6gm - Endpoint reached successfully deployment.apps/cray-uas-mgr created SUCCESSFUL REBUILD cray-uas-mgr-etcd. Could not find existing backup definition. If one exists, it should be deleted so a new one can be created that points to the new cluster IP. Example delete command: groot-ncn-w001:~ # kubectl delete etcdbackup -n services cray-bos-etcd-cluster-periodic-backup Rerun the etcd cluster health check (see Check the Health and Balance of etcd Clusters) after recovering one or more clusters. Ensure that the clusters are healthy and have the correct number of pods.\nManual Procedure for Clusters in the Services Namespace The following examples use the cray-bos etcd cluster, but these steps must be repeated for every unhealthy service.\n  Retrieve the .yaml file for the deployment and the etcd cluster objects.\nncn-w001# kubectl -n services get deployment cray-bos -o yaml \u0026gt; /root/etcd/cray-bos.yaml ncn-w001# kubectl -n services get etcd cray-bos-etcd -o yaml \u0026gt; /root/etcd/cray-bos-etcd.yaml Only two files must be retrieved in most cases. There is a third file needed if rebuilding clusters for the CPS. CPS must be unmounted before running the commands to rebuild the etcd cluster.\nncn-w001# kubectl -n services get deployment cray-cps -o yaml \u0026gt; /root/etcd/cray-cps.yaml ncn-w001# kubectl -n services get daemonset cray-cps-cm-pm -o yaml \u0026gt; /root/etcd/cray-cps-cm-pm.yaml ncn-w001# kubectl -n services get etcd cray-cps-etcd -o yaml \u0026gt; /root/etcd/cray-cps-etcd.yaml   Edit each .yaml file to remove the entire line for creationTimestamp, generation, resourceVersion, selfLink, uid, and everything after status (including status).\nFor example:\ncreationTimestamp: \u0026quot;2019-11-26T16:54:23Z\u0026quot; generation: 1 resourceVersion: \u0026quot;5340297\u0026quot; selfLink: /apis/extensions/v1beta1/namespaces/services/deployments/cray-bos uid: 65f4912e-106d-11ea-88b0-b42e993e060a status: availableReplicas: 1 conditions: - lastTransitionTime: \u0026quot;2019-11-26T16:54:23Z\u0026quot; lastUpdateTime: \u0026quot;2019-11-26T16:57:36Z\u0026quot; message: ReplicaSet \u0026quot;cray-bos-6f4475d59b\u0026quot; has successfully progressed. reason: NewReplicaSetAvailable status: \u0026quot;True\u0026quot; type: Progressing - lastTransitionTime: \u0026quot;2019-11-29T03:25:29Z\u0026quot; lastUpdateTime: \u0026quot;2019-11-29T03:25:29Z\u0026quot; message: Deployment has minimum availability. reason: MinimumReplicasAvailable status: \u0026quot;True\u0026quot; type: Available observedGeneration: 1 readyReplicas: 1 replicas: 1 updatedReplicas: 1   Delete the deployment and the etcd cluster objects.\nWait for the pods to terminate before proceeding to the next step.\nncn-w001# kubectl delete -f /root/etcd/cray-bos.yaml ncn-w001# kubectl delete -f /root/etcd/cray-bos-etcd.yaml In the use case of CPS clusters being rebuilt, the following files must be deleted:\nncn-w001# kubectl delete -f /root/etcd/cray-cps.yaml ncn-w001# kubectl delete -f /root/etcd/cray-cps-cm-pm.yaml ncn-w001# kubectl delete -f /root/etcd/cray-cps-etcd.yaml   Apply the etcd cluster file.\nncn-w001# kubectl apply -f /root/etcd/cray-bos-etcd.yaml Wait for all three pods to go into the Running state before proceeding to the next step. Use the following command to monitor the status of the pods:\nncn-w001# kubectl get pods -n services | grep bos-etcd cray-bos-etcd-hwcw4429b9 1/1 Running 1 7d18h cray-bos-etcd-mdnl28vq9c 1/1 Running 0 36h cray-bos-etcd-w5vv7j4ghh 1/1 Running 0 18h   Apply the deployment file.\nncn-w001# kubectl apply -f /root/etcd/cray-bos.yaml If using CPS, the etcd cluster file, deployment file, and daemonset file must be reapplied:\nncn-w001# kubectl apply -f /root/etcd/cray-cps.yaml ncn-w001# kubectl apply -f /root/etcd/cray-cps-cm-pm.yaml ncn-w001# kubectl apply -f /root/etcd/cray-cps-etcd.yaml Proceed to the next step to finish rebuilding the cluster.\n  Post-Rebuild   Update the IP address needed to interact with the rebuilt cluster.\nAfter recreating the etcd cluster, the IP address needed to interact with the cluster changes, which requires recreating the etcd backup. The IP address is created automatically via a cronjob that runs at the top of each hour.\n  Determine the periodic backup name for the cluster.\nThe following example is for the bos cluster:\nncn-w001# kubectl get etcdbackup -n services | grep bos.*periodic cray-bos-etcd-cluster-periodic-backup   Delete the etcd backup definition.\nA new backup will be created that points to the new IP address. Use the value returned in the previous substep.\nncn-w001# kubectl delete etcdbackup -n services \\ cray-bos-etcd-cluster-periodic-backup     Rerun the etcd cluster health check (see Check the Health and Balance of etcd Clusters) after recovering one or more clusters. Ensure that the clusters are healthy and have the correct number of pods.\n"
},
{
	"uri": "/docs-csm/en-11/operations/kubernetes/recover_from_postgres_wal_event/",
	"title": "Recover From Postgres Wal Event",
	"tags": [],
	"description": "",
	"content": "Recover from Postgres WAL Event A WAL event can occur because of lag, network communication, or bandwidth issues. This can cause the PVC hosted by Ceph and mounted inside the container on /home/postgres/pgdata to fill and the database to stop running. If no database dump exists, the disk space issue needs to be fixed so that a dump can be taken. Then the dump can be restored to a newly created postgresql cluster. If a dump already exists, skip to Rebuild the cluster and Restore the data.\nIf no database dump exists and neither option results in a successful dump, then services specific Disaster Recovery for Postgres will be required.\nThe Recovery Workflow:\n Option 1 : Clear logs and/or WAL files  Option 2 : Resize the Postgres PVCs Dump the data Rebuild the cluster and Restore the data  Attempt to Recover to a Running Database A running database is needed to be able to dump the current data.\nThe following example is based on cray-smd-postgres.\n Confirm that the database is down (no endpoint exists) and that the disk is full on one or more postgresql cluster member.  ncn-w001# POSTGRESQL=cray-smd-postgres $ NAMESPACE=services ncn-w001# kubectl get endpoints ${POSTGRESQL} -n ${NAMESPACE} NAME ENDPOINTS AGE cray-smd-postgres 3d22h ncn-w001# for i in {0..2}; do echo \u0026quot;${POSTGRESQL}-${i}:\u0026quot;; kubectl exec ${POSTGRESQL}-${i} -n ${NAMESPACE} -c postgres -- df -h pgdata; done cray-smd-postgres-0: Filesystem Size Used Avail Use% Mounted on /dev/rbd8 30G 28G 1.6G 95% /home/postgres/pgdata cray-smd-postgres-1: Filesystem Size Used Avail Use% Mounted on /dev/rbd15 30G 30G 0 100% /home/postgres/pgdata cray-smd-postgres-2: Filesystem Size Used Avail Use% Mounted on /dev/rbd6 30G 383M 30G 2% /home/postgres/pgdata If the database is down and the disk is full because of replication issues, there are two ways to attempt to get back to a running database: either delete files or resize the Postgres PVCs until the database is able to start running again.\nOption 1 : Clear logs and/or WAL files The following example is based on cray-smd-postgres.\n Clear files from /home/postgres/pgdata/pgroot/pg_log/ until the database is running again and you can successfully connect. For example, if the disk space is at 100%, exec into that pod, copy the logs off (optional) and then clear the logs to recover some disk space.  ncn-w001# kubectl cp \u0026quot;${POSTGRESQL}-1\u0026quot;:/home/postgres/pgdata/pgroot/pg_log /tmp -c postgres -n ${NAMESPACE} ncn-w001# kubectl exec \u0026quot;${POSTGRESQL}-1\u0026quot; -n ${NAMESPACE} -c postgres -it -- bash root@cray-smd-postgres-1:/home/postgres# for i in {0..7}; do \u0026gt; /home/postgres/pgdata/pgroot/pg_log/postgresql-$i.csv; done Restart the Postgres cluster and postgres-operator.  ncn-w001# kubectl delete pod -n ${NAMESPACE} \u0026quot;${POSTGRESQL}-0\u0026quot; \u0026quot;${POSTGRESQL}-1\u0026quot; \u0026quot;${POSTGRESQL}-2\u0026quot; ncn-w001# kubectl delete pod -n services -l app.kubernetes.io/name=postgres-operator Check if the database is running. If it is running, continue with Dumping the data.  ncn-w001# kubectl exec \u0026quot;${POSTGRESQL}-1\u0026quot; -n ${NAMESPACE} -c postgres -it -- psql -U postgres psql (12.2 (Ubuntu 12.2-1.pgdg18.04+1), server 11.7 (Ubuntu 11.7-1.pgdg18.04+1)) Type \u0026quot;help\u0026quot; for help. postgres=# \u0026lt;----- success!! Type \\q If the database is still not running, delete files from /home/postgres/pgdata/pgroot/data/pg_wal/. CAUTION: This method could result in unintended consequences for the Postgres database and long service downtime; do not use unless there is a known Disaster Recovery for Postgres procedure for repopulating the Postgres cluster.  ncn-w001# kubectl exec \u0026quot;${POSTGRESQL}-1\u0026quot; -n ${NAMESPACE} -c postgres -it -- bash root@cray-smd-postgres-1:/home/postgres# rm pgdata/pgroot/data/pg_wal/0* Restart the Postgres cluster and postgres-operator.  ncn-w001# kubectl delete pod -n ${NAMESPACE} \u0026quot;${POSTGRESQL}-0\u0026quot; \u0026quot;${POSTGRESQL}-1\u0026quot; \u0026quot;${POSTGRESQL}-2\u0026quot; ncn-w001# kubectl delete pod -n services -l app.kubernetes.io/name=postgres-operator Check if the database is running.  ncn-w001# kubectl exec \u0026quot;${POSTGRESQL}-1\u0026quot; -n ${NAMESPACE} -c postgres -it -- psql -U postgres psql (12.2 (Ubuntu 12.2-1.pgdg18.04+1), server 11.7 (Ubuntu 11.7-1.pgdg18.04+1)) Type \u0026quot;help\u0026quot; for help. postgres=# \u0026lt;----- success!! Type \\q If the database is still not running, try recovering using the other option listed in this document.  Option 2 : Resize the Postgres PVCs The following example is based on cray-smd-postgres, where the postgresql cray-smd-postgres resource and the pgdata-cray-smd-postgres PVCs will be resized from 100Gi to 120Gi.\n Determine the current size of the Postgres PVCs and set PGRESIZE to the desired new size (it must be larger than the current size).  ncn-w001# kubectl get postgresql -A | grep \u0026quot;smd\\|NAME\u0026quot; NAMESPACE NAME TEAM VERSION PODS VOLUME CPU-REQUEST MEMORY-REQUEST AGE STATUS services cray-smd-postgres cray-smd 11 3 100Gi 4 8Gi 18h Running ncn-w001# kubectl get pvc -A | grep \u0026quot;cray-smd-postgres\\|NAME\u0026quot; NAMESPACE NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE services pgdata-cray-smd-postgres-0 Bound pvc-c86859f4-a57f-4694-a66a-8120e96a1ab4 100Gi RWO k8s-block-replicated 18h services pgdata-cray-smd-postgres-1 Bound pvc-300f52e4-f88d-47ef-9a1e-e598fd919047 100Gi RWO k8s-block-replicated 18h services pgdata-cray-smd-postgres-2 Bound pvc-f33879f3-0e99-4299-b796-210fbb693a2f 100Gi RWO k8s-block-replicated 18h ncn-w001# PGRESIZE=120Gi ncn-w001# POSTGRESQL=cray-smd-postgres ncn-w001# PGDATA=pgdata-cray-smd-postgres ncn-w001# NAMESPACE=services Edit numberOfInstances in the postgresql resource from 3 to 1.  ncn-w001# kubectl patch postgresql ${POSTGRESQL} -n ${NAMESPACE} --type=json -p='[{\u0026quot;op\u0026quot; : \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;:\u0026quot;/spec/numberOfInstances\u0026quot;, \u0026quot;value\u0026quot; : 1}]' postgresql.acid.zalan.do/cray-smd-postgres patched Wait for 2 of the 3 postgresql pods to terminate.  ncn-w001# while [ $(kubectl get pods -l \u0026quot;application=spilo,cluster-name=${POSTGRESQL}\u0026quot; -n ${NAMESPACE} | grep -v NAME | wc -l) != 1 ] ; do echo \u0026quot; waiting for pods to terminate\u0026quot;; sleep 2; done Delete the PVCs from the non running Postgres pods.  ncn-w001# kubectl delete pvc \u0026quot;${PGDATA}-1\u0026quot; \u0026quot;${PGDATA}-2\u0026quot; -n ${NAMESPACE} persistentvolumeclaim \u0026quot;pgdata-cray-smd-postgres-1\u0026quot; deleted persistentvolumeclaim \u0026quot;pgdata-cray-smd-postgres-2\u0026quot; deleted Resize the remaining Postgres PVC resources.requests.storage to $PGRESIZE.  ncn-w001# kubectl patch -p '{\u0026quot;spec\u0026quot;: {\u0026quot;resources\u0026quot;: {\u0026quot;requests\u0026quot;: {\u0026quot;storage\u0026quot;: \u0026quot;'${PGRESIZE}'\u0026quot;}}}}' \u0026quot;pvc/${PGDATA}-0\u0026quot; -n ${NAMESPACE} persistentvolumeclaim/pgdata-cray-smd-postgres-0 patched Wait for the PVC to resize.  ncn-w001# while [ -z '$(kubectl describe pvc \u0026quot;${PGDATA}-0\u0026quot; -n ${NAMESPACE} | grep FileSystemResizeSuccessful' ] ; do echo \u0026quot; waiting for PVC to resize\u0026quot;; sleep 2; done Update the postgresql resource spec.volume.size to $PGRESIZE.  ncn-w001# kubectl get \u0026quot;postgresql/${POSTGRESQL}\u0026quot; -n ${NAMESPACE} -o json | jq '.spec.volume = {\u0026quot;size\u0026quot;: \u0026quot;'${PGRESIZE}'\u0026quot;}' | kubectl apply -f - postgresql.acid.zalan.do/cray-smd-postgres configured Restart the existing postgresql pod.  ncn-w001# kubectl delete pod \u0026quot;${POSTGRESQL}-0\u0026quot; -n services pod \u0026quot;cray-smd-postgres-0\u0026quot; deleted Check that the single instance pod is Running with 3/3 Ready, patronictl reports the member is running and postgresql resource is Running with new volume size ($PGRESIZE).  ncn-w001# kubectl get pods -l \u0026quot;application=spilo,cluster-name=${POSTGRESQL}\u0026quot; -n ${NAMESPACE} NAME READY STATUS RESTARTS AGE cray-smd-postgres-0 3/3 Running 0 14s $ kubectl exec \u0026quot;${POSTGRESQL}-0\u0026quot; -n ${NAMESPACE} -c postgres -it -- patronictl list +-------------------+---------------------+------------+--------+---------+----+-----------+ | Cluster | Member | Host | Role | State | TL | Lag in MB | +-------------------+---------------------+------------+--------+---------+----+-----------+ | cray-smd-postgres | cray-smd-postgres-0 | 10.44.0.38 | Leader | running | 2 | | +-------------------+---------------------+------------+--------+---------+----+-----------+ ncn-w001# kubectl get postgresql ${POSTGRESQL} -n ${NAMESPACE} NAME TEAM VERSION PODS VOLUME CPU-REQUEST MEMORY-REQUEST AGE STATUS cray-smd-postgres cray-smd 11 1 120Gi 500m 100Mi 11m Running Check that the database is running.  ncn-w001# kubectl exec \u0026quot;${POSTGRESQL}-0\u0026quot; -n services -c postgres -it -- psql -U postgres psql (12.2 (Ubuntu 12.2-1.pgdg18.04+1), server 11.7 (Ubuntu 11.7-1.pgdg18.04+1)) Type \u0026quot;help\u0026quot; for help. postgres=# \u0026lt;----- success!! Type \\q Scale numberOfInstances in postgresql resource from 1 back to 3.  ncn-w001# kubectl patch postgresql \u0026quot;${POSTGRESQL}\u0026quot; -n \u0026quot;${NAMESPACE}\u0026quot; --type='json' -p='[{\u0026quot;op\u0026quot; : \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;:\u0026quot;/spec/numberOfInstances\u0026quot;, \u0026quot;value\u0026quot; : 3}]' postgresql.acid.zalan.do/cray-smd-postgres patched Logs may indicate WAL error such as the following, but a dump can be taken at this point.  ncn-w001# kubectl logs \u0026quot;${POSTGRESQL}-0\u0026quot; -n ${NAMESPACE} -c postgres | grep -i error ncn-w001# kubectl logs \u0026quot;${POSTGRESQL}-1\u0026quot; -n ${NAMESPACE} -c postgres | grep -i error ncn-w001# kubectl logs \u0026quot;${POSTGRESQL}-2\u0026quot; -n ${NAMESPACE} -c postgres | grep -i error error: could not get write-ahead log end position from server: ERROR: invalid segment number In order to persist any Postgres PVC storage volume size changes, it is necessary that this change also be made to the customer-managed customizations.yaml file. See the Postgres PVC Resize information in the Post Install Customizations.  Dump the data If the recovery was successful such that the database is now running, then continue with the following steps to dump the data.\n\n Scale the client service to 0.  The following example is based on cray-smd. The cray-smd client service is deployed as a deployment. Other services may differ; e.g. statefulset.\nncn-w001# CLIENT=cray-smd ncn-w001# POSTGRESQL=cray-smd-postgres ncn-w001# NAMESPACE=services ncn-w001# kubectl scale deployment ${CLIENT} -n ${NAMESPACE} --replicas=0 deployment.apps/cray-smd scaled ncn-w001# while [ $(kubectl get pods -n ${NAMESPACE} -l app.kubernetes.io/name=\u0026quot;${CLIENT}\u0026quot; | grep -v NAME | wc -l) != 0 ] ; do echo \u0026quot; waiting for pods to terminate\u0026quot;; sleep 2; done Dump all the data.  Determine which Postgres member is the leader and exec into the leader pod to dump the data to a local file:\nncn-w001# kubectl exec \u0026quot;${POSTGRESQL}-0\u0026quot; -n ${NAMESPACE} -c postgres -it -- patronictl list +-------------------+---------------------+------------+--------+---------+----+-----------+ | Cluster | Member | Host | Role | State | TL | Lag in MB | +-------------------+---------------------+------------+--------+---------+----+-----------+ | cray-smd-postgres | cray-smd-postgres-0 | 10.42.0.25 | Leader | running | 1 | | | cray-smd-postgres | cray-smd-postgres-1 | 10.44.0.34 | | running | | 0 | | cray-smd-postgres | cray-smd-postgres-2 | 10.36.0.44 | | running | | 0 | +-------------------+---------------------+------------+--------+---------+----+-----------+ ncn-w001# POSTGRES_LEADER=cray-smd-postgres-0 ncn-w001# kubectl exec -it ${POSTGRES_LEADER} -n ${NAMESPACE} -c postgres -- pg_dumpall -c -U postgres \u0026gt; \u0026quot;${POSTGRESQL}-dumpall.sql\u0026quot; ncn-w001# ls \u0026quot;${POSTGRESQL}-dumpall.sql\u0026quot; cray-smd-postgres-dumpall.sql Rebuild the cluster and Restore the data If recovery was successful such that a dump could be taken or a dump already exists, then continue with the following steps to rebuild the postgresql cluster and restore the data.\nThe following example restores the dump to the cray-smd-postgres cluster.\n  If your client service is not yet scaled to 0, follow the step above to Scale the client service to 0.\n  Delete and re-create the postgresql resource (which includes the PVCs).\n  ncn-w001# CLIENT=cray-smd ncn-w001# POSTGRESQL=cray-smd-postgres ncn-w001# NAMESPACE=services ncn-w001# kubectl get postgresql ${POSTGRESQL} -n ${NAMESPACE} -o json | jq 'del(.spec.selector)' | jq 'del(.spec.template.metadata.labels.\u0026quot;controller-uid\u0026quot;)' | jq 'del(.status)' \u0026gt; postgres-cr.yaml ncn-w001# kubectl delete -f postgres-cr.yaml postgresql.acid.zalan.do \u0026quot;cray-smd-postgres\u0026quot; deleted ncn-w001# while [ $(kubectl get pods -l \u0026quot;application=spilo,cluster-name=${POSTGRESQL}\u0026quot; -n ${NAMESPACE} | grep -v NAME | wc -l) != 0 ] ; do echo \u0026quot; waiting for pods to terminate\u0026quot;; sleep 2; done ncn-w001# kubectl create -f postgres-cr.yaml postgresql.acid.zalan.do/cray-smd-postgres created ncn-w001# while [ $(kubectl get pods -l \u0026quot;application=spilo,cluster-name=${POSTGRESQL}\u0026quot; -n ${NAMESPACE} | grep -v NAME | wc -l) != 3 ] ; do echo \u0026quot; waiting for pods to start running\u0026quot;; sleep 2; done Determine which Postgres member is the leader.  ncn-w001# kubectl exec \u0026quot;${POSTGRESQL}-0\u0026quot; -n ${NAMESPACE} -c postgres -it -- patronictl list +-------------------+---------------------+------------+--------+---------+----+-----------+ | Cluster | Member | Host | Role | State | TL | Lag in MB | +-------------------+---------------------+------------+--------+---------+----+-----------+ | cray-smd-postgres | cray-smd-postgres-0 | 10.42.0.25 | Leader | running | 1 | | | cray-smd-postgres | cray-smd-postgres-1 | 10.44.0.34 | | running | | 0 | | cray-smd-postgres | cray-smd-postgres-2 | 10.36.0.44 | | running | | 0 | +-------------------+---------------------+------------+--------+---------+----+-----------+ ncn-w001# POSTGRES_LEADER=cray-smd-postgres-0 Copy the dump taken above to the Postgres leader pod and restore the data.  If the dump exists in a different location, adjust this example as needed.\nncn-w001# kubectl cp ./cray-smd-postgres-dumpall.sql ${POSTGRES_LEADER}:/home/postgres/cray-smd-postgres-dumpall.sql -c postgres -n ${NAMESPACE} ncn-w001# kubectl exec ${POSTGRES_LEADER} -c postgres -n ${NAMESPACE} -it -- psql -U postgres \u0026lt; cray-smd-postgres-dumpall.sql Restore the secrets.  Once the dump has been restored onto the newly built postgresql cluster, the current Kubernetes secrets need to be updated in the postgresql cluster, otherwise the service will experience readiness and liveness probe failures because it will be unable to authenticate to the database.\nDetermine which Postgres member is the leader.\nncn-w001# kubectl exec \u0026quot;${POSTGRESQL}-0\u0026quot; -n ${NAMESPACE} -c postgres -it -- patronictl list +-------------------+---------------------+------------+--------+---------+----+-----------+ | Cluster | Member | Host | Role | State | TL | Lag in MB | +-------------------+---------------------+------------+--------+---------+----+-----------+ | cray-smd-postgres | cray-smd-postgres-0 | 10.42.0.25 | Leader | running | 1 | | | cray-smd-postgres | cray-smd-postgres-1 | 10.44.0.34 | | running | | 0 | | cray-smd-postgres | cray-smd-postgres-2 | 10.36.0.44 | | running | | 0 | +-------------------+---------------------+------------+--------+---------+----+-----------+ ncn-w001# POSTGRES_LEADER=cray-smd-postgres-0 Determine what secrets are associated with the postgresql credentials.\nncn-w001# kubectl get secrets -n ${NAMESPACE} | grep \u0026quot;${POSTGRESQL}.credentials\u0026quot; services hmsdsuser.cray-smd-postgres.credentials Opaque 2 31m services postgres.cray-smd-postgres.credentials Opaque 2 31m services service-account.cray-smd-postgres.credentials Opaque 2 31m services standby.cray-smd-postgres.credentials Opaque 2 31m For each secret above, get the username and password from Kubernetes and update the Postgres database with this information. For example (hmsdsuser.cray-smd-postgres.credentials) :\nncn-w001# kubectl get secret hmsdsuser.cray-smd-postgres.credentials -n ${NAMESPACE} -ojsonpath='{.data.username}' | base64 -d hmsdsuser ncn-w001# kubectl get secret hmsdsuser.cray-smd-postgres.credentials -n ${NAMESPACE} -ojsonpath='{.data.password}'| base64 -d ABCXYZ Exec into the leader pod to reset the user\u0026rsquo;s password :\nncn-w001# kubectl exec ${POSTGRES_LEADER} -n ${NAMESPACE} -c postgres -it -- bash root@cray-smd-postgres-0:/home/postgres# /usr/bin/psql postgres postgres postgres=# ALTER USER hmsdsuser WITH PASSWORD 'ABCXYZ'; ALTER ROLE postgres=# Continue the above process until all ${POSTGRESQL}.credentials secrets have been updated in the database.\nRestart the postgresql cluster  ncn-w001# kubectl delete pod \u0026quot;${POSTGRESQL}-0\u0026quot; \u0026quot;${POSTGRESQL}-1\u0026quot; \u0026quot;${POSTGRESQL}-2\u0026quot; -n ${NAMESPACE} ncn-w001# while [ $(kubectl get postgresql ${POSTGRESQL} -n ${NAMESPACE} -o json | jq -r '.status.PostgresClusterStatus') != \u0026quot;Running\u0026quot; ]; do echo \u0026quot;waiting for ${POSTGRESQL} to start running\u0026quot;; sleep 2; done Scale the client service back to 3  ncn-w001# kubectl scale deployment ${CLIENT} -n ${NAMESPACE} --replicas=3 ncn-w001# while [ $(kubectl get pods -n ${NAMESPACE} -l app.kubernetes.io/name=\u0026quot;${CLIENT}\u0026quot; | grep -v NAME | wc -l) != 3 ] ; do echo \u0026quot; waiting for pods to start running\u0026quot;; sleep 2; done "
},
{
	"uri": "/docs-csm/en-11/operations/kubernetes/repopulate_data_in_etcd_clusters_when_rebuilding_them/",
	"title": "Repopulate Data In Etcd Clusters When Rebuilding Them",
	"tags": [],
	"description": "",
	"content": "Repopulate Data in etcd Clusters When Rebuilding Them When an etcd cluster is not healthy, it needs to be rebuilt. During that process, the pods that rely on etcd clusters lose data. That data needs to be repopulated in order for the cluster to go back to a healthy state.\nThe following services need their data repopulated in the etcd cluster:\n Boot Orchestration Service (BOS) Boot Script Service (BSS) Content Projection Service (CPS) Compute Rolling Upgrade Service (CRUS) External DNS Firmware Action Service (FAS) HMS Notification Fanout Daemon (HMNFD) Mountain Endpoint Discovery Service (MEDS) River Endpoint Discovery Service (REDS)  Prerequisites A etcd cluster was rebuilt. See Rebuild Unhealthy etcd Clusters.\nBOS   Reconstruct boot session templates for impacted product streams to repopulate data.\nBoot preparation information for other product streams can be found in the following locations:\n UANs: Refer to the UAN product stream repository and search for the \u0026ldquo;PREPARE UAN BOOT SESSION TEMPLATES\u0026rdquo; header in the \u0026ldquo;Install and Configure UANs\u0026rdquo; procedure. Cray Operating System (COS): Refer to the \u0026ldquo;Create a Boot Session Template\u0026rdquo; header in the \u0026ldquo;Boot COS\u0026rdquo; procedure in the COS product stream documentation.    CPS   Repopulate clusters for CPS.\n If there are no clients using CPS when the etcd cluster is rebuilt, then nothing needs to be done other than to rebuild the cluster and make sure all of the components are up and running. See Rebuild Unhealthy etcd Clusters for more information. If any clients have already mounted content provided by CPS, that content should be unmounted before rebuilding the etcd cluster, and then re-mounted after the etcd cluster is rebuilt. Compute nodes that use CPS to access their root file system must be shut down to unmount, and then booted to perform the re-mount.    CRUS   View the progress of existing CRUS sessions.\n  List the existing CRUS sessions to find the upgrade_id for the desired session.\nncn-w001# cray crus session list [[results]] api_version = \u0026#34;1.0.0\u0026#34; completed = false failed_label = \u0026#34;failed-nodes\u0026#34; kind = \u0026#34;ComputeUpgradeSession\u0026#34; messages = [ \u0026#34;Quiesce requested in step 0: moving to QUIESCING\u0026#34;, \u0026#34;All nodes quiesced in step 0: moving to QUIESCED\u0026#34;, \u0026#34;Began the boot session for step 0: moving to BOOTING\u0026#34;,] starting_label = \u0026#34;slurm-nodes\u0026#34; state = \u0026#34;UPDATING\u0026#34; upgrade_id = \u0026#34;e0131663-dbee-47c2-aa5c-13fe9b110242\u0026#34; \u0026lt;\u0026lt;-- Note this value upgrade_step_size = 50 upgrade_template_id = \u0026#34;boot-template\u0026#34; upgrading_label = \u0026#34;upgrading-nodes\u0026#34; workload_manager_type = \u0026#34;slurm\u0026#34;   Describe the CRUS session to see if the session failed or is stuck.\nIf the session continued and appears to be in a healthy state, proceed to the BSS section.\nncn-w001# cray crus session describe CRUS_UPGRADE_ID api_version = \u0026#34;1.0.0\u0026#34; completed = false failed_label = \u0026#34;failed-nodes\u0026#34; kind = \u0026#34;ComputeUpgradeSession\u0026#34; messages = [ \u0026#34;Quiesce requested in step 0: moving to QUIESCING\u0026#34;, \u0026#34;All nodes quiesced in step 0: moving to QUIESCED\u0026#34;, \u0026#34;Began the boot session for step 0: moving to BOOTING\u0026#34;,] starting_label = \u0026#34;slurm-nodes\u0026#34; state = \u0026#34;UPDATING\u0026#34; upgrade_id = \u0026#34;e0131663-dbee-47c2-aa5c-13fe9b110242\u0026#34; upgrade_step_size = 50 upgrade_template_id = \u0026#34;boot-template\u0026#34; upgrading_label = \u0026#34;upgrading-nodes\u0026#34; workload_manager_type = \u0026#34;slurm\u0026#34;     Find the name of the running CRUS pod.\nncn-w001# kubectl get pods -n services | grep cray-crus cray-crus-549cb9cb5d-jtpqg 3/4 Running 528 25h   Restart the CRUS pod.\nDeleting the pod will restart CRUS and start the discovery process for any data recovered in etcd.\nncn-w001# kubectl delete pods -n services POD_NAME   External DNS The etcd cluster for external DNS maintains an ephemeral cache for CoreDNS. There is no reason to back it up. If it is having any issues, delete it and recreate it.\n  Save the external DNS configuration.\n  Edit the end of each .yaml file to remove the .status, .metadata.uid, .metadata.selfLink, .metadata.resourceVersion, .metadata.generation, and .metadata.creationTimestamp.\nFor example:\napiVersion: etcd.database.coreos.com/v1beta2 kind: EtcdCluster metadata: annotations: etcd.database.coreos.com/scope: clusterwide labels: app.kubernetes.io/name: cray-externaldns-etcd name: cray-externaldns-etcd namespace: services spec: pod: ClusterDomain: \u0026#34; annotations: sidecar.istio.io/inject: \u0026#34;false\u0026#34; busyboxImage: registry.local/library/busybox:1.28.0-glibc persistentVolumeClaimSpec: accessModes: - ReadWriteOnce dataSource: null resources: requests: storage: 1Gi resources: {} repository: registry.local/coreos/etcd size: 3 version: 3.3.8   Delete the current cluster.\nncn-w001# kubectl -n services delete etcd cray-externaldns-etcd   Recreate the cluster.\nncn-w001# kubectl apply -f cray-externaldns-etcd.yaml   BSS Data is repopulated in BSS when the REDS init job is run.\n  Get the current REDS job.\nncn-w001# kubectl get -o json -n services job/cray-reds-init | \\ jq \u0026#39;del(.spec.template.metadata.labels[\u0026#34;controller-uid\u0026#34;], .spec.selector)\u0026#39; \u0026gt; cray-reds-init.json   Delete the reds-client-init job.\nncn-w001# kubectl delete -n services -f cray-reds-init.json   Restart the reds-client-init job.\nncn-w001# kubectl apply -n services -f cray-reds-init.json   REDS   Restart REDS.\nncn-w001# kubectl -n services delete pods --selector=\u0026#39;app.kubernetes.io/name=cray-reds\u0026#39;   MEDS   Restart MEDS.\nncn-w001# kubectl -n services delete pods --selector=\u0026#39;app.kubernetes.io/name=cray-meds\u0026#39;   FAS   Run the cray-fas-loader Kubernetes job.\nRefer to the \u0026ldquo;Use the cray-fas-loader Kubernetes Job\u0026rdquo; section in FAS Admin Procedures for more information.\nWhen the etcd cluster is rebuilt, all historic data for firmware actions and all recorded snapshots will be lost. Image data will need to be reloaded by following the cray-fas-loader Kubernetes job procedure. After images are reloaded any running actions at time of failure will need to be recreated.\n  HMNFD   Resubscribe the compute nodes and any NCNs that use the ORCA daemon for their State Change Notifications (SCN).\n  Resubscribe all compute nodes.\nncn-w001# pdsh -w nid00[0-9][0-9][0-9]-nmn \u0026#34;systemctl restart cray-dvs-orca\u0026#34;   Resubscribe the NCNs.\nncn-w001# pdsh -w ncn[ws]00[0-4]-can.local \u0026#34;systemctl restart cray-dvs-orca\u0026#34;     "
},
{
	"uri": "/docs-csm/en-11/operations/kubernetes/report_the_endpoint_status_for_etcd_clusters/",
	"title": "Report The Endpoint Status For Etcd Clusters",
	"tags": [],
	"description": "",
	"content": "Report the Endpoint Status for etcd Clusters Report etcd cluster end point status. The report includes a cluster\u0026rsquo;s endpoint, database size, and leader status.\nThis procedure provides the ability to view the etcd cluster endpoint status.\nPrerequisites  This procedure requires root privileges. The etcd clusters are in a healthy state.  Procedure   Report the endpoint status for all etcd clusters in a namespace.\nThe following example is for the services namespace.\nfor pod in $(kubectl get pods -l app=etcd -n services \\  -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;) do echo \u0026#34;### ${pod}Endpoint Status: ###\u0026#34; kubectl -n services exec ${pod} -- /bin/sh \\  -c \u0026#34;ETCDCTL_API=3 etcdctl endpoint status -w table\u0026#34; done ncn-w001# for pod in $(kubectl get pods -l app=etcd -n services -o \\ jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;); do echo \u0026#34;### ${pod}Endpoint Status: ###\u0026#34;; \\ kubectl -n services exec ${pod} -- /bin/sh -c \\ \u0026#34;ETCDCTL_API=3 etcdctl endpoint status -w table\u0026#34;; done; ### cray-bos-etcd-7cxq6qrhz5 Endpoint Status: ### +----------------+------------------+---------+---------+-----------+-----------+------------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX | +----------------+------------------+---------+---------+-----------+-----------+------------+ | 127.0.0.1:2379 | e57d42e2a85763bb | 3.3.22 | 139 kB | true | 26 | 78360 | +----------------+------------------+---------+---------+-----------+-----------+------------+ ### cray-bos-etcd-b9m4k5qfrd Endpoint Status: ### +----------------+------------------+---------+---------+-----------+-----------+------------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX | +----------------+------------------+---------+---------+-----------+-----------+------------+ | 127.0.0.1:2379 | 355baed6cb6e3022 | 3.3.22 | 139 kB | false | 26 | 78360 | +----------------+------------------+---------+---------+-----------+-----------+------------+ ### cray-bos-etcd-tnpv8x6cxv Endpoint Status: ### +----------------+------------------+---------+---------+-----------+-----------+------------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX | +----------------+------------------+---------+---------+-----------+-----------+------------+ | 127.0.0.1:2379 | 78949579ff08b422 | 3.3.22 | 139 kB | false | 26 | 78360 | +----------------+------------------+---------+---------+-----------+-----------+------------+ ### cray-bss-etcd-q4k54rbbfj Endpoint Status: ### +----------------+------------------+---------+---------+-----------+-----------+------------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX | +----------------+------------------+---------+---------+-----------+-----------+------------+ | 127.0.0.1:2379 | cbeb570f568c6ca6 | 3.3.22 | 70 kB | true | 29 | 41321 | +----------------+------------------+---------+---------+-----------+-----------+------------+ ### cray-bss-etcd-r75mlv6ffd Endpoint Status: ### +----------------+------------------+---------+---------+-----------+-----------+------------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX | +----------------+------------------+---------+---------+-----------+-----------+------------+ | 127.0.0.1:2379 | 7343edb17d8e6fd4 | 3.3.22 | 70 kB | false | 29 | 41321 | +----------------+------------------+---------+---------+-----------+-----------+------------+ ### cray-bss-etcd-xprv5ht5d4 Endpoint Status: ### +----------------+------------------+---------+---------+-----------+-----------+------------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX | +----------------+------------------+---------+---------+-----------+-----------+------------+ | 127.0.0.1:2379 | d7404ad66483bd37 | 3.3.22 | 70 kB | false | 29 | 41321 | +----------------+------------------+---------+---------+-----------+-----------+------------+ ...   Report the endpoint status for a singe etcd cluster in a namespace.\nThe following example is for the services namespace.\nfor pod in $(kubectl get pods -l etcd_cluster=cray-bos-etcd -n services \\  -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;) do echo \u0026#34;### ${pod}Endpoint Status: ###\u0026#34; kubectl -n services exec ${pod} -- /bin/sh \\  -c \u0026#34;ETCDCTL_API=3 etcdctl endpoint status -w table\u0026#34; done ncn-w001# for pod in $(kubectl get pods -l etcd_cluster=cray-bos-etcd -n services \\ -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;); do echo \u0026#34;### ${pod}Endpoint Status: \\ ###\u0026#34;; kubectl -n services exec ${pod} -- /bin/sh -c \\ \u0026#34;ETCDCTL\\_API=3 etcdctl endpoint status -w table\u0026#34;; done ### cray-bos-etcd-7cxq6qrhz5 Endpoint Status: ### +----------------+------------------+---------+---------+-----------+-----------+------------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX | +----------------+------------------+---------+---------+-----------+-----------+------------+ | 127.0.0.1:2379 | e57d42e2a85763bb | 3.3.22 | 139 kB | true | 26 | 78333 | +----------------+------------------+---------+---------+-----------+-----------+------------+ ### cray-bos-etcd-b9m4k5qfrd Endpoint Status: ### +----------------+------------------+---------+---------+-----------+-----------+------------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX | +----------------+------------------+---------+---------+-----------+-----------+------------+ | 127.0.0.1:2379 | 355baed6cb6e3022 | 3.3.22 | 139 kB | false | 26 | 78333 | +----------------+------------------+---------+---------+-----------+-----------+------------+ ### cray-bos-etcd-tnpv8x6cxv Endpoint Status: ### +----------------+------------------+---------+---------+-----------+-----------+------------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX | +----------------+------------------+---------+---------+-----------+-----------+------------+ | 127.0.0.1:2379 | 78949579ff08b422 | 3.3.22 | 139 kB | false | 26 | 78333 | +----------------+------------------+---------+---------+-----------+-----------+------------+   "
},
{
	"uri": "/docs-csm/en-11/operations/kubernetes/increase_pod_resource_limits/",
	"title": "Increase Pod Resource Limits",
	"tags": [],
	"description": "",
	"content": "Increase Pod Resource Limits Increase the appropriate resource limits for pods after determining if a pod is being CPU throttled or OOMKilled.\nReturn Kubernetes pods to a healthy state with resources available.\nPrerequisites  kubectl is installed. The names of the pods hitting their resource limits are known. See Determine if Pods are Hitting Resource Limits.  Procedure   Determine the current limits of a pod.\nIn the example below, cray-hbtd-etcd-8r2scmpb58 is the POD_ID being used.\nncn-w001# kubectl get po -n services POD_ID -o yaml ... **(look for this section)** resources: limits: cpu: \u0026#34;2\u0026#34; memory: 2Gi requests: cpu: 10m memory: 64Mi   Determine which Kubernetes entity (etcdcluster, deployment, statefulset) is creating the pod.\nThe Kubernetes entity can be found with either of the following options:\n  Find the Kubernetes entity and grep for the pod in question.\nReplace hbtd-etcd with the pod being used.\nncn-w001# kubectl get deployment,statefulset,etcdcluster,postgresql,daemonsets \\ -A | grep hbtd-etcd services etcdcluster.etcd.database.coreos.com/cray-hbtd-etcd 32d   Describe the pod and look in the Labels section.\nThis section is helpful for tracking down which entity is creating the pod.\nncn-w001# kubectl describe pod -n services POD_ID . . . Labels: app=etcd etcd_cluster=cray-hbtd-etcd etcd_node=cray-hbtd-etcd-8r2scmpb58 . .     Edit the entity.\nIn the example below, the ENTITY is etcdcluster and the CLUSTER_NAME is cray-hbtd-etcd.\nncn-w001# kubectl edit ENTITY -n services CLUSTER_NAME   Increase the resource limits for the pod.\n resources: {} Replace the text above with the following section, increasing the limits value(s):\n resources: limits: cpu: \u0026quot;4\u0026quot; memory: 8Gi requests: cpu: 10m memory: 64Mi   Run a rolling restart of the pods.\nncn-w001# kubectl get po -n services | grep CLUSTER_NAME cray-hbtd-etcd-8r2scmpb58 1/1 Running 0 5d11h cray-hbtd-etcd-qvz4zzjzw2 1/1 Running 0 5d11h cray-hbtd-etcd-vzjzmbn6nr 1/1 Running 0 5d11h   Kill the pods off one by one.\nncn-w001# kubectl -n services delete pod POD_ID   Wait for a replacement pod to come up and be in a Running state before proceeding to the next pod.\nThey should all be running with a more recent age.\nncn-w001# kubectl get po -n services | grep CLUSTER_NAME cray-hbtd-etcd-8r2scmpb58 1/1 Running 0 12s cray-hbtd-etcd-qvz4zzjzw2 1/1 Running 0 32s cray-hbtd-etcd-vzjzmbn6nr 1/1 Running 0 98s   "
},
{
	"uri": "/docs-csm/en-11/operations/kubernetes/kubernetes/",
	"title": "Kubernetes",
	"tags": [],
	"description": "",
	"content": "Kubernetes The system management components are broken down into a series of micro-services. Each service is independently deployable, fine-grained, and uses lightweight protocols. As a result, the system\u0026rsquo;s micro-services are modular, resilient, and can be updated independently. Services within this architecture communicate via REST APIs.\nAbout Kubernetes Kubernetes is a portable and extensible platform for managing containerized workloads and services. Kubernetes serves as a micro-services platform on the system that facilitates application deployment, scaling, and management. The system uses Kubernetes for container orchestration.\nResiliency The resiliency feature of Kubernetes ensures that the desired number of deployments of a micro-service are always running on one or more NCNs. In addition, Kubernetes ensures that if one NCN becomes unresponsive, the micro-services that were running on it are migrated to another NCN that is up and meets the requirements of the micro-services.\nKubernetes Components Kubernetes components can be divided into:\n Master components - Kubernetes master components provide the cluster\u0026rsquo;s control plane. These components make global decisions about the cluster, such as scheduling, and responding to cluster events. Worker components - A Kubernetes worker is a node that provides services necessary to run application containers. It is managed by the Kubernetes master. Node components run on every node and keep pods running, while providing the Kubernetes runtime environment.  An etcd cluster is used for storage and state management of the Kubernetes cluster.\n"
},
{
	"uri": "/docs-csm/en-11/operations/kubernetes/kubernetes_networking/",
	"title": "Kubernetes Networking",
	"tags": [],
	"description": "",
	"content": "Kubernetes Networking Every Kubernetes pod has an IP address in the pod network that is reachable within the cluster. The system uses the weave-net plugin for inter-node communication.\nAccess services from outside the cluster All services with a REST API must be accessed from outside the cluster using the Istio Ingress Gateway. This gateway can be accessed using a URL in the following format:\nhttps://api.SYSTEM-NAME_DOMAIN-NAME The API requests then get routed to the appropriate node running that service.\nAccess services from within the cluster All services running inside the cluster can access each other using their Pod IP address or the service\u0026rsquo;s cluster IP address, along with the service\u0026rsquo;s exposed port. The exception to this is a service that has a Cray REST API. These services are configured such that they must be accessed through the API gateway service.\nNetwork Policies Kubernetes supports network policies to limit access to pods. Therefore, services running inside the cluster generally cannot access each other using their Pod IP address or the service\u0026rsquo;s cluster IP address. Any other services that must be accessed through a protocol other than REST, can do so using the cluster VIP and the service\u0026rsquo;s NodePort. Only services that are configured to expose a NodePort or ExternalIP can be accessed from outside the cluster.\nAs part of the SMS installation, the following network policies are configured on the system:\n keycloak-database: Allows only keycloak to access the keycloak Postgres instance sma-zookeeper: Allows only Apache Kafka to access the SMA Zookeeper instance sma-postgres: Allows only Grafana to access the SMA Postgres instance hms-mariadb: Allows only SMD to access the MariaDB instance hms-badger: Allows only badger services to access the badger Postgres instance api-gateway-database: Allows only the API gateway to access the API gateway Postgres instance api-gateway-upstream: Allows only the API gateway to access the upstream services vcs-database: Allows only Gitea to access the VCS instance  To learn more about Kubernetes, refer to https://kubernetes.io/.\n"
},
{
	"uri": "/docs-csm/en-11/operations/kubernetes/kubernetes_storage/",
	"title": "Kubernetes Storage",
	"tags": [],
	"description": "",
	"content": "Kubernetes Storage Data belonging to micro-services in the management cluster is managed through persistent storage, which provides reliable and resilient data protection for containers running in the Kubernetes cluster.\nThe backing storage for this service is currently provided by JBOD disks that are spread across several nodes of the management cluster. These node disks are managed by Ceph, and are exposed to containers in the form of persistent volumes.\n"
},
{
	"uri": "/docs-csm/en-11/operations/kubernetes/pod_resource_limits/",
	"title": "Pod Resource Limits",
	"tags": [],
	"description": "",
	"content": "Pod Resource Limits Kubernetes uses resource requests and Quality of Service (QoS) for scheduling pods. Resource requests can be provided explicitly for pods and containers, whereas pod QoS is implicit, based on the resource requests and limits of the containers in the pod. There are three types of QoS:\n Guaranteed: All containers in a pod have explicit memory and CPU resource requests and limits. For each resource, the limit equals the request. Burstable: Does not meet Guaranteed requirements, but some of the containers have explicit memory and CPU resource requests or limits. BestEffort: None of the containers specify any resources.  Kubernetes will best be able to schedule pods when there are resources associated with each container in each pod.\nResource Limits For systems, all containers should have explicit resource requests and limits. Most pods should fall into the Burstable category. Containers that have resource requests equal to the resource limits should be reserved for very well behaved containers, and will usually be simple, single-function containers. One example of this could be an init container that is waiting for another resource to become available.\nResource limits are set by default at the namespace level, but pods within that namespace can increase or decrease their limits depending on the nature of the workload.\n"
},
{
	"uri": "/docs-csm/en-11/operations/kubernetes/containerd/",
	"title": "Containerd",
	"tags": [],
	"description": "",
	"content": "Containerd Containerd is a daemonset that runs on the host. It is used to run containers on the Kubernetes platform.\nRestarting containerd If the containerd service is restarted on a worker node, this may cause the sonar-jobs-watcher pod running on that worker node to fail when attempting to cleanup unneeded containers. For example:\nncn-w001 # systemctl restart containerd ncn-w001 # ncn-w001:~ # kubectl get pods -l name=sonar-jobs-watcher -n services -o wide | grep ncn-w001 sonar-jobs-watcher-8z6th 1/1 Running 0 95d 10.42.0.6 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ncn-w001 # kubectl logs sonar-jobs-watcher-8z6th -n services  Found pod cray-dns-unbound-manager-1631116980-h69h6 with restartPolicy \u0026#39;Never\u0026#39; and container \u0026#39;manager\u0026#39; with status \u0026#39;Completed\u0026#39; All containers of job pod cray-dns-unbound-manager-1631116980-h69h6 has completed. Killing istio-proxy (1c65dacb960c2f8ff6b07dfc9780c4621beb8b258599453a08c246bbe680c511) to allow job to complete time=\u0026#34;2021-09-08T16:44:18Z\u0026#34; level=fatal msg=\u0026#34;failed to connect: failed to connect, make sure you are running as root and the runtime has been started: context deadline exceeded\u0026#34; When this occurs, pods that are running on the node where containerd was restarted may remain in a NotReady state and never complete. For example:\nncn-w001 # kubectl get pods -o wide -A | cray-dns-unbound-manager services cray-dns-unbound-manager-1631116980-h69h6 1/2 NotReady 0 10m 10.42.0.100 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Restarting the sonar-jobs-watcher daemonset should resolve the issue. Once the sonar-jobs-watcher pods restart, the pod(s) that were in a NotReady state should complete within about a minute.\nncn-w001 # kubectl rollout restart -n services daemonset sonar-jobs-watcher daemonset.apps/sonar-jobs-watcher restarted To learn more in general about containerd, refer to https://containerd.io/.\n"
},
{
	"uri": "/docs-csm/en-11/operations/kubernetes/create_a_manual_backup_of_a_healthy_etcd_cluster/",
	"title": "Create A Manual Backup Of A Healthy Etcd Cluster",
	"tags": [],
	"description": "",
	"content": "Create a Manual Backup of a Healthy etcd Cluster Manually create a backup of a healthy etcd cluster and check to see if the backup was created successfully.\nBackups of healthy etcd clusters can be used to restore the cluster if it becomes unhealthy at any point.\nThe commands in this procedure can be run on any master node (ncn-mXXX) or worker node (ncn-wXXX) on the system.\nPrerequisites A healthy etcd cluster is available on the system. See Check the Health and Balance of etcd Clusters.\nProcedure   Create a backup for the desired etcd cluster.\nThe example below is backing up the etcd cluster for the Boot Orchestration Service (BOS). The returned backup name (cray-bos-etcd-cluster-manual-backup-25847) will be used in the next step.\nncn-w001# kubectl exec -it -n operators \\ $(kubectl get pod -n operators | grep etcd-backup-restore | head -1 | awk \u0026#39;{print $1}\u0026#39;) \\ -c util -- create_backup cray-bos wednesday-manual-backup etcdbackup.etcd.database.coreos.com/cray-bos-etcd-cluster-manual-backup-25847 created   Check the status of the backup using the name returned in the output of the previous step.\nncn-w001# kubectl -n services get etcdbackup BACKUP_NAME -o yaml . . status: etcdRevision: 1 etcdVersion: 3.3.8 lastSuccessDate: \u0026#34;2020-01-13T21:38:47Z\u0026#34; succeeded: true   "
},
{
	"uri": "/docs-csm/en-11/operations/kubernetes/determine_if_pods_are_hitting_resource_limits/",
	"title": "Determine If Pods Are Hitting Resource Limits",
	"tags": [],
	"description": "",
	"content": "Determine if Pods are Hitting Resource Limits Determine if a pod is being CPU throttled or hitting its memory limits (OOMKilled). Use the detect_cpu_throttling.sh script to determine if any pods are being CPU throttled, and check the Kubernetes events to see if any pods are hitting a memory limit.\nIMPORTANT: The presence of CPU throttling does not always indicate a problem, but if a service is being slow or experiencing latency issues, this procedure can be used to evaluate if it is not performing well as a result of CPU throttling.\nIdentify pods that are hitting resource limits in order to increase the resource limits for those pods.\nPrerequisites kubectl is installed.\nProcedure   Use the detect_cpu_throttling.sh script to determine if any pods are being CP throttled.\n  Install the script.\nThe script can be installed on ncn-w001, or any NCN that can SSH to worker nodes.\nncn-w001# cat detect_cpu_throttling.sh #!/bin/sh # Usage: detect_cpu_throttling.sh [pod_name_substr] (default evaluates all pods) str=$1 : ${str:=.} while read ns pod node; do echo \u0026#34; echo \u0026#34;Checking $pod\u0026#34; while read -r container; do uid=$(echo $container | awk \u0026#39;BEGIN { FS = \u0026#34;/\u0026#34; } ; {print $NF}\u0026#39;)ssh -T ${node}\u0026lt;\u0026lt;-EOF dir=$(find /sys/fs/cgroup/cpu,cpuacct/kubepods/burstable -name *${uid}* 2\u0026gt;/dev/null)[ \u0026#34;${dir}\u0026#34; = \u0026#34; ] \u0026amp;\u0026amp; { dir=$(find /sys/fs/cgroup/cpu,cpuacct/system.slice/containerd.service -name *${uid}* 2\u0026gt;/dev/null); } if [ \u0026#34;${dir}\u0026#34; != \u0026#34; ]; then num_periods=$(grep nr_throttled ${dir}/cpu.stat | awk \u0026#39;{print $NF}\u0026#39;)if [ ${num_periods}-gt 0 ]; then echo \u0026#34;*** CPU throttling for containerid ${uid}: ***\u0026#34; cat ${dir}/cpu.stat echo \u0026#34; fi fi EOF done \u0026lt;\u0026lt;\u0026lt; \u0026#34;`kubectl -n $nsget pod $pod-o yaml | grep \u0026#39; - containerID\u0026#39;`\u0026#34; done \u0026lt;\u0026lt;\u0026lt;\u0026#34;$(kubectl get pods -A -o wide | grep $str | grep Running | awk \u0026#39;{print $1 \u0026#34; \u0026#34; $2 \u0026#34; \u0026#34; $8}\u0026#39;)\u0026#34;   Determine if any pods are being CPU throttled.\nThe script can be used in two different ways:\n    Pass in a substring of the desired pod name(s).\nIn the example below, the externaldns pods are being used.\nncn-w001# ./detect_cpu_throttling.sh externaldns Checking cray-externaldns-coredns-58b5f8494-c45kh Checking cray-externaldns-coredns-58b5f8494-pjvz6 Checking cray-externaldns-etcd-2kn7w6gnsx Checking cray-externaldns-etcd-88x4drpv27 Checking cray-externaldns-etcd-sbnbph52vh Checking cray-externaldns-external-dns-5bb8765896-w87wb *** CPU throttling: *** nr_periods 1127304 nr_throttled 473554 throttled_time 71962850825439   Call the script without a parameter to evaluate all pods.\nIt can take two minutes or more to run when evaluating all pods:\nncn-w001# ./detect_cpu_throttling.sh Checking benji-k8s-fsfreeze-9zlfk Checking benji-k8s-fsfreeze-fgqmd Checking benji-k8s-fsfreeze-qgbcp Checking benji-k8s-maint-796b444bfc-qcrhx Checking benji-k8s-postgresql-0 Checking benji-k8s-pushgateway-777fd86545-qrmbr . .     Check if a pod was killed/restarted because it reached its memory limit.\n  Look for a Kubernetes event associated with the pod being killed/restarted.\nncn-w001# kubectl get events -A | grep -C3 OOM default 54m Warning OOMKilling node/ncn-w003 Memory cgroup out of memory: Kill process 1223856 (prometheus) score 1966 or sacrifice child default 44m Warning OOMKilling node/ncn-w003 Memory cgroup out of memory: Kill process 1372634 (prometheus) score 1966 or sacrifice child   Determine which pod was killed using the output above.\nUse grep on the string returned in the previous step to find the pod name. In this example, prometheus is used.\nncn-w001# kubectl get pod -A | grep prometheus     Follow the procedure to increase the resource limits for the pods identified in this procedure. See Increase Pod Resource Limits.\n"
},
{
	"uri": "/docs-csm/en-11/operations/kubernetes/disaster_recovery_postgres/",
	"title": "Disaster Recovery For Postgres",
	"tags": [],
	"description": "",
	"content": "Disaster Recovery for Postgres In the event that the Postgres cluster has failed to the point that it must be recovered and there is no dump available to restore the data, a full service specific disaster recovery is needed.\nBelow are the service specific steps required to cleanup any existing resources, redeploy the resources and repopulate the data.\nDisaster Recovery Procedures by Service:\n Restore HSM (Hardware State Manger) Postgres without a Backup Restore SLS (System Layout Service) Postgres without a Backup  Keycloak The following procedures are required to rebuild the automatically populated contents of Keycloak\u0026rsquo;s PostgreSQL database if the database has been lost and recreated.\n Re-run the keycloak-setup Job by running the following commands on a Kubernetes NCN:  Run the following command to fetch the current Job definition: kubectl get job -n services -l app.kubernetes.io/name=cray-keycloak -oyaml |\\ yq r - 'items[0]' | yq d - 'spec.selector' | \\ yq d - 'spec.template.metadata.labels' \u0026gt; keycloak-setup.yaml There should be no output.\n Run the following command to restart the keycloak-setup Job: kubectl replace --force -f keycloak-setup.yaml The output should be similar to the following:\njob.batch \u0026quot;keycloak-setup-1\u0026quot; deleted job.batch/keycloak-setup-1 replaced  Wait for the Job to finish by running the following command: kubectl wait --for=condition=complete -n services job -l app.kubernetes.io/name=cray-keycloak --timeout=-1s The output should be similar to the following:\njob.batch/keycloak-setup-1 condition met    Re-run the keycloak-users-localize Job by running the following commands on a Kubernetes NCN:  Run the following command to fetch the current Job definition: kubectl get job -n services -l app.kubernetes.io/name=cray-keycloak-users-localize -oyaml |\\ yq r - 'items[0]' | yq d - 'spec.selector' | \\ yq d - 'spec.template.metadata.labels' \u0026gt; keycloak-users-localize.yaml There should be no output.\n Run the following command to restart the keycloak-users-localize Job: kubectl replace --force -f keycloak-users-localize.yaml The output should be similar to the following:\njob.batch \u0026quot;keycloak-users-localize-1\u0026quot; deleted job.batch/keycloak-users-localize-1 replaced  Wait for the Job to finish by running the following command: kubectl wait --for=condition=complete -n services job -l app.kubernetes.io/name=cray-keycloak-users-localize --timeout=-1s The output should be similar to the following:\njob.batch/keycloak-users-localize-1 condition met    Restart keycloak-gatekeeper to pick up the newly generated client ID by running the following commands on a Kubernetes NCN:  Run the following command to restart the keycloak-gatekeeper Pod(s): kubectl rollout restart deployment -n services cray-keycloak-gatekeeper-ingress  The output should match the following: deployment.apps/cray-keycloak-gatekeeper-ingress restarted     Any other changes made to Keycloak, such as local users that have been created, will have to be manually re-applied.\n Restore Spire Postgres without a Backup  "
},
{
	"uri": "/docs-csm/en-11/operations/kubernetes/increase_kafka_pod_resource_limits/",
	"title": "Increase Kafka Pod Resource Limits",
	"tags": [],
	"description": "",
	"content": "Increase Kafka Pod Resource Limits For larger scale systems, the Kafka resource limits may need to be increased. See Increase Pod Resource Limits for details on how to increase limits. Increase Kafka Resource Limits Example\nFor a 1500 compute node system, increasing the cpu count to 6 and memory limits to 128G should be adequate.\n"
},
{
	"uri": "/docs-csm/en-11/operations/kubernetes/check_for_and_clear_etcd_cluster_alarms/",
	"title": "Check For And Clear Etcd Cluster Alarms",
	"tags": [],
	"description": "",
	"content": "Check for and Clear etcd Cluster Alarms Check for any etcd cluster alarms and clear them as needed. An etcd cluster alarm must be manually cleared.\nFor example, a cluster\u0026rsquo;s database \u0026ldquo;NOSPACE\u0026rdquo; alarm is set when database storage space is no longer available. A subsequent defrag may free up database storage space, but writes to the database will continue to fail while the \u0026ldquo;NOSPACE\u0026rdquo; alarm is set.\nPrerequisites  This procedure requires root privileges. The etcd clusters are in a healthy state.  Procedure   Check for etcd cluster alarms.\nAn empty list will be returned if no alarms are set.\n  Check if any etcd alarms are set for etcd clusters in the services namespace.\nfor pod in $(kubectl get pods -l etcd_cluster=cray-bos-etcd \\  -n services -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;) do echo \u0026#34;### ${pod}Alarms Set: ###\u0026#34; kubectl -n services exec ${pod} -- /bin/sh \\  -c \u0026#34;ETCDCTL_API=3 etcdctl alarm list\u0026#34; done ncn-w001# for pod in $(kubectl get pods -l app=etcd -n services \\ -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;); \\ do echo \u0026#34;### ${pod}Alarms Set: ###\u0026#34;; kubectl -n services exec ${pod} -- /bin/sh -c \\ \u0026#34;ETCDCTL_API=3 etcdctl alarm list\u0026#34;; done ### cray-bos-etcd-7cxq6qrhz5 Alarms Set: ### ### cray-bos-etcd-b9m4k5qfrd Alarms Set: ### ### cray-bos-etcd-tnpv8x6cxv Alarms Set: ### ### cray-bss-etcd-q4k54rbbfj Alarms Set: ### ### cray-bss-etcd-r75mlv6ffd Alarms Set: ### ### cray-bss-etcd-xprv5ht5d4 Alarms Set: ### ### cray-cps-etcd-8hpztfkjdp Alarms Set: ### ### cray-cps-etcd-fp4kfsf799 Alarms Set: ### ### cray-cps-etcd-g6gz9vmmdn Alarms Set: ### ### cray-crus-etcd-6z9zskl6cr Alarms Set: ### ### cray-crus-etcd-krp255f97q Alarms Set: ### ### cray-crus-etcd-tpclqfln67 Alarms Set: ### ### cray-externaldns-etcd-2vnb5t4657 Alarms Set: ### ### cray-externaldns-etcd-sc4b88ptg2 Alarms Set: ### ### cray-externaldns-etcd-smhxd9mb8n Alarms Set: ### ### cray-fas-etcd-j9qmtrxnhh Alarms Set: ### ### cray-fas-etcd-w8xl7vbn84 Alarms Set: ### ### cray-fas-etcd-zr2vnvhdwk Alarms Set: ### ### cray-hbtd-etcd-jcxl65xwwd Alarms Set: ### ### cray-hbtd-etcd-rpwx7qdtxb Alarms Set: ### ### cray-hbtd-etcd-vswmwrmhpl Alarms Set: ### ### cray-hmnfd-etcd-2rpvswtpd2 Alarms Set: ### ### cray-hmnfd-etcd-6pm4tm5d6x Alarms Set: ### ### cray-hmnfd-etcd-776b2g5d4l Alarms Set: ### ### cray-reds-etcd-m8wgp24k9p Alarms Set: ### ### cray-reds-etcd-wghvvfbnjp Alarms Set: ### ### cray-reds-etcd-zpzw8mpkfk Alarms Set: ### ### cray-uas-mgr-etcd-4xq5swfsr2 Alarms Set: ### ### cray-uas-mgr-etcd-kfd64zwpbz Alarms Set: ### ### cray-uas-mgr-etcd-nmqkdh8n2d Alarms Set: ###   Check if any etcd alarms are set for a particular etcd cluster in the services namespace.\nfor pod in $(kubectl get pods -l etcd_cluster=cray-bos-etcd \\  -n services -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;) do echo \u0026#34;### ${pod}Alarms Set: ###\u0026#34; kubectl -n services exec ${pod} -- /bin/sh \\  -c \u0026#34;ETCDCTL_API=3 etcdctl alarm list\u0026#34; done ncn-w001# for pod in $(kubectl get pods -l etcd_cluster=cray-bos-etcd \\ -n services -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;); do echo \u0026#34;### \\ ${pod}Alarms Set: ###\u0026#34;; kubectl -n services exec ${pod} -- /bin/sh -c \\ \u0026#34;ETCDCTL_API=3 etcdctl alarm list\u0026#34;; done ### cray-bos-etcd-7cxq6qrhz5 Alarms Set: ### ### cray-bos-etcd-b9m4k5qfrd Alarms Set: ### ### cray-bos-etcd-tnpv8x6cxv Alarms Set: ###     Clear any etcd cluster alarms.\nA list of disarmed alarms will be returned. An empty list is returned if no alarms were set.\n  Clear all etcd alarms set in etcd clusters.\nfor pod in $(kubectl get pods -l app=etcd -n services \\  -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;) do echo \u0026#34;### ${pod}Disarmed Alarms: ###\u0026#34; kubectl -n services exec ${pod} -- /bin/sh \\  -c \u0026#34;ETCDCTL_API=3 etcdctl alarm disarm\u0026#34; done ncn-w001# for pod in $(kubectl get pods -l app=etcd -n services -o \\ jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;); do echo \u0026#34;### ${pod}Disarmed Alarms: \\ ###\u0026#34;; kubectl -n services exec ${pod} -- /bin/sh -c \\ \u0026#34;ETCDCTL_API=3 etcdctl alarm disarm\u0026#34;; done ### cray-bos-etcd-7cxq6qrhz5 Disarmed Alarms: ### ### cray-bos-etcd-b9m4k5qfrd Disarmed Alarms: ### ### cray-bos-etcd-tnpv8x6cxv Disarmed Alarms: ### ### cray-bss-etcd-q4k54rbbfj Disarmed Alarms: ### ### cray-bss-etcd-r75mlv6ffd Disarmed Alarms: ### ### cray-bss-etcd-xprv5ht5d4 Disarmed Alarms: ### ### cray-cps-etcd-8hpztfkjdp Disarmed Alarms: ### ### cray-cps-etcd-fp4kfsf799 Disarmed Alarms: ### ### cray-cps-etcd-g6gz9vmmdn Disarmed Alarms: ### ### cray-crus-etcd-6z9zskl6cr Disarmed Alarms: ### ### cray-crus-etcd-krp255f97q Disarmed Alarms: ### ### cray-crus-etcd-tpclqfln67 Disarmed Alarms: ### ### cray-externaldns-etcd-2vnb5t4657 Disarmed Alarms: ### ### cray-externaldns-etcd-sc4b88ptg2 Disarmed Alarms: ### ### cray-externaldns-etcd-smhxd9mb8n Disarmed Alarms: ### ### cray-fas-etcd-j9qmtrxnhh Disarmed Alarms: ### ### cray-fas-etcd-w8xl7vbn84 Disarmed Alarms: ### ### cray-fas-etcd-zr2vnvhdwk Disarmed Alarms: ### ### cray-hbtd-etcd-jcxl65xwwd Disarmed Alarms: ### ### cray-hbtd-etcd-rpwx7qdtxb Disarmed Alarms: ### ### cray-hbtd-etcd-vswmwrmhpl Disarmed Alarms: ### ### cray-hmnfd-etcd-2rpvswtpd2 Disarmed Alarms: ### ### cray-hmnfd-etcd-6pm4tm5d6x Disarmed Alarms: ### ### cray-hmnfd-etcd-776b2g5d4l Disarmed Alarms: ### ### cray-reds-etcd-m8wgp24k9p Disarmed Alarms: ### ### cray-reds-etcd-wghvvfbnjp Disarmed Alarms: ### ### cray-reds-etcd-zpzw8mpkfk Disarmed Alarms: ### ### cray-uas-mgr-etcd-4xq5swfsr2 Disarmed Alarms: ### ### cray-uas-mgr-etcd-kfd64zwpbz Disarmed Alarms: ### ### cray-uas-mgr-etcd-nmqkdh8n2d Disarmed Alarms: ###   Clear all alarms in one particular etcd cluster.\nfor pod in $(kubectl get pods -l etcd_cluster=cray-bos-etcd \\  -n services -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;) do echo \u0026#34;### ${pod}Disarmed Alarms: ###\u0026#34; kubectl -n services exec ${pod} -- /bin/sh \\  -c \u0026#34;ETCDCTL_API=3 etcdctl alarm disarm\u0026#34; done ncn-w001# for pod in $(kubectl get pods -l etcd_cluster=cray-bos-etcd \\ -n services -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;); do echo \u0026#34;### ${pod}\\ Disarmed Alarms: ###\u0026#34;; kubectl -n services exec ${pod} -- /bin/sh \\ -c \u0026#34;ETCDCTL_API=3 etcdctl alarm disarm\u0026#34;; done ### cray-bos-etcd-7cxq6qrhz5 Disarmed Alarms: ### memberID:14039380531903955557 alarm:NOSPACE memberID:10060051157615504224 alarm:NOSPACE memberID:9418794810465807950 alarm:NOSPACE ### cray-bos-etcd-b9m4k5qfrd Disarmed Alarms: ### ### cray-bos-etcd-tnpv8x6cxv Disarmed Alarms: ###     "
},
{
	"uri": "/docs-csm/en-11/operations/kubernetes/check_the_health_and_balance_of_etcd_clusters/",
	"title": "Check The Health And Balance Of Etcd Clusters",
	"tags": [],
	"description": "",
	"content": "Check the Health and Balance of etcd Clusters Check to see if all of the etcd clusters have healthy pods, are balanced, and have a healthy cluster database. There needs to be the same number of pods running on each worker node for the etcd clusters to be balanced. If the number of pods is not the same for each worker node, the cluster is not balanced.\nAny clusters that do not have healthy pods will need to be rebuilt. Kubernetes cluster data will not be stored as efficiently when etcd clusters are not balanced.\nPrerequisites This procedure requires root privileges.\nProcedure   Check the health of the clusters.\nTo check the health of the etcd clusters in the services namespace without TLS authentication:\nncn-w001# for pod in $(kubectl get pods -l app=etcd -n services \\ -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;); do echo \u0026#34;### ${pod}###\u0026#34;; \\ kubectl -n services exec ${pod} -- /bin/sh -c \u0026#34;ETCDCTL_API=3 etcdctl endpoint health\u0026#34;; done ### cray-bos-etcd-6nkn6dzhv7 ### 127.0.0.1:2379 is healthy: successfully committed proposal: took = 1.670457ms ### cray-bos-etcd-6xtp2gqs64 ### 127.0.0.1:2379 is healthy: successfully committed proposal: took = 954.462µs ### cray-bos-etcd-gnt9rxcbvl ### 127.0.0.1:2379 is healthy: successfully committed proposal: took = 1.313505ms ### cray-bss-etcd-4jsn7p49rj ### 127.0.0.1:2379 is healthy: successfully committed proposal: took = 2.054509ms ### cray-bss-etcd-9q6xf5wl5q ### 127.0.0.1:2379 is healthy: successfully committed proposal: took = 1.174929ms ### cray-bss-etcd-ncwkjmlq8b ### 127.0.0.1:2379 is healthy: successfully committed proposal: took = 1.632738ms ### cray-cps-etcd-8ml5whzhjh ### 127.0.0.1:2379 is healthy: successfully committed proposal: took = 1.792795ms ### cray-cps-etcd-9xh5dbvgvp ### 127.0.0.1:2379 is healthy: successfully committed proposal: took = 1.952686ms ### cray-cps-etcd-cmjb9hvfgb ### 127.0.0.1:2379 is healthy: successfully committed proposal: took = 1.132689ms ### cray-crus-etcd-4ccjwq9g2g ### 127.0.0.1:2379 is healthy: successfully committed proposal: took = 1.000064ms ### cray-crus-etcd-5p7xgk9fj4 ### 127.0.0.1:2379 is healthy: successfully committed proposal: took = 1.415419ms ### cray-crus-etcd-qmdcb6xvjg ### 127.0.0.1:2379 is healthy: successfully committed proposal: took = 1.383509ms ### cray-externaldns-etcd-2b2rkfgj6s ### 127.0.0.1:2379 is healthy: successfully committed proposal: took = 1.39374ms ### cray-externaldns-etcd-7frkfnbncx ### 127.0.0.1:2379 is healthy: successfully committed proposal: took = 1.405028ms ### cray-externaldns-etcd-7gwlgc47pf ### 127.0.0.1:2379 is healthy: successfully committed proposal: took = 974.093µs ### cray-fw-update-etcd-brplp6r44r ### 127.0.0.1:2379 is healthy: successfully committed proposal: took = 1.030956ms ### cray-fw-update-etcd-qzwf2k4ccs ### 127.0.0.1:2379 is healthy: successfully committed proposal: took = 8.40234ms ### cray-fw-update-etcd-wp29sfq25j ### 127.0.0.1:2379 is healthy: successfully committed proposal: took = 1.740369ms ### cray-hbtd-etcd-b5lsrllglj ### 127.0.0.1:2379 is healthy: successfully committed proposal: took = 1.639258ms ### cray-hbtd-etcd-dnsgsr4xqb ### 127.0.0.1:2379 is healthy: successfully committed proposal: took = 1.761431ms ### cray-hbtd-etcd-zzbrp6hmdv ### 127.0.0.1:2379 is healthy: successfully committed proposal: took = 1.437008ms ### cray-hmnfd-etcd-4fp999tl7c ### 127.0.0.1:2379 is healthy: successfully committed proposal: took = 1.200601ms ### cray-hmnfd-etcd-5k9nw25pdc ### 127.0.0.1:2379 is healthy: successfully committed proposal: took = 1.75563ms ### cray-hmnfd-etcd-8tzmz869gx ### 127.0.0.1:2379 is healthy: successfully committed proposal: took = 1.1993ms ### cray-reds-etcd-mmnqnd45bw ### 127.0.0.1:2379 is healthy: successfully committed proposal: took = 1.034105ms ### cray-reds-etcd-swkghllr6n ### 127.0.0.1:2379 is healthy: successfully committed proposal: took = 1.456472ms ### cray-reds-etcd-wspl4fbt97 ### 127.0.0.1:2379 is healthy: successfully committed proposal: took = 1.351777ms If any of the etcd clusters are not healthy, refer to Rebuild Unhealthy etcd Clusters.\n  Check the number of pods in each cluster and verify they are balanced.\nEach cluster should contain at least three pods, but may contain more. Ensure that no two pods in a given cluster exist on the same worker node.\nncn-w001# kubectl get pod -n services -o wide | head -n 1; for cluster in \\ $(kubectl get etcdclusters.etcd.database.coreos.com -n services | grep -v NAME | \\ awk \u0026#39;{print $1}\u0026#39;); do kubectl get pod -n services -o wide | grep $cluster; echo \u0026#34;; done NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATE cray-bos-etcd-7gl9dccmrq 1/1 Running 0 8d 10.40.0.88 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-bos-etcd-g65fjhhlbg 1/1 Running 0 8d 10.42.0.36 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-bos-etcd-lbsppj5kt7 1/1 Running 0 20h 10.47.0.98 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-bss-etcd-dbhxvz824w 1/1 Running 0 8d 10.42.0.45 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-bss-etcd-hzpbrcn2pb 1/1 Running 0 20h 10.47.0.99 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-bss-etcd-kpc64v64wd 1/1 Running 0 8d 10.40.0.43 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-cps-etcd-8ndvn4dlx4 1/1 Running 0 20h 10.47.0.100 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-cps-etcd-gvlql48gwk 1/1 Running 0 8d 10.40.0.89 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-cps-etcd-wsvhmp4f7p 1/1 Running 0 8d 10.42.0.64 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-crus-etcd-2wvb2bpczb 1/1 Running 0 20h 10.47.0.117 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-crus-etcd-fhbcvknghh 1/1 Running 0 8d 10.42.0.34 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-crus-etcd-nrxqzftrzr 1/1 Running 0 8d 10.40.0.45 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-externaldns-etcd-7skqmr825d 1/1 Running 0 20h 10.47.0.119 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-externaldns-etcd-gm2s7nkjgl 1/1 Running 0 8d 10.42.0.13 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-externaldns-etcd-ttnchdrwjl 1/1 Running 0 8d 10.40.0.22 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-fas-etcd-29qcrd8qdt 1/1 Running 0 20h 10.47.0.102 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-fas-etcd-987c87m4mv 1/1 Running 0 8d 10.40.0.66 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-fas-etcd-9fxbzkzrsv 1/1 Running 0 8d 10.42.0.43 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-hbtd-etcd-2sf24nw5zs 1/1 Running 0 8d 10.40.0.78 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-hbtd-etcd-5r6mgvjct8 1/1 Running 0 20h 10.47.0.105 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-hbtd-etcd-t78x5wqkjt 1/1 Running 0 8d 10.42.0.51 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-hmnfd-etcd-99j5zt5ln6 1/1 Running 0 8d 10.40.0.74 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-hmnfd-etcd-h9gnvvs7rs 1/1 Running 0 8d 10.42.0.39 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-hmnfd-etcd-lj72f8xjkv 1/1 Running 0 20h 10.47.0.103 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-reds-etcd-97wr66d4pj 1/1 Running 0 20h 10.47.0.129 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-reds-etcd-kmggscpzrf 1/1 Running 0 8d 10.40.0.64 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-reds-etcd-zcwrhm884l 1/1 Running 0 8d 10.42.0.53 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-uas-mgr-etcd-7gmh92t2hx 1/1 Running 0 20h 10.47.0.94 ncn-w001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-uas-mgr-etcd-7m4qmtgp6t 1/1 Running 0 8d 10.42.0.67 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cray-uas-mgr-etcd-pldlkpr48w 1/1 Running 0 8d 10.40.0.94 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; If the etcd clusters are not balanced, see Rebalance Healthy etcd Clusters.\n  Check the health of an etcd cluster database.\n  To check the health of an etcd cluster\u0026rsquo;s database in the services namespace:\nfor pod in $(kubectl get pods -l app=etcd -n services \\  -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;) do echo \u0026#34;### ${pod}Etcd Database Check: ###\u0026#34; dbc=$(kubectl -n services exec ${pod} -- /bin/sh \\  -c \u0026#34;ETCDCTL_API=3 etcdctl put foo fooCheck \u0026amp;\u0026amp; \\ ETCDCTL_API=3 etcdctl get foo \u0026amp;\u0026amp; \\ ETCDCTL_API=3 etcdctl del foo \u0026amp;\u0026amp; \\ ETCDCTL_API=3 etcdctl get foo\u0026#34; 2\u0026gt;\u0026amp;1) echo $dbc | awk \u0026#39;{ if ( $1==\u0026#34;OK\u0026#34; \u0026amp;\u0026amp; $2==\u0026#34;foo\u0026#34; \u0026amp;\u0026amp; \\ $3==\u0026#34;fooCheck\u0026#34; \u0026amp;\u0026amp; $4==\u0026#34;1\u0026#34; \u0026amp;\u0026amp; $5==\u0026#34; ) print \\ \u0026#34;PASS: \u0026#34; PRINT $0; else \\ print \u0026#34;FAILED DATABASE CHECK - EXPECTED: OK foo fooCheck 1 \\ GOT: \u0026#34; PRINT $0 }\u0026#39; done For example:\nncn-w001# for pod in $(kubectl get pods -l app=etcd -n services -o \\ jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;); do echo \u0026#34;### ${pod}\\ Etcd Database Check: ###\u0026#34;; dbc=$(kubectl -n services exec ${pod} \\ -- /bin/sh -c \u0026#34;ETCDCTL_API=3 etcdctl put foo fooCheck \u0026amp;\u0026amp; ETCDCTL_API=3 \\ etcdctl get foo \u0026amp;\u0026amp; ETCDCTL_API=3 etcdctl del foo \u0026amp;\u0026amp; ETCDCTL_API=3 \\ etcdctl get foo\u0026#34; 2\u0026gt;\u0026amp;1); echo $dbc | awk \u0026#39;{ if ( $1==\u0026#34;OK\u0026#34; \u0026amp;\u0026amp; \\ $2==\u0026#34;foo\u0026#34; \u0026amp;\u0026amp; $3==\u0026#34;fooCheck\u0026#34; \u0026amp;\u0026amp; $4==\u0026#34;1\u0026#34; \u0026amp;\u0026amp; $5==\u0026#34; ) print \u0026#34;PASS: \\ \u0026#34; PRINT $0; else print \u0026#34;FAILED DATABASE CHECK - \\ EXPECTED: OK foo fooCheck 1 GOT: \u0026#34; PRINT $0 \\}\u0026#39;; done ### cray-bos-etcd-7cxq6qrhz5 Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-bos-etcd-b9m4k5qfrd Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-bos-etcd-tnpv8x6cxv Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-bss-etcd-q4k54rbbfj Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-bss-etcd-r75mlv6ffd Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-bss-etcd-xprv5ht5d4 Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-cps-etcd-8hpztfkjdp Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-cps-etcd-fp4kfsf799 Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-cps-etcd-g6gz9vmmdn Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-crus-etcd-6z9zskl6cr Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-crus-etcd-krp255f97q Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-crus-etcd-tpclqfln67 Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-externaldns-etcd-2vnb5t4657 Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-externaldns-etcd-sc4b88ptg2 Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-externaldns-etcd-smhxd9mb8n Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-fas-etcd-j9qmtrxnhh Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-fas-etcd-w8xl7vbn84 Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-fas-etcd-zr2vnvhdwk Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-hbtd-etcd-jcxl65xwwd Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-hbtd-etcd-rpwx7qdtxb Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-hbtd-etcd-vswmwrmhpl Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-hmnfd-etcd-2rpvswtpd2 Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-hmnfd-etcd-6pm4tm5d6x Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-hmnfd-etcd-776b2g5d4l Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-reds-etcd-m8wgp24k9p Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-reds-etcd-wghvvfbnjp Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-reds-etcd-zpzw8mpkfk Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-uas-mgr-etcd-4xq5swfsr2 Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-uas-mgr-etcd-kfd64zwpbz Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-uas-mgr-etcd-nmqkdh8n2d Etcd Database Check: ### PASS: OK foo fooCheck 1   To check one cluster:\nfor pod in $(kubectl get pods -l etcd_cluster=cray-bos-etcd -n services \\  -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;) do echo \u0026#34;### ${pod}Etcd Database Check: ###\u0026#34; dbc=$(kubectl -n services exec ${pod} -- /bin/sh \\  -c \u0026#34;ETCDCTL_API=3 etcdctl put foo fooCheck \u0026amp;\u0026amp; \\ ETCDCTL_API=3 etcdctl get foo \u0026amp;\u0026amp; \\ ETCDCTL_API=3 etcdctl del foo \u0026amp;\u0026amp; \\ ETCDCTL_API=3 etcdctl get foo\u0026#34; 2\u0026gt;\u0026amp;1) echo $dbc | awk \u0026#39;{ if ( $1==\u0026#34;OK\u0026#34; \u0026amp;\u0026amp; $2==\u0026#34;foo\u0026#34; \u0026amp;\u0026amp; \\ $3==\u0026#34;fooCheck\u0026#34; \u0026amp;\u0026amp; $4==\u0026#34;1\u0026#34; \u0026amp;\u0026amp; $5==\u0026#34; ) print \\ \u0026#34;PASS: \u0026#34; PRINT $0; else \\ print \u0026#34;FAILED DATABASE CHECK - EXPECTED: OK foo fooCheck 1 \\ GOT: \u0026#34; PRINT $0 }\u0026#39; done For example:\nncn-w001# for pod in $(kubectl get pods -l etcd_cluster=cray-bos-etcd \\ -n services -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;); do echo \\ \u0026#34;### ${pod}Etcd Database Check: ###\u0026#34;; dbc=$(kubectl -n \\ services exec ${pod} -- /bin/sh -c \u0026#34;ETCDCTL_API=3 etcdctl \\ put foo fooCheck \u0026amp;\u0026amp; ETCDCTL_API=3 etcdctl get foo \u0026amp;\u0026amp; \\ ETCDCTL_API=3 etcdctl del foo \u0026amp;\u0026amp; ETCDCTL_API=3 etcdctl get \\ foo\u0026#34; 2\u0026gt;\u0026amp;1); echo $dbc | awk \u0026#39;{ if ( $1==\u0026#34;OK\u0026#34; \u0026amp;\u0026amp; $2==\u0026#34;foo\u0026#34; \u0026amp;\u0026amp; \\ $3==\u0026#34;fooCheck\u0026#34; \u0026amp;\u0026amp; $4==\u0026#34;1\u0026#34; \u0026amp;\u0026amp; $5==\u0026#34; ) print \u0026#34;PASS: \u0026#34; PRINT $0; \\ else print \u0026#34;FAILED DATABASE CHECK - EXPECTED: \\ OK foo fooCheck 1 GOT: \u0026#34; PRINT $0 }\u0026#39;; done ### cray-bos-etcd-7cxq6qrhz5 Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-bos-etcd-b9m4k5qfrd Etcd Database Check: ### PASS: OK foo fooCheck 1 ### cray-bos-etcd-tnpv8x6cxv Etcd Database Check: ### PASS: OK foo fooCheck 1   If any of the etcd cluster databases are not healthy, refer to the following procedures:\n Refer to Check for and Clear etcd Cluster Alarms Refer to Clear Space in an etcd Cluster Database    "
},
{
	"uri": "/docs-csm/en-11/operations/kubernetes/clear_space_in_an_etcd_cluster_database/",
	"title": "Clear Space In An Etcd Cluster Database",
	"tags": [],
	"description": "",
	"content": "Clear Space in an etcd Cluster Database Use this procedure to clear the etcd cluster NOSPACE alarm. Once it is set it will remain set. If needed, defrag the database cluster before clearing the NOSPACE alarm.\nDefragging the database cluster and clearing the etcd cluster NOSPACE alarm will free up database space.\nPrerequisites  This procedure requires root privileges. The etcd clusters are in a healthy state.  Procedure   Clear up space when the etcd database space has exceeded and has been defragged, but the NOSPACE alarm remains set.\n  Verify that the attempt to store a new key-value fails.\nkubectl -n services exec -it hbtd-etcd-h59j42knjv -- sh ncn-w001# kubectl -n services exec -it hbtd-ETCD_CLUSTER -- sh # export ETCDCTL_API=3 # etcdctl put foo bar {\u0026#34;level\u0026#34;:\u0026#34;warn\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2020-10-23T23:56:48.408Z\u0026#34;,\u0026#34;caller\u0026#34;:\u0026#34;clientv3/retry_interceptor.go:62\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;retrying of unary invoker failed\u0026#34;,\u0026#34;target\u0026#34;:\u0026#34;endpoint://client-208534eb-2ab4-4c58-8853-58bff088c394/127.0.0.1:2379\u0026#34;,\u0026#34;attempt\u0026#34;:0,\u0026#34;error\u0026#34;:\u0026#34;rpc error: code = ResourceExhausted desc = etcdserver: mvcc: database space exceeded\u0026#34;} Error: etcdserver: mvcc: database space exceeded   Check to see if the default 2G disk usage space (unless defined different in the helm chart) is currently exceeded.\nIn the following example, the disk usage is 375.5 M, which means the disk space has not been exceeded.\nkubectl -n services exec -it hbtd-etcd-h59j42knjv -- sh ncn-w001# kubectl -n services exec -it hbtd-ETCD_CLUSTER -- sh # df -h Filesystem Size Used Available Use% Mounted on overlay 396.3G 59.6G 316.5G 16% / tmpfs 64.0M 0 64.0M 0% /dev tmpfs 125.7G 0 125.7G 0% /sys/fs/cgroup /dev/rbd21 2.9G 375.5M 2.5G 13% /var/etcd. \u0026lt;------/dev/sdc4 396.3G 22.0G 354.1G 6% /etc/hosts /dev/sdc4 396.3G 22.0G 354.1G 6% /dev/termination-log /dev/sdc5 396.3G 59.6G 316.5G 16% /etc/hostname /dev/sdc5 396.3G 59.6G 316.5G 16% /etc/resolv.conf   Clear the NOSPACE alarm.\nfor pod in $(kubectl get pods -l etcd_cluster=hbtd-etcd \\  -n services -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;) do echo \u0026#34;### ${pod}###\u0026#34; kubectl -n services exec ${pod} -- /bin/sh \\  -c \u0026#34;ETCDCTL_API=3 etcdctl alarm disarm\u0026#34; done ncn-w001# for pod in $(kubectl get pods -l etcd_cluster=hbtd-etcd \\ -n services -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;); do echo \u0026#34;### ${pod}###\u0026#34;; \\ kubectl -n services exec ${pod} -- /bin/sh \\ -c \u0026#34;ETCDCTL_API=3 etcdctl alarm disarm\u0026#34;; done ### hbtd-etcd-h59j42knjv ### memberID:6004340417806974740 alarm:NOSPACE memberID:10618826089438871005 alarm:NOSPACE memberID:6927946043724325475 alarm:NOSPACE ### hbtd-etcd-jfwh9l49lm ### ### hbtd-etcd-mhklm4n5qd ###   Verify a new key-value can now be successfully stored.\nkubectl -n services exec -it hbtd-etcd-h59j42knjv -- sh ncn-w001# kubectl -n services exec -it hbtd-ETCD_CLUSTER -- sh ncn-w001# export ETCDCTL_API=3 ncn-w001# etcdctl put foo bar OK     Clear the NOSPACE alarm. If the database needs to be defragged, the alarm will be reset.\n  Confirm the \u0026ldquo;database space exceeded\u0026rdquo; message is present.\nkubectl logs -n services --tail=-1 --prefix=true -l \u0026#34;app.kubernetes.io/name=cray-hbtd\u0026#34; -c cray-hbtd | grep \u0026#34;x3005c0s19b1n0\u0026#34; ncn-w001# kubectl logs -n services --tail=-1 --prefix=true -l \\ \u0026#34;app.kubernetes.io/name=cray-hbtd\u0026#34; -c cray-hbtd | grep \u0026#34;x3005c0s19b1n0\u0026#34; [pod/cray-hbtd-56bc4f6fdb-92bqx/cray-hbtd] 2020/09/15 20:00:44 INTERNAL ERROR storing key {\u0026#34;Component\u0026#34;:\u0026#34;x3005c0s19b1n0\u0026#34;,\u0026#34;Last_hb_rcv_time\u0026#34;:\u0026#34;5f611d6c\u0026#34;,\u0026#34;Last_hb_timestamp\u0026#34;:\u0026#34;2020-09-15T15:00:44.878876-06:00\u0026#34;,\u0026#34;Last_hb_status\u0026#34;:\u0026#34;OK\u0026#34;,\u0026#34;Had_warning\u0026#34;:\u0026#34;} : etcdserver: mvcc: database space exceeded [pod/cray-hbtd-56bc4f6fdb-92bqx/cray-hbtd] 2020/09/15 20:00:47 INTERNAL ERROR storing key {\u0026#34;Component\u0026#34;:\u0026#34;x3005c0s19b1n0\u0026#34;,\u0026#34;Last_hb_rcv_time\u0026#34;:\u0026#34;5f611d6f\u0026#34;,\u0026#34;Last_hb_timestamp\u0026#34;:\u0026#34;2020-09-15T15:00:47.893757-06:00\u0026#34;,\u0026#34;Last_hb_status\u0026#34;:\u0026#34;OK\u0026#34;,\u0026#34;Had_warning\u0026#34;:\u0026#34;} : etcdserver: mvcc: database space exceeded [pod/cray-hbtd-56bc4f6fdb-92bqx/cray-hbtd] 2020/09/15 20:00:53 INTERNAL ERROR storing key {\u0026#34;Component\u0026#34;:\u0026#34;x3005c0s19b1n0\u0026#34;,\u0026#34;Last_hb_rcv_time\u0026#34;:\u0026#34;5f611d75\u0026#34;,\u0026#34;Last_hb_timestamp\u0026#34;:\u0026#34;2020-09-15T15:00:53.926195-06:00\u0026#34;,\u0026#34;Last_hb_status\u0026#34;:\u0026#34;OK\u0026#34;,\u0026#34;Had_warning\u0026#34;:\u0026#34;} : etcdserver: mvcc: database space exceeded [pod/cray-hbtd-56bc4f6fdb-92bqx/cray-hbtd] 2020/09/15 20:01:02 INTERNAL ERROR storing key {\u0026#34;Component\u0026#34;:\u0026#34;x3005c0s19b1n0\u0026#34;,\u0026#34;Last_hb_rcv_time\u0026#34;:\u0026#34;5f611d7e\u0026#34;,\u0026#34;Last_hb_timestamp\u0026#34;:\u0026#34;2020-09-15T15:01:02.970168-06:00\u0026#34;,\u0026#34;Last_hb_status\u0026#34;:\u0026#34;OK\u0026#34;,\u0026#34;Had_warning\u0026#34;:\u0026#34;} : etcdserver: mvcc: database space exceeded [pod/cray-hbtd-56bc4f6fdb-92bqx/cray-hbtd] 2020/09/15 20:01:05 INTERNAL ERROR storing key {\u0026#34;Component\u0026#34;:\u0026#34;x3005c0s19b1n0\u0026#34;,\u0026#34;Last_hb_rcv_time\u0026#34;:\u0026#34;5f611d81\u0026#34;,\u0026#34;Last_hb_timestamp\u0026#34;:\u0026#34;2020-09-15T15:01:05.983828-06:00\u0026#34;,\u0026#34;Last_hb_status\u0026#34;:\u0026#34;OK\u0026#34;,\u0026#34;Had_warning\u0026#34;:\u0026#34;} : etcdserver: mvcc: database space exceeded   Check if the default 2G (unless defined different in the helm chart) disk usage has been exceeded.\nkubectl exec -it -n services cray-hbtd-etcd-6p4tc4jdgm -- sh ncn-w001# kubectl exec -it -n services ETCD_CLUSTER_NAME -- sh # df -h Filesystem Size Used Available Use% Mounted on overlay 439.1G 15.2G 401.5G 4% / tmpfs 64.0M 0 64.0M 0% /dev tmpfs 125.7G 0 125.7G 0% /sys/fs/cgroup /dev/rbd3 7.8G 2.4G 5.4G 31% /var/etcd   Resolve the space issue by either increasing the frequency of how often the etcd-defrag cron job is run, or by triggering it manually.\nSelect one of the following options:\n  Increase the frequency of the kube-etcd-defrag from every 24 hours to 12 hours.\nkubectl edit -n operators cronjob.batch/kube-etcd-defrag ncn-w001# kubectl edit -n operators cronjob.batch/kube-etcd-defrag ... ... name: etcd-defrag name: etcd-defrag schedule: 0 */12 * * * successfulJobsHistoryLimit: 1 suspend: false status: ... ...   Trigger the job manually.\nkubectl -n operators create job --from=cronjob/kube-etcd-defrag kube-etcd-defrag ncn-w001# kubectl -n operators create job \\ --from=cronjob/kube-etcd-defrag kube-etcd-defrag     Check the log messages after the defrag job is triggered.\nncn-w001# kubectl logs -f -n operators pod/kube-etcd-defrag-1600171200-fxpn7 Defragging cray-bos-etcd-j7czpr9pbr Defragging cray-bos-etcd-k4qtjtgqjb Defragging cray-bos-etcd-wcm8cs7dvc Defragging cray-bss-etcd-2h6k4l4j2g Defragging cray-bss-etcd-5dqwvrdtnf Defragging cray-bss-etcd-zlwmzkcjhz Defragging cray-cps-etcd-6cqw8sw5k6 Defragging cray-cps-etcd-psjm9lpw66 Defragging cray-cps-etcd-rp6fp94ccv Defragging cray-crus-etcd-228mdpm2h6 Defragging cray-crus-etcd-hldtxr6f9s Defragging cray-crus-etcd-sfsckpv4vw Defragging cray-externaldns-etcd-6l772b2cdv Defragging cray-externaldns-etcd-khbgl45pf7 Defragging cray-externaldns-etcd-qphz6lmhns Defragging cray-fas-etcd-7ktd4h47jv Defragging cray-fas-etcd-b27mknzs2q Defragging cray-fas-etcd-vccgfhcnnt Defragging cray-hbtd-etcd-6p4tc4jdgm Defragging cray-hbtd-etcd-j4xs7zj6v5 Defragging cray-hbtd-etcd-pb9tnrj6bw Defragging cray-hmnfd-etcd-558sqc22lw Defragging cray-hmnfd-etcd-prt7k224br Defragging cray-hmnfd-etcd-rf2x7sth84 Defragging cray-reds-etcd-h94pq6w7cv Defragging cray-reds-etcd-hgxdbgc65j Defragging cray-reds-etcd-qxfh756zhm Defragging cray-uas-mgr-etcd-5548zzfw8w Defragging cray-uas-mgr-etcd-ngbm48qz2g Defragging cray-uas-mgr-etcd-qnrlg6r4n6   Verify the disk space is less than the size limit.\nkubectl exec -it -n services cray-hbtd-etcd-6p4tc4jdgm -- sh / # df -h ncn-w001# kubectl exec -it -n services ETCD_CLUSTER_NAME -- sh # df -h Filesystem Size Used Available Use% Mounted on overlay 439.1G 15.2G 401.5G 4% / tmpfs 64.0M 0 64.0M 0% /dev tmpfs 125.7G 0 125.7G 0% /sys/fs/cgroup /dev/rbd3 7.8G 403.0M 7.4G 5% /var/etcd.   Turn off the NOSPACE alarm.\nkubectl exec -it -n services cray-hbtd-etcd-6p4tc4jdgm -- sh ncn-w001# kubectl exec -it -n services ETCD_CLUSTER_NAME -- sh # ETCDCTL_API=3 etcdctl alarm disarm memberID:14039380531903955557 alarm:NOSPACE memberID:10060051157615504224 alarm:NOSPACE memberID:9418794810465807950 alarm:NOSPACE     "
},
{
	"uri": "/docs-csm/en-11/operations/kubernetes/configure_kubectl_credentials_to_access_the_kubernetes_apis/",
	"title": "Configure Kubectl Credentials To Access The Kubernetes Apis",
	"tags": [],
	"description": "",
	"content": "Configure kubectl Credentials to Access the Kubernetes APIs The credentials for kubectl are located in the admin configuration file on all non-compute node (NCN) master and worker nodes. They can be found at /etc/kubernetes/admin.conf for the root user. Use kubectl to access the Kubernetes cluster from a device outside the cluster.\nFor more information, refer to https://kubernetes.io/\nPrerequisites This procedure requires administrative privileges and assumes that the device being used has:\n kubectl is installed Access to the site admin network  Procedure   Access the credentials file used by kubectl at /etc/kubernetes/admin.conf on any one of the master or worker NCNs.\nIf copying this file to another system, be sure to set the environmental variable KUBECONFIG to the new location on that system.\n  Verify access by executing the following command:\nncn# kubectl get nodes If the command was successful, the system will return output similar to the following:\nNAME STATUS ROLES AGE VERSION ncn-m001 Ready master 23h v1.14.3 ncn-m002 Ready master 23h v1.14.3 ncn-m003 Ready master 23h v1.14.3 ncn-w001 Ready \u0026lt;none\u0026gt; 23h v1.14.3 ncn-w002 Ready \u0026lt;none\u0026gt; 23h v1.14.3 ncn-w003 Ready \u0026lt;none\u0026gt; 23h v1.14.3 The information above is only an example and may appear differently than it is shown above.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/kubernetes/about_etcd/",
	"title": "About Etcd",
	"tags": [],
	"description": "",
	"content": "About etcd The system uses etcd for storing all of its cluster data. It is an open source database that is excellent for maintaining the state of Kubernetes. Failures in the etcd cluster at the heart of Kubernetes will cause a failure of Kubernetes. To mitigate this risk, the system is deployed with etcd on dedicated disks and with a specific configuration to optimize Kubernetes workloads. The system also provides additional etcd cluster(s) as necessary to help maintain an operational state of services. These additional clusters are managed by a Kubernetes operator and do not interact with the core Kubernetes etcd service.\nTo learn more about etcd, refer to the following links:\n General documentation - https://github.com/etcd-io/etcd README - https://github.com/etcd-io/etcd/tree/master/etcdctl etcd upstream performance - https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/performance.md#benchmarks  Usage of etcd on the System Communication between etcd machines is handled via the Raft consensus algorithm. Latency from the etcd leader is the most important metric to track because severe latency will introduce instability within the cluster. Raft is only as fast as the slowest machine in the majority. This problem can be mitigated by properly tuning the cluster.\netcd is a highly available key value store that runs on the three non-compute nodes (NCNs) that act as Kubernetes worker nodes. The three node cluster size deployment is used to meet the minimum requirements for resiliency. Scaling to more nodes will provide more resiliency, but it will not provide more speed. For example, one write to the cluster is actually three writes, so one to each instance. Scaling to five or more instances in a cluster would mean that one write will actually equal five writes to the cluster.\nThe system utilizes etcd in two major ways:\n etcd running on bare-metal with a dedicated disk partition  Supports only Kubernetes Includes a dedicated partition to provide the best throughput and scalability  Enables the Kubernetes services to be scaled, as well as the physical nodes running those services   Run on the Kubernetes master nodes and will not relocate  Handles replication and instance re-election in the event of a node failure   Backed up to a Ceph Rados Gateway (S3 compatible) bucket   etcd running via a Kubernetes operator  Services utilize this to deploy an etcd cluster on the worker nodes The etcd pods are mobile and will relocate in the event of a pod or node failure Each etcd cluster can be backed up to a Ceph Rados Gateway (S3 compatible) bucket  This option is decided by the service owner or developer as some information has an extremely short lifespan, and by the time the restore could be performed, the data would be invalid      "
},
{
	"uri": "/docs-csm/en-11/operations/kubernetes/about_kubectl/",
	"title": "About Kubectl",
	"tags": [],
	"description": "",
	"content": "About kubectl kubectl is a CLI that can be used to run commands against a Kubernetes cluster. The format of the kubectl command is shown below:\nncn# kubectl COMMAND RESOURCE_TYPE RESOURCE_NAME FLAGS An example of using kubectl to retrieve information about a pod is shown below:\nncn# kubectl get pod POD_NAME1 POD_NAME2 kubectl is installed by default on the non-compute node (NCN) image. To learn more about kubectl, refer to https://kubernetes.io/docs\n"
},
{
	"uri": "/docs-csm/en-11/operations/kubernetes/about_postgres/",
	"title": "About Postgres",
	"tags": [],
	"description": "",
	"content": "About Postgres The system uses PostgreSQL (known as Postgres) as a database solution. Postgres databases use SQL language to store and manage databases on the system.\nTo learn more about Postgres, see https://www.postgresql.org/docs/.\nThe Patroni tool can be used to manage and maintain information in a Postgres database. It handles tasks such as listing cluster members and the replication status, configuring and restarting databases, and more. For more information about this tool, refer to Troubleshoot Postgres Database.\n"
},
{
	"uri": "/docs-csm/en-11/operations/kubernetes/backups_for_etcd-operator_clusters/",
	"title": "Backups For Etcd-operator Clusters",
	"tags": [],
	"description": "",
	"content": "Backups for etcd-operator Clusters Backups are periodically created for etcd clusters. These backups are stored in the Ceph Rados Gateway (S3). Not all services are backed up automatically. Services that are not backed up automatically will need to be manually rediscovered if the cluster is unhealthy.\nClusters with Automated Backups The following services are backed up (daily, one week\u0026rsquo;s worth of backups retained) as part of the automated solution:\n Boot Orchestration Service (BOS) Boot Script Service (BSS) Compute Rolling Upgrade Service (CRUS) External DNS Firmware Action Service (FAS)  Run the following command on any master node (ncn-mXXX) or the first worker node (ncn-w001) to list the backups for a specific project. In the example below, the backups for BSS are listed.\nncn-w001# kubectl exec -it -n operators $(kubectl get pod -n operators \\ | grep etcd-backup-restore | head -1 | awk \u0026#39;{print $1}\u0026#39;) -c boto3 -- list_backups cray-bss cray-bss/etcd.backup_v1450_2020-01-30-20:44:41 cray-bss/etcd.backup_v4183_2020-02-01-20:45:48 cray-bss/etcd.backup_v5771_2020-02-02-20:45:48 cray-bss/etcd.backup_v7210_2020-02-03-20:45:48 To view all available backups across all projects:\nncn-w001# kubectl exec -it -n operators $(kubectl get pod -n operators \\ | grep etcd-backup-restore | head -1 | awk \u0026#39;{print $1}\u0026#39;) -c boto3 -- list_backups \u0026#34; bare-metal/etcd-backup-2020-02-03-14-40-07.tar.gz bare-metal/etcd-backup-2020-02-03-14-50-03.tar.gz bare-metal/etcd-backup-2020-02-03-15-00-10.tar.gz bare-metal/etcd-backup-2020-02-03-15-10-06.tar.gz bare-metal/etcd-backup-2020-02-03-15-30-05.tar.gz bare-metal/etcd-backup-2020-02-03-15-40-01.tar.gz bare-metal/etcd-backup-2020-02-03-15-50-08.tar.gz cray-bos/etcd.backup_v1200_2020-02-03-20:45:48 cray-bos/etcd.backup_v240_2020-01-30-20:44:34 cray-bos/etcd.backup_v480_2020-01-31-20:44:34 cray-bos/etcd.backup_v720_2020-02-01-20:45:48 cray-bos/etcd.backup_v960_2020-02-02-20:45:48 cray-bss/etcd.backup_v1450_2020-01-30-20:44:41 cray-bss/etcd.backup_v4183_2020-02-01-20:45:48 cray-bss/etcd.backup_v5771_2020-02-02-20:45:48 cray-bss/etcd.backup_v7210_2020-02-03-20:45:48 cray-crus/etcd.backup_v1_2020-01-30-20:44:48 cray-crus/etcd.backup_v1_2020-02-01-20:45:48 cray-crus/etcd.backup_v1_2020-02-02-20:45:48 cray-crus/etcd.backup_v1_2020-02-03-20:45:48 cray-externaldns/etcd.backup_v2_2020-01-30-20:44:55 cray-externaldns/etcd.backup_v2_2020-02-01-20:45:48 cray-externaldns/etcd.backup_v2_2020-02-02-20:45:48 cray-fas/etcd.backup_v60303_2020-02-03-20:45:48 cray-fas/etcd.backup_v63195_2020-01-30-20:44:55 cray-fas/etcd.backup_v66092_2020-02-01-20:45:48 cray-fas/etcd.backup_v68972_2020-02-02-20:45:48 cray-fas/etcd.backup_v71858_2020-02-03-20:45:48 The returned output includes the date and time of the latest backup for each service. If a recent backup for any service is not included, it is an indication that the service is not backed up automatically. Create a manual backup for that service by following the Create a Manual Backup of a Healthy etcd Cluster procedure.\nClusters without Automated Backups The following projects are not backed up as part of the automated solution:\n Heartbeat Tracking Daemon (HBTD) HMS Notification Fanout Daemon (HMNFD) River Endpoint Discovery Service (REDS) User Access Service (UAS) Manager Content Projection Service (CPS)  If these clusters become unhealthy, the process for rediscovering their data should be followed. See Repopulate Data in etcd Clusters When Rebuilding Them.\n"
},
{
	"uri": "/docs-csm/en-11/operations/kubernetes/cert_renewal_for_kubernetes_and_bare_metal_etcd/",
	"title": "Kubernetes And Bare Metal Etcd Certificate Renewal",
	"tags": [],
	"description": "",
	"content": "Kubernetes and Bare Metal EtcD Certificate Renewal Scope As part of the installation, Kubernetes generates certificates for the required subcomponents. This document will help walk through the process of renewing the certificates.\nIMPORTANT: Depending on the version of Kubernetes, the command may or may not reside under the alpha category. Use kubectl certs --help and kubectl alpha certs --help to determine this. The overall command syntax should be the same and this is just whether or not the command structure will require alpha in it.\nIMPORTANT: When you pick your master node to renew the certs on, then that is the node that will be referenced in this document as ncn-m.\nIMPORTANT: This document is based off a base hardware configuration of 3 masters and 3 workers (We are leaving off utility storage since they are not running Kubernetes). Please make sure to update any commands that run on multiple nodes accordingly.\nFile locations IMPORTANT: Master nodes will have certificates for both Kubernetes services and the Kubernetes client. Workers will only have the certificates for the Kubernetes client.\nServices (master nodes):\n/etc/kubernetes/pki/apiserver.crt /etc/kubernetes/pki/apiserver-etcd-client.crt /etc/kubernetes/pki/apiserver-etcd-client.key /etc/kubernetes/pki/apiserver.key /etc/kubernetes/pki/apiserver-kubelet-client.crt /etc/kubernetes/pki/apiserver-kubelet-client.key /etc/kubernetes/pki/ca.crt /etc/kubernetes/pki/ca.key /etc/kubernetes/pki/front-proxy-ca.crt /etc/kubernetes/pki/front-proxy-ca.key /etc/kubernetes/pki/front-proxy-client.crt /etc/kubernetes/pki/front-proxy-client.key /etc/kubernetes/pki/sa.key /etc/kubernetes/pki/sa.pub /etc/kubernetes/pki/etcd/ca.crt /etc/kubernetes/pki/etcd/ca.key /etc/kubernetes/pki/etcd/healthcheck-client.crt /etc/kubernetes/pki/etcd/healthcheck-client.key /etc/kubernetes/pki/etcd/peer.crt /etc/kubernetes/pki/etcd/peer.key /etc/kubernetes/pki/etcd/server.crt /etc/kubernetes/pki/etcd/server.key Client (master and worker nodes):\n/var/lib/kubelet/pki/kubelet-client-2021-09-07-17-06-36.pem /var/lib/kubelet/pki/kubelet-client-current.pem /var/lib/kubelet/pki/kubelet.crt /var/lib/kubelet/pki/kubelet.key Procedure Check the expiration of the certificates.\n  Log into a master node and run the following:\nncn-m# kubeadm alpha certs check-expiration --config /etc/kubernetes/kubeadmcfg.yaml WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] CERTIFICATE EXPIRES RESIDUAL TIME CERTIFICATE AUTHORITY EXTERNALLY MANAGED admin.conf Sep 24, 2021 15:21 UTC 14d no apiserver Sep 24, 2021 15:21 UTC 14d ca no apiserver-etcd-client Sep 24, 2021 15:20 UTC 14d ca no apiserver-kubelet-client Sep 24, 2021 15:21 UTC 14d ca no controller-manager.conf Sep 24, 2021 15:21 UTC 14d no etcd-healthcheck-client Sep 24, 2021 15:19 UTC 14d etcd-ca no etcd-peer Sep 24, 2021 15:19 UTC 14d etcd-ca no etcd-server Sep 24, 2021 15:19 UTC 14d etcd-ca no front-proxy-client Sep 24, 2021 15:21 UTC 14d front-proxy-ca no scheduler.conf Sep 24, 2021 15:21 UTC 14d no CERTIFICATE AUTHORITY EXPIRES RESIDUAL TIME EXTERNALLY MANAGED ca Sep 02, 2030 15:21 UTC 8y no etcd-ca Sep 02, 2030 15:19 UTC 8y no front-proxy-ca Sep 02, 2030 15:21 UTC 8y no   Backing up existing certificates   Backup existing certificates.\nMaster Nodes:\nncn-m# pdsh -w ncn-m00[1-3] tar cvf /root/cert_backup.tar /etc/kubernetes/pki/ /var/lib/kubelet/pki/ ncn-m001: tar: Removing leading / from member names ncn-m001: /etc/kubernetes/pki/ ncn-m001: /etc/kubernetes/pki/front-proxy-client.key ncn-m001: tar: Removing leading / from hard link targets ncn-m001: /etc/kubernetes/pki/apiserver-etcd-client.key ncn-m001: /etc/kubernetes/pki/sa.key . . .. shortened output Worker Nodes:\nIMPORTANT: The range of nodes below should reflect the size of the environment. This should run on every worker node.\nncn-m# pdsh -w ncn-w00[1-3] tar cvf /root/cert_backup.tar /var/lib/kubelet/pki/ ncn-w003: tar: Removing leading / from member names ncn-w003: /var/lib/kubelet/pki/ ncn-w003: /var/lib/kubelet/pki/kubelet.key ncn-w003: /var/lib/kubelet/pki/kubelet-client-2021-09-07-17-06-36.pem ncn-w003: /var/lib/kubelet/pki/kubelet.crt . . .. shortened output   Renewing Certificates On each master node   Renew the Certificates.\nncn-m# kubeadm alpha certs renew all --config /etc/kubernetes/kubeadmcfg.yaml WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] certificate embedded in the kubeconfig file for the admin to use and for kubeadm itself renewed certificate for serving the Kubernetes API renewed certificate the apiserver uses to access etcd renewed certificate for the API server to connect to kubelet renewed certificate embedded in the kubeconfig file for the controller manager to use renewed certificate for liveness probes to healthcheck etcd renewed certificate for etcd nodes to communicate with each other renewed certificate for serving etcd renewed certificate for the front proxy client renewed certificate embedded in the kubeconfig file for the scheduler manager to use renewed   Check the new expiration.\nncn-m# kubeadm alpha certs check-expiration --config /etc/kubernetes/kubeadmcfg.yaml WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] CERTIFICATE EXPIRES RESIDUAL TIME CERTIFICATE AUTHORITY EXTERNALLY MANAGED admin.conf Sep 22, 2022 17:13 UTC 364d no apiserver Sep 22, 2022 17:13 UTC 364d ca no apiserver-etcd-client Sep 22, 2022 17:13 UTC 364d etcd-ca no apiserver-kubelet-client Sep 22, 2022 17:13 UTC 364d ca no controller-manager.conf Sep 22, 2022 17:13 UTC 364d no etcd-healthcheck-client Sep 22, 2022 17:13 UTC 364d etcd-ca no etcd-peer Sep 22, 2022 17:13 UTC 364d etcd-ca no etcd-server Sep 22, 2022 17:13 UTC 364d etcd-ca no front-proxy-client Sep 22, 2022 17:13 UTC 364d front-proxy-ca no scheduler.conf Sep 22, 2022 17:13 UTC 364d no CERTIFICATE AUTHORITY EXPIRES RESIDUAL TIME EXTERNALLY MANAGED ca Sep 02, 2030 15:21 UTC 8y no etcd-ca Sep 02, 2030 15:19 UTC 8y no front-proxy-ca Sep 02, 2030 15:21 UTC 8y no   This command may have only updated some certificates.\nncn-m# ncn-m001:~ # ls -l /etc/kubernetes/pki -rw-r--r-- 1 root root 1249 Sep 22 17:13 apiserver.crt -rw-r--r-- 1 root root 1090 Sep 22 17:13 apiserver-etcd-client.crt -rw------- 1 root root 1675 Sep 22 17:13 apiserver-etcd-client.key -rw------- 1 root root 1679 Sep 22 17:13 apiserver.key -rw-r--r-- 1 root root 1099 Sep 22 17:13 apiserver-kubelet-client.crt -rw------- 1 root root 1679 Sep 22 17:13 apiserver-kubelet-client.key -rw------- 1 root root 1025 Sep 21 20:50 ca.crt -rw------- 1 root root 1679 Sep 21 20:50 ca.key drwxr-xr-x 2 root root 162 Sep 21 20:50 etcd -rw------- 1 root root 1038 Sep 21 20:50 front-proxy-ca.crt -rw------- 1 root root 1679 Sep 21 20:50 front-proxy-ca.key -rw-r--r-- 1 root root 1058 Sep 22 17:13 front-proxy-client.crt -rw------- 1 root root 1675 Sep 22 17:13 front-proxy-client.key -rw------- 1 root root 1675 Sep 21 20:50 sa.key -rw------- 1 root root 451 Sep 21 20:50 sa.pub ncn-m# ls -l /etc/kubernetes/pki/etcd -rw-r--r-- 1 root root 1017 Sep 21 20:50 ca.crt -rw-r--r-- 1 root root 1675 Sep 21 20:50 ca.key -rw-r--r-- 1 root root 1094 Sep 22 17:13 healthcheck-client.crt -rw------- 1 root root 1679 Sep 22 17:13 healthcheck-client.key -rw-r--r-- 1 root root 1139 Sep 22 17:13 peer.crt -rw------- 1 root root 1679 Sep 22 17:13 peer.key -rw-r--r-- 1 root root 1139 Sep 22 17:13 server.crt -rw------- 1 root root 1675 Sep 22 17:13 server.key As we can see not all the certificate files were updated.\nIMPORTANT: Some certificates were not updated because they have a distant expiration time and did not need to be updated. This is expected.\nCertificates most likely to not be updated due to a distant expiration:\nCERTIFICATE AUTHORITY EXPIRES RESIDUAL TIME EXTERNALLY MANAGED ca Sep 02, 2030 15:21 UTC 8y no etcd-ca Sep 02, 2030 15:19 UTC 8y no front-proxy-ca Sep 02, 2030 15:21 UTC 8y no This means we can ignore the fact that our ca.crt/key, front-proxy-ca.crt/key, and etcd ca.crt/key were not updated.\n  Check the expiration of the certificates files that do not have a current date and are of the .crt or .pem format. See File Locations for the list of files.\nThis task is for each master node and the below example checks each certificate in File Locations.\nfor i in $(ls /etc/kubernetes/pki/*.crt;ls /etc/kubernetes/pki/etcd/*.crt;ls /var/lib/kubelet/pki/*.crt;ls /var/lib/kubelet/pki/*.pem);do echo ${i}; openssl x509 -enddate -noout -in ${i};done /etc/kubernetes/pki/apiserver.crt notAfter=Sep 22 17:13:28 2022 GMT /etc/kubernetes/pki/apiserver-etcd-client.crt notAfter=Sep 22 17:13:28 2022 GMT /etc/kubernetes/pki/apiserver-kubelet-client.crt notAfter=Sep 22 17:13:28 2022 GMT /etc/kubernetes/pki/ca.crt notAfter=Sep 4 09:31:10 2031 GMT /etc/kubernetes/pki/front-proxy-ca.crt notAfter=Sep 4 09:31:11 2031 GMT /etc/kubernetes/pki/front-proxy-client.crt notAfter=Sep 22 17:13:29 2022 GMT /etc/kubernetes/pki/etcd/ca.crt notAfter=Sep 4 09:30:28 2031 GMT /etc/kubernetes/pki/etcd/healthcheck-client.crt notAfter=Sep 22 17:13:29 2022 GMT /etc/kubernetes/pki/etcd/peer.crt notAfter=Sep 22 17:13:29 2022 GMT /etc/kubernetes/pki/etcd/server.crt notAfter=Sep 22 17:13:29 2022 GMT /var/lib/kubelet/pki/kubelet.crt notAfter=Sep 21 19:50:16 2022 GMT /var/lib/kubelet/pki/kubelet-client-2021-09-07-17-06-36.pem notAfter=Sep 4 17:01:38 2022 GMT /var/lib/kubelet/pki/kubelet-client-current.pem notAfter=Sep 4 17:01:38 2022 GMT IMPORTANT: DO NOT forget to verify certificates in /etc/kubernetes/pki/etcd.\n As noted in our above output all certificates including those for etcd were updated. Please note apiserver-etcd-client.crt is a Kubernetes api cert not an etcd only cert. Also the /var/lib/kubelet/pki/ certificates will be updated in the Kubernetes client section that follows.    Restart etcd.\nOnce the steps to renew the needed certs have been completed on all the master nodes, then log into each master node one at a time and do:\nncn-m# systemctl restart etcd.service   On master and worker nodes   Restart kubelet.\nOn each Kubernetes node do:\nIMPORTANT: The below example will need to be adjusted to reflect the correct amount of master and worker nodes in your environment.\nncn-m# pdsh -w ncn-m00[1-3] -w ncn-w00[1-3] systemctl restart kubelet.service   Fix kubectl command access.\nNOTE: Only if your certificates have expired will the following command respond with Unauthorized. In any case, the new client certificates will need to be distributed in the following steps.\nncn-m# kubectl get nodes error: You must be logged in to the server (Unauthorized) ncn-m# cp /etc/kubernetes/admin.conf /root/.kube/config ncn-m# # kubectl get nodes NAME STATUS ROLES AGE VERSION ncn-m001 Ready master 370d v1.18.6 ncn-m002 Ready master 370d v1.18.6 ncn-m003 Ready master 370d v1.18.6 ncn-w001 Ready \u0026lt;none\u0026gt; 370d v1.18.6 ncn-w002 Ready \u0026lt;none\u0026gt; 370d v1.18.6 ncn-w003 Ready \u0026lt;none\u0026gt; 370d v1.18.6   Distribute the client certificate to the rest of the cluster.\nNOTE: You may have errors copying files. The target may or may not exist depending on the version of Shasta.\n You DO NOT need to copy this to the master node where you are performing this work. Copy /etc/kubernetes/admin.conf to all master and worker nodes.  Client access:\nNOTE: Please update the below command with the appropriate amount of worker nodes.\nncn-m# pdcp -w ncn-m00[2-3] -w ncn-w00[1-3] /etc/kubernetes/admin.conf /etc/kubernetes/   Regenerating kubelet .pem certificates   Backup certificates for kubelet on each master and worker node:\nIMPORTANT: The below example will need to be adjusted to reflect the correct amount of master and worker nodes in your environment.\nncn-m# pdsh -w ncn-m00[1-3] -w ncn-w00[1-3] tar cvf /root/kubelet_certs.tar /etc/kubernetes/kubelet.conf /var/lib/kubelet/pki/   On the master node where you updated the other certificates do:\nGet your current apiserver-advertise-address.\nncn# kubectl config view|grep server server: https://10.252.120.2:6442 Using the IP address from the above output do:\n The apiserver-advertise-address may vary, so make sure you are not copy and pasting without verifying.  ncn-m# for node in $(kubectl get nodes -o json|jq -r \u0026#39;.items[].metadata.name\u0026#39;); do kubeadm alpha kubeconfig user --org system:nodes --client-name system:node:$node --apiserver-advertise-address 10.252.120.2 --apiserver-bind-port 6442 \u0026gt; /root/$node.kubelet.conf; done This will generate a new kubelet.conf file in the /root/ directory. There should be a new file per node running Kubernetes.\n  Copy each file to the corresponding node shown in the filename.\nNOTE: Please update the below command with the appropriate amount of master and worker nodes.\nncn-m# for node in ncn-m00{1..3} ncn-w00{1..3}; do scp /root/$node.kubelet.conf $node:/etc/kubernetes/; done   Log into each node one at a time and do the following.\n systemctl stop kubelet.service rm /etc/kubernetes/kubelet.conf rm /var/lib/kubelet/pki/* cp /etc/kubernetes/\u0026lt;node\u0026gt;.kubelet.conf /etc/kubernetes/kubelet.conf systemctl start kubelet.service kubeadm init phase kubelet-finalize all \u0026ndash;cert-dir /var/lib/kubelet/pki/    Check the expiration of the kubectl certificates files. See File Locations for the list of files.\nThis task is for each master and worker node. The example checks each kubelet certificate in File Locations.\nfor i in $(ls /var/lib/kubelet/pki/*.crt;ls /var/lib/kubelet/pki/*.pem);do echo ${i}; openssl x509 -enddate -noout -in ${i};done /var/lib/kubelet/pki/kubelet.crt notAfter=Sep 22 17:37:30 2022 GMT /var/lib/kubelet/pki/kubelet-client-2021-09-22-18-37-30.pem notAfter=Sep 22 18:32:30 2022 GMT /var/lib/kubelet/pki/kubelet-client-current.pem notAfter=Sep 22 18:32:30 2022 GMT   Perform a rolling reboot of master nodes.\n Follow the Reboot_NCNs process.  IMPORTANT: Please ensure you are verifying pods are running on the master node that was rebooted before proceeding to the next node.\n  Perform a rolling reboot of worker nodes.\n Follow the Reboot_NCNs process.    "
},
{
	"uri": "/docs-csm/en-11/operations/kubernetes/about_kubernetes_taints_and_labels/",
	"title": "About Kubernetes Taints And Labels",
	"tags": [],
	"description": "",
	"content": "About Kubernetes Taints and Labels Kubernetes labels control node affinity, which is the property of pods that attracts them to a set of nodes. On the other hand, Kubernetes taints enable a node to repel a set of pods. In addition, pods can have tolerances for taints to allow them to run on nodes with certain taints.\nTaints are controlled with the kubectl taint nodes command, while node labels for various nodes can be customized with a configmap that contains the desired values. For a description of how to modify the default node labels, refer to the Customer Access Network (CAN) documentation.\nThe list of existing labels can be retrieved using the following command:\nncn# kubectl get nodes --show-labels To learn more, refer to https://kubernetes.io/.\n"
},
{
	"uri": "/docs-csm/en-11/operations/",
	"title": "Cray System Management Administration Guide",
	"tags": [],
	"description": "",
	"content": "Cray System Management (CSM) Administration Guide The Cray System Management (CSM) operational activities are administrative procedures required to operate an HPE Cray EX system with CSM software installed.\nThe following administrative topics can be found in this guide:\n Cray System Management (CSM) Administration Guide  CSM Product Management Image Management Boot Orchestration System Power Off Procedures System Power On Procedures Power Management Artifact Management Compute Rolling Upgrades Configuration Management Kubernetes Package Repository Management Security and Authentication Resiliency ConMan Utility Storage System Management Health System Layout Service (SLS) System Configuration Service Hardware State Manager (HSM) Hardware Management (HM) Collector Node Management River Endpoint Discovery Service (REDS) Network  Management Network Customer Access Network (CAN) Dynamic Host Configuration Protocol (DHCP) Domain Name Service (DNS) External DNS MetalLB in BGP-Mode   Update Firmware with FAS User Access Service (UAS)    CSM Product Management  Validate CSM Health Configure Keycloak Account Configure the Cray Command Line Interface (cray CLI) Change Passwords and Credentials Configure Non-Compute Nodes with CFS Perform NCN Personalization Access the LiveCD USB Device After Reboot Post-Install Customizations Validate Signed RPMs  Image Management Build and customize image recipes with the Image Management Service (IMS).\n Image Management Image Management Workflows Upload and Register an Image Recipe Build a New UAN Image Using the Default Recipe Build an Image Using IMS REST Service Customize an Image Root Using IMS  Create UAN Boot Images Convert TGZ Archives to SquashFS Images   Delete or Recover Deleted IMS Content  Boot Orchestration Use the Boot Orchestration Service (BOS) to boot, configure, and shut down collections of nodes.\n Boot Orchestration Service (BOS) BOS Workflows BOS Session Templates  Manage a Session Template Create a Session Template to Boot Compute Nodes with CPS Boot UANs   BOS Sessions  Manage a BOS Session View the Status of a BOS Session Limit the Scope of a BOS Session Configure the BOS Timeout When Booting Compute Nodes Kernel Boot Parameters Check the Progress of BOS Session Operations Clean Up Logs After a BOA Kubernetes Job Clean Up After a BOS/BOA Job is Completed or Cancelled Troubleshoot UAN Boot Issues Troubleshoot Booting Nodes with Hardware Issues   BOS Limitations for Gigabyte BMC Hardware Stage Changes without BOS Compute Node Boot Sequence  Healthy Compute Node Boot Process Node Boot Root Cause Analysis  Compute Node Boot Issue Symptom: Duplicate Address Warnings and Declined DHCP Offers in Logs Compute Node Boot Issue Symptom: Node is Not Able to Download the Required Artifacts Compute Node Boot Issue Symptom: Message About Invalid EEPROM Checksum in Node Console or Log Boot Issue Symptom: Node HSN Interface Does Not Appear or Show Detected Links Detected Compute Node Boot Issue Symptom: Node Console or Logs Indicate that the Server Response has Timed Out Tools for Resolving Compute Node Boot Issues Troubleshoot Compute Node Boot Issues Related to Unified Extensible Firmware Interface (UEFI) Troubleshoot Compute Node Boot Issues Related to Dynamic Host Configuration Protocol (DHCP) Troubleshoot Compute Node Boot Issues Related to the Boot Script Service Troubleshoot Compute Node Boot Issues Related to Trivial File Transfer Protocol (TFTP) Troubleshoot Compute Node Boot Issues Using Kubernetes Log File Locations and Ports Used in Compute Node Boot Troubleshooting Troubleshoot Compute Node Boot Issues Related to Slow Boot Times   Edit the iPXE Embedded Boot Script Redeploy the iPXE and TFTP Services Upload Node Boot Information to Boot Script Service (BSS)    System Power Off Procedures Procedures required for a full power off of an HPE Cray EX system.\n System Power Off Procedures Prepare the System for Power Off Shut Down and Power Off Compute and User Access Nodes Save Management Network Switch Configuration Settings Power Off Compute and IO Cabinets Shut Down and Power Off the Management Kubernetes Cluster Power Off the External Lustre File System  System Power On Procedures Procedures required for a full power on of an HPE Cray EX system.\n System Power On Procedures Power On and Start the Management Kubernetes Cluster Power On the External Lustre File System Power On Compute and IO Cabinets Bring Up the Slingshot Fabric Power On and Boot Compute and User Access Nodes Recover from a Liquid Cooled Cabinet EPO Event  Power Management HPE Cray System Management (CSM) software manages and controls power out-of-band through Redfish APIs.\n Power Management Cray Advanced Platform Monitoring and Control (CAPMC) Liquid Cooled Node Power Management  User Access to Compute Node Power Data   Standard Rack Node Power Management Ignore Nodes with CAPMC Set the Turbo Boost Limit  Artifact Management Use the Ceph Object Gateway Simple Storage Service (S3) API to manage artifacts on the system.\n Artifact Management Manage Artifacts with the Cray CLI Use S3 Libraries and Clients Generate Temporary S3 Credentials  Compute Rolling Upgrades Upgrade sets of compute nodes with the Compute Rolling Upgrade Service (CRUS) without requiring an entire set of nodes to be out of service at once. CRUS enables administrators to limit the impact on production caused from upgrading compute nodes by working through one step of the upgrade process at a time.\n Compute Rolling Upgrade Service (CRUS) CRUS Workflow Upgrade Compute Nodes with CRUS Troubleshoot Nodes Failing to Upgrade in a CRUS Session Troubleshoot a Failed CRUS Session Because of Unmet Conditions Troubleshoot a Failed CRUS Session Because of Bad Parameters  Configuration Management The Configuration Framework Service (CFS) is available on systems for remote execution and configuration management of nodes and boot images.\n Configuration Management Configuration Layers  Create a CFS Configuration Update a CFS Configuration   Ansible Inventory  Manage Multiple Inventories in a Single Location   Configuration Sessions  Create a CFS Session with Dynamic Inventory Create an Image Customization CFS Session Set Limits for a Configuration Session Use a Specific Inventory for a Configuration Session Change the Ansible Verbosity Logs Set the ansible.cfg for a Session Delete CFS Sessions Automatic Session Deletion with sessionTTL Track the Status of a Session View Configuration Session Logs Troubleshoot Ansible Play Failures in CFS Sessions Troubleshoot CFS Session Failing to Complete   Configuration Management with the CFS Batcher Configuration Management of System Components Ansible Execution Environments  Use a Custom ansible-cfg File Enable Ansible Profiling   CFS Global Options Version Control Service (VCS)  Git Operations VCS Branching Strategy Customize Configuration Values Update the Privacy Settings for Gitea Configuration Content Repositories Create and Populate a VCS Configuration Repository   Write Ansible Code for CFS  Target Ansible Tasks for Image Customization   Ansible Inventory  Manage Multiple Inventories in a Single Location   Configuration Sessions  Create a CFS Session with Dynamic Inventory Create an Image Customization CFS Session Set Limits for a Configuration Session Change the Ansible Verbosity Logs Set the ansible.cfg for a Session Delete CFS Sessions Automatic Session Deletion with sessionTTL Track the Status of a Session View Configuration Session Logs Troubleshoot Ansible Play Failures in CFS Sessions Troubleshoot CFS Session Failing to Complete   Configuration Management with the CFS Batcher Configuration Management of System Components Ansible Execution Environments  Use a Custom ansible-cfg File Enable Ansible Profiling   CFS Global Options Version Control Service (VCS)  Git Operations VCS Branching Strategy Customize Configuration Values Update the Privacy Settings for Gitea Configuration Content Repositories Create and Populate a VCS Configuration Repository   Write Ansible Code for CFS * Target Ansible Tasks for Image Customization  Kubernetes The system management components are broken down into a series of micro-services. Each service is independently deployable, fine-grained, and uses lightweight protocols. As a result, the system\u0026rsquo;s micro-services are modular, resilient, and can be updated independently. Services within the Kubernetes architecture communicate via REST APIs.\n Kubernetes Architecture About kubectl  Configure kubectl Credentials to Access the Kubernetes APIs   About Kubernetes Taints and Labels Kubernetes Storage Kubernetes Networking Retrieve Cluster Health Information Using Kubernetes Pod Resource Limits  Determine if Pods are Hitting Resource Limits Increase Pod Resource Limits Increase Kafka Pod Resource Limits   About etcd  Check the Health and Balance of etcd Clusters Rebuild Unhealthy etcd Clusters Backups for etcd-operator Clusters Create a Manual Backup of a Healthy etcd Cluster Restore an etcd Cluster from a Backup Repopulate Data in etcd Clusters When Rebuilding Them Restore Bare-Metal etcd Clusters from an S3 Snapshot Rebalance Healthy etcd Clusters Check for and Clear etcd Cluster Alarms Report the Endpoint Status for etcd Clusters Clear Space in an etcd Cluster Database   About Postgres  Troubleshoot Postgres Database Recover from Postgres WAL Event Restore Postgres Disaster Recovery for Postgres View Postgres Information for System Databases    Package Repository Management Repositories are added to systems to extend the system functionality beyond what is initially delivered. The Sonatype Nexus Repository Manager is the primary method for repository management. Nexus hosts the Yum, Docker, raw, and Helm repositories for software and firmware content.\n Package Repository Management Package Repository Management with Nexus Manage Repositories with Nexus Nexus Configuration Nexus Deployment Restrict Admin Privileges in Nexus Repair Yum Repository Metadata  Security and Authentication Mechanisms used by the system to ensure the security and authentication of internal and external requests.\n System Security and Authentication Manage System Passwords * Update NCN Passwords * Change Root Passwords for Compute Nodes * Change NCN Image Root Password and SSH Keys * Change EX Liquid-Cooled Cabinet Global Default Password * Provisioning a Liquid-Cooled EX Cabinet CEC with Default Credentials * Updating the Liquid-Cooled EX Cabinet Default Credentials after a CEC Password Change SSH Keys Authenticate an Account with the Command Line Default Keycloak Realms, Accounts, and Clients  Certificate Types Change the Keycloak Admin Password Create a Service Account in Keycloak Retrieve the Client Secret for Service Accounts Get a Long-Lived Token for a Service Account Access the Keycloak User Management UI Create Internal User Accounts in the Keycloak Shasta Realm Delete Internal User Accounts in the Keycloak Shasta Realm Create Internal User Groups in the Keycloak Shasta Realm Remove Internal Groups from the Keycloak Shasta Realm Remove the Email Mapper from the LDAP User Federation Re-Sync Keycloak Users to Compute Nodes Keycloak Operations Configure Keycloak for LDAP/AD authentication Configure the RSA Plugin in Keycloak Preserve Username Capitalization for Users Exported from Keycloak Change the LDAP Server IP Address for Existing LDAP Server Content Change the LDAP Server IP Address for New LDAP Server Content Remove the LDAP User Federation from Keycloak Add LDAP User Federation   Public Key Infrastructure (PKI)  PKI Certificate Authority (CA) Make HTTPS Requests from Sources Outside the Management Kubernetes Cluster Transport Layer Security (TLS) for Ingress Services PKI Services HashiCorp Vault Backup and Restore Vault Clusters Troubleshoot Common Vault Cluster Issues   Public Key Infrastructure (PKI)  PKI Certificate Authority (CA) Make HTTPS Requests from Sources Outside the Management Kubernetes Cluster Transport Layer Security (TLS) for Ingress Services PKI Services HashiCorp Vault Backup and Restore Vault Clusters Troubleshoot Common Vault Cluster Issues   Troubleshoot SPIRE Failing to Start on NCNs API Authorization  Resiliency HPE Cray EX systems are designed so that system management services (SMS) are fully resilient and that there is no single point of failure.\n Resiliency Resilience of System Management Services Restore System Functionality if a Kubernetes Worker Node is Down Recreate StatefulSet Pods on Another Node NTP Resiliency  ConMan ConMan is a tool used for connecting to remote consoles and collecting console logs. These node logs can then be used for various administrative purposes, such as troubleshooting node boot issues.\n Access Compute Node Logs Access Console Log Data Via the System Monitoring Framework (SMF) Manage Node Consoles Log in to a Node Using ConMan Establish a Serial Connection to NCNs Disable ConMan After System Software Installation Troubleshoot ConMan Blocking Access to a Node BMC Troubleshoot ConMan Failing to Connect to a Console Troubleshoot ConMan Asking for Password on SSH Connection  Utility Storage Ceph is the utility storage platform that is used to enable pods to store persistent data. It is deployed to provide block, object, and file storage to the management services running on Kubernetes, as well as for telemetry data coming from the compute nodes.\n Utility Storage Collect Information about the Ceph Cluster Manage Ceph Services Adjust Ceph Pool Quotas Add Ceph OSDs Shrink Ceph OSDs Ceph Health States Dump Ceph Crash Data Identify Ceph Latency Issues Cephadm Reference Material Restore Nexus Data After Data Corruption Troubleshoot Failure to Get Ceph Health Troubleshoot a Down OSD Troubleshoot Ceph OSDs Reporting Full Troubleshoot System Clock Skew Troubleshoot an Unresponsive S3 Endpoint Troubleshoot Ceph-Mon Processes Stopping and Exceeding Max Restarts Troubleshoot Pods Failing to Restart on Other Worker Nodes Troubleshoot Large Object Map Objects in Ceph Health Troubleshoot Failure of RGW Health Check  System Management Health Enable system administrators to assess the health of their system. Operators need to quickly and efficiently troubleshoot system issues as they occur and be confident that a lack of issues indicates the system is operating normally.\n System Management Health System Management Health Checks and Alerts Access System Management Health Services Configure Prometheus Email Alert Notifications  System Layout Service (SLS) The System Layout Service (SLS) holds information about the system design, such as the physical locations of network hardware, compute nodes, and cabinets. It also stores information about the network, such as which port on which switch should be connected to each compute node.\n System Layout Service (SLS) Dump SLS Information Load SLS Database with Dump File Add UAN CAN IP Addresses to SLS   Update SLS with UAN Aliases   Create a Backup of the SLS Postgres Database Restore SLS Postgres Database from Backup Restore SLS Postgres without an Existing Backup  System Configuration Service The System Configuration Service (SCSD) allows administrators to set various BMC and controller parameters. These parameters are typically set during discovery, but this tool enables parameters to be set before or after discovery. The operations to change these parameters are available in the Cray CLI under the scsd command.\n System Configuration Service Configure BMC and Controller Parameters with SCSD Manage Parameters with the scsd Service Set BMC Credentials  Hardware State Manager (HSM) Use the Hardware State Manager (HSM) to monitor and interrogate hardware components in the HPE Cray EX system, tracking hardware state and inventory information, and making it available via REST queries and message bus events when changes occur.\n Hardware State Manager (HSM) Hardware Management Services (HMS) Locking API  Lock and Unlock Management Nodes Manage HMS Locks   Component Groups and Partitions  Manage Component Groups Component Group Members Manage Component Partitions Component Partition Members Component Memberships   Hardware State Manager (HSM) State and Flag Fields HSM Roles and Subroles Add an NCN to the HSM Database Add a Switch to the HSM Database Create a Backup of the HSM Postgres Database Restore HSM Postgres from a Backup Restore HSM Postgres without a Backup  Hardware Management (HM) Collector The Hardware Management (HM) Collector is used to collect telemetry and Redfish events from hardware in the system.\n Adjust HM Collector resource limits and requests  Node Management Monitor and manage compute nodes (CNs) and non-compute nodes (NCNs) used in the HPE Cray EX system.\n Node Management Node Management Workflows Rebuild NCNs Reboot NCNs  Check and Set the metalno-wipe Setting on NCNs   Enable Nodes Disable Nodes Find Node Type and Manufacturer Add a Standard Rack Node  Move a Standard Rack Node Move a Standard Rack Node (Same Rack/Same HSN Ports)   Clear Space in Root File System on Worker Nodes Troubleshoot Issues with Redfish Endpoint DiscoveryCheck for Redfish Events from Nodes Reset Credentials on Redfish Devices Access and Update Settings for Replacement NCNs Change Settings for HMS Collector Polling of Air Cooled Nodes Use the Physical KVM Launch a Virtual KVM on Gigabyte Servers Launch a Virtual KVM on Intel Servers Change Java Security Settings Verify Accuracy of the System Clock Configuration of NCN Bonding  Change Interfaces in the Bond Troubleshoot Interfaces with IP Address Issues   Troubleshoot Loss of Console Connections and Logs on Gigabyte Nodes Check the BMC Failover Mode Update Compute Node Mellanox HSN NIC Firmware TLS Certificates for Redfish BMCs  Add TLS Certificates to BMCs   Dump a Non-Compute Node Enable Passwordless Connections to Liquid Cooled Node BMCs  View BIOS Logs for Liquid Cooled Nodes   Configure NTP on NCNs  River Endpoint Discovery Service (REDS) The River Endpoint Discovery Service (REDS) performs geolocation and initialization of compute nodes, based on a mapping file that is provided with each system.\n Configure a Management Switch for REDS Initialize and Geolocate Nodes Verify Node Removal Troubleshoot Common REDS Issues  Troubleshot Common Error Messages in REDS Logs Clear State and Restart REDS    Network Overview of the several different networks supported by the HPE Cray EX system.\n Network Access to System Management Services Default IP Address Ranges Connect to the HPE Cray EX Environment  Management Network HPE Cray EX systems can have network switches in many roles: spine switches, leaf switches, aggregation switches, and CDU switches. Newer systems have HPE Aruba switches, while older systems have Dell and Mellanox switches. Switch IP addresses are generated by Cray Site Init (CSI).\n Management Network Switch Rename Management Network ACL configuration Management Network CAN setup Management Network Flow Control Settings Management Network Access Port configurations Update Management Network Firmware  Customer Access Network (CAN) The Customer Access Network (CAN) provides access from outside the customer network to services, NCNs, and User Access Nodes (UANs) in the system.\n Customer Access Network (CAN) Required Labels if CAN is Not Configured Externally Exposed Services Connect to the CAN CAN with Dual-Spine Configuration Troubleshoot CAN Issues  Dynamic Host Configuration Protocol (DHCP) The DHCP service on the HPE Cray EX system uses the Internet Systems Consortium (ISC) Kea tool. Kea provides more robust management capabilities for DHCP servers.\n DHCP Troubleshoot DHCP Issues  Domain Name Service (DNS) The central DNS infrastructure provides the structural networking hierarchy and datastore for the system.\n DNS Manage the DNS Unbound Resolver Enable ncsd on UANs Troubleshoot Common DNS Issues Troubleshoot PowerDNS  External DNS External DNS, along with the Customer Access Network (CAN), Border Gateway Protocol (BGP), and MetalLB, makes it simpler to access the HPE Cray EX API and system management services. Services are accessible directly from a laptop without needing to tunnel into a non-compute node (NCN) or override /etc/hosts settings.\n External DNS External DNS csi config init Input Values Update the system-name.site-domain Value Post-Installation Update the can-external-dns Value Post-Installation Ingress Routing Add NCNs and UANs to External DNS External DNS Failing to Discover Services Workaround Troubleshoot Connectivity to Services with External IP addresses Troubleshoot DNS Configuration Issues  MetalLB in BGP-Mode MetalLB is a component in Kubernetes that manages access to LoadBalancer services from outside the Kubernetes cluster. There are LoadBalancer services on the Node Management Network (NMN), Hardware Management Network (HMN), and Customer Access Network (CAN).\nMetalLB can run in either Layer2-mode or BGP-mode for each address pool it manages. BGP-mode is used for the NMN, HMN, and CAN. This enables true load balancing (Layer2-mode does failover, not load balancing) and allows for a more robust layer 3 configuration for these networks.\n MetalLB in BGP-Mode MetalLB in BGP-Mode Configuration Check BGP Status and Reset Sessions Update BGP Neighbors Troubleshoot Services without an Allocated IP Address Troubleshoot BGP not Accepting Routes from MetalLB  Update Firmware with FAS The Firmware Action Service (FAS) provides an interface for managing firmware versions of Redfish-enabled hardware in the system. FAS interacts with the Hardware State Managers (HSM), device data, and image data in order to update firmware.\nSee Update Firmware with FAS for a list components that are upgradable with FAS. Refer to the HPC Firmware Pack (HFP) product stream to update firmware on other components.\n Update Firmware with FAS FAS CLI FAS Filters FAS Recipes FAS Admin Procedures FAS Use Cases Upload Olympus BMC Recovery Firmware into TFTP Server  User Access Service (UAS) The User Access Service (UAS) is a containerized service managed by Kubernetes that enables application developers to create and run user applications. Users launch a User Access Instance (UAI) using the cray command. Users can also transfer data between the Cray system and external systems using the UAI.\n User Access Service (UAS) End-User UAIs Special Purpose UAIs Elements of a UAI UAI Host Nodes UAI macvlans Network Attachments UAI Host Node Selection UAI Network Attachments Configure UAIs in UAS  UAI Images  Listing Registered UAI Images Register a UAI Image Retrieve UAI Image Registration Information Update a UAI Image Registration Delete a UAI Image Registration   Volumes  List Volumes Registered in UAS Add a Volume to UAS Obtain Configuration of a UAS Volume Update a UAS Volume Delete a Volume Configuration   Resource Specifications  List UAI Resource Specifications Create a UAI Resource Specification Retrieve Resource Specification Details Update a Resource Specification Delete a UAI Resource Specification   UAI Classes  List Available UAI Classes Create a UAI Class View a UAI Class Modify a UAI Class Delete a UAI Class     UAI Management  List UAIs Creating a UAI Examining a UAI Using a Direct Administrative Command Deleting a UAI   Legacy Mode User-Driven UAI Management  Configure A Default UAI Class for Legacy Mode Create and Use Default UAIs in Legacy Mode List Available UAI Images in Legacy Mode Create UAIs From Specific UAI Images in Legacy Mode   Broker Mode UAI Management  Configure End-User UAI Classes for Broker Mode Configure a Broker UAI class Start a Broker UAI Log in to a Broker UAI   UAI Images  Customize the Broker UAI Image Customize End-User UAI Images   Troubleshoot UAS Issues  Troubleshoot UAS by Viewing Log Output Troubleshoot UAIs by Viewing Log Output Troubleshoot Stale Brokered UAIs Troubleshoot UAI Stuck in \u0026ldquo;ContainerCreating\u0026rdquo; Troubleshoot Duplicate Mount Paths in a UAI Troubleshoot Missing or Incorrect UAI Images Troubleshoot UAIs with Administrative Access Troubleshoot Common Mistakes when Creating a Custom End-User UAI Image    "
},
{
	"uri": "/docs-csm/en-11/operations/image_management/image_management/",
	"title": "Image Management",
	"tags": [],
	"description": "",
	"content": "Image Management The Image Management Service (IMS) uses the open source Kiwi-NG tool to build image roots from compressed Kiwi image descriptions. These compressed Kiwi image descriptions are referred to as \u0026ldquo;recipes.\u0026rdquo; Kiwi-NG builds images based on a variety of different Linux distributions, specifically SUSE, RHEL, and their derivatives. Kiwi image descriptions must follow the Kiwi development schema. More information about the development schema and the Kiwi-NG tool can be found in the documentation: https://doc.opensuse.org/projects/kiwi/doc/.\nEven though Kiwi recipes can be developed from scratch or found on the Internet, Cray suggests that recipes are based on existing Cray image recipes. Cray provides multiple types of recipes including, but not limited to the following:\n Barebones Image Recipes - The barebones recipes contain only the upstream Linux packages needed to successfully boot the image on a Cray compute node using upstream packages. Bare-bones recipes are primarily meant to be used to validate the Cray IMS tools, without requiring HPE Cray Operating System (COS) content. COS Recipes - COS recipes contain a Linux environment with a Cray customized kernel and optimized Cray services for our most demanding customers and workloads.  Cray provided recipes are uploaded to the Simple Storage Service (S3) and registered with IMS as part of the install.\nImages built by IMS contain only the packages and settings that are referenced in the Kiwi-NG recipe used to build the image. The only exception is that IMS will dynamically install the system\u0026rsquo;s root CA certificate to allow zypper (via Kiwi-NG) to talk securely with the required Nexus RPM repositories. Images that are intended to be used to boot a CN or other node must be configured with DNS and other settings that enable the image to talk to vital services. A base level of customization is provided by the default Ansible plays used by the Configuration Framework Service (CFS) to enable DNS resolution, which are typically run against an image after it is built by IMS.\nWhen customizing an image via IMS image customization, once chrooted into the image root (or if using a `jailed` environment), the image will only have access to whatever configuration the image already contains. In order to talk to services, including Nexus RPM repositories, the image root must first be configured with DNS and other settings. A base level of customization is provided by the default Ansible plays used by the CFS to enable DNS resolution.\nThe Nexus Repository Manager service provides local RPM repositories for use when building or customizing an image. The Kiwi image descriptions should reference these repositories as needed. In order to include the custom-repo repository in an IMS Kiwi-NG recipe, the repository source path should be modified to the URI for a repository instance hosted by Nexus.\n\u0026lt;repository type=\u0026quot;rpm-md\u0026quot; alias=\u0026quot;custom-repo\u0026quot; imageinclude=\u0026quot;true\u0026quot;\u0026gt; \u0026lt;source path=\u0026quot;https://packages.local/repository/REPO_NAME\u0026quot; /\u0026gt; \u0026lt;/repository\u0026gt; "
},
{
	"uri": "/docs-csm/en-11/operations/image_management/image_management_workflows/",
	"title": "Image Management Workflows",
	"tags": [],
	"description": "",
	"content": "Image Management Workflows Overview of how to create an image and how to customize and image.\nThe following workflows are intended to be high-level overviews of image management tasks. These workflows depict how services interact with each other during image management and help to provide a quicker and deeper understanding of how the system functions.\nThe workflows in this section include:\n Create a New Image Customize an Image  Create a New Image Use Case: The system administrator creates an image root from a customized recipe. The new image can be used to boot compute nodes.\nComponents: This workflow is based on the interaction of the Image Management Service (IMS) with other services during the image build process. The process of image creation builds an image from a recipe. An administrator may choose to use the Cray-provided recipes or customize Kiwi recipes to define the image to be built.\nMentioned in this workflow:\n Image Management Service (IMS) allows administrators and users to build or customize (pre-boot) images from kiwi-ng recipes. This service is responsible for enabling the creation of bootable and non-bootable images, enabling image customization via an SSH-able environment, and packaging and association of new/customized image artifacts (kernel, rootfs, initrd, etc) with a new IMS image record. Nexus is needed for image creation and image customization. Nexus provides local RPM repositories for use when building or customizing an image. You can define zypper or Yum package repositories and provide the RPM content for installing and updating software for every compute and non-compute node in the system. The Simple Storage Service (Ceph S3) is an artifact repository that stores boot artifacts. Recipes are stored in the ims bucket and images are stored in the boot-images bucket.  Workflow Overview: The following sequence of steps occurs during this workflow.\n  Administrator downloads an existing recipe from S3\nTo create a custom recipe, the administrator can first download an existing recipe from S3. A Kiwi recipe consists of multiple files and directories, which together define the repositories, packages and post-install actions to take during the Kiwi build process.\n  Administrator modifies the recipe and uploads to S3\nModify the recipe by editing the files and subdirectories in the image-recipe directory. Edit the config.xml file to modify the name of the recipe, the set of RPM packages being installed or the RPM repositories being referenced. The recipe should be uploaded to S3 in a .tgz archive via CLI.\n  Administrator registers the recipe with IMS\nRegistering the recipe creates a recipe record for your custom recipe in IMS.\nncn# cray ims recipes list   Administrator uploads public key\nA public key is uploaded to allow them to access SSH shells that IMS provides.\nncn# cray ims public-keys create --name \u0026#34;username public key\u0026#34; --public-key ~/.ssh/id_rsa.pub   Administrator starts the creation job\nCreate a new IMS image by providing request body parameter, job_type=\u0026ldquo;create\u0026rdquo;. The following steps 6-10 happen automatically as a part of the image creation process.\nncn# cray ims jobs create \\ --job-type create \\ --image-root-archive-name cray-sles15-barebones \\ --artifact-id $IMS_RECIPE_ID \\ --public-key-id $IMS_PUBLIC_KEY_ID \\ --enable-debug False   IMS to Ceph S3\nIMS fetches the recipe from S3 and decompresses the recipe to a temporary directory.\n  IMS to Nexus\nIMS waits for repositories to be available from Nexus. The repositories are needed to build the image.\n  IMS creates a custom RPM\nIMS creates a custom RPM to install the CA root certificate from the system into the image. The build-ca-rpm container creates an RPM with the private root-CA certificate for the system and this RPM is installed automatically by Kiwi-NG. The CA root certificate is required to enable secure HTTPS access to the RPM repositories when building the image root.\n  IMS calls Kiwi-NG to build the image\nIMS calls Kiwi-NG to build the image root from the recipe and accesses packages in zypper/yum repositories. The building of the image using kiwi happens in the build-image container. After kiwi is done building the image (either success or fail), the buildenv-sidecar container packages the artifacts, or in the case of failure, enables the debug shell if enable-debug is True. In the buildenv-sidecar container, the image artifacts are packaged and new image artifact records are created for each.\nIf there is a failure and enable-debug is true, a debug SSH shell is established. Admin can inspect image build root. Use commands touch /mnt/image/complete in a non-jailed environment or touch /tmp/complete in a jailed (chroot) environment to exit.\n  Save the new image record in IMS\nThe metadata for the new image artifacts is stored in IMS.\n  Upload the new image artifacts to Ceph S3\nThe new image artifacts are uploaded to Ceph S3.\n  Customize an Image Use Case: The system administrator customizes an existing image and makes desired changes.\nComponents: This workflow is based on the interaction of the Image Management Service (IMS) with Ceph S3 during the image customization process. The customization workflow sets up a temporary image customization environment within a Kubernetes pod and mounts the image to be customized in that environment. A system administrator then makes the desired changes to the image root within the customization environment. IMS then compresses the customized image root and uploads it and its associated initrd image and kernel image (needed to boot a node) to Ceph S3.\nMentioned in this workflow:\n Image Management Service (IMS) allows administrators and users to build or customize (pre-boot) images from kiwi-ng recipes. This service is responsible for enabling the creation of bootable and non-bootable images, enabling image customization via an SSH-able environment, and packaging and association of new/customized image artifacts (kernel, rootfs, initrd, etc) with a new IMS image record. The Simple Storage Service (Ceph S3) is an artifact repository that stores artifacts. Recipes are stored in the ims bucket and images are stored in the boot-images bucket.  Workflow Overview: The following sequence of steps occurs during this workflow.\n  Administrator identifies an existing image to be customized\nRetrieve a list of ImageRecords indicating images that have been registered with IMS. IMS uses the ImageRecord to read the image\u0026rsquo;s manifest.yaml to find the image\u0026rsquo;s root file system (rootfs) artifact. Note the id of the image that you want to customize.\nncn# cray ims images list   Administrator uploads public key\nA public key is uploaded to allow them to access SSH shells that IMS provides.\nncn# cray ims public-keys create --name \u0026#34;username public key\u0026#34; --public-key ~/.ssh/id_rsa.pub   Administrator starts the image customization job\nCreate a new IMS image by providing request body parameter,job_type=\u0026ldquo;customize\u0026rdquo;. The following steps (4-8) happen automatically as a part of the image customization process.\nncn# cray ims jobs create \\ --job-type customize \\ --image-root-archive-name my_customized_image \\ --kernel-file-name vmlinuz \\ --initrd-file-name initrd \\ --artifact-id $IMS_IMAGE_ID \\ --public-key-id $IMS_PUBLIC_KEY_ID \\ --enable-debug False   IMS to Ceph S3\nIMS downloads the image root (rootfs) from Ceph S3 and decompresses the image root to a temporary directory.\n  IMS creates an SSH environment for image customization\nIMS spins up an sshd container so that the administrator can modify the image. Administrator accesses the sshd container and makes changes to the image. For example, it may be necessary to modify the timezone, or modify the programming environment, etc. Use touch /mnt/image/complete in a non-jailed environment or touch /tmp/complete in a jailed (chroot) environment to exit. The shell can be run in either a jailed or non-jailed mode.\nThe output is a new image. Note that the original image also exists. IMS customizes a copy of the original image.\n  Buildenv-sidecar container packages new image artifacts\nThe buildenv-sidecar container waits for the admin to exit the SSH and upon completion new records are created for each image artifact. It also adds the root CA certificate to the image and packages the new image artifacts (kernel, initrd, rootfs).\n  Save the new image record in IMS\nThe metadata for the new image artifacts is stored in IMS.\n  Upload the new image artifacts to Ceph S3\nThe new image artifacts are uploaded to Ceph S3.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/image_management/upload_and_register_an_image_recipe/",
	"title": "Upload And Register An Image Recipe",
	"tags": [],
	"description": "",
	"content": "Upload and Register an Image Recipe Download and expand recipe archives from S3 and IMS. Modify and upload a recipe archive, and then register that recipe archive with IMS.\nPrerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system. System management services (SMS) are running in a Kubernetes cluster on non-compute nodes (NCN) and include the following deployment:  cray-ims, the Image Management Service (IMS)   The NCN Certificate Authority (CA) public key has been properly installed into the CA cache for this system. A token providing Simple Storage Service (S3) credentials has been generated.  Limitations  The commands in this procedure must be run as the root user. The IMS tool currently only supports Kiwi-NG recipe types.  Procedure   Locate the desired recipe to download from S3.\nThere may be multiple records returned. Ensure that the correct record is selected in the returned data.\nncn-m001# cray ims recipes list ... [[results]] id = \u0026#34;76ef564d-47d5-415a-bcef-d6022a416c3c\u0026#34; name = \u0026#34;cray-sles15-barebones\u0026#34; created = \u0026#34;2020-02-05T19:24:22.621448+00:00\u0026#34; [results.link] path = \u0026#34;s3://ims/recipes/76ef564d-47d5-415a-bcef-d6022a416c3c/cray-sles15-barebones.tgz\u0026#34; etag = \u0026#34;28f3d78c8cceca2083d7d3090d96bbb7\u0026#34; type = \u0026#34;s3\u0026#34; ... If successful, create variables for the S3 bucket and key values from the S3 path in the returned data.\nncn-m001# export S3_ARTIFACT_BUCKET=ims ncn-m001# export S3_ARTIFACT_KEY=recipes/76ef564d-47d5-415a-bcef-d6022a416c3c/cray-sles15-barebones.tgz ncn-m001# export ARTIFACT_FILENAME=cray-sles15-barebones.tgz   Download the recipe archive.\nUse the variables created in the previous step when running the following command.\nncn-m001# cray artifacts get $S3_ARTIFACT_BUCKET $S3_ARTIFACT_KEY $ARTIFACT_FILENAME   Expand the recipe with tar.\nncn-m001# mkdir image-recipe ncn-m001# tar xvf $ARTIFACT_FILENAME -C image-recipe   Modify the recipe by editing the files and subdirectories in the image-recipe directory.\nA Kiwi recipe consists of multiple files and directories, which together define the repositories, packages and post-install actions to take during the Kiwi build process.\n Edit the config.xml file to modify the name of the recipe, the set of RPM packages being installed or the RPM repositories being referenced. Kiwi-NG supports multiple ways to modify the post-install configuration of the image root, including several shell scripts (config.sh, images.sh) and the root/overlay directory. To learn how these can be used to add specific configuration to the image root, reference the Kiwi-NG documentation. See https://doc.opensuse.org/projects/kiwi/doc/. Recipes built by IMS are required to reference repositories that are hosted on the NCN by the Nexus.    Locate the directory containing the Kiwi-NG image description files.\nThis step should be done after the recipe has been changed.\nncn-m001# cd image-recipe   Set an environment variable for the name of the file that will contain the archive of the image recipe.\nncn-m001# export ARTIFACT_FILE=my_recipe.tgz   Create a tgz archive of the image recipe.\nncn-m001# tar cvfz ../$ARTIFACT_FILE . ncn-m001# cd ..   Create a new IMS recipe record.\nncn-m001# cray ims recipes create --name \u0026#34;My Recipe\u0026#34; \\ --recipe-type kiwi-ng --linux-distribution sles15 created = \u0026#34;2018-12-04T17:25:52.482514+00:00\u0026#34; id = \u0026#34;2233c82a-5081-4f67-bec4-4b59a60017a6\u0026#34; linux_distribution = \u0026#34;sles15\u0026#34; name = \u0026#34;my_recipe.tgz\u0026#34; recipe_type = \u0026#34;kiwi-ng\u0026#34; If successful, create a variable for the id value in the returned data.\nncn-m001# export IMS_RECIPE_ID=2233c82a-5081-4f67-bec4-4b59a60017a6   Upload the customized recipe to S3.\nIt is suggested as a best practice that the S3 object name start with recipes/ and contain the IMS recipe ID to remove ambiguity.\nncn-m001# cray artifacts create ims recipes/$IMS_RECIPE_ID/$ARTIFACT_FILE $ARTIFACT_FILE   Update the IMS recipe record with the S3 path to the recipe archive.\nncn-m001# cray ims recipes update $IMS_RECIPE_ID \\ --link-type s3 \\ --link-path s3://ims/recipes/$IMS_RECIPE_ID/$ARTIFACT_FILE id = \u0026#34;2233c82a-5081-4f67-bec4-4b59a60017a6\u0026#34; recipe_type = \u0026#34;kiwi-ng\u0026#34; linux_distribution = \u0026#34;sles15\u0026#34; name = \u0026#34;my_recipe.tgz\u0026#34; created = \u0026#34;2020-02-05T19:24:22.621448+00:00\u0026#34; [link] path = \u0026#34;s3://ims/recipes/2233c82a-5081-4f67-bec4-4b59a60017a6/my_recipe.tgz\u0026#34; etag = \u0026#34; type = \u0026#34;s3\u0026#34;   "
},
{
	"uri": "/docs-csm/en-11/operations/image_management/build_an_image_using_ims_rest_service/",
	"title": "Build An Image Using IMS Rest Service",
	"tags": [],
	"description": "",
	"content": "Build an Image Using IMS REST Service Create an image root from an IMS recipe.\nPrerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system. System management services (SMS) are running in a Kubernetes cluster on non-compute nodes (NCN) and include the following deployments:  cray-ims, the Image Management Service (IMS) cray-nexus, the Nexus repository manager service   The NCN Certificate Authority (CA) public key has been properly installed into the CA cache for this system. kubectl is installed locally and configured to point at the SMS Kubernetes cluster. A Kiwi image recipe uploaded as a gzipped tar file and registered with IMS. See Upload and Register an Image Recipe. A token providing Simple Storage Service (S3) credentials has been generated.  Limitations The commands in this procedure must be run as the root user.\nProcedure Prepare to Create the Image\n  Check for an existing IMS public key id.\nSkip this step if it is known that a public key associated with the user account being used was not previously uploaded to the IMS service.\nThe following query may return multiple public key records. The correct one will have a name value including the current username in use.\nncn# cray ims public-keys list ... [[results]] public_key = \u0026#34;ssh-rsa AAAAB3NzaC1yc2EA ... AsVruw1Zeiec2IWt\u0026#34; id = \u0026#34;a252ff6f-c087-4093-a305-122b41824a3e\u0026#34; name = \u0026#34;username public key\u0026#34; created = \u0026#34;2018-11-21T17:19:07.830000+00:00\u0026#34; ... If a public key associated with the username in use is not returned, proceed to the next step. If a public key associated with the username does exist, create a variable for the IMS public key id value in the returned data and then proceed to step 3.\nncn# export IMS_PUBLIC_KEY_ID=a252ff6f-c087-4093-a305-122b41824a3e   Upload the SSH public key to the IMS service.\nSkip this step if an IMS public key record has already been created for the account being used.\nThe IMS debug/configuration shell relies on passwordless SSH. This SSH public key needs to be uploaded to IMS to enable interaction with the image customization environment later in this procedure.\nReplace the username value with the actual username being used on the system when setting the public key name.\nncn# cray ims public-keys create --name \u0026#34;username public key\u0026#34; --public-key ~/.ssh/id\\_rsa.pub public_key = \u0026#34;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCl50gK4l9uupxC2KHxMpTNxPTJbnwEdWy1jst5W5LqJx9fdTrc9uNJ33HAq+WIOhPVGbLm2N4GX1WTUQ4+wVOSmmBBJnlu/l5rmO9lEGT6U8lKG8dA9c7qhguGHy7M7WBgdW/gWA16gwE/u8Qc2fycFERRKmFucL/Er9wA0/Qvz7/U59yO+HOtk5hvEz/AUkvaaoY0IVBfdNBCl59CIdZHxDzgXlXzd9PAlrXZNO8jDD3jyFAOvMMRG7py78zj2NUngvsWYoBcV3FcREZJU529uJ0Au8Vn9DRADyB4QQS2o+fa6hG9i2SzfY8L6vAVvSE7A2ILAsVruw1Zeiec2IWt\u0026#34; id = \u0026#34;a252ff6f-c087-4093-a305-122b41824a3e\u0026#34; name = \u0026#34;username public key\u0026#34; created = \u0026#34;2018-11-21T17:19:07.830000+00:00\u0026#34; If successful, create a variable for the IMS public key id value in the returned data.\nncn# export IMS_PUBLIC_KEY_ID=a252ff6f-c087-4093-a305-122b41824a3e   Get the IMS Recipe to Build\n Locate the IMS recipe needed to build the image.\nncn# cray ims recipes list ... [[results]] id = \u0026#34;2233c82a-5081-4f67-bec4-4b59a60017a6\u0026#34; name = \u0026#34;my_recipe.tgz\u0026#34; created = \u0026#34;2020-02-05T19:24:22.621448+00:00\u0026#34; [results.link] path = \u0026#34;s3://ims/recipes/2233c82a-5081-4f67-bec4-4b59a60017a6/my_recipe.tgz\u0026#34; etag = \u0026#34;28f3d78c8cceca2083d7d3090d96bbb7\u0026#34; type = \u0026#34;s3\u0026#34; ... If successful, create a variable for the IMS recipe id in the returned data.\nncn# export IMS_RECIPE_ID=2233c82a-5081-4f67-bec4-4b59a60017a6   Submit the Kubernetes Image Create Job\n Create an IMS job record and start the image creation job.\nAfter building an image, IMS will automatically upload any build artifacts (root file system, kernel and initrd) to the artifact repository, and associate them with IMS. IMS is not able to dynamically determine the Linux kernel and initrd to look for because the file name for these vary depending upon Linux distribution, Linux version, dracut configuration, and more. Thus, the user must pass the name of the kernel and initrd that IMS will look for in the resultant image root\u0026rsquo;s /boot directory.\nUse the following table to help determine the default kernel and initrd file names to specify when submitting the job to customize an image. These are just default names. Please consult with the site administrator to determine if these names have been changed for a given image or recipe.\n   Recipe Recipe Name Kernel File Name Initrd File Name     SLES 15 SP1 Barebones cray-sles15sp1-barebones vmlinuz initrd   COS cray-shasta-compute-sles15sp1.x86_64-1.4.66 vmlinuz initrd    ncn# cray ims jobs create \\ --job-type create \\ --image-root-archive-name cray-sles15-barebones \\ --artifact-id $IMS_RECIPE_ID \\ --public-key-id $IMS_PUBLIC_KEY_ID \\ --enable-debug False status = \u0026#34;creating\u0026#34; enable_debug = false kernel_file_name = \u0026#34;vmlinuz\u0026#34; artifact_id = \u0026#34;2233c82a-5081-4f67-bec4-4b59a60017a6\u0026#34; build_env_size = 10 job_type = \u0026#34;create\u0026#34; kubernetes_service = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-service\u0026#34; kubernetes_job = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-create\u0026#34; id = \u0026#34;ad5163d2-398d-4e93-94f0-2f439f114fe7\u0026#34; image_root_archive_name = \u0026#34;cray-sles15-barebones\u0026#34; initrd_file_name = \u0026#34;initrd\u0026#34; created = \u0026#34;2018-11-21T18:22:53.409405+00:00\u0026#34; public_key_id = \u0026#34;a252ff6f-c087-4093-a305-122b41824a3e\u0026#34; kubernetes_configmap = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-configmap\u0026#34; If successful, create variables for the IMS job id and kubernetes_job values in the returned data.\nncn# export IMS_JOB_ID=ad5163d2-398d-4e93-94f0-2f439f114fe7 ncn# export IMS_KUBERNETES_JOB=cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-create   Use kubectl and the returned IMS_KUBERNETES_JOB value to describe the image create job.\nncn# kubectl -n ims describe job $IMS_KUBERNETES_JOB Name: ims-myimage-create Namespace: default ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulCreate 4m job-controller Created pod: cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-create-lt69t If successful, create a variable for the pod name that was created above, which will be used in the next step.\nncn# export POD=cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-create-lt69t   Watch the logs from the fetch-recipe, wait-for-repos, build-ca-rpm, build-image, and buildenv-sidecar containers to monitor the image creation process.\nUse kubectl and the returned pod name from the previous step to retrieve this information.\nThe fetch-recipe container is responsible for fetching the recipe archive from S3 and uncompressing the recipe.\nncn# kubectl -n ims logs -f $POD -c fetch-recipe INFO:/scripts/fetch.py:IMS_JOB_ID=ad5163d2-398d-4e93-94f0-2f439f114fe7 INFO:/scripts/fetch.py:Setting job status to \u0026#39;fetching_recipe\u0026#39;. INFO:ims_python_helper:image_set_job_status: {{ims_job_id: ad5163d2-398d-4e93-94f0-2f439f114fe7, job_status: fetching_recipe}} INFO:ims_python_helper:PATCH https://api-gw-service-nmn.local/apis/ims/jobs/ad5163d2-398d-4e93-94f0-2f439f114fe7 status=fetching_recipe INFO:/scripts/fetch.py:Fetching recipe http://rgw.local:8080/ims/recipes/2233c82a-5081-4f67-bec4-4b59a60017a6/my_recipe.tgz?AWSAccessKeyId=GQZKV1HAM80ZFDZJFFS7\u0026amp;Expires=1586891507\u0026amp;Signature=GzRzuTWo3p5CoKHzT2mIuPQXLGM%3D INFO:/scripts/fetch.py:Saving file as \u0026#39;/mnt/recipe/recipe.tgz\u0026#39; INFO:/scripts/fetch.py:Verifying md5sum of the downloaded file. INFO:/scripts/fetch.py:Successfully verified the md5sum of the downloaded file. INFO:/scripts/fetch.py:Uncompressing recipe into /mnt/recipe INFO:/scripts/fetch.py:Deleting compressed recipe /mnt/recipe/recipe.tgz INFO:/scripts/fetch.py:Done The wait-for-repos container will ensure that any HTTP/HTTPS repositories referenced by the Kiwi-NG recipe can be accessed and are available. This helps ensure that the image will be built successfully. If 301 responses are returned instead of 200 responses, that does not indicate an error.\nncn# kubectl -n ims logs -f $POD -c wait-for-repos ... 2019-05-17 09:53:47,381 - INFO - __main__ - Recipe contains the following repos: [\u0026#39;http://api-gw-service-nmn.local/repositories/sle15-Module-Basesystem/\u0026#39;, \u0026#39;http://api-gw-service-nmn.local/repositories/sle15-Product-SLES/\u0026#39;, \u0026#39;http://api-gw-service-nmn.local/repositories/cray-sle15\u0026#39;] 2019-05-17 09:53:47,381 - INFO - __main__ - Attempting to get http://api-gw-service-nmn.local/repositories/sle15-Module-Basesystem/repodata/repomd.xml 2019-05-17 09:53:47,404 - INFO - __main__ - 200 response getting http://api-gw-service-nmn.local/repositories/sle15-Module-Basesystem/repodata/repomd.xml 2019-05-17 09:53:47,404 - INFO - __main__ - Attempting to get http://api-gw-service-nmn.local/repositories/sle15-Product-SLES/repodata/repomd.xml 2019-05-17 09:53:47,431 - INFO - __main__ - 200 response getting http://api-gw-service-nmn.local/repositories/sle15-Product-SLES/repodata/repomd.xml 2019-05-17 09:53:47,431 - INFO - __main__ - Attempting to get http://api-gw-service-nmn.local/repositories/cray-sle15/repodata/repomd.xml 2019-05-17 09:53:47,458 - INFO - __main__ - 200 response getting http://api-gw-service-nmn.local/repositories/cray-sle15/repodata/repomd.xml The build-ca-rpm container creates an RPM with the private root-CA certificate for the system. This RPM is installed automatically by Kiwi-NG to ensure that Kiwi can securely talk to the Nexus repositories when building the image root.\nncn# kubectl -n ims logs -f $POD -c build-ca-rpm cray_ca_cert-1.0.0/ cray_ca_cert-1.0.0/etc/ cray_ca_cert-1.0.0/etc/cray/ cray_ca_cert-1.0.0/etc/cray/ca/ cray_ca_cert-1.0.0/etc/cray/ca/certificate_authority.crt Executing(%prep): /bin/sh -e /var/tmp/rpm-tmp.pgDBLk + umask 022 + cd /root/rpmbuild/BUILD + cd /root/rpmbuild/BUILD + rm -rf cray_ca_cert-1.0.0 + /bin/gzip -dc /root/rpmbuild/SOURCES/cray_ca_cert-1.0.0.tar.gz + /bin/tar -xof - + STATUS=0 + \u0026#39;[\u0026#39; 0 -ne 0 ] + cd cray_ca_cert-1.0.0 + /bin/chmod -Rf a+rX,u+w,g-w,o-w . + exit 0 Executing(%build): /bin/sh -e /var/tmp/rpm-tmp.gILKaJ + umask 022 + cd /root/rpmbuild/BUILD + cd cray_ca_cert-1.0.0 + exit 0 Executing(%install): /bin/sh -e /var/tmp/rpm-tmp.PGhobB + umask 022 + cd /root/rpmbuild/BUILD + cd cray_ca_cert-1.0.0 + install -d /root/rpmbuild/BUILDROOT/cray_ca_cert-1.0.0-1.x86_64/usr/share/pki/trust/anchors + install -m 644 /etc/cray/ca/certificate_authority.crt /root/rpmbuild/BUILDROOT/cray_ca_cert-1.0.0-1.x86_64/usr/share/pki/trust/anchors/cray_certificate_authority.crt + /usr/lib/rpm/brp-compress + /usr/lib/rpm/brp-strip /usr/bin/strip + /usr/lib/rpm/brp-strip-static-archive /usr/bin/strip find: file: No such file or directory + /usr/lib/rpm/brp-strip-comment-note /usr/bin/strip /usr/bin/objdump Processing files: cray_ca_cert-1.0.0-1.x86_64 Provides: cray_ca_cert = 1.0.0-1 cray_ca_cert(x86-64) = 1.0.0-1 Requires(interp): /bin/sh Requires(rpmlib): rpmlib(CompressedFileNames) \u0026lt;= 3.0.4-1 rpmlib(PayloadFilesHavePrefix) \u0026lt;= 4.0-1 Requires(post): /bin/sh Checking for unpackaged file(s): /usr/lib/rpm/check-files /root/rpmbuild/BUILDROOT/cray_ca_cert-1.0.0-1.x86_64 Wrote: /root/rpmbuild/RPMS/x86_64/cray_ca_cert-1.0.0-1.x86_64.rpm Executing(%clean): /bin/sh -e /var/tmp/rpm-tmp.jHaFMC + umask 022 + cd /root/rpmbuild/BUILD + cd cray_ca_cert-1.0.0 + exit 0 The build-image container builds the recipe using the Kiwi-NG tool.\nncn# kubectl -n ims logs -f $POD -c build-image + RECIPE_ROOT_PARENT=/mnt/recipe + IMAGE_ROOT_PARENT=/mnt/image + PARAMETER_FILE_BUILD_FAILED=/mnt/image/build_failed + PARAMETER_FILE_KIWI_LOGFILE=/mnt/image/kiwi.log ... + kiwi-ng --logfile=/mnt/image/kiwi.log --type tbz system build --description /mnt/recipe --target /mnt/image [ INFO ]: 16:14:31 | Loading XML description [ INFO ]: 16:14:31 | --\u0026gt; loaded /mnt/recipe/config.xml [ INFO ]: 16:14:31 | --\u0026gt; Selected build type: tbz [ INFO ]: 16:14:31 | Preparing new root system [ INFO ]: 16:14:31 | Setup root directory: /mnt/image/build/image-root [ INFO ]: 16:14:31 | Setting up repository http://api-gw-service-nmn.local/repositories/sle15-Module-Basesystem/ [ INFO ]: 16:14:31 | --\u0026gt; Type: rpm-md [ INFO ]: 16:14:31 | --\u0026gt; Translated: http://api-gw-service-nmn.local/repositories/sle15-Module-Basesystem/ [ INFO ]: 16:14:31 | --\u0026gt; Alias: SLES15_Module_Basesystem [ INFO ]: 16:14:32 | Setting up repository http://api-gw-service-nmn.local/repositories/sle15-Product-SLES/ [ INFO ]: 16:14:32 | --\u0026gt; Type: rpm-md [ INFO ]: 16:14:32 | --\u0026gt; Translated: http://api-gw-service-nmn.local/repositories/sle15-Product-SLES/ [ INFO ]: 16:14:32 | --\u0026gt; Alias: SLES15_Product_SLES [ INFO ]: 16:14:32 | Setting up repository http://api-gw-service-nmn.local/repositories/cray-sle15 [ INFO ]: 16:14:32 | --\u0026gt; Type: rpm-md [ INFO ]: 16:14:32 | --\u0026gt; Translated: http://api-gw-service-nmn.local/repositories/cray-sle15 [ INFO ]: 16:14:32 | --\u0026gt; Alias: DST_built_rpms ... [ INFO ]: 16:19:19 | Calling images.sh script [ INFO ]: 16:19:55 | Creating system image [ INFO ]: 16:19:55 | Creating XZ compressed tar archive [ INFO ]: 16:21:31 | --\u0026gt; Creating archive checksum [ INFO ]: 16:21:51 | Export rpm packages metadata [ INFO ]: 16:21:51 | Export rpm verification metadata [ INFO ]: 16:22:09 | Result files: [ INFO ]: 16:22:09 | --\u0026gt; image_packages: /mnt/image/cray-sles15-barebones.x86_64-1.0.1.packages [ INFO ]: 16:22:09 | --\u0026gt; image_verified: /mnt/image/cray-sles15-barebones.x86_64-1.0.1.verified [ INFO ]: 16:22:09 | --\u0026gt; root_archive: /mnt/image/cray-sles15-barebones.x86_64-1.0.1.tar.xz [ INFO ]: 16:22:09 | --\u0026gt; root_archive_md5: /mnt/image/cray-sles15-barebones.x86_64-1.0.1.md5 + rc=0 + \u0026#39;[\u0026#39; 0 -ne 0 \u0026#39;]\u0026#39; + exit 0 The buildenv-sidecar container determines if the Kiwi-NG build was successful or not.\n If the Kiwi-NG build completed successfully, the image root, kernel, and initrd artifacts are uploaded to the artifact repository. If the Kiwi-NG build failed to complete successfully, an optional SSH Debug shell is enabled so the image build can be debugged.  ncn# kubectl -n ims logs -f $POD -c buildenv-sidecar Not running user shell for successful create action Copying SMS CA Public Certificate to target image root + IMAGE_ROOT_PARENT=/mnt/image + IMAGE_ROOT_DIR=/mnt/image/build/image-root + KERNEL_FILENAME=vmlinuz + INITRD_FILENAME=initrd + IMAGE_ROOT_ARCHIVE_NAME=sles15_barebones_image + echo Copying SMS CA Public Certificate to target image root + mkdir -p /mnt/image/build/image-root/etc/cray + cp -r /etc/cray/ca /mnt/image/build/image-root/etc/cray/ + mksquashfs /mnt/image/build/image-root /mnt/image/sles15_barebones_image.sqsh Parallel mksquashfs: Using 4 processors Creating 4.0 filesystem on /mnt/image/sles15_barebones_image.sqsh, block size 131072. [===========================================================\\] 26886/26886 100% Exportable Squashfs 4.0 filesystem, gzip compressed, data block size 131072 compressed data, compressed metadata, compressed fragments, compressed xattrs ... + python -m ims_python_helper image upload_artifacts sles15_barebones_image 7de80ccc-1e7d-43a9-a6e4-02cad10bb60b \\  -v -r /mnt/image/sles15_barebones_image.sqsh -k /mnt/image/image-root/boot/vmlinuz -i /mnt/image/image-root/boot/initrd { \u0026#34;ims_image_artifacts\u0026#34;: [ { \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;4add976679c7e955c4b16d7e2cfa114e-32\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/d88521c3-b339-43bc-afda-afdfda126388/rootfs\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;md5\u0026#34;: \u0026#34;94165af4373e5ace3e817eb4baba2284\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;application/vnd.cray.image.rootfs.squashfs\u0026#34; }, { \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;f836412241aae79d160556ed6a4eb4d4\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/d88521c3-b339-43bc-afda-afdfda126388/kernel\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;md5\u0026#34;: \u0026#34;f836412241aae79d160556ed6a4eb4d4\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;application/vnd.cray.image.kernel\u0026#34; }, { \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;ec8793c07f94e59a2a30abdb1bd3d35a-4\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/d88521c3-b339-43bc-afda-afdfda126388/initrd\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;md5\u0026#34;: \u0026#34;86832ee3977ca0515592e5d00271d2fe\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;application/vnd.cray.image.initrd\u0026#34; }, { \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;13af343f3e76b0f8c7fbef7ee3588ac1\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/d88521c3-b339-43bc-afda-afdfda126388/manifest.json\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;md5\u0026#34;: \u0026#34;13af343f3e76b0f8c7fbef7ee3588ac1\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;application/json\u0026#34; } ], \u0026#34;ims_image_record\u0026#34;: { \u0026#34;created\u0026#34;: \u0026#34;2018-12-17T22:59:43.264129+00:00\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;d88521c3-b339-43bc-afda-afdfda126388\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;sles15_barebones_image\u0026#34; \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;13af343f3e76b0f8c7fbef7ee3588ac1\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/d88521c3-b339-43bc-afda-afdfda126388/manifest.json\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, }, \u0026#34;ims_job_record\u0026#34;: { \u0026#34;artifact_id\u0026#34;: \u0026#34;2233c82a-5081-4f67-bec4-4b59a60017a6\u0026#34;, \u0026#34;build_env_size\u0026#34;: 10, \u0026#34;created\u0026#34;: \u0026#34;2018-11-21T18:22:53.409405+00:00\u0026#34;, \u0026#34;enable_debug\u0026#34;: false, \u0026#34;id\u0026#34;: \u0026#34;ad5163d2-398d-4e93-94f0-2f439f114fe7\u0026#34;, \u0026#34;image_root_archive_name\u0026#34;: \u0026#34;sles15_barebones_image\u0026#34;, \u0026#34;initrd_file_name\u0026#34;: \u0026#34;initrd\u0026#34;, \u0026#34;job_type\u0026#34;: \u0026#34;create\u0026#34;, \u0026#34;kernel_file_name\u0026#34;: \u0026#34;vmlinuz\u0026#34;, \u0026#34;kubernetes_configmap\u0026#34;: \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-configmap\u0026#34;, \u0026#34;kubernetes_job\u0026#34;: \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-create\u0026#34;, \u0026#34;kubernetes_service\u0026#34;: \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-service\u0026#34;, \u0026#34;public_key_id\u0026#34;: \u0026#34;a252ff6f-c087-4093-a305-122b41824a3e\u0026#34;, \u0026#34;resultant_image_id\u0026#34;: \u0026#34;d88521c3-b339-43bc-afda-afdfda126388\u0026#34;, \u0026#34;ssh_port\u0026#34;: 0, \u0026#34;status\u0026#34;: \u0026#34;creating\u0026#34; }, \u0026#34;result\u0026#34;: \u0026#34;success\u0026#34; } ... IMPORTANT: The IMS image creation workflow automatically copies the NCN Certificate Authority\u0026rsquo;s public certificate to /etc/cray/ca/certificate_authority.crt within the image root being built. This can be used to enable secure communications between the NCN and the client node.\nIf the image creation operation fails, the build artifacts will not be uploaded to S3. If enable_debug is set to true, the IMS creation job will enable a debug SSH shell that is accessible by one or more dynamic host names. The user needs to know if they will SSH from inside or outside the Kubernetes cluster to determine which host name to use. Typically, customers access the system from outside the Kubernetes cluster using the Customer Access Network (CAN).\n  Use the IMS_JOB_ID to look up the ID of the newly created image.\nSteps 7-9 should only be run if the image creation job fails. If it was successful, proceed to step 10.\nThere may be multiple records returned. Ensure that the correct record is selected in the returned data.\nncn# cray ims jobs describe $IMS_JOB_ID status = \u0026#34;waiting_on_user\u0026#34; enable_debug = false kernel_file_name = \u0026#34;vmlinuz\u0026#34; artifact_id = \u0026#34;4e78488d-4d92-4675-9d83-97adfc17cb19\u0026#34; build_env_size = 10 job_type = \u0026#34;create\u0026#34; kubernetes_service = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-service\u0026#34; kubernetes_job = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-create\u0026#34; id = \u0026#34;ad5163d2-398d-4e93-94f0-2f439f114fe7\u0026#34; image_root_archive_name = \u0026#34;my_customized_image\u0026#34; initrd_file_name = \u0026#34;initrd\u0026#34; created = \u0026#34;2018-11-21T18:22:53.409405+00:00\u0026#34; kubernetes_namespace = \u0026#34;ims\u0026#34; public_key_id = \u0026#34;a252ff6f-c087-4093-a305-122b41824a3e\u0026#34; kubernetes_configmap = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-configmap\u0026#34; [[ssh_containers]] status = \u0026#34;pending\u0026#34; jail = false name = \u0026#34;debug\u0026#34; [ssh_containers.connection_info.\u0026#34;cluster.local\u0026#34;] host = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-service.ims.svc.cluster.local\u0026#34; port = 22 [ssh_containers.connection_info.customer_access] host = \u0026#34;ad5163d2-398d-4e93-94f0-2f439f114fe7.ims.shasta.cray.com\u0026#34; \u0026lt;\u0026lt;-- Note this host port = 22 \u0026lt;\u0026lt;-- Note this port If successful, create variables for the SSH connection information.\nncn# IMS_SSH_HOST=ad5163d2-398d-4e93-94f0-2f439f114fe7.ims.shasta.cray.com ncn# IMS_SSH_PORT=22   Connect to the IMS debug shell.\nTo access the debug shell, SSH to the container using the private key that matches the public key used to create the IMS Job.\nIMPORTANT: The following command will not work when run on a node within the Kubernetes cluster.\nncn# ssh -p IMS_SSH_PORT root@IMS_SSH_HOST Last login: Tue Sep 4 18:06:27 2018 from gateway [root@POD ~]#   Investigate the IMS debug shell.\n  Change to the cd /mnt/image/ directory.\n[root@POD image]# cd /mnt/image/   Use chroot to access the image root.\n[root@POD image]# chroot image-root/   Investigate the image debug shell.\n  Exit the image root.\n:/ # exit [root@POD image]#   Touch the complete file once investigations are complete.\n[root@POD image]# touch /mount/image/complete     Verify the new image was created correctly.\nncn# cray ims jobs describe $IMS_JOB_ID status = \u0026#34;success\u0026#34; enable_debug = false kernel_file_name = \u0026#34;vmlinuz\u0026#34; artifact_id = \u0026#34;2233c82a-5081-4f67-bec4-4b59a60017a6\u0026#34; build_env_size = 10 job_type = \u0026#34;create\u0026#34; kubernetes_service = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-service\u0026#34; kubernetes_job = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-customize\u0026#34; id = \u0026#34;ad5163d2-398d-4e93-94f0-2f439f114fe7\u0026#34; image_root_archive_name = \u0026#34;sles15_barebones_image\u0026#34; resultant_image_id = d88521c3-b339-43bc-afda-afdfda126388 initrd_file_name = \u0026#34;initrd\u0026#34; created = \u0026#34;2018-11-21T18:22:53.409405+00:00\u0026#34; kubernetes_namespace = \u0026#34;ims\u0026#34; public_key_id = \u0026#34;a252ff6f-c087-4093-a305-122b41824a3e\u0026#34; kubernetes_configmap = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-configmap\u0026#34; If successful, create a variable for the IMS resultant_image_id.\nncn# export IMS_RESULTANT_IMAGE_ID=d88521c3-b339-43bc-afda-afdfda126388   Verify the new IMS image record exists.\nncn# cray ims images describe $IMS_RESULTANT_IMAGE_ID created = \u0026#34;2018-12-17T22:59:43.264129+00:00\u0026#34; id = \u0026#34;d88521c3-b339-43bc-afda-afdfda126388\u0026#34; name = \u0026#34;sles15_barebones_image\u0026#34; [link] path = \u0026#34;s3://boot-images/d88521c3-b339-43bc-afda-afdfda126388/manifest.json\u0026#34; etag = \u0026#34;180883770442235de747e9d69855f269\u0026#34; type = \u0026#34;s3\u0026#34;   Clean Up the Create Environment\n Delete the IMS job record using the IMS_JOB_ID.\nncn# cray ims jobs delete $IMS_JOB_ID Deleting the job record will delete the underlying Kubernetes job, service, and ConfigMap that were created when the job record was submitted.\n  Images built by IMS contain only the packages and settings that are referenced in the Kiwi-NG recipe used to build the image. The only exception is that IMS will dynamically install the system\u0026rsquo;s root CA certificate to allow zypper (via Kiwi-NG) to talk securely with the required Nexus RPM repositories. Images that are intended to be used to boot a CN or other node must be configured with DNS and other settings that enable the image to talk to vital services. A base level of customization is provided by the default Ansible plays used by the Configuration Framework Service (CFS) to enable DNS resolution, which are typically run against an image after it is built by IMS.\nWhen customizing an image via Customize an Image Root Using IMS, once chrooted into the image root (or if using a `jailed` environment), the image will only have access to whatever configuration the image already contains. In order to talk to services, including Nexus RPM repositories, the image root must first be configured with DNS and other settings. That base level of customization is provided by the default Ansible plays used by the CFS to enable DNS resolution.\n"
},
{
	"uri": "/docs-csm/en-11/operations/image_management/convert_tgz_archives_to_squashfs_images/",
	"title": "Convert Tgz Archives To SqUAShfs Images",
	"tags": [],
	"description": "",
	"content": "Convert TGZ Archives to SquashFS Images If customizing a pre-built image root archive compressed as a txz or other non-SquashFS format, convert the image root to SquashFS and upload the SquashFS archive to S3.\nThe steps in this section only apply if the image root is not in SquashFS format.\nPrerequisites There is a pre-built image that is not currently in SquashFS format.\nProcedure   Locate the image root to be converted to SquashFS.\nImages and recipes are uploaded to IMS and S3 via containers.\n  Uncompress the desired file to a temporary directory.\nReplace the TXZ_COMPRESSED_IMAGE value with the name of the image root being used that was returned in the previous step.\nncn-m001# mkdir -p ~/tmp/image-root ncn-m001# cd ~/tmp/ ncn-m001# tar xvf TXZ_COMPRESSED_IMAGE -C image-root   Recompress the image root with SquashFS.\nncn-m001# export IMS_ROOTFS_FILENAME=IMAGE_NAME.squashfs ncn-m001# mksquashfs image-root $IMS_ROOTFS_FILENAME   "
},
{
	"uri": "/docs-csm/en-11/operations/image_management/create_uan_boot_images/",
	"title": "Create UAN Boot Images",
	"tags": [],
	"description": "",
	"content": "Create UAN Boot Images Update configuration management git repository to match the installed version of the UAN product. Then use that updated configuration to create UAN boot images and a BOS session template.\nThis is the overall workflow for preparing UAN images for booting UANs:\n Clone the UAN configuration git repository and create a branch based on the branch imported by the UAN installation. Update the configuration content and push the changes to the newly created branch. Create a Configuration Framework Service (CFS) configuration for the UANs, specifying the git configuration and the UAN image to apply the configuration to. More Cray products can also be added to the CFS configuration so that the UANs can install multiple Cray products into the UAN image at the same time. Configure the UAN image using CFS and generate a newly configured version of the UAN image. Create a Boot Orchestration Service (BOS) boot session template for the UANs. This template maps the configured image, the CFS configuration to be applied post-boot, and the nodes which will receive the image and configuration.  Once the UAN BOS session template is created, the UANs will be ready to be booted by a BOS session.\nReplace PRODUCT_VERSION and CRAY_EX_HOSTNAME in the example commands in this procedure with the current UAN product version installed (See Step 1) and the hostname of the HPE Cray EX system, respectively.\nPrerequisites The UAN product stream must be installed.\nLimitations This guide only details how to apply UAN-specific configuration to the UAN image and nodes. Consult the manuals for the individual HPE products (for example, workload managers and the HPE Cray Programming Environment) that must be configured on the UANs.\nUAN Image Pre-Boot Configuration   Obtain the artifact IDs and other information from the cray-product-catalog Kubernetes ConfigMap. Record the information labeled in the following example.\nUpon successful installation of the UAN product, the UAN configuration, image recipes, and prebuilt boot images are cataloged in this ConfigMap. This information is required for this procedure.\nncn-m001# kubectl get cm -n services cray-product-catalog -o json | jq -r .data.uan PRODUCT_VERSION: configuration: clone_url: https://vcs.CRAY_EX_HOSTNAME/vcs/cray/uan-config-management.git # \u0026lt;--- Gitea clone url commit: 6658ea9e75f5f0f73f78941202664e9631a63726 # \u0026lt;--- Git commit id import_branch: cray/uan/PRODUCT_VERSION # \u0026lt;--- Git branch with configuration import_date: 2021-02-02 19:14:18.399670 ssh_url: git@vcs.CRAY_EX_HOSTNAME:cray/uan-config-management.git images: cray-shasta-uan-cos-sles15sp1.x86_64-0.1.17: # \u0026lt;--- IMS image name id: c880251d-b275-463f-8279-e6033f61578b # \u0026lt;--- IMS image id recipes: cray-shasta-uan-cos-sles15sp1.x86_64-0.1.17: # \u0026lt;--- IMS recipe name id: cbd5cdf6-eac3-47e6-ace4-aa1aecb1359a # \u0026lt;--- IMS recipe id   Generate the password hash for the root user. Replace PASSWORD with the root password you wish to use.\nncn-m001# openssl passwd -6 -salt $(\u0026lt; /dev/urandom tr -dc _A-Z-a-z-0-9 | head -c4) PASSWORD   Obtain the HashiCorp Vault root token.\nncn-m001# kubectl get secrets -n vault cray-vault-unseal-keys -o jsonpath=\u0026#39;{.data.vault-root}\u0026#39; \\ | base64 -d; echo   Write the password hash obtained in Step 2 to the HashiCorp Vault.\nThe vault login command will request a token. That token value is the output of the previous step. The vault read secret/uan command verifies that the hash was stored correctly. This password hash will be written to the UAN for the root user by CFS.\nncn-m001# kubectl exec -itn vault cray-vault-0 -- sh export VAULT_ADDR=http://cray-vault:8200 vault login vault write secret/uan root_password=\u0026#39;HASH\u0026#39; vault read secret/uan   Obtain the password for the crayvcs user from the Kubernetes secret for use in the next command.\nncn-m001# kubectl get secret -n services vcs-user-credentials \\ --template={{.data.vcs_password}} | base64 --decode   Clone the UAN configuration management repository. Replace CRAY_EX_HOSTNAME in the clone_url with api-gw-service-nmn.local when cloning the repository.\nThe repository is in the VCS/Gitea service and the location is reported in the cray-product-catalog Kubernetes ConfigMap in the configuration.clone_url key. The CRAY_EX_HOSTNAME from the clone_url is replaced with api-gw-service-nmn.local in the command that clones the repository.\nncn-m001# git clone https://api-gw-service-nmn.local/vcs/cray/uan-config-management.git . . . ncn-m001# cd uan-config-management \u0026amp;\u0026amp; git checkout cray/uan/PRODUCT_VERSION \u0026amp;\u0026amp; git pull Branch \u0026#39;cray/uan/PRODUCT_VERSION\u0026#39; set up to track remote branch \u0026#39;cray/uan/PRODUCT_VERSION\u0026#39; from \u0026#39;origin\u0026#39;. Already up to date.   Create a branch using the imported branch from the installation to customize the UAN image.\nThis imported branch will be reported in the cray-product-catalog Kubernetes ConfigMap in the configuration.import_branch key under the UAN section. The format is cray/uan/PRODUCT_VERSION. In this guide, an integration branch is used for examples, but the name can be any valid git branch name.\nModifying the cray/uan/PRODUCT_VERSION branch that was created by the UAN product installation is not allowed by default.\nncn-m001# git checkout -b integration \u0026amp;\u0026amp; git merge cray/uan/PRODUCT_VERSION Switched to a new branch \u0026#39;integration\u0026#39; Already up to date.   Configure a root user in the UAN image by adding the encrypted password of the root user from /etc/shadow on an NCN worker to the file group_vars/Application/passwd.yml. Skip this step if the root user is already configured in the image.\nHewlett Packard Enterprise recommends configuring a root user in the UAN image for troubleshooting purposes. The entry for root user password will resemble the following example:\nroot_passwd: $6$LmQ/PlWlKixK$VL4ueaZ8YoKOV6yYMA9iH0gCl8F4C/3yC.jMIGfOK6F61h6d.iZ6/QB0NLyex1J7AtOsYvqeycmLj2fQcLjfE1   Apply any site-specific customizations and modifications to the Ansible configuration for the UAN nodes and commit the changes.\nThe default Ansible play to configure UAN nodes is site.yml in the base of the uan-config-management repository. The roles that are executed in this play allow for nondefault configuration as required for the system.\nConsult the individual Ansible role README.md files in the uan-config-management repository roles directory to configure individual role variables. Roles prefixed with uan_ are specific to UAN configuration and include network interfaces, disk, LDAP, software packages, and message of the day roles.\nVariables should be defined and overridden in the Ansible inventory locations of the repository as shown in the following example and not in the Ansible plays and roles defaults. See https://docs.ansible.com/ansible/2.9/user_guide/playbooks_best_practices.html#content-organization for directory layouts for inventory.\nWarning: Never place sensitive information such as passwords in the git repository.\nThe following example shows how to add a vars.yml file containing site-specific configuration values to the Application group variable location.\nThese and other Ansible files do not necessarily need to be modified for UAN image creation.\nncn-m001# vim group_vars/Application/vars.yml ncn-m001# git add group_vars/Application/vars.yml ncn-m001# git commit -m \u0026#34;Add vars.yml customizations\u0026#34; [integration ecece54] Add vars.yml customizations 1 file changed, 1 insertion(+) create mode 100644 group_vars/Application/vars.yml   Verify that the System Layout Service (SLS) and the uan_interfaces configuration role refer to the Mountain Node Management Network by the same name. Skip this step if there are no Mountain cabinets in the HPE Cray EX system.\n  Edit the roles/uan_interfaces/tasks/main.yml file and change the line that reads url: http://cray-sls/v1/search/networks?name=MNMN to read url: http://cray-sls/v1/search/networks?name=NMN_MTN.\nThe following excerpt of the relevant section of the file shows the result of the change.\n- name: Get Mountain NMN Services Network info from SLS local_action: module: uri url: http://cray-sls/v1/search/networks?name=NMN_MTN method: GET register: sls_mnmn_svcs ignore_errors: yes   Stage and commit the network name change\nncn-m# git add roles/uan_interfaces/tasks/main.yml ncn-m# git commit -m \u0026#34;Add Mountain cabinet support\u0026#34;     Push the changes to the repository using the proper credentials, including the password obtained previously.\nncn-m001# git push --set-upstream origin integration Username for \u0026#39;https://api-gw-service-nmn.local\u0026#39;: crayvcs Password for \u0026#39;https://crayvcs@api-gw-service-nmn.local\u0026#39;: . . . remote: Processed 1 references in total To https://api-gw-service-nmn.local/vcs/cray/uan-config-management.git * [new branch] integration -\u0026gt; integration Branch \u0026#39;integration\u0026#39; set up to track remote branch \u0026#39;integration\u0026#39; from \u0026#39;origin\u0026#39;.   Capture the most recent commit for reference in setting up a CFS configuration and navigate to the parent directory.\nncn-m001# git rev-parse --verify HEAD ecece54b1eb65d484444c4a5ca0b244b329f4667 ncn-m001# cd .. The configuration parameters have been stored in a branch in the UAN git repository. The next phase of the process is initiating the Configuration Framework Service (CFS) to customize the image.\n  Configure UAN Images  Create a JSON input file for generating a CFS configuration for the UAN.\nGather the git repository clone URL, commit, and top-level play for each configuration layer (that is, Cray product). Add them to the CFS configuration for the UAN, if wanted.\nFor the commit value for the UAN layer, use the Git commit value obtained in the previous step.\nSee the product manuals for further information on configuring other Cray products, as this procedure documents only the configuration of the UAN. More layers can be added to be configured in a single CFS session.\nThe following configuration example can be used for preboot image customization as well as post-boot node configuration.\n{ \u0026#34;layers\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;uan-integration-PRODUCT\\_VERSION\u0026#34;, \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/uan-config-management.git\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;ecece54b1eb65d484444c4a5ca0b244b329f4667\u0026#34; } # **{ ... add configuration layers for other products here, if desired ... }** ] }   Add the configuration to CFS using the JSON input file.\nIn the following example, the JSON file created in the previous step is named uan-config-PRODUCT_VERSION.json only the details for the UAN layer are shown.\nncn-m001# cray cfs configurations update uan-config-PRODUCT_VERSION \\  --file ./uan-config-PRODUCT_VERSION.json \\  --format json { \u0026#34;lastUpdated\u0026#34;: \u0026#34;2021-07-28T03:26:00:37Z\u0026#34;, \u0026#34;layers\u0026#34;: [ { \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/uan-config-management.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;ecece54b1eb65d484444c4a5ca0b244b329f4667\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;uan-integration-PRODUCT_VERSION\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34; } # \u0026lt;-- Additional layers not shown, but would be inserted here ], \u0026#34;name\u0026#34;: \u0026#34;uan-config-PRODUCT_VERSION\u0026#34; }   Modify the UAN image to include the 1.4.0 day zero RPMs.\n  Untar the 1.4.0 Day Zero Patch tarball if it is not untarred already.\nncn-m001# tar -xvf shasta-1.4.0-p2.tar 1.4.0-p2/ 1.4.0-p2/csm/ 1.4.0-p2/csm/csm-0.8.22-0.9.0.patch.gz 1.4.0-p2/csm/csm-0.8.22-0.9.0.patch.gz.md5sum 1.4.0-p2/uan/ 1.4.0-p2/uan/uan-2.0.0-uan-2.0.0.patch.gz 1.4.0-p2/uan/uan-2.0.0-uan-2.0.0.patch.gz.md5sum 1.4.0-p2/rpms/ 1.4.0-p2/rpms/cray-dvs-compute-2.12_4.0.102-7.0.1.0_8.1__g30d29e7a.x86_64.rpm 1.4.0-p2/rpms/cray-dvs-devel-2.12_4.0.102-7.0.1.0_8.1__g30d29e7a.x86_64.rpm 1.4.0-p2/rpms/cray-dvs-kmp-cray_shasta_c-2.12_4.0.102_k4.12.14_197.78_9.1.58-7.0.1.0_8.1__g30d29e7a.x86_64.rpm 1.4.0-p2/rpms/cray-network-config-1.1.7-20210318094806_b409053-sles15sp1.x86_64.rpm 1.4.0-p2/rpms/slingshot-network-config-1.1.7-20210318093253_83fab52-sles15sp1.x86_64.rpm 1.4.0-p2/rpms/slingshot-network-config-full-1.1.7-20210318093253_83fab52-sles15sp1.x86_64.rpm 1.4.0-p2/rpms/cray-dvs-compute-2.12_4.0.102-7.0.1.0_8.1__g30d29e7a.x86_64.rpm.md5sum 1.4.0-p2/rpms/cray-dvs-devel-2.12_4.0.102-7.0.1.0_8.1__g30d29e7a.x86_64.rpm.md5sum 1.4.0-p2/rpms/cray-dvs-kmp-cray_shasta_c-2.12_4.0.102_k4.12.14_197.78_9.1.58-7.0.1.0_8.1__g30d29e7a.x86_64.rpm.md5sum 1.4.0-p2/rpms/cray-network-config-1.1.7-20210318094806_b409053-sles15sp1.x86_64.rpm.md5sum 1.4.0-p2/rpms/slingshot-network-config-1.1.7-20210318093253_83fab52-sles15sp1.x86_64.rpm.md5sum 1.4.0-p2/rpms/slingshot-network-config-full-1.1.7-20210318093253_83fab52-sles15sp1.x86_64.rpm.md5sum   Download the rootfs image specified in the UAN product catalog.\nReplace IMAGE_ID in the following export command with the IMS image id recorded in Step 1.\nncn-m001# export UAN_IMAGE_ID=IMAGE_ID ncn-m001# cray artifacts get boot-images ${UAN_IMAGE_ID}/rootfs \\ ${UAN_IMAGE_ID}.squashfs ncn-m001# la ${UAN_IMAGE_ID}.squashfs -rw-r--r-- 1 root root 1.5G Mar 17 19:35 f3ba09d7-e3c2-4b80-9d86-0ee2c48c2214.squashfs   Mount the squashfs file and copy its contents to a different directory.\nncn-m001# mkdir mnt ncn-m001# mkdir UAN-1.4.0-day-zero ncn-m001# mount -t squashfs ${UAN_IMAGE_ID}.squashfs mnt -o ro,loop ncn-m001# cp -a mnt UAN-1.4.0-day-zero ncn-m001# umount mnt ncn-m001# rmdir mnt   Copy the new RPMs into the new image directory.\nncn-m001# cp 1.4.0-p2/rpms/* UAN-1.4.0-day-zero/ ncn-m001# cd UAN-1.4.0-day-zero/   Chroot into the new image directory.\nncn-m001# chroot . bash   Update, erase, and install RPMs in the new image directory.\nchroot-ncn-m001# rpm -Uv cray-dvs-*.rpm chroot-ncn-m001# rpm -e cray-network-config chroot-ncn-m001# rpm -e slingshot-network-config-full chroot-ncn-m001# rpm -e slingshot-network-config chroot-ncn-m001# rpm -iv slingshot-network-config-full-1.1.7-20210318093253_83fab52-sles15sp1.x86_64.rpm \\ slingshot-network-config-1.1.7-20210318093253_83fab52-sles15sp1.x86_64.rpm \\ cray-network-config-1.1.7-20210318094806_b409053-sles15sp1.x86_64.rpm   Generate a new initrd to match the updated image by running the /tmp/images.sh script. Then wait for this script to complete before continuing.\nchroot-ncn-m001# /tmp/images.sh The output of this script will contain error messages. These error messages can be ignored as long as the message dracut: *** Creating initramfs image file appears at the end.\n  Copy the /boot/initrd and /boot/vmlinuz files out of the chroot environment and into a temporary location on the file system of the node.\n  Exit the chroot environment and delete the packages.\nchroot-ncn-m001# exit exit ncn-m001# rm *.rpm ncn-m001# cd ..   Verify that there is only one subdirectory in the lib/modules directory of the image.\nThe existence of more than one subdirectory indicates a mismatch between the kernel of the image and the DVS RPMS that were installed in the previous step.\nncn-m001# la UAN-1.4.0-day-zero/lib/modules/ total 8.0K drwxr-xr-x 3 root root 49 Feb 25 17:50 ./ drwxr-xr-x 8 root root 4.0K Feb 25 17:52 ../ drwxr-xr-x 6 root root 4.0K Mar 17 19:49 4.12.14-197.78_9.1.58-cray_shasta_c/   Resquash the new image directory.\nncn-m001# mksquashfs UAN-1.4.0-day-zero UAN-1.4.0-day-zero.squashfs Parallel mksquashfs: Using 64 processors Creating 4.0 filesystem on UAN-1.4.0-day-zero.squashfs, block size 131072. ...   Create a new IMS image registration and save the id field in an environment variable.\nncn-m001# cray ims images create --name UAN-1.4.0-day-zero name = \u0026#34;UAN-1.4.0-day-zero\u0026#34; created = \u0026#34;2021-03-17T20:23:05.576754+00:00\u0026#34; id = \u0026#34;ac31e971-f990-4b5f-821d-c0c18daefb6e\u0026#34; ncn-m001# export NEW_IMAGE_ID=ac31e971-f990-4b5f-821d-c0c18daefb6e   Upload the new image, initrd, and kernel to S3 using the id from the previous step.\nncn-m001# cray artifacts create boot-images ${NEW_IMAGE_ID}/rootfs \\ UAN-1.4.0-day-zero.squashfs artifact = \u0026#34;ac31e971-f990-4b5f-821d-c0c18daefb6e/UAN-1.4.0-day-zero.rootfs\u0026#34; Key = \u0026#34;ac31e971-f990-4b5f-821d-c0c18daefb6e/UAN-1.4.0-day-zero.rootfs\u0026#34; ncn-m001# cray artifacts create boot-images ${NEW_IMAGE_ID}/initrd \\ initrd artifact = \u0026#34;ac31e971-f990-4b5f-821d-c0c18daefb6e/UAN-1.4.0-day-zero.initrd\u0026#34; Key = \u0026#34;ac31e971-f990-4b5f-821d-c0c18daefb6e/UAN-1.4.0-day-zero.initrd\u0026#34; ncn-m001# cray artifacts create boot-images ${NEW_IMAGE_ID}/kernel \\ vmlinuz artifact = \u0026#34;ac31e971-f990-4b5f-821d-c0c18daefb6e/UAN-1.4.0-day-zero.kernel\u0026#34; Key = \u0026#34;ac31e971-f990-4b5f-821d-c0c18daefb6e/UAN-1.4.0-day-zero.kernel\u0026#34;   Obtain the md5sum of the squashfs image, initrd, and kernel.\nncn-m001# md5sum UAN-1.4.0-day-zero.squashfs initrd vmlinuz cb6a8934ad3c483e740c648238800e93 UAN-1.4.0-day-zero.squashfs 3fd8a72a49a409f70140fabe11bdac25 initrd 5edcf3fd42ab1eccfbf1e52008dac5b9 vmlinuz   Use the image id from Step 1 to print out all the IMS details about the current UAN image.\nncn-m001# cray ims images describe c880251d-b275-463f-8279-e6033f61578b created = \u0026#34;2021-03-24T18:00:24.464755+00:00\u0026#34; id = \u0026#34;c880251d-b275-463f-8279-e6033f61578b\u0026#34; name = \u0026#34;cray-shasta-uan-cos-sles15sp1.x86_64-0.1.32\u0026#34;[link] etag = \u0026#34;d4e09fb028d5d99e4a0d4d9b9d930e13\u0026#34; path = \u0026#34;s3://boot-images/c880251d-b275-463f-8279-e6033f61578b/manifest.json\u0026#34; type = \u0026#34;s3\u0026#34;   Use the path of the manifest.json file to download that JSON to a local file. Omit everything before the image id in the cray artifacts get boot-images command, as shown in the following example what does this sentence even mean? should we cut it?.\nncn-m001# cray artifacts get boot-images \\ c880251d-b275-463f-8279-e6033f61578b/manifest.json uan-manifest.json ncn-m001# cat uan-manifest.json { \u0026#34;artifacts\u0026#34;: [ { \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;6d04c3a4546888ee740d7149eaecea68\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/c880251d-b275-463f-8279-e6033f61578b/rootfs\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;md5\u0026#34;: \u0026#34;a159b94238fc5bfe80045889226b33a3\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;application/vnd.cray.image.rootfs.squashfs\u0026#34; }, { \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;6d04c3a4546888ee740d7149eaecea68\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/c880251d-b275-463f-8279-e6033f61578b/kernel\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;md5\u0026#34;: \u0026#34;175f0c1363c9e3a4840b08570a923bc5\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;application/vnd.cray.image.kernel\u0026#34; }, { \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;6d04c3a4546888ee740d7149eaecea68\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/c880251d-b275-463f-8279-e6033f61578b/initrd\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;md5\u0026#34;: \u0026#34;0094629e4da25226c75b113760eeabf7\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;application/vnd.cray.image.initrd\u0026#34; } ], \u0026#34;created\u0026#34; : \u0026#34;20210317153136\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0\u0026#34; } Alternatively, a manifest.json can be created from scratch. In that case, create a new hexadecimal value for the etag if the image referred to by the manifest does not already have one. The etag field cannot be left blank.\n  Replace the path and md5 values of the initrd, kernel, and rootfs with the values obtained in substeps m and n.\n  Update the value for the \u0026quot;created\u0026quot; line in the manifest with the output of the following command:\nncn-m001# date \u0026#39;+%Y%m%d%H%M%S\u0026#39;   Verify that the modified JSON file is still valid.\nncn-m001# cat manifest.json | jq   Save the changes to the file.\n  Upload the updated manifest.json file.\nncn-m001# cray artifacts create boot-images \\ ${NEW_IMAGE_ID}/manifest.json uan-manifest.json artifact = \u0026#34;ac31e971-f990-4b5f-821d-c0c18daefb6e/manifest.json\u0026#34; Key = \u0026#34;ac31e971-f990-4b5f-821d-c0c18daefb6e/manifest.json\u0026#34;   Update the IMS image to use the new uan-manifest.json file.\nncn-m001# cray ims images update ${NEW_IMAGE_ID} \\ --link-type s3 --link-path s3://boot-images/${NEW_IMAGE_ID}/manifest.json \\ --link-etag 6d04c3a4546888ee740d7149eaecea68 created = \u0026#34;2021-03-17T20:23:05.576754+00:00\u0026#34; id = \u0026#34;ac31e971-f990-4b5f-821d-c0c18daefb6e\u0026#34; name = \u0026#34;UAN-1.4.0-day-zero\u0026#34; [link] etag = \u0026#34;6d04c3a4546888ee740d7149eaecea68\u0026#34; path = \u0026#34;s3://boot-images/ac31e971-f990-4b5f-821d-c0c18daefb6e/manifest.json\u0026#34; type = \u0026#34;s3\u0026#34;     Create a CFS session to perform preboot image customization of the UAN image.\nncn-m001# cray cfs sessions create --name uan-config-PRODUCT_VERSION \\  --configuration-name uan-config-PRODUCT_VERSION \\  --target-definition image \\  --target-group Application $NEW_IMAGE_ID \\  --format json   Wait until the CFS configuration session for the image customization to complete. Then record the ID of the IMS image created by CFS.\nThe following command will produce output while the process is running. If the CFS session completes successfully, an IMS image ID will appear in the output.\nncn-m001# cray cfs sessions describe uan-config-PRODUCT_VERSION --format json | jq   Prepare UAN Boot Session Templates  Retrieve the xnames of the UAN nodes from the Hardware State Manager (HSM).\nThese xnames are needed for Step 20.\nncn-m001# cray hsm state components list --role Application --subrole UAN --format json | jq -r .Components[].ID x3000c0s19b0n0 x3000c0s24b0n0 x3000c0s20b0n0 x3000c0s22b0n0   Determine the correct value for the ifmap option in the kernel_parameters string for the type of UAN.\n Use ifmap=net0:nmn0,lan0:hsn0,lan1:hsn1 if the UANs are:  Either HPE DL325 or DL385 server that have a single OCP PCIe card installed. Gigabyte servers that do not have additional PCIe network cards installed other than the built-in LOM ports.   Use ifmap=net2:nmn0,lan0:hsn0,lan1:hsn1 if the UANs are:  Either HPE DL325 or DL385 servers which have a second OCP PCIe card installed, regardless if it is being used or not. Gigabyte servers that have a PCIe network card installed in addition to the built-in LOM ports, regardless if it is being used or not.      Construct a JSON BOS boot session template for the UAN.\n  Populate the template with the following information:\n The value of the ifmap option for the kernel_parameters string that was determined in the previous step. The xnames of Application nodes from Step 18 The customized image ID from Step 17 for The CFS configuration session name from Step 17    Verify that the session template matches the format and structure in the following example:\n{ \u0026#34;boot_sets\u0026#34;: { \u0026#34;uan\u0026#34;: { \u0026#34;boot_ordinal\u0026#34;: 2, \u0026#34;kernel_parameters\u0026#34;: \u0026#34;console=ttyS0,115200 bad_page=panic crashkernel=360M hugepagelist=2m-2g intel_iommu=off intel_pstate=disable iommu=pt ip=nmn0:dhcp numa_interleave_omit=headless numa_zonelist_order=node oops=panic pageblock_order=14 pcie_ports=native printk.synchronous=y quiet rd.neednet=1 rd.retry=10 rd.shell turbo_boost_limit=999 ifmap=net2:nmn0,lan0:hsn0,lan1:hsn1 spire_join_token=${SPIRE_JOIN_TOKEN}\u0026#34;, \u0026#34;network\u0026#34;: \u0026#34;nmn\u0026#34;, \u0026#34;node_list\u0026#34;: [ # \\[ ... List of Application Nodes from cray hsm state command ...\\] ], \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/IMS\\_IMAGE\\_ID/manifest.json\u0026#34;, # \u0026lt;-- result\\_id from CFS image customization session \u0026#34;rootfs_provider\u0026#34;: \u0026#34;cpss3\u0026#34;, \u0026#34;rootfs_provider_passthrough\u0026#34;: \u0026#34;dvs:api-gw-service-nmn.local:300:nmn0\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; } }, \u0026#34;cfs\u0026#34;: { \u0026#34;configuration\u0026#34;: \u0026#34;uan-config-PRODUCT\\_VERSION\u0026#34; }, \u0026#34;enable_cfs\u0026#34;: true, \u0026#34;name\u0026#34;: \u0026#34;uan-sessiontemplate-PRODUCT\\_VERSION\u0026#34; }   Save the template with a descriptive name, such as uan-sessiontemplate-PRODUCT_VERSION.json.\n    Register the session template with BOS.\nThe following command uses the JSON session template file to save a session template in BOS. This step allows administrators to boot UANs by referring to the session template name.\nncn-m001# cray bos sessiontemplate create \\  --name uan-sessiontemplate-PRODUCT_VERSION \\  --file uan-sessiontemplate-PRODUCT_VERSION.json /sessionTemplate/uan-sessiontemplate-PRODUCT_VERSION   Perform Boot UANs to boot the UANs with the new image and BOS session template.\n"
},
{
	"uri": "/docs-csm/en-11/operations/image_management/customize_an_image_root_using_ims/",
	"title": "Customize An Image Root Using IMS",
	"tags": [],
	"description": "",
	"content": "Customize an Image Root Using IMS The Image Management Service (IMS) customization workflow sets up a temporary image customization environment within a Kubernetes pod and mounts the image to be customized in that environment. A system administrator then makes the desired changes to the image root within the customization environment.\nAfterwards, the IMS customization workflow automatically copies the NCN CA public key to /etc/cray/ca/certificate_authority.crt within the image root being customized to enable secure communications between NCNs and client nodes. IMS then compresses the customized image root and uploads it and its associated initrd image and kernel image (needed to boot a node) to the artifact repository.\nPrerequisites  System management services (SMS) are running in a Kubernetes cluster on non-compute nodes (NCN) and include the following deployments:  cray-ims, the Image Management Service (IMS) cray-nexus, the Nexus repository manager service   kubectl is installed locally and configured to point at the SMS Kubernetes cluster. An IMS created image root archive or a pre-built image root SquashFS archive is available to customize. The NCN Certificate Authority (CA) public key has been properly installed into the CA cache for this system. A token providing Simple Storage Service (S3) credentials has been generated. When customizing an image using IMS Image Customization, once chrooted into the image root (if using a `jailed` environment), the image will only have access to whatever configuration the image already contains. In order to talk to services, including Nexus RPM repositories, the image root must first be configured with DNS and other settings. A base level of customization is provided by the default Ansible plays used by the Configuration Framework Service (CFS) to enable DNS resolution.  Limitations  The commands in this procedure must be run as the root user. Currently, the initrd image and kernel image are not regenerated automatically when the image root is changed. The admin must manually regenerate them while in the customization environment, if needed. Images in the .txz compressed format need to be converted to SquashFS in order to use IMS image customization.  Procedure Enable Passwordless SSH\n  Check for an existing IMS public key id.\nSkip this step if it is known that a public key associated with the user account being used was not previously uploaded to the IMS service.\nThe following query may return multiple public key records. The correct one will have a name value including the current username in use.\nncn# cray ims public-keys list ... [[results]] public_key = \u0026#34;ssh-rsa AAAAB3NzaC1yc2EA ... AsVruw1Zeiec2IWt\u0026#34; id = \u0026#34;a252ff6f-c087-4093-a305-122b41824a3e\u0026#34; name = \u0026#34;username public key\u0026#34; created = \u0026#34;2018-11-21T17:19:07.830000+00:00\u0026#34; ... If a public key associated with the username in use is not returned, proceed to the next step. If a public key associated with the username does exist, create a variable for the IMS public key id value in the returned data and then proceed to step 3.\nncn# export IMS_PUBLIC_KEY_ID=a252ff6f-c087-4093-a305-122b41824a3e   Upload the SSH public key to the IMS service.\nSkip this step if an IMS public key record has already been created for the account being used.\nThe IMS debug/configuration shell relies on passwordless SSH. This SSH public key needs to be uploaded to IMS to enable interaction with the image customization environment later in this procedure.\nReplace the username value with the actual username being used on the system when setting the public key name.\nncn# cray ims public-keys create --name \u0026#34;username public key\u0026#34; --public-key ~/.ssh/id_rsa.pub public_key = \u0026#34;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCl50gK4l9uupxC2KHxMpTNxPTJbnwEdWy1jst5W5LqJx9fdTrc9uNJ33HAq+WIOhPVGbLm2N4GX1WTUQ4+wVOSmmBBJnlu/l5rmO9lEGT6U8lKG8dA9c7qhguGHy7M7WBgdW/gWA16gwE/u8Qc2fycFERRKmFucL/Er9wA0/Qvz7/U59yO+HOtk5hvEz/AUkvaaoY0IVBfdNBCl59CIdZHxDzgXlXzd9PAlrXZNO8jDD3jyFAOvMMRG7py78zj2NUngvsWYoBcV3FcREZJU529uJ0Au8Vn9DRADyB4QQS2o+fa6hG9i2SzfY8L6vAVvSE7A2ILAsVruw1Zeiec2IWt\u0026#34; id = \u0026#34;a252ff6f-c087-4093-a305-122b41824a3e\u0026#34; name = \u0026#34;username public key\u0026#34; created = \u0026#34;2018-11-21T17:19:07.830000+00:00\u0026#34; If successful, create a variable for the IMS public key id value in the returned data.\nncn# export IMS_PUBLIC_KEY_ID=a252ff6f-c087-4093-a305-122b41824a3e   Locate or Register an Image Root Archive to Customize\n Determine if the image root being used is in IMS and ready to be customized.\nIMS requires that the image root being used meets the following criteria:\n  It is in SquashFS format.\n  It has been uploaded to S3 via the Cray CLI.\n  It is registered with the IMS service. Select one of the following options based on the current state of the image root being used:\n  If the image root being customized meets the above requirements, proceed to Locate an IMS Image to Customize.\n  If the image root being customized is not in SquashFS format, refer to Convert TGZ Archives to SquashFS Images.\n  If the image root being customized is in SquashFS format and in S3, but not registered with the IMS service, proceed to Register the Image Root with the IMS Service.\n    Create an IMS Image Record\n Create a new IMS image record for the image.\nncn# cray ims images create --name $IMS_ROOTFS_FILENAME created = \u0026#34;2018-12-04T17:25:52.482514+00:00\u0026#34; id = \u0026#34;4e78488d-4d92-4675-9d83-97adfc17cb19\u0026#34; name = \u0026#34;sles_15_image.squashfs\u0026#34; If successful, create a variable for the id value in the returned data.\nncn# export IMS_IMAGE_ID=4e78488d-4d92-4675-9d83-97adfc17cb19   Upload Image Artifacts to S3\nThe steps in this section apply only if the SquashFS image root is not yet in S3.\n Upload the image root to S3.\nncn# cray artifacts create boot-images $IMS_IMAGE_ID $IMS_ROOTFS_FILENAME $IMS_ROOTFS_FILENAME ncn# export IMS_ROOTFS_MD5SUM=`md5sum $IMS_ROOTFS_FILENAME | awk \u0026#39;{ print $1 }\u0026#39;`   Upload the kernel for the image to S3.\nncn# export IMS_KERNEL_FILENAME=vmlinuz ncn# cray artifacts create boot-images $IMS_IMAGE_ID $IMS_KERNEL_FILENAME \\ image-root/boot/$IMS_KERNEL_FILENAME ncn# export IMS_KERNEL_MD5SUM=`md5sum image-root/boot/$IMS_KERNEL_FILENAME | awk \u0026#39;{ print $1 }\u0026#39;`   Upload the initrd for the image to S3.\nncn# export IMS_INITRD_FILENAME=initrd ncn# cray artifacts create boot-images $IMS_IMAGE_ID $IMS_INITRD_FILENAME \\ image-root/boot/$IMS_INITRD_FILENAME ncn# export IMS_INITRD_MD5SUM=`md5sum image-root/boot/$IMS_INITRD_FILENAME | awk \u0026#39;{ print $1 }\u0026#39;`   Create an Image Manifest and Upload it to S3\nCray uses a manifest file that associates multiple related boot artifacts (kernel, initrd, rootfs) into an image description that is used by IMS and other services to boot nodes. Artifacts listed within the manifest are identified by a type value:\n application/vnd.cray.image.rootfs.squashfs application/vnd.cray.image.initrd application/vnd.cray.image.kernel application/vnd.cray.image.parameters.boot   Generate an image manifest file.\nncn# cat \u0026lt;\u0026lt;EOF\u0026gt; manifest.json { \u0026#34;created\u0026#34;: \u0026#34;`date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;`\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0\u0026#34;, \u0026#34;artifacts\u0026#34;: [ { \u0026#34;link\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/$IMS_IMAGE_ID/$IMS_ROOTFS_FILENAME\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;md5\u0026#34;: \u0026#34;$IMS_ROOTFS_MD5SUM\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;application/vnd.cray.image.rootfs.squashfs\u0026#34; }, { \u0026#34;link\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/$IMS_IMAGE_ID/$IMS_KERNEL_FILENAME\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;md5\u0026#34;: \u0026#34;$IMS_KERNEL_MD5SUM\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;application/vnd.cray.image.kernel\u0026#34; }, { \u0026#34;link\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/$IMS_IMAGE_ID/$IMS_INITRD_FILENAME\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;md5\u0026#34;: \u0026#34;$IMS_INITRD_MD5SUM\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;application/vnd.cray.image.initrd\u0026#34; } ] } EOF   Upload the manifest to S3.\nncn# cray artifacts create boot-images $IMS_IMAGE_ID/manifest.json manifest.json    Register the Image Root with the IMS Service\n Update the IMS image record.\nncn# cray ims images update $IMS_IMAGE_ID \\ --link-type s3 \\ --link-path s3://boot-images/$IMS_IMAGE_ID/manifest.json created = \u0026#34;2018-12-04T17:25:52.482514+00:00\u0026#34; id = \u0026#34;4e78488d-4d92-4675-9d83-97adfc17cb19\u0026#34; name = \u0026#34;sles_15_image.squashfs\u0026#34; [link] type = \u0026#34;s3\u0026#34; path = \u0026#34;s3://boot-images/4e78488d-4d92-4675-9d83-97adfc17cb19/manifest.json\u0026#34; etag = \u0026#34;   \nLocate an IMS Image to Customize\n Locate the IMS image record for the image that is being customized.\nncn# cray ims images list ... [[results]] created = \u0026#34;2018-12-04T17:25:52.482514+00:00\u0026#34; id = \u0026#34;4e78488d-4d92-4675-9d83-97adfc17cb19\u0026#34; name = \u0026#34;sles_15_image.squashfs\u0026#34; [results.link] type = \u0026#34;s3\u0026#34; path = \u0026#34;/4e78488d-4d92-4675-9d83-97adfc17cb19/sles_15_image.squashfs\u0026#34; etag = \u0026#34; ... If successful, create a variable for the id for the image that is being customized.\nncn# export IMS_IMAGE_ID=4e78488d-4d92-4675-9d83-97adfc17cb19   Submit the Kubernetes Image Customization Job\n Create an IMS job record and start the image customization job.\nAfter customizing the image, IMS will automatically upload any build artifacts (root file system, kernel, and initrd) to S3, and associate the S3 artifacts with IMS. Unfortunately, IMS is not able to dynamically determine the names of the Linux kernel and initrd to look for, because the file name for these vary depending upon Linux distribution, Linux version, dracut configuration, and more. Thus, the user must pass the name of the kernel and initrd that IMS is to look for in the resultant image root\u0026rsquo;s /boot directory.\nUse the following table to help determine the default kernel and initrd file names to specify when submitting the job to customize an image. These are just default names. Please consult with the site administrator to determine if these names have been changed for a given image or recipe.\n   Recipe Recipe Name Kernel File Name Initrd File Name     SLES 15 SP1 Barebones cray-sles15sp1-barebones vmlinuz initrd   COS cray-shasta-compute-sles15sp1.x86_64-1.4.66 vmlinuz initrd      Start the image customization job.\nBefore running the following command, replace the MY_CUSTOMIZED_IMAGE value with the name of the image root being used.\nncn# cray ims jobs create \\ --job-type customize \\ --kernel-file-name vmlinuz \\ --initrd-file-name initrd \\ --artifact-id $IMS_IMAGE_ID \\ --public-key-id $IMS_PUBLIC_KEY_ID \\ --enable-debug False \\ --image-root-archive-name MY_CUSTOMIZED_IMAGE status = \u0026#34;creating\u0026#34; enable_debug = false kernel_file_name = \u0026#34;vmlinuz\u0026#34; artifact_id = \u0026#34;4e78488d-4d92-4675-9d83-97adfc17cb19\u0026#34; build_env_size = 10 job_type = \u0026#34;customize\u0026#34; kubernetes_service = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-service\u0026#34; kubernetes_job = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-customize\u0026#34; id = \u0026#34;ad5163d2-398d-4e93-94f0-2f439f114fe7\u0026#34; image_root_archive_name = \u0026#34;MY_CUSTOMIZED_IMAGE\u0026#34; initrd_file_name = \u0026#34;initrd\u0026#34; created = \u0026#34;2018-11-21T18:22:53.409405+00:00\u0026#34; kubernetes_namespace = \u0026#34;ims\u0026#34; public_key_id = \u0026#34;a252ff6f-c087-4093-a305-122b41824a3e\u0026#34; kubernetes_configmap = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-configmap\u0026#34; [[ssh_containers]] status = \u0026#34;pending\u0026#34; jail = false name = \u0026#34;customize\u0026#34; [ssh_containers.connection_info.\u0026#34;cluster.local\u0026#34;] host = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-service.ims.svc.cluster.local\u0026#34; port = 22 [ssh_containers.connection_info.customer_access] host = \u0026#34;ad5163d2-398d-4e93-94f0-2f439f114fe7.ims.shasta.cray.com\u0026#34; port = 22   Create variables for the IMS job ID, Kubernetes job ID, and the SSH connection values in the returned data.\nBefore setting the SSH values, determine the appropriate method to SSH into the customization pod:\n [ssh_containers.connection_info.customer_access] values (preferred): The customer_access address is a dynamic hostname that is made available for use by the customer to access the IMS Job from outside the Kubernetes cluster. [ssh_containers.connection_info.\u0026quot;cluster.local\u0026quot;] values: The cluster.local address is used when trying to access an IMS Job from a pod that is running within the HPE Cray EX Kubernetes cluster. For example, this is the address that CFS uses to talk to the IMS Job during a pre-boot customization session.  The external IP address should only be used if the dynamic customer_access hostname does not resolve properly. In the following example, the admin could then SSH to the 10.103.2.160 IP address.\nncn# kubectl get services -n ims | grep IMS_JOB_ID NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE cray-ims-06c3dd57-f347-4229-85b3-1d024a947b3f-service LoadBalancer 10.29.129.204 10.103.2.160 22:31627/TCP 21h To create the variables:\nncn# export IMS_JOB_ID=ad5163d2-398d-4e93-94f0-2f439f114fe7 ncn# export IMS_KUBERNETES_JOB=cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-customize ncn# export IMS_SSH_HOST=ad5163d2-398d-4e93-94f0-2f439f114fe7.ims.shasta.cray.com ncn# export IMS_SSH_PORT=22 The IMS customization job enables customization of the image root via an SSH shell accessible by one or more dynamic host names. The user needs to know if they will SSH from inside or outside the Kubernetes cluster to determine which host name to use. Typically, customers access the system from outside the Kubernetes cluster using the Customer Access Network (CAN).\nUnder normal circumstances, IMS customization jobs will download and mount the rootfs for the specified IMS image under the /mnt/image/image-root directory within the SSH shell. After SSHing into the job container, cd or chroot into the /mnt/image/image-root directory in order to interact with the image root being customized.\nOptionally, IMS can be told to create a jailed SSH environment by specifying the --ssh-containers-jail True parameter.\nA jailed environment lets users SSH into the SSH container and be immediately within the image root for the image being customized. Users do not need to cd or chroot into the image root. Using a jailed environment has some advantages, such as making the IMS SSH job shell look more like a compute node. This allows applications like the CFS to perform actions on both IMS job pods (pre-boot) and compute nodes (post-boot).\n    Use kubectl and the returned IMS_KUBERNETES_JOB value to describe the image create job.\nncn# kubectl -n ims describe job $IMS_KUBERNETES_JOB Name: cray-ims-cfa864b3-4e08-49b1-9c57-04573228fd3f-customize Namespace: default ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulCreate 4m job-controller Created pod: cray-ims-cfa864b3-4e08-49b1-9c57-04573228fd3f-customize-xh2jf If successful, create a variable for the pod name using the value indicated in the output above, which will be used in future steps.\nncn# export POD=cray-ims-cfa864b3-4e08-49b1-9c57-04573228fd3f-customize-xh2jf   Verify that the status of the IMS job is waiting_on_user.\nncn# cray ims jobs describe $IMS_JOB_ID status = \u0026#34;waiting_on_user\u0026#34; enable_debug = false kernel_file_name = \u0026#34;vmlinuz\u0026#34; artifact_id = \u0026#34;4e78488d-4d92-4675-9d83-97adfc17cb19\u0026#34; build_env_size = 10 job_type = \u0026#34;customize\u0026#34; kubernetes_service = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-service\u0026#34; kubernetes_job = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-customize\u0026#34; id = \u0026#34;ad5163d2-398d-4e93-94f0-2f439f114fe7\u0026#34; image_root_archive_name = \u0026#34;my_customized_image\u0026#34; initrd_file_name = \u0026#34;initrd\u0026#34; created = \u0026#34;2018-11-21T18:22:53.409405+00:00\u0026#34; kubernetes_namespace = \u0026#34;ims\u0026#34; public_key_id = \u0026#34;a252ff6f-c087-4093-a305-122b41824a3e\u0026#34; kubernetes_configmap = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-configmap\u0026#34; [[ssh_containers]] status = \u0026#34;pending\u0026#34; jail = false name = \u0026#34;customize\u0026#34; [ssh_containers.connection_info.\u0026#34;cluster.local\u0026#34;] host = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-service.ims.svc.cluster.local\u0026#34; port = 22 [ssh_containers.connection_info.customer_access] host = \u0026#34;ad5163d2-398d-4e93-94f0-2f439f114fe7.ims.shasta.cray.com\u0026#34; port = 22   Customize the image in the image customization environment.\nOnce chrooted into the image root (or if using a `jailed` environment) during image customization, the image will only have access to whatever configuration the image already contains. In order to talk to services, including Nexus RPM repositories, the image root must first be configured with DNS and other settings. A base level of customization is provided by the default Ansible plays used by the CFS to enable DNS resolution.\n  Option 1: SSH to the image customization environment.\nThe image root is available under /mnt/image/image-root. For passwordless SSH to work, ensure that the correct public/private key pair is used. The private key will need to match the public key that was uploaded to the IMS service and specified in the IMS Job.\nIMPORTANT: The following command will work when run on any of the master nodes and worker nodes, except for ncn-w001.\nncn# ssh -p $IMS_SSH_PORT root@$IMS_SSH_HOST Last login: Tue Sep 4 18:06:27 2018 from gateway [root@POD ~]# Once connected to the IMS image customization shell, perform any customizations required. If the SSH shell was created without using the --ssh-containers-jail True parameter, cd or chroot into the image root. Refer to the following sections for examples of custom configurations:\n \u0026ldquo;Running ML Applications Using Singularity\u0026rdquo; in the Analytics product stream documentation. \u0026ldquo;Customize an Image Root to Install Compute Kubernetes\u0026rdquo; in the Cray Operating System (COS) product stream documentation.  After changes have been made, run the touch command on the complete file. The location of the complete file depends on whether or not the SSH job shell was created using the --ssh-containers-jail True parameter. See the table below for more information.\n   \u0026ndash;ssh-containers-jail Command used to create the complete file     False (default) touch /mnt/image/complete   True touch /tmp/complete    [root@POD image]# cd /mnt/image/ [root@POD image]# chroot image-root/ :/ # (do touch complete flag) :/ # exit [root@POD image]# When the complete file has been created, the following actions will occur:\n The job SSH container will close any active SSH connections The buildenv-sidecar container will compresses the image root The customized artifacts will be uploaded to S3 and associated with a new IMS image record    Option 2: Use Ansible to run playbooks against the image root.\nncn# ansible all -i $IMS_SSH_HOST, -m ping --ssh-extra-args \u0026#34; -p $IMS_SSH_PORT-i ./pod_rsa_key \\ -o StrictHostKeyChecking=no\u0026#34; -u root ad5163d2-398d-4e93-94f0-2f439f114fe7.ims.shasta.cray.com | SUCCESS =\u0026gt; { \u0026#34;changed\u0026#34;: false, \u0026#34;ping\u0026#34;: \u0026#34;pong\u0026#34; } This Ansible inventory file below can also be used. The private key (./pod_rsa_key) corresponds to the public key file the container has in its authorized_keys file.\nmyimage-customize ansible_user=root ansible_host=ad5163d2-398d-4e93-94f0-2f439f114fe7.ims.shasta.cray.com ansible_port=22 \\  ansible_ssh_private_key_file=./pod_rsa_key ansible_ssh_common_args=\u0026#39;-o \\ StrictHostKeyChecking=no\u0026#39; A sample playbook can be run on the image root:\n--- # The playbook creates a new database test and populates data in the database to test the sharding. - hosts: all remote_user: root tasks: - name: Look at the image root command: \u0026#34;ls -l /mnt/image/image-root\u0026#34; - name: chroot and run dracut command: \u0026#34;chroot /mnt/image/image-root dracut --force --kver 4.4.143-94.47-default\u0026#34; - name: example copying file with owner and permissions copy: src: sample_playbook.yml dest: /mnt/image/image-root/tmp - name: Exit the build container copy: src: nothing_file dest: /mnt/image/complete The sample playbook can be run with the following command:\nncn# ansible-playbook -i ./inventory.ini sample_playbook.yml     Tail the buildenv-sidecar to ensure that any artifacts are properly uploaded to S3 and associated with IMS.\nncn# kubectl -n ims logs -f $POD -c buildenv-sidecar + python -m ims_python_helper image upload_artifacts sles15_barebones_image 7de80ccc-1e7d-43a9-a6e4-02cad10bb60b -v -r /mnt/image/sles15_barebones_image.sqsh -k /mnt/image/image-root/boot/vmlinuz -i /mnt/image/image-root/boot/initrd { \u0026#34;ims_image_artifacts\u0026#34;: [ { \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;4add976679c7e955c4b16d7e2cfa114e-32\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/d88521c3-b339-43bc-afda-afdfda126388/rootfs\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;md5\u0026#34;: \u0026#34;94165af4373e5ace3e817eb4baba2284\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;application/vnd.cray.image.rootfs.squashfs\u0026#34; }, { \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;f836412241aae79d160556ed6a4eb4d4\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/d88521c3-b339-43bc-afda-afdfda126388/kernel\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;md5\u0026#34;: \u0026#34;f836412241aae79d160556ed6a4eb4d4\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;application/vnd.cray.image.kernel\u0026#34; }, { \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;ec8793c07f94e59a2a30abdb1bd3d35a-4\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/d88521c3-b339-43bc-afda-afdfda126388/initrd\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;md5\u0026#34;: \u0026#34;86832ee3977ca0515592e5d00271d2fe\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;application/vnd.cray.image.initrd\u0026#34; }, { \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;13af343f3e76b0f8c7fbef7ee3588ac1\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/d88521c3-b339-43bc-afda-afdfda126388/manifest.json\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;md5\u0026#34;: \u0026#34;13af343f3e76b0f8c7fbef7ee3588ac1\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;application/json\u0026#34; } ], \u0026#34;ims_image_record\u0026#34;: { \u0026#34;created\u0026#34;: \u0026#34;2018-12-17T22:59:43.264129+00:00\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;d88521c3-b339-43bc-afda-afdfda126388\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;sles15_barebones_image\u0026#34; \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;13af343f3e76b0f8c7fbef7ee3588ac1\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/d88521c3-b339-43bc-afda-afdfda126388/manifest.json\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, }, \u0026#34;ims_job_record\u0026#34;: { \u0026#34;artifact_id\u0026#34;: \u0026#34;2233c82a-5081-4f67-bec4-4b59a60017a6\u0026#34;, \u0026#34;build_env_size\u0026#34;: 10, \u0026#34;created\u0026#34;: \u0026#34;2018-11-21T18:22:53.409405+00:00\u0026#34;, \u0026#34;enable_debug\u0026#34;: false, \u0026#34;id\u0026#34;: \u0026#34;ad5163d2-398d-4e93-94f0-2f439f114fe7\u0026#34;, \u0026#34;image_root_archive_name\u0026#34;: \u0026#34;sles15_barebones_image\u0026#34;, \u0026#34;initrd_file_name\u0026#34;: \u0026#34;initrd\u0026#34;, \u0026#34;job_type\u0026#34;: \u0026#34;create\u0026#34;, \u0026#34;kernel_file_name\u0026#34;: \u0026#34;vmlinuz\u0026#34;, \u0026#34;kubernetes_configmap\u0026#34;: \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-configmap\u0026#34;, \u0026#34;kubernetes_job\u0026#34;: \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-create\u0026#34;, \u0026#34;kubernetes_service\u0026#34;: \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-service\u0026#34;, \u0026#34;public_key_id\u0026#34;: \u0026#34;a252ff6f-c087-4093-a305-122b41824a3e\u0026#34;, \u0026#34;resultant_image_id\u0026#34;: \u0026#34;d88521c3-b339-43bc-afda-afdfda126388\u0026#34;, \u0026#34;ssh_port\u0026#34;: 0, \u0026#34;status\u0026#34;: \u0026#34;packaging_artifacts\u0026#34; }, \u0026#34;result\u0026#34;: \u0026#34;success\u0026#34; } The IMS customization workflow automatically copies the NCN Certificate Authority\u0026rsquo;s public certificate to /etc/cray/ca/certificate_authority.crt within the image root being customized. This can be used to enable secure communications between the NCN and the client node.\n  Look up the ID of the newly created image.\nncn# cray ims jobs describe $IMS_JOB_ID status = \u0026#34;success\u0026#34; enable_debug = false kernel_file_name = \u0026#34;vmlinuz\u0026#34; artifact_id = \u0026#34;4e78488d-4d92-4675-9d83-97adfc17cb19\u0026#34; build_env_size = 10 job_type = \u0026#34;customize\u0026#34; kubernetes_service = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-service\u0026#34; kubernetes_job = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-customize\u0026#34; id = \u0026#34;ad5163d2-398d-4e93-94f0-2f439f114fe7\u0026#34; image_root_archive_name = \u0026#34;my_customized_image\u0026#34; initrd_file_name = \u0026#34;initrd\u0026#34; resultant_image_id = \u0026#34;d88521c3-b339-43bc-afda-afdfda126388\u0026#34; created = \u0026#34;2018-11-21T18:22:53.409405+00:00\u0026#34; kubernetes_namespace = \u0026#34;ims\u0026#34; public_key_id = \u0026#34;a252ff6f-c087-4093-a305-122b41824a3e\u0026#34; kubernetes_configmap = \u0026#34;cray-ims-ad5163d2-398d-4e93-94f0-2f439f114fe7-configmap\u0026#34; If successful, create a variable for the IMS resultant_image_id value in the returned data.\nncn# export IMS_RESULTANT_IMAGE_ID=d88521c3-b339-43bc-afda-afdfda126388   Verify the new IMS image record exists.\nncn# cray ims images describe $IMS_RESULTANT_IMAGE_ID created = \u0026#34;2018-12-04T17:25:52.482514+00:00\u0026#34; id = \u0026#34;d88521c3-b339-43bc-afda-afdfda126388\u0026#34; name = \u0026#34;my_customized_image.squashfs\u0026#34; [link] type = \u0026#34;s3\u0026#34; path = \u0026#34;/d88521c3-b339-43bc-afda-afdfda126388/my_customized_image.squashfs\u0026#34; etag = \u0026#34;28f3d78c8cceca2083d7d3090d96bbb7\u0026#34;   Clean Up the Image Customization Environment\n Delete the IMS job record.\nncn# cray ims jobs delete $IMS_JOB_ID Deleting the job record also deletes the underlying Kubernetes job, service, and ConfigMap that were created when the job record was submitted.\n  The image root has been modified, compressed, and uploaded to S3, along with its associated initrd and kernel files. The image customization environment has also been cleaned up.\n"
},
{
	"uri": "/docs-csm/en-11/operations/image_management/delete_or_recover_deleted_ims_content/",
	"title": "Delete Or Recover Deleted IMS Content",
	"tags": [],
	"description": "",
	"content": "Delete or Recover Deleted IMS Content The Image Management System (IMS) manages user supplied SSH public Keys, customizable image recipes, images, and IMS jobs that are used to build or customize images. In previous versions of IMS, deleting an IMS public key, recipe, or image resulted in that item being permanently deleted. Additionally, IMS recipes and images store linked artifacts in the Simple Storage Service (S3) datastore. These artifacts are referenced by the IMS recipe and image records. The default option when deleting an IMS recipe and image record was to also delete these linked S3 artifacts.\nncn# cray ims recipes list ... [[results]] id = \u0026#34;76ef564d-47d5-415a-bcef-d6022a416c3c\u0026#34; name = \u0026#34;cray-sles15-barebones\u0026#34; created = \u0026#34;2020-02-05T19:24:22.621448+00:00\u0026#34; [results.link] path = \u0026#34;s3://ims/recipes/76ef564d-47d5-415a-bcef-d6022a416c3c/cray-sles15-barebones.tgz\u0026#34; etag = \u0026#34;28f3d78c8cceca2083d7d3090d96bbb7\u0026#34; type = \u0026#34;s3\u0026#34; ... ncn# cray ims images list ... [[results]] created = \u0026#34;2018-12-04T17:25:52.482514+00:00\u0026#34; id = \u0026#34;4e78488d-4d92-4675-9d83-97adfc17cb19\u0026#34; name = \u0026#34;sles_15_image.squashfs\u0026#34; [results.link] type = \u0026#34;s3\u0026#34; path = \u0026#34;/4e78488d-4d92-4675-9d83-97adfc17cb19/sles_15_image.squashfs\u0026#34; etag = \u0026#34; ... Deleting an IMS image can create a situation where boot artifacts referenced by a Boot Orchestration Service (BOS) session template no longer exist, making that template unable to boot. Previously, to recover from this situation, an admin would have had to rebuild the boot image using IMS and/or reinstall the prebuilt image from the installer, reapply any Cray and site customizations, and recreate a new BOS template for the IMS image.\nNew functionality has been added to IMS to enable administrators to soft delete, recover (undelete), or hard delete public-keys, recipes, and images. The added functionality provides a way to recover IMS items that were mistakenly deleted. There is no undelete operation for IMS Jobs.\nSoft deleting an IMS record effectively removes the record being deleted from the default collection, and moves it to a new deleted collection. Recovering a deleted IMS record (undelete operation) moves the IMS record from the deleted collection back to the collection of available items. Hard deleting an IMS record permanently deletes it from the deleted collection.\nDelete an IMS Artifact Use the cray CLI utility to delete either soft delete or hard delete an IMS public-key, recipe, or image.\nSoft deleting an IMS public key, recipe, or image removes the record(s) from the collection of available items. Hard deleting permanently removes the item from the deleted collection. Additionally, any linked artifacts are also permanently removed.\nDeleting an IMS public-key, recipe, or image record performs the following actions:\n The IMS record(s) being deleted are moved from the collection of available items to a new deleted collection. Any newly created records within the deleted collection will have the same IMS ID value as it did before being moved there. Any Simple Storage Service (S3) artifacts that are associated with the record or records being deleted are renamed within their S3 buckets so as to make them unavailable under their original key name.  Prerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system. System management services (SMS) are running in a Kubernetes cluster on non-compute nodes (NCN) and include the following deployments:  cray-ims, the Image Management Service (IMS) cray-nexus, the Nexus repository manager service   kubectl is installed locally and configured to point at the SMS Kubernetes cluster.  Procedure   Soft delete the desired IMS artifact.\nThe following substeps assume that an image is being deleted. The same process can be followed if deleting a public-key or recipe.\n  List the existing images in IMS.\nncn# cray ims images list ... [[results]] created = \u0026#34;2018-12-04T17:25:52.482514+00:00\u0026#34; id = \u0026#34;4e78488d-4d92-4675-9d83-97adfc17cb19\u0026#34; \u0026lt;\u0026lt;-- Note this ID name = \u0026#34;sles_15_image.squashfs\u0026#34; [results.link] type = \u0026#34;s3\u0026#34; path = \u0026#34;/4e78488d-4d92-4675-9d83-97adfc17cb19/sles_15_image.squashfs\u0026#34; etag = \u0026#34; ...   Delete the image.\nncn# cray ims images delete IMS_IMAGE_ID   Verify the image was successfully deleted.\nncn# cray ims images list   View the recently deleted item in the deleted images list.\nncn# cray ims deleted images list ... [[results]] created = \u0026#34;2018-12-04T17:25:52.482514+00:00\u0026#34; \u0026lt;\u0026lt;-- Date the record was originally created deleted = \u0026#34;2020-11-03T09:57:31.746521+00:00\u0026#34; \u0026lt;\u0026lt;-- Date the record was deleted id = \u0026#34;4e78488d-4d92-4675-9d83-97adfc17cb19\u0026#34; name = \u0026#34;sles_15_image.squashfs\u0026#34; [results.link] type = \u0026#34;s3\u0026#34; path = \u0026#34;/deleted/4e78488d-4d92-4675-9d83-97adfc17cb19/sles_15_image.squashfs\u0026#34; \u0026lt;\u0026lt;-- S3 path to linked artifact was renamed etag = \u0026#34; ... If the admin desires the public-key, recipe, or image to be permanently deleted, proceed to the next step. If the deleted image might need to be recovered in the future, no more work is needed.\n    Hard delete the desired IMS artifact.\nDo not proceed with this step if the IMS artifact might be needed in the future. The following substeps assume that an image is being deleted. The same process can be followed if deleting a public-key or recipe.\n  List the deleted images.\nncn# cray ims deleted images list ... [[results]] created = \u0026#34;2018-12-04T17:25:52.482514+00:00\u0026#34; deleted = \u0026#34;2020-11-03T09:57:31.746521+00:00\u0026#34; id = \u0026#34;4e78488d-4d92-4675-9d83-97adfc17cb19\u0026#34; \u0026lt;\u0026lt;-- Note this ID name = \u0026#34;sles_15_image.squashfs\u0026#34; [results.link] type = \u0026#34;s3\u0026#34; path = \u0026#34;/deleted/4e78488d-4d92-4675-9d83-97adfc17cb19/sles_15_image.squashfs\u0026#34; etag = \u0026#34; ...   Permanently delete the desired image from the deleted images list.\nncn# cray ims deleted images delete IMS_IMAGE_ID     Recover Deleted IMS Artifacts Use the IMS undelete command to update the record(s) within the deleted collection for an IMS public-key, recipe, or image.\nRecovering a deleted IMS public-key, recipe, or image record uses the following workflow:\n The record(s) being undeleted are moved to from the deleted collection to the collection of available items. Any restored records will have the same IMS ID value as it did before being undeleted. Any Simple Storage Service (S3) artifacts that are associated with the record(s) being undeleted are renamed within their S3 buckets so as to make them available under their original key name.  Prerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system. System management services (SMS) are running in a Kubernetes cluster on non-compute nodes (NCN) and include the following deployments:  cray-ims, the Image Management Service (IMS) cray-nexus, the Nexus repository manager service   kubectl is installed locally and configured to point at the SMS Kubernetes cluster.  Procedure The steps in this procedure assume that a deleted image is being recovered. The same process can be followed if recovering a deleted public-key or recipe.\n  List the deleted image.\nncn# cray ims deleted images list ... [[results]] created = \u0026#34;2018-12-04T17:25:52.482514+00:00\u0026#34; deleted = \u0026#34;2020-11-03T09:57:31.746521+00:00\u0026#34; id = \u0026#34;4e78488d-4d92-4675-9d83-97adfc17cb19\u0026#34; \u0026lt;\u0026lt;-- Note this ID name = \u0026#34;sles_15_image.squashfs\u0026#34; [results.link] type = \u0026#34;s3\u0026#34; path = \u0026#34;/deleted/4e78488d-4d92-4675-9d83-97adfc17cb19/sles_15_image.squashfs\u0026#34; etag = \u0026#34; ...   Use the undelete operation to recover the image.\nncn# cray ims deleted images update IMS_IMAGE_ID --operation undelete   List the deleted images to verify the recovered image is no longer in the collection of deleted items.\nncn# cray ims deleted images list   List the IMS images to verify the image was recovered.\nncn# cray ims images list ... [[results]] created = \u0026#34;2018-12-04T17:25:52.482514+00:00\u0026#34; id = \u0026#34;4e78488d-4d92-4675-9d83-97adfc17cb19\u0026#34; name = \u0026#34;sles_15_image.squashfs\u0026#34; [results.link] type = \u0026#34;s3\u0026#34; path = \u0026#34;/4e78488d-4d92-4675-9d83-97adfc17cb19/sles_15_image.squashfs\u0026#34; \u0026lt;\u0026lt;-- The restored artifact path etag = \u0026#34; ...   "
},
{
	"uri": "/docs-csm/en-11/operations/hmcollector/adjust_hmcollector_resource_limits_requests/",
	"title": "Adjust Hm Collector Resource Limits And Requests",
	"tags": [],
	"description": "",
	"content": "Adjust HM Collector resource limits and requests  Resource Limit Tuning Guidance Customize cray-hms-hmcollector resource limits and requests in customizations.yaml Redeploy cray-hms-hmcollector with new resource limits and requests  Resource Limit Tuning Guidance Inspect current resource usage in the cray-hms-hmcollector pod View resource usage of the containers in the cray-hms-hmcollector pod:\nncn-m001# kubectl -n services top pod -l app.kubernetes.io/name=cray-hms-hmcollector --containers POD NAME CPU(cores) MEMORY(bytes) cray-hms-hmcollector-7c5b797c5c-zxt67 istio-proxy 187m 275Mi cray-hms-hmcollector-7c5b797c5c-zxt67 cray-hms-hmcollector 4398m 296Mi The default resource limits for the cray-hms-hmcollector container are:\n CPU: 4 or 4000m Memory: 5Gi  The default resource limits for the istio-proxy container are:\n CPU: 2 or 2000m Memory: 1Gi  Inspect the cray-hms-hmcollector pod for OOMKilled events Describe the collector-hms-hmcollector pod to determine if it has been OOMKilled in the recent past:\nncn-m001# kubectl -n services describe pod -l app.kubernetes.io/name=cray-hms-hmcollector Look for the cray-hms-hmcollector container and check its Last State (if present) to see if the container has been perviously terminated due to it running out of memory:\n... Containers: cray-hms-hmcollector: Container ID: containerd://a35853bacdcea350e70c57fe1667b5b9d3c82d41e1e7c1f901832bae97b722fb Image: dtr.dev.cray.com/cray/hms-hmcollector:2.10.6 Image ID: dtr.dev.cray.com/cray/hms-hmcollector@sha256:b043617f83b9ff7e542e56af5bbf47f4ca35876f83b5eb07314054726c895b08 Ports: 80/TCP, 443/TCP Host Ports: 0/TCP, 0/TCP State: Running Started: Tue, 21 Sep 2021 20:52:13 +0000 Last State: Terminated Reason: OOMKilled Exit Code: 137 Started: Tue, 21 Sep 2021 20:51:08 +0000 Finished: Tue, 21 Sep 2021 20:52:12 +0000 ...  In the above example output the cray-hms-hmcollector container was perviously OOMKilled, but the container is currently running.\n Look for the isitio-proxy container and check its Last State (if present) to see if the container has been perviously terminated due to it running out of memory:\n... istio-proxy: Container ID: containerd://f439317c16f7db43e87fbcec59b7d36a0254dabd57ab71865d9d7953d154bb1a Image: dtr.dev.cray.com/cray/proxyv2:1.7.8-cray1 Image ID: dtr.dev.cray.com/cray/proxyv2@sha256:8f2bccd346381e0399564142f9534c6c76d8d0b8bd637e9440d53bf96a9d86c7 Port: 15090/TCP Host Port: 0/TCP Args: proxy sidecar --domain $(POD_NAMESPACE).svc.cluster.local --serviceCluster cray-hms-hmcollector.services --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --trust-domain=cluster.local --concurrency 2 State: Running Started: Tue, 21 Sep 2021 20:51:09 +0000 Last State: Terminated Reason: OOMKilled Exit Code: 137 Started: Tue, 21 Sep 2021 20:51:08 +0000 Finished: Tue, 21 Sep 2021 20:52:12 +0000 ...  In the above example output the istio-proxy container was perviously OOMKilled, but the container is currently running.\n How to adjust CPU and Memory limits If the cray-hms-hmcollector container is hitting its CPU limit and memory usage is steadily increasing till it gets OOMKilled, then the CPU limit for the cray-hms-hmcollector should be increased. It can be increased in increments of 8 or 8000m This is a situation were the collector is unable to process events fast enough and they start to collect build up inside of it.\nIf the cray-hms-hmcollector container is consistency hitting its CPU limit, then its CPU limit should be increased. It can be increased in increments of 8 or 8000m.\nIf the cray-hms-hmcollector container is consistency hitting its memory limit, then its memory limit should be increased. It can be increased in increments of 5Gi.\nIf the istio-proxy container is getting OOMKilled, then its memory limit should be increased in increments of 5 Gigabytes (5Gi) at a time.\nOtherwise, if the cray-hms-hmcollector and istio-proxy containers are not hitting their CPU or memory limits\nFor reference, on a system with 4 fully populated liquid cooled cabinets the cray-hms-hmcollector was consuming ~5 or ~5000m of CPU and ~300Mi of memory.\nCustomize cray-hms-hmcollector resource limits and requests in customizations.yaml   If the site-init repository is available as a remote repository then clone it on the host orchestrating the upgrade:\nncn-m001# git clone \u0026#34;$SITE_INIT_REPO_URL\u0026#34; site-init Otherwise, create a new site-init working tree:\nncn-m001# git init site-init   Download customizations.yaml:\nncn-m001# kubectl get secrets -n loftsman site-init -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | base64 -d \u0026gt; site-init/customizations.yaml   Review, add, and commit customizations.yaml to the local site-init repository as appropriate.\n NOTE: If site-init was cloned from a remote repository in step 1, there may not be any differences and hence nothing to commit. This is okay. If there are differences between what is in the repository and what was stored in the site-init, then it suggests settings were improperly changed at some point. If that is the case then be cautious, there may be dragons ahead.\n ncn-m001# cd site-init ncn-m001# git diff ncn-m001# git add customizations.yaml ncn-m001# git commit -m \u0026#39;Add customizations.yaml from site-init secret\u0026#39;   Update customizations.yaml with the existing cray-hms-hmcollector resource limits and requests settings:\nPersist resource requests and limits from the cray-hms-hmcollector deployment:\nncn-m001# kubectl -n services get deployments cray-hms-hmcollector \\  -o jsonpath=\u0026#39;{.spec.template.spec.containers[].resources}\u0026#39; | yq r -P - | \\  yq w -f - -i ./customizations.yaml spec.kubernetes.services.cray-hms-hmcollector.resources Persist annotations manually added to cray-hms-hmcollector deployment:\nncn-m001# kubectl -n services get deployments cray-hms-hmcollector \\  -o jsonpath=\u0026#39;{.spec.template.metadata.annotations}\u0026#39; | \\  yq d -P - \u0026#39;\u0026#34;traffic.sidecar.istio.io/excludeOutboundPorts\u0026#34;\u0026#39; | \\  yq w -f - -i ./customizations.yaml spec.kubernetes.services.cray-hms-hmcollector.podAnnotations View the updated overrides added to customizations.yaml. If the value overrides look different to the sample output below then the resource limits and requests have been manually modified in the past.\nncn-m001# yq r ./customizations.yaml spec.kubernetes.services.cray-hms-hmcollector hmcollector_external_ip: \u0026#39;{{ network.netstaticips.hmn_api_gw }}\u0026#39; resources: limits: cpu: \u0026#34;4\u0026#34; memory: 5Gi requests: cpu: 500m memory: 256Mi podAnnotations: {}   If desired adjust the resource limits and requests for the cray-hms-hmcollector. Otherwise this step can be skipped. Refer to Resource Limit Tuning Guidance for information on how the resource limits could be adjusted.\nEdit customizations.yaml and the value overrides for the cray-hms-hmcollector Helm chart are defined at spec.kubernetes.services.cray-hms-hmcollector\nAdjust the resource limits and requests for the cray-hms-hmcollector deployment in customizations.yaml:\ncray-hms-hmcollector: hmcollector_external_ip: \u0026#39;{{ network.netstaticips.hmn_api_gw }}\u0026#39; resources: limits: cpu: \u0026#34;4\u0026#34; memory: 5Gi requests: cpu: 500m memory: 256Mi To specify a non-default memory limit for the Istio proxy used by the cray-hms-hmcollector to pod annotation sidecar.istio.io/proxyMemoryLimit can added under podAnnotations. By default the Istio proxy memory limit is 1Gi.\ncray-hms-hmcollector: podAnnotations: sidecar.istio.io/proxyMemoryLimit: 5Gi   Review the changes to customizations.yaml and verify baseline system customizations and any customer-specific settings are correct.\nncn-m001# git diff   Add and commit customizations.yaml if there are any changes:\nncn-m001# git add customizations.yaml ncn-m001# git commit -m \u0026quot;Update customizations.yaml consistent with CSM $CSM_RELEASE_VERSION\u0026quot;   Update site-init sealed secret in loftsman namespace:\nncn-m001# kubectl delete secret -n loftsman site-init ncn-m001# kubectl create secret -n loftsman generic site-init --from-file=customizations.yaml   Push to the remote repository as appropriate:\nncn-m001# git push   If this document was referenced during an upgrade procure, then skip Otherwise, continue on to Redeploy cray-hms-hmcollector with new resource limits and requests for the the new resource limits and requests to take effect.\n  Redeploy cray-hms-hmcollector with new resource limits and requests   Determine the version of HM Collector:\nncn-m001# HMCOLLECTOR_VERSION=$(kubectl -n loftsman get cm loftsman-sysmgmt -o jsonpath=\u0026#39;{.data.manifest\\.yaml}\u0026#39; | yq r - \u0026#39;spec.charts.(name==cray-hms-hmcollector).version\u0026#39;) ncn-m001# echo $HMCOLLECTOR_VERSION   Create hmcollector-manifest.yaml:\nncn-m001# cat \u0026gt; hmcollector-manifest.yaml \u0026lt;\u0026lt; EOF apiVersion: manifests/v1beta1 metadata: name: hmcollector spec: charts: - name: cray-hms-hmcollector version: $HMCOLLECTOR_VERSION namespace: services EOF   Acquire customizations.yaml:\nncn-m001# kubectl get secrets -n loftsman site-init -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | base64 -d \u0026gt; customizations.yaml   Merge customizations.yaml with hmcollector-manifest.yaml:\nncn-m001# manifestgen -c customizations.yaml -i ./hmcollector-manifest.yaml \u0026gt; ./hmcollector-manifest.out.yaml   Redeploy the HM Collector helm chart:\nncn-m001# loftsman ship \\  --charts-repo https://packages.local/repository/charts \\  --manifest-path hmcollector-manifest.out.yaml   "
},
{
	"uri": "/docs-csm/en-11/operations/image_management/build_a_new_uan_image_using_the_default_recipe/",
	"title": "Build A New UAN Image Using The Default Recipe",
	"tags": [],
	"description": "",
	"content": "Build a New UAN Image Using the Default Recipe Build or rebuild the UAN image using either the default UAN image or image recipe. Both of these are supplied by the User Access Node (UAN) product stream installer.\nPrerequisites  Both the Cray Operation System (COS) and UAN product streams must be installed. The Cray administrative CLI must be initialized.  Procedure The Cray EX User Access Node (UAN) recipe currently requires the Slingshot Diagnostics package, which is not installed with the UAN product itself. Therefore, the UAN recipe can only be built after either the Slingshot product is installed, or the Slingshot Diagnostics package is removed from the recipe.\n  Determine if the Slingshot product stream is installed on the HPE Cray EX system.\nThe Slingshot Diagnostics RPM must be removed from the default recipe if the Slingshot product is not installed.\n  Modify the default UAN recipe to remove the Slingshot diagnostic package. Skip this step if the Slingshot package is installed.\n  Perform Upload and Register an Image Recipe to download and extract the UAN image recipe, cray-sles15sp1-uan-cos, but stop before the step that modifies the recipe.\n  Open the file config-template.xml.j2 within the recipe for editing and remove these lines:\n\u0026lt;!-- SECTION: Slingshot Diagnostic package --\u0026gt; \u0026lt;package name=\u0026#34;cray-diags-fabric\u0026#34;/\u0026gt;   Resume the procedure Upload and Register an Image Recipe, starting with the step that locates the directory that contains the Kiwi-NG image description files.\nThe next substep requires the id of the new image recipe record.\n  Perform the procedure Build an Image Using IMS REST Service to build the UAN image from the modified recipe. Use the id of the new image recipe.\nSkip the remaining steps of this current procedure.\n    (Optional) Build the UAN image using IMS. Skip this step to build the UAN image manually.\n  Identify the UAN image recipe.\nncn-m001# cray ims recipes list --format json | jq \u0026#39;.[] | select(.name | contains(\u0026#34;uan\u0026#34;))\u0026#39; { \u0026#34;created\u0026#34;: \u0026#34;2021-02-17T15:19:48.549383+00:00\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;4a5d1178-80ad-4151-af1b-bbe1480958d1\u0026#34;, \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;3c3b292364f7739da966c9cdae096964\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://ims/recipes/4a5d1178-80ad-4151-af1b-bbe1480958d1/recipe.tar.gz\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;linux_distribution\u0026#34;: \u0026#34;sles15\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;cray-shasta-uan-cos-sles15sp1.x86_64-@product_version@\u0026#34;, \u0026#34;recipe_type\u0026#34;: \u0026#34;kiwi-ng\u0026#34; }   Save the ID of the IMS recipe in an environment variable.\nncn-m001# export IMS_RECIPE_ID=4a5d1178-80ad-4151-af1b-bbe1480958d1   Use the saved IMS recipe id in the procedure Build an Image Using IMS REST Service to build the UAN image.\n    (Optional) Build the UAN image by customizing it manually. Skip this step if the UAN image was built automatically in the previous step.\n  Identify the base UAN image to customize.\nncn-m001# cray ims images list --format json | jq \u0026#39;.[] | select(.name | contains(\u0026#34;uan\u0026#34;))\u0026#39; { \u0026#34;created\u0026#34;: \u0026#34;2021-02-18T17:17:44.168655+00:00\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;6d46d601-c41f-444d-8b49-c9a2a55d3c21\u0026#34;, \u0026#34;link\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;371b62c9f0263e4c8c70c8602ccd5158\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/6d46d601-c41f-444d-8b49-c9a2a55d3c21/manifest.json\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;uan-PRODUCT_VERSION-image\u0026#34; }   Save the ID of the IMS image in an environment variable.\nncn-m001# export IMS_IMAGE_ID=4a5d1178-80ad-4151-af1b-bbe1480958d1   Use the saved IMS image ID in the Customize an Image Root Using IMS procedure to build the UAN image.\n    "
},
{
	"uri": "/docs-csm/en-11/operations/hardware_state_manager/manage_hms_locks/",
	"title": "Manage Hms Locks",
	"tags": [],
	"description": "",
	"content": "Manage HMS Locks This section describes how to check the status of a lock, disable reservations, and repair reservations. The disable and repair operations only affect the ability to make reservations on hardware devices.\nSome of the common scenarios an admin might encounter when working with the Hardware State Manager (HSM) Locking API are also described.\nCheck Lock Status Use the following command to verify if an xname is locked or not. The command will show if its locked (admin), reserved (service command), or reservation disabled (either an EPO or an admin command).\nThe following shows how to interpret the output:\n Locked: Shows if the xname has been locked with the cray hsm locks lock create command. Reserved: Shows if the xname has been locked for a time-boxed event. Only service can reserve xnames; administrators are not able to reserve xnames. ReservationDisable: Shows if the ability to reserved an xname has been changed by an EPO or admin command.  ncn-m001# cray hsm locks status create --component-ids x1003c5s2b1n1 NotFound = [] [[Components]] ID = \u0026#34;x1003c5s2b1n1\u0026#34; Locked = false Reserved = false ReservationDisabled = false Disable Reservations Disabling a lock prevents a service from being able to make a reservation on it, and it releases/ends any current reservations. Even though SMD removes the reservation when disabling a lock, it does not mean that the Firmware Action Service (FAS) is aware that it has lost the reservation. Additionally, if CAPMC has a reservation that is cancelled, disabled, or broken, it will do nothing to the existing CAPMC operation. There are no checks by CAPMC to make sure things are still reserved at any time during a power operation.\nThis is a way to stop new operations from happening, not a way to prevent currently executing operations.\nncn-m001# cray hsm locks disable create --component-ids x1003c5s2b1n1 Failure = [] [Counts] Total = 1 Success = 1 Failure = 0 [Success] ComponentIDs = [ \u0026#34;x1003c5s2b1n1\u0026#34;,] The following is an example of a when a lock is disabled:\nncn-m001# cray hsm locks status create --component-ids x1003c5s2b1n1 NotFound = [] [[Components]] ID = \u0026#34;x1003c5s2b1n1\u0026#34; Locked = false Reserved = false ReservationDisabled = true Repair Reservations Locks must be manually repaired after disabling a component or performing a manual EPO. This prevents the system from automatically re-issuing reservations or giving out lock requests.\nncn-m001# cray hsm locks repair create --component-ids x1003c5s2b1n1 Failure = [] [Counts] Total = 1 Success = 1 Failure = 0 [Success] ComponentIDs = [ \u0026#34;x1003c5s2b1n1\u0026#34;,] To verify if the lock was successfully repaired:\nncn-m001# cray hsm locks status create --component-ids x1003c5s2b1n1 NotFound = [] [[Components]] ID = \u0026#34;x1003c5s2b1n1\u0026#34; Locked = false Reserved = false ReservationDisabled = false Scenario: What Happens to a Lock if a disable is Issued? Before issuing a disable command, verify that a lock is already in effect:\nncn-m001# cray hsm locks lock create --component-ids x1003c5s2b1n1 Failure = [] [Counts] Total = 1 Success = 1 Failure = 0 [Success] ComponentIDs = [ \u0026#34;x1003c5s2b1n1\u0026#34;,] ncn-m001# cray hsm locks status create --component-ids x1003c5s2b1n1 NotFound = [] [[Components]] ID = \u0026#34;x1003c5s2b1n1\u0026#34; Locked = true Reserved = false ReservationDisabled = false When attempting to disable, the lock will stay in effect, but the reservation ability will be disabled. For example:\nncn-m001# cray hsm locks disable create --component-ids x1003c5s2b1n1 Failure = [] [Counts] Total = 1 Success = 1 Failure = 0 [Success] ComponentIDs = [ \u0026#34;x1003c5s2b1n1\u0026#34;,] ncn-m001# cray hsm locks status create --component-ids x1003c5s2b1n1 NotFound = [] [[Components]] ID = \u0026#34;x1003c5s2b1n1\u0026#34; Locked = true Reserved = false ReservationDisabled = true Scenario: Can a lock be Issued to a Currently Locked Component? A lock cannot be issued to a component that is already locked. The following example shows a component that is already locked, and the returned error message when trying to lock the component again.\nncn-m001# cray hsm locks status create --component-ids x1003c5s2b1n1 NotFound = [] [[Components]] ID = \u0026#34;x1003c5s2b1n1\u0026#34; Locked = true \u0026lt;\u0026lt;-- xname is locked Reserved = false ReservationDisabled = true ncn-m001# cray hsm locks lock create --component-ids x1003c5s2b1n1 Usage: cray hsm locks lock create [OPTIONS] Try \u0026#39;cray hsm locks lock create --help\u0026#39; for help. Error: Bad Request: Component is Locked "
},
{
	"uri": "/docs-csm/en-11/operations/hardware_state_manager/restore_hsm_postgres_from_backup/",
	"title": "Restore Hardware State Manager (HSM) Postgres Database From Backup",
	"tags": [],
	"description": "",
	"content": "Restore Hardware State Manager (HSM) Postgres Database from Backup This procedure can be used to restore the HSM Postgres database from a previously taken backup. This can be a manual backup created by the Create a Backup of the HSM Postgres Database procedure, or an automatic backup created by the cray-smd-postgresql-db-backup Kubernetes cronjob.\nPrerequisites   Healthy System Layout Service (SLS). Recovered first if also affected.\n  Healthy HSM Postgres Cluster.\nUse patronictl list on the HSM Postgres cluster to determine the current state of the cluster, and a healthy cluster will look similar to the following:\nncn# kubectl exec cray-smd-postgres-0 -n services -c postgres -it -- patronictl list + Cluster: cray-smd-postgres (6975238790569058381) ---+----+-----------+ | Member | Host | Role | State | TL | Lag in MB | +---------------------+------------+--------+---------+----+-----------+ | cray-smd-postgres-0 | 10.44.0.40 | Leader | running | 1 | | | cray-smd-postgres-1 | 10.36.0.37 | | running | 1 | 0 | | cray-smd-postgres-2 | 10.42.0.42 | | running | 1 | 0 | +---------------------+------------+--------+---------+----+-----------+   Previously taken backup of the HSM Postgres cluster either a manual or automatic backup.\nCheck for any available automatic HSM Postgres backups:\nncn# cray artifacts list postgres-backup --format json | jq -r \u0026#39;.artifacts[].Key | select(contains(\u0026#34;smd\u0026#34;))\u0026#39; cray-smd-postgres-2021-07-11T23:10:08.manifest cray-smd-postgres-2021-07-11T23:10:08.psql   Procedure   Retrieve a previously taken HSM Postgres backup. This can be either a previously taken manual HSM backup or an automatic Postgres backup in the postgres-backup S3 bucket.\n  From a previous manual backup:\n  Copy over the folder or tarball containing the Postgres backup to be restored. If it is a tarball, extract it.\n  Set the environment variable POSTGRES_SQL_FILE to point toward the .psql file in the backup folder:\nncn# export POSTGRES_SQL_FILE=/root/cray-smd-postgres-backup_2021-07-07_16-39-44/cray-smd-postgres-backup_2021-07-07_16-39-44.psql   Set the environment variable POSTGRES_SECRET_MANIFEST to point toward the .manifest file in the backup folder:\nncn# export POSTGRES_SECRET_MANIFEST=/root/cray-smd-postgres-backup_2021-07-07_16-39-44/cray-smd-postgres-backup_2021-07-07_16-39-44.manifest     From a previous automatic Postgres backup:\n  Check for available backups.\nncn# cray artifacts list postgres-backup --format json | jq -r \u0026#39;.artifacts[].Key | select(contains(\u0026#34;smd\u0026#34;))\u0026#39; cray-smd-postgres-2021-07-11T23:10:08.manifest cray-smd-postgres-2021-07-11T23:10:08.psql Set the following environment variables for the name of the files in the backup:\nncn# export POSTGRES_SECRET_MANIFEST_NAME=cray-smd-postgres-2021-07-11T23:10:08.manifest ncn# export POSTGRES_SQL_FILE_NAME=cray-smd-postgres-2021-07-11T23:10:08.psql   Download the .psql file for the Postgres backup.\nncn# cray artifacts get postgres-backup \u0026#34;$POSTGRES_SQL_FILE_NAME\u0026#34; \u0026#34;$POSTGRES_SQL_FILE_NAME\u0026#34;   Download the .manifest file for the HSM backup.\nncn# cray artifacts get postgres-backup \u0026#34;$POSTGRES_SECRET_MANIFEST_NAME\u0026#34; \u0026#34;$POSTGRES_SECRET_MANIFEST_NAME\u0026#34;   Setup environment variables pointing to the full path of the .psql and .manifest files.\nncn# export POSTGRES_SQL_FILE=$(realpath \u0026#34;$POSTGRES_SQL_FILE_NAME\u0026#34;) ncn# export POSTGRES_SECRET_MANIFEST=$(realpath \u0026#34;$POSTGRES_SECRET_MANIFEST_NAME\u0026#34;)       Verify the POSTGRES_SQL_FILE and POSTGRES_SECRET_MANIFEST environment variables are set correctly.\nncn# echo \u0026#34;$POSTGRES_SQL_FILE\u0026#34; /root/cray-smd-postgres-backup_2021-07-07_16-39-44/cray-smd-postgres-backup_2021-07-07_16-39-44.psql ncn# echo \u0026#34;$POSTGRES_SECRET_MANIFEST\u0026#34; /root/cray-smd-postgres-backup_2021-07-07_16-39-44/cray-smd-postgres-backup_2021-07-07_16-39-44.manifest   Scale HSM to 0.\nncn-w001# CLIENT=cray-smd ncn-w001# POSTGRESQL=cray-smd-postgres ncn-w001# NAMESPACE=services ncn-w001# kubectl scale deployment ${CLIENT} -n ${NAMESPACE} --replicas=0 deployment.apps/cray-smd scaled ncn-w001# while [ $(kubectl get pods -n ${NAMESPACE} -l app.kubernetes.io/name=\u0026#34;${CLIENT}\u0026#34; | grep -v NAME | wc -l) != 0 ] ; do echo \u0026#34; waiting for pods to terminate\u0026#34;; sleep 2; done   Re-run the HSM loader job.\nncn# kubectl -n services get job cray-smd-init -o json | jq \u0026#39;del(.spec.selector)\u0026#39; | jq \u0026#39;del(.spec.template.metadata.labels.\u0026#34;controller-uid\u0026#34;)\u0026#39; | kubectl replace --force -f - Wait for the job to complete:\nncn# kubectl wait -n services job cray-smd-init --for=condition=complete --timeout=5m   Determine which Postgres member is the leader.\nncn-w001# kubectl exec \u0026#34;${POSTGRESQL}-0\u0026#34; -n ${NAMESPACE} -c postgres -it -- patronictl list +-------------------+---------------------+------------+--------+---------+----+-----------+ | Cluster | Member | Host | Role | State | TL | Lag in MB | +-------------------+---------------------+------------+--------+---------+----+-----------+ | cray-smd-postgres | cray-smd-postgres-0 | 10.42.0.25 | Leader | running | 1 | | | cray-smd-postgres | cray-smd-postgres-1 | 10.44.0.34 | | running | | 0 | | cray-smd-postgres | cray-smd-postgres-2 | 10.36.0.44 | | running | | 0 | +-------------------+---------------------+------------+--------+---------+----+-----------+ ncn-w001# POSTGRES_LEADER=cray-smd-postgres-0   Determine the database schema version of the currently running HSM database, and then verify that it matches the database schema version from the Postgres backup:\nDatabase schema of the currently running HSM Postgres instance.\nncn# kubectl exec $POSTGRES_LEADER -n services -c postgres -it -- bash -c \u0026#34;psql -U hmsdsuser -d hmsds -c \u0026#39;SELECT * FROM system\u0026#39;\u0026#34; id | schema_version | system_info ----+----------------+------------- 0 | 17 | {} (1 row)  The output above shows the database schema is at version 17.\n Database schema version from the Postgres backup:\nncn# cat \u0026#34;$POSTGRES_SQL_FILE\u0026#34; | grep \u0026#34;COPY public.system\u0026#34; -A 2 COPY public.system (id, schema_version, dirty) FROM stdin; 0 17 f \\.  The output above shows the database schema is at version 17.\n If the database schema versions match, proceed to the next step. Otherwise, the Postgres backup taken is not applicable to the currently running instance of HSM.\nWARNING: If the database schema versions do not match the version of HSM deployed, they will need to be either upgraded/downgraded to a version with a compatible database schema version. Ideally, it will be to the same version of HSM that was used to create the Postgres backup.\n  Delete and re-create the postgresql resource (which includes the PVCs).\nncn-w001# CLIENT=cray-smd ncn-w001# POSTGRESQL=cray-smd-postgres ncn-w001# NAMESPACE=services ncn-w001# kubectl get postgresql ${POSTGRESQL} -n ${NAMESPACE} -o json | jq \u0026#39;del(.spec.selector)\u0026#39; | jq \u0026#39;del(.spec.template.metadata.labels.\u0026#34;controller-uid\u0026#34;)\u0026#39; | jq \u0026#39;del(.status)\u0026#39; \u0026gt; postgres-cr.yaml ncn-w001# kubectl delete -f postgres-cr.yaml postgresql.acid.zalan.do \u0026#34;cray-smd-postgres\u0026#34; deleted ncn-w001# while [ $(kubectl get pods -l \u0026#34;application=spilo,cluster-name=${POSTGRESQL}\u0026#34; -n ${NAMESPACE} | grep -v NAME | wc -l) != 0 ] ; do echo \u0026#34; waiting for pods to terminate\u0026#34;; sleep 2; done ncn-w001# kubectl create -f postgres-cr.yaml postgresql.acid.zalan.do/cray-smd-postgres created ncn-w001# while [ $(kubectl get pods -l \u0026#34;application=spilo,cluster-name=${POSTGRESQL}\u0026#34; -n ${NAMESPACE} | grep -v NAME | wc -l) != 3 ] ; do echo \u0026#34; waiting for pods to start running\u0026#34;; sleep 2; done   Determine which Postgres member is the new leader.\nncn-w001# kubectl exec \u0026#34;${POSTGRESQL}-0\u0026#34; -n ${NAMESPACE} -c postgres -it -- patronictl list +-------------------+---------------------+------------+--------+---------+----+-----------+ | Cluster | Member | Host | Role | State | TL | Lag in MB | +-------------------+---------------------+------------+--------+---------+----+-----------+ | cray-smd-postgres | cray-smd-postgres-0 | 10.42.0.25 | Leader | running | 1 | | | cray-smd-postgres | cray-smd-postgres-1 | 10.44.0.34 | | running | | 0 | | cray-smd-postgres | cray-smd-postgres-2 | 10.36.0.44 | | running | | 0 | +-------------------+---------------------+------------+--------+---------+----+-----------+ ncn-w001# POSTGRES_LEADER=cray-smd-postgres-0   Copy the dump taken above to the Postgres leader pod and restore the data.\nIf the dump exists in a different location, adjust this example as needed.\nncn-w001# kubectl cp ${POSTGRES_SQL_FILE} ${POSTGRES_LEADER}:/home/postgres/cray-smd-postgres-dumpall.sql -c postgres -n ${NAMESPACE} ncn-w001# kubectl exec ${POSTGRES_LEADER} -c postgres -n ${NAMESPACE} -it -- psql -U postgres \u0026lt; cray-smd-postgres-dumpall.sql   Clear out of sync data from tables in postgres.\nThe backup will have restored tables that may contain out of date information. To refresh this data, it must first be deleted.\nDelete the entries in the EthernetInterfaces table. These will automatically get repopulated during rediscovery.\nncn# kubectl exec $POSTGRES_LEADER -n services -c postgres -it -- bash -c \u0026#34;psql -U hmsdsuser -d hmsds -c \u0026#39;DELETE FROM comp_eth_interfaces\u0026#39;\u0026#34;   Restore the secrets.\nOnce the dump has been restored onto the newly built postgresql cluster, the Kubernetes secrets need to match with the postgresql cluster, otherwise the service will experience readiness and liveness probe failures because it will be unable to authenticate to the database.\n  With secrets manifest from an existing backup If the Postgres secrets were auto-backed up, then re-create the secrets in Kubernetes.\nDelete and re-create the four cray-smd-postgres secrets using the manifest set to POSTGRES_SECRET_MANIFEST in step 1 above.\nncn-w001# kubectl delete secret postgres.cray-smd-postgres.credentials service-account.cray-smd-postgres.credentials hmsdsuser.cray-smd-postgres.credentials standby.cray-smd-postgres.credentials -n ${NAMESPACE} ncn-w001# kubectl apply -f ${POSTGRES_SECRET_MANIFEST}   Without the previous secrets from a backup If the Postgres secrets were not backed up, then update the secrets in Postgres.\nDetermine which Postgres member is the leader.\nncn-w001# kubectl exec \u0026#34;${POSTGRESQL}-0\u0026#34; -n ${NAMESPACE} -c postgres -it -- patronictl list +-------------------+---------------------+------------+--------+---------+----+-----------+ | Cluster | Member | Host | Role | State | TL | Lag in MB | +-------------------+---------------------+------------+--------+---------+----+-----------+ | cray-smd-postgres | cray-smd-postgres-0 | 10.42.0.25 | Leader | running | 1 | | | cray-smd-postgres | cray-smd-postgres-1 | 10.44.0.34 | | running | | 0 | | cray-smd-postgres | cray-smd-postgres-2 | 10.36.0.44 | | running | | 0 | +-------------------+---------------------+------------+--------+---------+----+-----------+ ncn-w001# POSTGRES_LEADER=cray-smd-postgres-0 Determine what secrets are associated with the postgresql credentials.\nncn-w001# kubectl get secrets -n ${NAMESPACE} | grep \u0026#34;${POSTGRESQL}.credentials\u0026#34; services hmsdsuser.cray-smd-postgres.credentials Opaque 2 31m services postgres.cray-smd-postgres.credentials Opaque 2 31m services service-account.cray-smd-postgres.credentials Opaque 2 31m services standby.cray-smd-postgres.credentials Opaque 2 31m For each secret above, get the username and password from Kubernetes and update the Postgres database with this information.\nFor example (hmsdsuser.cray-smd-postgres.credentials):\nncn-w001# kubectl get secret hmsdsuser.cray-smd-postgres.credentials -n ${NAMESPACE} -ojsonpath=\u0026#39;{.data.username}\u0026#39; | base64 -d hmsdsuser ncn-w001# kubectl get secret hmsdsuser.cray-smd-postgres.credentials -n ${NAMESPACE} -ojsonpath=\u0026#39;{.data.password}\u0026#39;| base64 -d ABCXYZ Exec into the leader pod to reset the user\u0026rsquo;s password:\nncn-w001# kubectl exec ${POSTGRES_LEADER} -n ${NAMESPACE} -c postgres -it -- bash root@cray-smd-postgres-0:/home/postgres# /usr/bin/psql postgres postgres postgres=# ALTER USER hmsdsuser WITH PASSWORD \u0026#39;ABCXYZ\u0026#39;; ALTER ROLE postgres=# Continue the above process until all ${POSTGRESQL}.credentials secrets have been updated in the database.\n    Restart the postgresql cluster.\nncn-w001# kubectl delete pod \u0026#34;${POSTGRESQL}-0\u0026#34; \u0026#34;${POSTGRESQL}-1\u0026#34; \u0026#34;${POSTGRESQL}-2\u0026#34; -n ${NAMESPACE} ncn-w001# while [ $(kubectl get postgresql ${POSTGRESQL} -n ${NAMESPACE} -o json | jq -r \u0026#39;.status.PostgresClusterStatus\u0026#39;) != \u0026#34;Running\u0026#34; ]; do echo \u0026#34;waiting for ${POSTGRESQL}to start running\u0026#34;; sleep 2; done   Scale the client service back to 3.\nncn-w001# kubectl scale deployment ${CLIENT} -n ${NAMESPACE} --replicas=3 ncn-w001# while [ $(kubectl get pods -n ${NAMESPACE} -l app.kubernetes.io/name=\u0026#34;${CLIENT}\u0026#34; | grep -v NAME | wc -l) != 3 ] ; do echo \u0026#34; waiting for pods to start running\u0026#34;; sleep 2; done   Verify that the service is functional.\nncn# cray hsm service ready code = 0 message = \u0026#34;HSM is healthy\u0026#34; Get the number of node objects stored in HSM:\nncn# cray hsm state components list --type node --format json | jq .[].ID | wc -l 1000   Resync the component state and inventory.\nAfter restoring HSM\u0026rsquo;s postgres from a back up, some of the transient data like component state and hardware inventory may be out of sync with reality. This involves kicking off an HSM rediscovery.\nncn# endpoints=$(cray hsm inventory redfishEndpoints list --format json | jq -r \u0026#39;.[]|.[]|.ID\u0026#39;) ncn# for e in $endpoints; do cray hsm inventory discover create --xnames ${e}; done Wait for discovery to complete. Discovery is complete after there are no redfishEndpoints left in the \u0026lsquo;DiscoveryStarted\u0026rsquo; state\nncn# cray hsm inventory redfishEndpoints list --format json | grep -c \u0026#34;DiscoveryStarted\u0026#34; 0   Check for discovery errors.\nncn# cray hsm inventory redfishEndpoints list --format json | grep LastDiscoveryStatus | grep -v -c \u0026#34;DiscoverOK\u0026#34; If any of the RedfishEndpoint entries have a LastDiscoveryStatus other than DiscoverOK after discovery has completed, refer to the Troubleshoot Issues with Redfish Endpoint Discovery procedure for guidance.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/hardware_state_manager/restore_hsm_postgres_without_a_backup/",
	"title": "Restore Hardware State Manager (HSM) Postgres Without An Existing Backup",
	"tags": [],
	"description": "",
	"content": "Restore Hardware State Manager (HSM) Postgres without an Existing Backup This procedure is intended to repopulate HSM in the event when no Postgres backup exists.\nPrerequisite   Healthy System Layout Service (SLS). Recovered first if also affected.\n  Healthy HSM service.\nVerify all 3 HSM postgres replicas are up and running:\nncn# kubectl -n services get pods -l cluster-name=cray-smd-postgres NAME READY STATUS RESTARTS AGE cray-smd-postgres-0 3/3 Running 0 18d cray-smd-postgres-1 3/3 Running 0 18d cray-smd-postgres-2 3/3 Running 0 18d   Procedure   Re-run the HSM loader job.\nncn# kubectl -n services get job cray-smd-init -o json | jq \u0026#39;del(.spec.selector)\u0026#39; | jq \u0026#39;del(.spec.template.metadata.labels.\u0026#34;controller-uid\u0026#34;)\u0026#39; | kubectl replace --force -f - Wait for the job to complete:\nncn# kubectl wait -n services job cray-smd-init --for=condition=complete --timeout=5m   Verify that the service is functional.\nncn# cray hsm service ready code = 0 message = \u0026#34;HSM is healthy\u0026#34;   Get the number of node objects stored in HSM.\nncn# cray hsm state components list --type node --format json | jq .[].ID | wc -l 0   Restart MEDS and REDS.\nTo repopulate HSM with components, restart MEDS and REDS so that they will add known RedfishEndpoints back in to HSM. This will also kick off HSM rediscovery to repopulate components and hardware inventory.\nncn# kubectl scale deployment cray-meds -n services --replicas=0 ncn# kubectl scale deployment cray-meds -n services --replicas=1 ncn# kubectl scale deployment cray-reds -n services --replicas=0 ncn# kubectl scale deployment cray-reds -n services --replicas=1 Wait for the RedfishEndpoints table to get repopulated and discovery to complete.\nncn# cray hsm inventory RedfishEndpoints list --format json | jq .[].ID | wc -l 100 ncn# cray hsm inventory redfishEndpoints list --format json | grep -c \u0026#34;DiscoveryStarted\u0026#34; 0   Check for Discovery Errors.\nncn# cray hsm inventory redfishEndpoints list --format json | grep LastDiscoveryStatus | grep -v -c \u0026#34;DiscoverOK\u0026#34; If any of the RedfishEndpoint entries have a LastDiscoveryStatus other than DiscoverOK after discovery has completed, refer to the Troubleshoot Issues with Redfish Endpoint Discovery procedure for guidance.\n  Re-apply any component group or partition customizations.\nAny component groups or partitions created before HSM\u0026rsquo;s Postgres information was lost will need to be manually re-entered.\n Manage Component Groups Manage Component Partitions    "
},
{
	"uri": "/docs-csm/en-11/operations/hardware_state_manager/hardware_state_manager/",
	"title": "Hardware State Manager (HSM)",
	"tags": [],
	"description": "",
	"content": "Hardware State Manager (HSM) The Hardware State Manager (HSM) monitors and interrogates hardware components in the HPE Cray EX system, tracking hardware state and inventory information, and making it available via REST queries and message bus events when changes occur.\nIn the CSM 0.9.3 release, v1 of the HSM API has begun its deprecation process in favor of the new HSM v2 API. Refer to the HSM API documentation for more information on the changes.\n"
},
{
	"uri": "/docs-csm/en-11/operations/hardware_state_manager/hardware_state_manager_hsm_state_and_flag_fields/",
	"title": "Hardware State Manager (HSM) State And Flag Fields",
	"tags": [],
	"description": "",
	"content": "Hardware State Manager (HSM) State and Flag Fields HSM manages important information for hardware components in the system. Administrators can use the data returned by HSM to learn about the state of the system. To do so, it is critical that the State and Flag fields are understood, and the next steps to take are known when viewing output returned by HSM commands. It is also beneficial to understand what services can cause State or Flag changes in HSM.\nThe following describes what causes State and Flag changes for all HSM components:\n Initial State/Flag is set upon discovery. This is generally Off/OK or On/OK. BMCs go to Ready/OK instead of On/OK. A component in the Populated state after discovery has an unknown power state. If one is expected for nodes, BMCs, or another component, this is likely due to a firmware issue. Flags can be set to Warning or Alert if the component\u0026rsquo;s Status.Health reads as Warning or Critical via Redfish during discovery. State for all components associated with a BMC is set to Empty if that BMC is removed from the network. State change events from components are consumed by HSM via subscriptions to Redfish events. These are subscribed to and placed on the Kafka bus by hmcollector for HSM\u0026rsquo;s consumption. HSM will update component state based on the information in the Redfish events.  The following describes what causes State and Flag changes for nodes only:\n Heartbeat Tracking Daemon (HBTD) updates the state of nodes based on heartbeats it receives from nodes. HBTD sets the node to Ready/OK when it starts heartbeats. HBTD sets the node to Ready/Warning after a few missed heartbeats. HBTD sets the node to Standby after many missed heartbeats and the node is presumed dead.  State descriptions:\n  Empty\nThe location is not populated with a component.\n  Populated\nPresent (not empty), but no further track can or is being done.\n  Off\nPresent but powered off.\n  On\nPowered on. If no heartbeat mechanism is available, its software state may be unknown.\n  Standby\nNo longer Ready and presumed dead. It typically means the heartbeat has been lost (w/ alert).\n  Ready\nBoth On and Ready to provide its expected services. For example, used for jobs.\n  Flag descriptions:\n  OK\nComponent is OK.\n  Warning\nThere is a non-critical error. Generally coupled with a Ready state.\n  Alert\nThere is a critical error. Generally coupled with a Standby state. Otherwise, reported via Redfish.\n  Hardware State Transitions The following table describes how to interpret when the state of hardware changes:\n   Prior State New State Reason     Ready Standby HBTD if node has many missed heartbeats   Ready Ready/Warning HBTD if node has a few missed heartbeats   Standby Ready HBTD node re-starts heartbeating   On Ready HBTD node started heartbeating   Off Ready HBTD sees heartbeats before Redfish Event (On)   Standby On Redfish Event (On) or if re-discovered while in the standby state   Off On Redfish Event (On)   Standby Off Redfish Event (Off)   Ready Off Redfish Event (Off)   On Off Redfish Event (Off)   Any State Empty Redfish Endpoint is disabled meaning component removal    Generally, nodes transition from Off to On to Ready when going from Off to booted, and from Ready to Ready/Warning to Standby to Off when shut down.\n"
},
{
	"uri": "/docs-csm/en-11/operations/hardware_state_manager/lock_and_unlock_management_nodes/",
	"title": "Lock And Unlock Management Nodes",
	"tags": [],
	"description": "",
	"content": "Lock and Unlock Management Nodes The ability to ignore non-compute nodes (NCNs) is turned off by default. Management nodes and NCNs are also not locked by default. The administrator must lock the NCNs to prevent unwanted actions from affecting these nodes.\nThis section only covers using locks with the Hardware State Manager (HSM). For more information on ignoring nodes, refer to the following sections:\n Firmware Action Service (FAS): See Ignore Node within FAS. Cray Advanced Platform Monitoring and Control (CAPMC): See Ignore Nodes with CAPMC  The following actions can be prevented when a node is locked.\n Firmware upgrades with FAS Power off operations with CAPMC Reset operations with CAPMC  Doing any of these actions by accident will shut down a management node. If the node is a Kubernetes master or worker node, this can have serious negative effects on system operations. If a single node is taken down by mistake, it is possible that services will recover. If all management nodes are taken down, or all Kubernetes worker nodes are taken down by mistake, the system must be restarted.\nAfter critical nodes are locked, power/reset (CAPMC) or firmware (FAS) operations cannot affect the nodes unless they are unlocked. For example, any locked node that is included in a list of nodes to be reset will result in a failure.\nTopics:  When To Lock Management Nodes When To Unlock Management Nodes How To Lock Management Nodes How To Unlock Management Nodes  When To Lock Management Nodes To best protect system health, NCNs should be locked as early as possible in the install/upgrade cycle. The later in the process, the more risk there is of accidentally taking down a critical node. NCN locking must be done after Kubernetes is running and the HSM service is operational.\nCheck whether HSM is running with the following command:\nncn# kubectl -n services get pods | grep smd cray-smd-848bcc875c-6wqsh 2/2 Running 0 9d cray-smd-848bcc875c-hznqj 2/2 Running 0 9d cray-smd-848bcc875c-tp6gf 2/2 Running 0 6d22h cray-smd-init-2tnnq 0/2 Completed 0 9d cray-smd-postgres-0 2/2 Running 0 19d cray-smd-postgres-1 2/2 Running 0 6d21h cray-smd-postgres-2 2/2 Running 0 19d cray-smd-wait-for-postgres-4-7c78j 0/3 Completed 0 9d The cray-smd pods need to be in the \u0026lsquo;Running\u0026rsquo; state, except for cray-smd-init and cray-smd-wait-for-postgres which should be in \u0026lsquo;Completed\u0026rsquo; state.\nWhen To Unlock Management Nodes Any time a management NCN has to be power cycled, reset, or have its firmware updated it will first need to be unlocked. After the operation is complete the targeted nodes should once again be locked.\nHow To Lock Management Nodes Use the cray hsm locks lock command to perform locking.\n  To lock all nodes with the Management role.\nThe processing-model rigid parameter means that the operation must succeed on all target nodes or the entire operation will fail.\nncn# cray hsm locks lock create --role Management --processing-model rigid Failure = [] [Counts] Total = 8 Success = 8 Failure = 0 [Success] ComponentIDs = [ \u0026#34;x3000c0s5b0n0\u0026#34;, \u0026#34;x3000c0s4b0n0\u0026#34;, \u0026#34;x3000c0s7b0n0\u0026#34;, \u0026#34;x3000c0s6b0n0\u0026#34;, \u0026#34;x3000c0s3b0n0\u0026#34;, \u0026#34;x3000c0s2b0n0\u0026#34;, \u0026#34;x3000c0s9b0n0\u0026#34;, \u0026#34;x3000c0s8b0n0\u0026#34;,]   To lock single nodes or lists of specific nodes.\nncn# cray hsm locks lock create --role Management --component-ids x3000c0s6b0n0 --processing-model rigid Failure = [] [Counts] Total = 1 Success = 1 Failure = 0 [Success] ComponentIDs = [ \u0026#34;x3000c0s6b0n0\u0026#34;,]   How To Unlock Management Nodes Use the cray hsm locks unlock command to perform unlocking.\n  To lock all nodes with the Management role.\nncn# cray hsm locks unlock create --role Management --processing-model rigid Failure = [] [Counts] Total = 8 Success = 8 Failure = 0 [Success] ComponentIDs = [ \u0026#34;x3000c0s7b0n0\u0026#34;, \u0026#34;x3000c0s6b0n0\u0026#34;, \u0026#34;x3000c0s3b0n0\u0026#34;, \u0026#34;x3000c0s2b0n0\u0026#34;, \u0026#34;x3000c0s9b0n0\u0026#34;, \u0026#34;x3000c0s8b0n0\u0026#34;, \u0026#34;x3000c0s5b0n0\u0026#34;, \u0026#34;x3000c0s4b0n0\u0026#34;,]   To unlock single nodes or lists of specific nodes.\nncn# cray hsm locks unlock create --role Management --component-ids x3000c0s6b0n0 --processing-model rigid Failure = [] [Counts] Total = 1 Success = 1 Failure = 0 [Success] ComponentIDs = [ \u0026#34;x3000c0s6b0n0\u0026#34;,]   "
},
{
	"uri": "/docs-csm/en-11/operations/hardware_state_manager/manage_component_groups/",
	"title": "Manage Component Groups",
	"tags": [],
	"description": "",
	"content": "Manage Component Groups The creation, deletion, and modification of groups is enabled by the Hardware State Manager (HSM) APIs.\nThe following is an example group that contains the optional fields tags and exclusiveGroup:\n{ \u0026#34;label\u0026#34; : \u0026#34;blue\u0026#34;, \u0026#34;description\u0026#34; : \u0026#34;blue node group\u0026#34;, \u0026#34;tags\u0026#34; : [ \u0026#34;tag1\u0026#34;, \u0026#34;tag2\u0026#34; ], \u0026#34;members\u0026#34; : { \u0026#34;ids\u0026#34; : [ \u0026#34;x0c0s0b0n0\u0026#34;, \u0026#34;x0c0s0b0n1\u0026#34;, \u0026#34;x0c0s0b1n0\u0026#34;, \u0026#34;x0c0s0b1n1\u0026#34; ] }, \u0026#34;exclusiveGroup\u0026#34; : \u0026#34;colors\u0026#34; } Troubleshooting: If the Cray CLI has not been initialized, the CLI commands will not work.\nCreate a New Group A group is defined by its members list and identifying label. It is also possible to add a description and a free form set of tags to help organize groups.\nThe members list may be set initially with the full list of member IDs, or can begin empty and have components added individually. The following examples show two different ways to create a new group.\nCreate a new non-exclusive group with an empty members list and two optional tags:\nncn-m# cray hsm groups create --label GROUP_LABEL \\ --tags TAG1,TAG2 \\ --description DESCRIPTION_OF_GROUP_LABEL Create a new group with a pre-set members list, which is part of an exclusive group:\nncn-m# cray hsm groups create --label GROUP_LABEL \\ --description DESCRIPTION_OF_GROUP_LABEL \\ --exclusive-group EXCLUSIVE_GROUP_LABEL \\ --members-ids MEMBER_ID,MEMBER_ID,MEMBER_ID Create a new group:\nncn-m# cray hsm groups create -v --label GROUP_LABEL Add a description of the group:\nncn-m# cray hsm groups update test_group --description \u0026#34;Description of group\u0026#34; Add a new component to a group:\nncn-m# cray hsm groups members create --id XNAME GROUP_LABEL Retrieve a Group Retrieve the complete group object to learn more about a group. This is also submitted when the group is created, except it is up-to-date with any additions or deletions from the members set.\nRetrieve all fields for a group, including the members list:\nncn-m# cray hsm groups describe GROUP_LABEL Delete a Group Entire groups can be removed. The group label is deleted and removed from all members who were formerly a part of the group.\nDelete a group with the following command:\nncn-m# cray hsm groups delete GROUP_LABEL "
},
{
	"uri": "/docs-csm/en-11/operations/hardware_state_manager/manage_component_partitions/",
	"title": "Manage Component Partitions",
	"tags": [],
	"description": "",
	"content": "Manage Component Partitions The creation, deletion, and modification of partitions is enabled by the Hardware State Manager (HSM) APIs.\nThe following is an example partition that contains the optional tags field:\n{ \u0026quot;name\u0026quot; : \u0026quot;partition 1\u0026quot;, \u0026quot;description\u0026quot; : \u0026quot;partition 1\u0026quot;, \u0026quot;tags\u0026quot; : [ \u0026quot;tag2\u0026quot; ], \u0026quot;members\u0026quot; : { \u0026quot;ids\u0026quot; : [ \u0026quot;x0c0s0b0n0\u0026quot;, \u0026quot;x0c0s0b0n1\u0026quot;, \u0026quot;x0c0s0b1n0\u0026quot; ] }, } Troubleshooting: If the Cray CLI has not been initialized, the CLI commands will not work.\nCreate a New Partition Creating a partition is very similar to creating a group. Members can either be provided in an initial list, or the list can be initially empty and added to later. There is no exclusiveGroups field because partition memberships are always exclusive. The following are two different ways to create a partition.\nCreate a new partition with an empty members list and two optional tags:\nncn-m# cray hsm partitions create --name PARTITION_NAME \\ --tags TAG1,TAG2 \\ --description DESCRIPTION_OF_PARTITION_NAME Create a new partition with a pre-set members list:\nncn-m# cray hsm partitions create --name PARTITION_NAME \\ --description DESCRIPTION OF PARTITION_NAME \\ --members-ids MEMBER_ID,MEMBER_ID,MEMBER_ID,MEMBER_ID Create a new partition:\nncn-m# cray hsm partitions create -v --label PARTITION_LABEL Add a description of the partition:\nncn-m# cray hsm partitions update test_group --description \u0026quot;Description of partition\u0026quot; Add a new component to the partition:\nncn-m# cray hsm partitions members create --id XNAME PARTITION_LABEL Retrieve Partition Information Information about a partition is retrieved with the partition name.\nRetrieve all fields for a partition, including the members list:\nncn-m# cray hsm partitions describe PARTITION_NAME Delete a Partition Once a partition is deleted, the former members will not have a partition assigned to them and are ready to be assigned to a new partition.\nDelete a partition so all members are no longer in it:\nncn-m# cray hsm partitions delete PARTITION_NAME "
},
{
	"uri": "/docs-csm/en-11/operations/hardware_state_manager/component_memberships/",
	"title": "Component Memberships",
	"tags": [],
	"description": "",
	"content": "Component Memberships Memberships are a read-only resource that is generated automatically by changes to groups and partitions. Each component in /hsm/v2/State/Components is represented. Filter options are available to prune the list, or a specific xname ID can be given. All groups and the partition (if any) of each component are listed.\nAt this point in time, only information about node components is needed. The --type node filter option is used in the commands below to retrieve information about node memberships only.\nThe following is an example membership:\n{ \u0026#34;id\u0026#34; : \u0026#34;x2c3s0b0n0\u0026#34;, \u0026#34;groupLabels\u0026#34; : [ \u0026#34;grp1\u0026#34;, \u0026#34;red\u0026#34;, \u0026#34;my_nodes\u0026#34; ], \u0026#34;partitionName\u0026#34; : \u0026#34;partition2\u0026#34; } Troubleshooting: If the Cray CLI has not been initialized, the CLI commands will not work.\nRetrieve Group and Partition Memberships By default, the memberships collection contains all components, regardless of if they are in a group. However, a filtered subset is desired more frequently. Querying the memberships collection supports the same query options as /hsm/v2/State/Components.\nRetrieve all node memberships:\nncn-m# cray hsm memberships list --type node Retrieve only nodes not in a partition:\nncn-m# cray hsm memberships list --type node --partition NULL Retrieve Membership Data for a Given Component Any components in /hsm/v2/State/Components can have its group and memberships looked up with its individual component xname ID.\nncn-m# cray hsm memberships describe MEMBER_ID "
},
{
	"uri": "/docs-csm/en-11/operations/hardware_state_manager/component_partition_members/",
	"title": "Component Partition Members",
	"tags": [],
	"description": "",
	"content": "Component Partition Members The members object in the partition definition has additional actions available for managing the members after the partition has been created.\nThe following is an example of partition members:\n{ \u0026quot;ids\u0026quot; : [ \u0026quot;x0c0s0b0n0\u0026quot;,\u0026quot;x0c0s0b0n1\u0026quot;,\u0026quot;x0c0s0b1n0\u0026quot;,\u0026quot;x0c0s0b1n1\u0026quot; ] } Retrieve Partition Members Retrieving members of a partition is very similar to how group members are retrieved and modified. No filtering options are available in partitions. However, there are partition and group filtering parameters for the /hsm/v2/State/Components and /hsm/v2/memberships collections, with both essentially working the same way.\nRetrieve only the members array for a single partition:\nncn-m# cray hsm partitions members list PARTITION_NAME Add a Component to Partition Components can be added to a partition\u0026rsquo;s member list, assuming it is not already a member or in another partition. This can be verified by looking at the membership information.\nAdd a component to a partition:\nncn-m# cray hsm partitions members create --id COMPONENT_ID PARTITION_NAME For example:\nncn-m# cray hsm partitions members create --id x1c0s0b0n0 partition1 Remove a Partition Member Remove a single component from a partition, assuming it is a current member. It will no longer be in any partition and is free to be assigned to a new one.\nncn-m# cray hsm partitions members delete MEMBER_ID PARTITION_NAME For example:\nncn-m# cray hsm partitions members delete x1c0s0b0n0 partition1 "
},
{
	"uri": "/docs-csm/en-11/operations/hardware_state_manager/create_a_backup_of_the_hsm_postgres_database/",
	"title": "Create A Backup Of The HSM Postgres Database",
	"tags": [],
	"description": "",
	"content": "Create a Backup of the HSM Postgres Database Perform a manual backup of the contents of the Hardware State Manager (HSM) Postgres database. This backup can be used to restore the contents of the HSM Postgres database at a later point in time using the Restore HSM Postgres from Backup procedure.\nPrerequisites   Healthy HSM Postgres Cluster.\nUse patronictl list on the HSM Postgres cluster to determine the current state of the cluster, and a healthy cluster will look similar to the following:\nncn# kubectl exec cray-smd-postgres-0 -n services -c postgres -it -- patronictl list + Cluster: cray-smd-postgres (6975238790569058381) ---+----+-----------+ | Member | Host | Role | State | TL | Lag in MB | +---------------------+------------+--------+---------+----+-----------+ | cray-smd-postgres-0 | 10.44.0.40 | Leader | running | 1 | | | cray-smd-postgres-1 | 10.36.0.37 | | running | 1 | 0 | | cray-smd-postgres-2 | 10.42.0.42 | | running | 1 | 0 | +---------------------+------------+--------+---------+----+-----------+   Healthy HSM Service.\nVerify all 3 HSM replicas are up and running:\nncn# kubectl -n services get pods -l cluster-name=cray-smd-postgres NAME READY STATUS RESTARTS AGE cray-smd-postgres-0 3/3 Running 0 18d cray-smd-postgres-1 3/3 Running 0 18d cray-smd-postgres-2 3/3 Running 0 18d   Procedure   Create a directory to store the HSM backup files.\nncn# BACKUP_LOCATION=\u0026#34;/root\u0026#34; ncn# export BACKUP_NAME=\u0026#34;cray-smd-postgres-backup_`date \u0026#39;+%Y-%m-%d_%H-%M-%S\u0026#39;`\u0026#34; ncn# export BACKUP_FOLDER=\u0026#34;${BACKUP_LOCATION}/${BACKUP_NAME}\u0026#34; ncn# mkdir -p \u0026#34;$BACKUP_FOLDER\u0026#34; The HSM backup will be located in the following directory:\nncn# echo $BACKUP_FOLDER /root/cray-smd-postgres-backup_2021-07-07_16-39-44   Run the backup_smd_postgres.sh script to take a backup of the HSM Postgres.\nncn# /usr/share/doc/csm/operations/hardware_state_manager/scripts/backup_smd_postgres.sh ~/cray-smd-postgres-backup_2021-07-07_16-39-44 ~ HSM postgres backup file will land in /root/cray-smd-postgres-backup_2021-07-07_16-39-44 Determining the postgres leader... The HSM postgres leader is cray-smd-postgres-0 Using pg_dumpall to dump the contents of the HSM database... PSQL dump is available at /root/cray-smd-postgres-backup_2021-07-07_16-39-44/cray-smd-postgres-backup_2021-07-07_16-39-44.psql Saving Kubernetes secret service-account.cray-smd-postgres.credentials Saving Kubernetes secret hmsdsuser.cray-smd-postgres.credentials Saving Kubernetes secret postgres.cray-smd-postgres.credentials Saving Kubernetes secret standby.cray-smd-postgres.credentials Removing extra fields from service-account.cray-smd-postgres.credentials.yaml Removing extra fields from hmsdsuser.cray-smd-postgres.credentials.yaml Removing extra fields from postgres.cray-smd-postgres.credentials.yaml Removing extra fields from standby.cray-smd-postgres.credentials.yaml Adding Kubernetes secret service-account.cray-smd-postgres.credentials to secret manifest Adding Kubernetes secret hmsdsuser.cray-smd-postgres.credentials to secret manifest Adding Kubernetes secret postgres.cray-smd-postgres.credentials to secret manifest Adding Kubernetes secret standby.cray-smd-postgres.credentials to secret manifest Secret manifest is located at /root/cray-smd-postgres-backup_2021-07-07_16-39-44/cray-smd-postgres-backup_2021-07-07_16-39-44.manifest HSM Postgres backup is available at: /root/cray-smd-postgres-backup_2021-07-07_16-39-44   Copy the backup folder off of the cluster, and store it in a secure location.\nThe BACKUP_FOLDER environment variable is the name of the folder to backup.\nncn# echo $BACKUP_FOLDER /root/cray-smd-postgres-backup_2021-07-07_16-39-44 Optionally, create a tarball of the Postgres backup files:\nncn# cd $BACKUP_FOLDER \u0026amp;\u0026amp; cd .. ncn# tar -czvf $BACKUP_NAME.tar.gz $BACKUP_NAME   "
},
{
	"uri": "/docs-csm/en-11/operations/hardware_state_manager/hardware_management_services_hms_locking_api/",
	"title": "Hardware Management Services (hms) Locking Api",
	"tags": [],
	"description": "",
	"content": "Hardware Management Services (HMS) Locking API The locking feature is a part of the Hardware State Manager (HSM) API. The locking API enables administrators to lock xnames on the system. Locking xnames ensures other system actors, such as administrators or running services, cannot perform a firmware update with the Firmware Action Service (FAS) or a power state change with the Cray Advanced Platform Monitoring and Control (CAPMC). Locks only constrain FAS and CAPMC from each other and help ensure that a firmware update action will not be interfered with by a request to power off the device through CAPMC. Locks only work with HMS services and will not impact other system services.\nLocks can only be used to prevent actions firmware updates with FAS or power state changes with CAPMC. Administrators can still use HMS APIs to view the state of various hardware components on the system, even if a lock is in place. There is no automatic locking for hardware devices. Locks need to be manually set or unset by an admin. A scenario that might be encountered is when a larger hardware state change job is run, and one of the xnames in the job has a lock on it. If FAS is the service running the job, FAS will attempt to update the firmware on each xname, and will update all devices that do not have a lock on it. The job will not complete until the node lock ends, or if a timeout is set for the job.\nThe locking API also includes actions to repair or disable a node\u0026rsquo;s locking ability with respect to HMS services. The disable function will make it so a device cannot be firmware updated or power controlled (via an HMS service) until a repair is done. Future requests to perform a firmware update via FAS or power state change via CAPMC cannot be made on that xname until the repair action is used.\nWarning: System administrators should LOCK NCNs after the system has been brought up to prevent an admin from unintentionally firmware updating or powering off an NCN. If this lock is not engaged, an authorized request to FAS or CAPMC could power off the NCNs, which will negatively impact system stability and the health of services running on those NCNs.\n"
},
{
	"uri": "/docs-csm/en-11/operations/hardware_state_manager/hsm_roles_and_subroles/",
	"title": "HSM Roles And Subroles",
	"tags": [],
	"description": "",
	"content": "HSM Roles and Subroles The Hardware State Manager (HSM) contains several pre-defined roles and subroles that can be assigned to components and used to target specific hardware devices.\nRoles and subroles assignments come from the System Layout Service (SLS) and are applied by HSM when a node is discovered.\nHSM Roles The following is a list of all pre-defined roles:\n Management Compute Application Service System Storage  The Management role refers to NCNs and will generally have the Master, Worker, or Storage subrole assigned.\nThe Compute role generally refers to compute nodes.\nThe Application role is used for more specific node uses and will generally have the UAN, LNETRouter, Visualization, Gateway, or UserDefined subrole assigned.\nHSM Subroles The following is a list of all pre-defined subroles:\n Worker Master Storage UAN Gateway LNETRouter Visualization UserDefined  The Master, Worker, and Storage subroles are generally used with the Master role to indicate NCN types.\nThe UAN, LNETRouter, Visualization, Gateway, and UserDefined subroles are generally used with the Application role to indicate specific use nodes.\nAdd Custom Roles and Subroles Custom roles and subroles can also be created and added to the HSM. New roles or subroles can be added anytime after SMD has been deployed.\nTo add new roles/subroles, add them to the cray-hms-base-config configmap under data-\u0026gt;hms_config.json.HMSExtendedDefinitions.(Sub)Role:\nncn# kubectl edit configmap -n services cray-hms-base-config data: hms_config.json: |- { \u0026#34;HMSExtendedDefinitions\u0026#34;:{ \u0026#34;Role\u0026#34;:[ \u0026#34;Compute\u0026#34;, \u0026#34;Service\u0026#34;, \u0026#34;System\u0026#34;, \u0026#34;Application\u0026#34;, \u0026#34;Storage\u0026#34;, \u0026#34;Management\u0026#34; ], \u0026#34;SubRole\u0026#34;:[ \u0026#34;Worker\u0026#34;, \u0026#34;Master\u0026#34;, \u0026#34;Storage\u0026#34;, \u0026#34;UAN\u0026#34;, \u0026#34;Gateway\u0026#34;, \u0026#34;LNETRouter\u0026#34;, \u0026#34;Visualization\u0026#34;, \u0026#34;UserDefined\u0026#34; ] } } Deleting roles/subroles from this list will also remove them from HSM. However, deleting any of the pre-defined roles or subroles will have no affect.\n"
},
{
	"uri": "/docs-csm/en-11/operations/hardware_state_manager/add_a_switch_to_the_hsm_database/",
	"title": "Add A Switch To The HSM Database",
	"tags": [],
	"description": "",
	"content": "Add a Switch to the HSM Database Manually add a switch to the Hardware State Manager (HSM) database. Switches need to be in the HSM database in order to update their firmware with the Firmware Action Service (FAS).\nPrerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system.  Procedure   Add the switch to the HSM database.\nThe --rediscover-on-update true flag forces HSM to discover the switch.\nncn-m001# cray hsm inventory redfishEndpoints create --id XNAME --fqdn IP_ADDRESS --user USERNAME \\ --password PASSWORD --rediscover-on-update true For example:\nncn-m001# cray hsm inventory redfishEndpoints create --id x3000c0r41b0 --fqdn 10.254.2.17 --user root \\ --password YourPassword --rediscover-on-update true [[results]] URI = \u0026#34;/hsm/v2/Inventory/RedfishEndpoints/x3000c0r41b0\u0026#34;   Verify that HSM successfully discovered the switch.\nncn-m001# cray hsm inventory redfishEndpoints list --id XNAME [[RedfishEndpoints]] Domain = \u0026#34; RediscoverOnUpdate = true Hostname = \u0026#34;10.254.2.17\u0026#34; Enabled = true FQDN = \u0026#34;10.254.2.17\u0026#34; User = \u0026#34;root\u0026#34; Password = \u0026#34; Type = \u0026#34;RouterBMC\u0026#34; ID = \u0026#34;x3000c0r41b0\u0026#34; [RedfishEndpoints.DiscoveryInfo] LastDiscoveryAttempt = \u0026#34;2020-02-05T18:41:08.823059Z\u0026#34; RedfishVersion = \u0026#34;1.2.0\u0026#34; LastDiscoveryStatus = \u0026#34;DiscoverOK\u0026#34;   The switch is now discovered by the HSM.\n"
},
{
	"uri": "/docs-csm/en-11/operations/hardware_state_manager/add_an_ncn_to_the_hsm_database/",
	"title": "Add An NCN To The HSM Database",
	"tags": [],
	"description": "",
	"content": "Add an NCN to the HSM Database This procedure details how to customize the bare-metal non-compute node (NCN) on a system and add the NCN to the Hardware State Manager (HSM) database.\nThe examples in this procedure use ncn-w0003-nmn as the Customer Access Node (CAN). Use the correct CAN for the system.\nPrerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system. The initial software installation is complete. Keycloak authentication is complete.  Procedure   Locate the xname of the system.\nThe xname is located in the /etc/hosts file.\nncn-m001# grep ncn-w003-nmn /etc/hosts 0.252.1.15 ncn-w003.local ncn-w003 ncn-w003-nmn ncn-w003-nmn.local sms03-nmn x3000c0s24b0n0 #-label-10.252.1.15   Create an entry with the following keypairs.\nget_token needs to be created or exist in the script with the following curl command. The get_token function is defined below:\nfunction get_token () { ADMIN_SECRET=$(kubectl get secrets admin-client-auth -ojsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d) curl -s -d grant_type=client_credentials -d client_id=admin-client -d client_secret=$ADMIN_SECRET \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | python -c \u0026#39;import sys, json; print json.load(sys.stdin)[\u0026#34;access_token\u0026#34;]\u0026#39; } The get_token script adds the authorization required by the HTTPS security token. -H options tell the REST API to accept the data as JSON and that the information is for a JSON-enabled application.\nncn-m001# curl -X POST -k https://api-gw-service-nmn.local/apis/smd/hsm/v2/State/Components \\ -H \u0026#34;Authorization: Bearer $(get_token)\u0026#34; -H \u0026#34;accept: application/json\u0026#34; -H \\ \u0026#34;Content-Type: application/json\u0026#34; -d \\ \u0026#39;{\u0026#34;Components\u0026#34;:[{\u0026#34;ID\u0026#34;:\u0026#34;x3000c0s24b0\u0026#34;,\u0026#34;State\u0026#34;:\u0026#34;On\u0026#34;,\u0026#34;NetType\u0026#34;:\u0026#34;Sling\u0026#34;,\u0026#34;Arch\u0026#34;:\u0026#34;X86\u0026#34;,\u0026#34;Role\u0026#34;:\u0026#34;Management\u0026#34;}]}\u0026#39;   List HSM state components and verify information is correct.\nncn-m001# cray hsm state components list --id x3000c0s24b0 [[Components]] Arch = \u0026#34;X86\u0026#34; Enabled = true Flag = \u0026#34;OK\u0026#34; State = \u0026#34;On\u0026#34; Role = \u0026#34;Management\u0026#34; NetType = \u0026#34;Sling\u0026#34; Type = \u0026#34;NodeBMC\u0026#34; ID = \u0026#34;x3000c0s24b0\u0026#34;   Find the daemonset pod that is running on the NCN being added to the HSM database.\nncn-m001# kubectl get pods -l app.kubernetes.io/instance=ncn-customization -n services -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ncn-customization-cray-service-4tqcg 2/2 Running 2 4d2h 10.47.0.3 ncn-m001 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ncn-customization-cray-service-dh8gb 2/2 Running 1 4d2h 10.42.0.4 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ncn-customization-cray-service-gwxc2 2/2 Running 2 4d2h 10.40.0.8 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ncn-customization-cray-service-rjms5 2/2 Running 2 4d2h 10.35.0.3 ncn-w004 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ncn-customization-cray-service-wgl44 2/2 Running 2 4d2h 10.39.0.3 ncn-w005 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   Delete the daemonset pod.\nDeleting the pod will restart it and enable the changes to be picked up.\nncn-m001# kubectl -n services delete pod ncn-customization-cray-service-dh8gb   Verify the daemonset restarts on the NCN with the CAN configuration.\n  Retrieve the new pod name.\nncn-m001# kubectl get pods -l app.kubernetes.io/instance=ncn-customization \\ -n services -o wide | grep ncn-w003 ncn-customization-cray-service-dh8gb 2/2 Running 2 22d 10.36.0.119 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   Wait for the daemonset pod to cycle through the unload session.\nThis may take up to 5 minutes.\nncn-m001# cray cfs sessions list | grep \u0026#34;name =\u0026#34; name = \u0026#34;ncn-customization-ncn-w003-unload\u0026#34;   Wait for the daemonset pod to cycle through the load session.\nThis may take up to 5 minutes.\nncn-m001# cray cfs sessions list | grep \u0026#34;name =\u0026#34; name = \u0026#34;ncn-customization-ncn-w003-load\u0026#34; Once the load job completes, if there are no errors returned, the session is removed.\nRunning cray cfs sessions list | grep \u0026quot;name =\u0026quot; again should return with no sessions active. If Ansible errors were encountered during the unload or load sessions, the dormant CFS session artifacts remain for CFS Ansible failure troubleshooting.\n    "
},
{
	"uri": "/docs-csm/en-11/operations/hardware_state_manager/component_group_members/",
	"title": "Component Group Members",
	"tags": [],
	"description": "",
	"content": "Component Group Members The members object in the group definition has additional actions available for managing the members after the group has been created.\nThe following is an example of group members:\n{ \u0026#34;ids\u0026#34; : [ \u0026#34;x0c0s0b0n0\u0026#34;,\u0026#34;x0c0s0b0n1\u0026#34;,\u0026#34;x0c0s0b1n0\u0026#34; ] } Retrieve Group Members Retrieve just the members array for a group:\nncn-m# cray hsm groups members list GROUP_LABEL Retrieve only the members of a group that are also in a specific partition:\nncn-m# cray hsm groups members list --partition PARTITION_NAME GROUP_LABEL Retrieve only the members of a group that are not in any partition currently:\nncn-m# cray hsm groups members list --partition NULL GROUP_LABEL Add Group Members Add a single component to a group. The only time this is not permitted is if the component already exists, or the group has an exclusiveGroup label and the component is already a member of a group with that exclusive label.\nAdd a component to a group:\nncn-m# cray hsm groups members create --id MEMBER_ID GROUP_LABEL For example:\nncn-m# cray hsm groups members create --id x1c0s0b0n0 blue Remove Group Members Single members are removed with the component xname ID from the given group.\nRemove a member from a group:\nncn-m# cray hsm groups members delete MEMBER_ID GROUP_LABEL For example:\nncn-m# cray hsm groups members delete x1c0s0b0n0 blue "
},
{
	"uri": "/docs-csm/en-11/operations/hardware_state_manager/component_groups_and_partitions/",
	"title": "Component Groups And Partitions",
	"tags": [],
	"description": "",
	"content": "Component Groups and Partitions The Hardware State Manager (HSM) provides the group and partition services. Both are means of grouping (also known as labeling) system components that are tracked by HSM. Components include the nodes, blades, controllers, and more on a system.\nThere is no limit to the number of members a group or partition contains. The only limitation is that all members must be actual members of the system. The HSM needs to know that the components exist.\nGroups Groups are collections of components (primarily nodes) in /hsm/v2/State/Components. Components can be members of any number of groups. Groups can be created freely, and HSM does not assign them any predetermined meaning.\nIf a group has exclusiveGroup=EXCLUSIVE_LABEL_NAME set, then a component may only be a member of one group that matches that exclusive label. For example, if the exclusive group label colors is associated with groups blue, red, and green, then a node that is part of the green group could not also be placed in the red group.\nPartitions Partitions are isolated, non-overlapping groups. Each component can be a member of only one partition at a time, and partitions are used as an access control mechanism. Partitions have a specific predefined meaning, intended to provide logical divisions of a single physical system.\n"
},
{
	"uri": "/docs-csm/en-11/operations/firmware/upload_olympus_bmc_recovery_firmware_into_tftp_server/",
	"title": "Upload BMC Recovery Firmware Into Tftp Server",
	"tags": [],
	"description": "",
	"content": "Upload BMC Recovery Firmware into TFTP Server cray-upload-recovery-images is a utility for uploading the BMC recovery files for ChassisBMCs, NodeBMCs, and RouterBMCs to be served by the cray-tftp service. The tool uses the cray cli (fas, artifacts) and cray-tftp to download the s3 recovery images (as remembered by FAS) then upload them into the PVC that is used by cray-tftp. cray-upload-recovery-images should be run on every system.\nProcedure   Execute the cray-upload-recovery-images script.\nncn# cray-upload-recovery-images Attempting to retrieve ChassisBMC .itb file s3:/fw-update/d7bb5be9eecc11eab18c26c5771395a4/cc-1.3.10.itb d7bb5be9eecc11eab18c26c5771395a4/cc-1.3.10.itb Uploading file: /tmp/cc.itb Defaulting container name to cray-ipxe. Successfully uploaded /tmp/cc.itb! removed /tmp/cc.itb ChassisBMC recovery image upload complete ======================================== Attempting to retrieve NodeBMC .itb file s3:/fw-update/d81157f7eecc11ea943d26c5771395a4/nc-1.3.10.itb d81157f7eecc11ea943d26c5771395a4/nc-1.3.10.itb Uploading file: /tmp/nc.itb Defaulting container name to cray-ipxe. Successfully uploaded /tmp/nc.itb! removed /tmp/nc.itb NodeBMC recovery image upload complete ======================================== Attempting to retrieve RouterBMC .itb file s3:/fw-update/d85398f2eecc11ea94ff26c5771395a4/rec-1.3.10.itb d85398f2eecc11ea94ff26c5771395a4/rec-1.3.10.itb Uploading file: /tmp/rec.itb Defaulting container name to cray-ipxe. Successfully uploaded /tmp/rec.itb! removed /tmp/rec.itb RouterBMC recovery image upload complete ``\n  "
},
{
	"uri": "/docs-csm/en-11/operations/firmware/fas_filters/",
	"title": "FAS Filters",
	"tags": [],
	"description": "",
	"content": "FAS Filters FAS uses five primary filters for actions and snapshots to determine what operations to create. The filters are listed below:\n Selection Filters - Determine what operations will be created. The following selection filters are available:  stateComponentFilter targetFilter inventoryHardwareFilter  imageFilter   Command Filters - Determine how the operations will be executed. The following command filters are available:  command    All filters are logically connected with AND logic. Only the stateComponentFilter, targetFilter, and inventoryHardwareFilter are used for snapshots.\n Selection Filters stateComponentFilter The state component filter allows users to select hardware to update. Hardware can be selected individually with xnames, or in groups by leveraging the Hardware State Manager (HSM) groups and partitions features.\nParameters  xnames - A list of xnames to target. partitions - A partition to target. groups- A group to target. deviceTypes Set to NodeBMC, RouterBMC, or ChassisBMC. These are the ONLY three allowed types and come from the Hardware State Manager (HSM).   inventoryHardwareFilter The inventory hardware filter takes place after the state component filter has been applied. It will remove any devices that do not conform to the identified manufacturer or models determined by querying the Redfish endpoint.\nIMPORTANT: There can be a mismatch of hardware models. The model field is human-readable and is human-programmable. In some cases, there can be typos where the wrong model is programmed, which causes issues filtering. If this occurs, query the hardware, find the model name, and add it to the images repository on the desired image.\nParameters  manufacturer - Set to Cray, HPE, or Gigabyte. model - The Redfish reported model, which can be specified.   imageFilter FAS applies images to xname/targets. The image filter is a way to specify an explicit image that should be used. When included with other filters, the image filter reduces the devices considered to only those devices where the image can be applied.\nFor example, if a user specifies an image that only applies to gigabyte, nodeBMCs, BIOS targets. If all hardware in the system is targeted with an empty stateComponentFilter, FAS would find all devices in the system that can be updated via Redfish, and then the image filter would remove all xname/ targets that this image could not be applied. In this example, FAS would remove any device that is not a Gigabyte nodeBMC, as well as any target that is not BIOS.\nParameters  imageID - The ID of the image to force onto the system. overrideImage - If this is combined with imageID; it will FORCE the selected image onto all hardware identified, even if it is not applicable. WARNING: This may cause undesirable outcomes, but most hardware will prevent a bad image from being loaded.   targetFilter The target filter selects targets that match against the list. For example, if the user specifies only the BIOS target, FAS will include only operations that explicitly have BIOS as a target. A Redfish device has potentially many targets (members). Targets for FAS are case sensitive and must match Redfish.\nParameters  targets - The actual \u0026lsquo;members\u0026rsquo; that will be upgraded. Examples include, but are not limited to the following:  BIOS BMC NIC Node0.BIOS Node1.BIOS Recovery     Command Filters command The command group is the most important part of an action command and controls if the action is executed as dry-run or a live update.\nIt also determines whether or not to override an operation that would normally not be executed if there is no way to return the xname/target to the previous firmware version. This happens if an image does not exist in the image repository.\nThese filters are then applied; and then command parameter applies settings for the overall action. The swagger file is a great reference.\nParameters  version - Usually latest because that is the most common use case. tag - Usually default because the default image is the most useful one to use. This parameter can usually be ignored. overrideDryrun - This determines if this is a LIVE UPDATE or a DRY-RUN. If doing an override; then it will provide a live update. restoreNotPossibleOverride - This determines if an update (live or dry-run) will be attempted if a restore cannot be performed. Typically there is not enough firmware to be able to do a rollback, which means if the system is an UPDATE away from a particular version, it cannot go back to a previous version. It is most likely that this value will ALWAYS need to be set true. overwriteSameImage - This will cause a firmware update to be performed EVEN if the device is already at the identified, selected version. timeLimit - This is the amount of time in seconds that any operation should be allowed to execute. Most cray hardware can be completed in approximately 1000 seconds or less; but the gigabyte hardware will commonly take 1,500 seconds or greater. Setting the value to 2000 is recommended as a stop gap to prevent the operation from never ending, should something get stuck. description- A human-friendly description that should be set to give useful information about the firmware operation.  "
},
{
	"uri": "/docs-csm/en-11/operations/firmware/fas_use_cases/",
	"title": "FAS Use Cases",
	"tags": [],
	"description": "",
	"content": "FAS Use Cases Use the Firmware Action Service (FAS) to update the firmware on supported hardware devices. Each procedure includes the prerequisites and example recipes required to update the firmware.\n Update Liquid-Cooled Node Update Chassis Management Module (CMM) Firmware Update NCN BIOS and BMC Firmware with FAS Update Liquid-Cooled Compute Node BIOS Firmware Compute Node BIOS Workaround for HPE CRAY EX425  NOTE to update Switch Controllers (sC) or RouterBMC refer to the Rosetta Documentation Update Liquid-Cooled Nodes Update a liquid-cooled node controller (nC) firmware using FAS. This procedure uses the dry-run feature to verify that the update will be successful before initiating the actual update.\nThis procedure updates the following hardware:\n Node controller (nC) firmware  Prerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system.  Example Recipes Manufacturer: Cray | Device Type: NodeBMC | Target: BMC\nThe compute nodes must be powered off before upgrading the BMC image for a nodeBMC. BMC firmware with FPGA updates require the nodes to be off. If the nodes are not off when the update command is issued, the update will get deferred until the next power cycle of the BMC, which may be a long period of time.\n{ \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;BMC\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of Olympus node BMCs\u0026#34; } } Manufacturer: Cray | Device Type: NodeBMC | Target: Redstone FPGA\nIMPORTANT: The Nodes themselves must be powered on in order to update the firmware of the Redstone FPGA on the nodes.\n{ \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;Node0.AccFPGA0\u0026#34;, \u0026#34;Node1.AccFPGA0\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of Node Redstone FPGA\u0026#34; } } Procedure   Create a JSON file using one of the example recipes with the command parameters required for updating the BMC firmware.\n  Initiate a dry-run to verify that the firmware can be updated.\n  Create the dry-run session.\nThe overrideDryrun = false value indicates that the command will do a dry run.\nncn-w001# cray fas actions create nodeBMC.json overrideDryrun = false actionID = \u0026#34;fddd0025-f5ff-4f59-9e73-1ca2ef2a432d\u0026#34;   Describe the actionID for firmware update dry-run job.\nReplace the actionID value with the string returned in the previous step. In this example, \u0026quot;fddd0025-f5ff-4f59-9e73-1ca2ef2a432d\u0026quot; is used.\nncn-w001# cray fas actions describe {actionID} blockedBy = [] state = \u0026#34;completed\u0026#34; actionID = \u0026#34;fddd0025-f5ff-4f59-9e73-1ca2ef2a432d\u0026#34; startTime = \u0026#34;2020-08-31 15:49:44.568271843 +0000 UTC\u0026#34; snapshotID = \u0026#34;00000000-0000-0000-0000-000000000000\u0026#34; endTime = \u0026#34;2020-08-31 15:51:35.426714612 +0000 UTC\u0026#34; [command] description = \u0026#34;Update Cray Node BMCs Dryrun\u0026#34; tag = \u0026#34;default\u0026#34; restoreNotPossibleOverride = true timeLimit = 10000 version = \u0026#34;latest\u0026#34; overrideDryrun = false If state = \u0026quot;completed\u0026quot;, the dry-run has found and checked all the nodes. Check the following sections for more information:\n  Lists the nodes that have a valid image for updating:\n[operationSummary.succeeded]   Lists the nodes that will not be updated because they are already at the correct version:\n[operationSummary.noOperation]   Lists the nodes that had an error when attempting to update:\n[operationSummary.failed]   Lists the nodes that do not have a valid image for updating:\n[operationSummary.noSolution]       Update the firmware after verifying that the dry-run worked as expected.\n  Edit the JSON file and update the values so an actual firmware update can be run.\nThe following example is for the nodeBMC.json file. Update the following values:\n\u0026#34;overrideDryrun\u0026#34;:true, \u0026#34;description\u0026#34;:\u0026#34;Update Cray Node BMCs\u0026#34;   Run the firmware update.\nThe returned overrideDryrun = true indicates that an actual firmware update job was created. A new actionID will also be returned.\nncn-w001# cray fas actions create nodeBMC.json overrideDryrun = true actionID = \u0026#34;bc40f10a-e50c-4178-9288-8234b336077b\u0026#34; The time it takes for a firmware update varies. It can be a few minutes or over 20 minutes depending on response time.\nThe liquid-cooled node BMC automatically reboots after the firmware has been loaded.\n    Retrieve the operationID and verify that the update is complete.\nncn-w001# cray fas actions describe {actionID} [operationSummary.failed] [[operationSummary.failed.operationKeys]] stateHelper = \u0026#34;unexpected change detected in firmware version. Expected nc.1.3.10-shasta-release.arm.2020-07-21T23:58:22+00:00.d479f59 got: nc.cronomatic-dev.arm.2019-09-24T13:20:24+00:00.9d0f8280\u0026#34; fromFirmwareVersion = \u0026#34;nc.cronomatic-dev.arm.2019-09-24T13:20:24+00:00.9d0f8280\u0026#34; xname = \u0026#34;x1005c6s4b0\u0026#34; target = \u0026#34;BMC\u0026#34; operationID = \u0026#34;e910c6ad-db98-44fc-bdc5-90477b23386f\u0026#34;   View more details for an operation using the operationID from the previous step.\nCheck the list of nodes for the failed or completed state.\nncn-w001# cray fas operations describe {operationID} For example:\nncn-w001# cray fas operations describe \u0026#34;e910c6ad-db98-44fc-bdc5-90477b23386f\u0026#34; fromFirmwareVersion = \u0026#34;nc.cronomatic-dev.arm.2019-09-24T13:20:24+00:00.9d0f8280\u0026#34; fromTag = \u0026#34; fromImageURL = \u0026#34; endTime = \u0026#34;2020-08-31 16:40:13.464321212 +0000 UTC\u0026#34; actionID = \u0026#34;bc40f10a-e50c-4178-9288-8234b336077b\u0026#34; startTime = \u0026#34;2020-08-31 16:28:01.228524446 +0000 UTC\u0026#34; fromSemanticFirmwareVersion = \u0026#34; toImageURL = \u0026#34; model = \u0026#34;WNC_REV_B\u0026#34; operationID = \u0026#34;e910c6ad-db98-44fc-bdc5-90477b23386f\u0026#34; fromImageID = \u0026#34;00000000-0000-0000-0000-000000000000\u0026#34; target = \u0026#34;BMC\u0026#34; toImageID = \u0026#34;39c0e553-281d-4776-b68e-c46a2993485e\u0026#34; toSemanticFirmwareVersion = \u0026#34;1.3.10\u0026#34; refreshTime = \u0026#34;2020-08-31 16:40:13.464325422 +0000 UTC\u0026#34; blockedBy = [] toTag = \u0026#34; state = \u0026#34;failed\u0026#34; stateHelper = \u0026#34;unexpected change detected in firmware version. Expected nc.1.3.10-shasta-release.arm.2020-07-21T23:58:22+00:00.d479f59 got: nc.cronomatic-dev.arm.2019-09-24T13:20:24+00:00.9d0f8280\u0026#34; deviceType = \u0026#34;NodeBMC\u0026#34;    Update Chassis Management Module Firmware Update the Chassis Management Module (CMM) controller (cC) firmware using FAS. This procedure uses the dry-run feature to verify that the update will be successful.\nThe CMM firmware update process also checks and updates the Cabinet Environmental Controller (CEC) firmware.\nPrerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system.  Example Recipes Manufacturer: Cray | Device Type: ChassisBMC | Target: BMC\nIMPORTANT: Before updating a CMM, make sure all slot and rectifier power is off.\n{ \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34; }, \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;chassisBMC\u0026#34; ] }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;BMC\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of Cray Chassis Controllers\u0026#34; } } Procedure   Power off the liquid-cooled chassis slots and chassis rectifiers.\n  Disable the hms-discovery Kubernetes cronjob:\nncn# kubectl -n services patch cronjobs hms-discovery -p \u0026#39;{\u0026#34;spec\u0026#34; : {\u0026#34;suspend\u0026#34; : true }}\u0026#39;   Power off all the components; for example, in chassis 0-7, cabinets 1000-1003.\nncn# cray capmc xname_off create --xnames x[1000-1003]c[0-7] --recursive true --continue true This command powers off all the node cards, then all the compute blades, then all the Slingshot switch ASICS, then all the Slingshot switch enclosures, and finally all chassis enclosures in cabinets 1000-1003.\nWhen power is removed from a chassis, the high-voltage DC rectifiers that support the chassis are powered off. If a component is not populated, the --continue option enables the command to continue instead of returning error messages.\n    Create a JSON file using the example recipe above with the command parameters required for updating the CMM firmware.\n  Initiate a dry-run to verify that the firmware can be updated.\n  Create the dry-run session.\nThe overrideDryrun = false value indicates that the command will do a dry-run.\nncn# cray fas actions create chassisBMC.json overrideDryrun = false actionID = \u0026#34;fddd0025-f5ff-4f59-9e73-1ca2ef2a432d\u0026#34;   Describe the actionID to see the firmware update dry-run job status.\nReplace the actionID value with the string returned in the previous step. In this example, \u0026quot;fddd0025-f5ff-4f59-9e73-1ca2ef2a432d\u0026quot; is used.\nncn# cray fas actions describe {actionID} blockedBy = [] state = \u0026#34;completed\u0026#34; actionID = \u0026#34;fddd0025-f5ff-4f59-9e73-1ca2ef2a432d\u0026#34; startTime = \u0026#34;2020-08-31 15:49:44.568271843 +0000 UTC\u0026#34; snapshotID = \u0026#34;00000000-0000-0000-0000-000000000000\u0026#34; endTime = \u0026#34;2020-08-31 15:51:35.426714612 +0000 UTC\u0026#34; [command] description = \u0026#34;Update Cray Chassis Management Module controllers Dryrun\u0026#34; tag = \u0026#34;default\u0026#34; restoreNotPossibleOverride = true timeLimit = 10000 version = \u0026#34;latest\u0026#34; overrideDryrun = false If state = \u0026quot;completed\u0026quot;, the dry-run has found and checked all the nodes. Check the following sections for more information:\n  Lists the nodes that have a valid image for updating:\n[operationSummary.succeeded]   Lists the nodes that will not be updated because they are already at the correct version:\n[operationSummary.noOperation]   Lists the nodes that had an error when attempting to update:\n[operationSummary.failed]   Lists the nodes that do not have a valid image for updating:\n[operationSummary.noSolution]       Update the firmware after verifying that the dry-run worked as expected.\n  Edit the JSON file and update the values so an actual firmware update can be run.\nThe following example is for the chassisBMC.json file. Update the following values:\n\u0026#34;overrideDryrun\u0026#34;:true, \u0026#34;description\u0026#34;:\u0026#34;Update Cray Chassis Management Module controllers\u0026#34;   Run the firmware update.\nThe returned overrideDryrun = true indicates that an actual firmware update job was created. A new actionID will also be returned.\nncn# cray fas actions create chassisBMC.json overrideDryrun = true actionID = \u0026#34;bc40f10a-e50c-4178-9288-8234b336077b\u0026#34; The time it takes for a firmware update varies. It can be a few minutes or over 20 minutes depending on response time.\n    Restart the hms-discovery cronjob.\nncn# kubectl -n services patch cronjobs hms-discovery -p \u0026#39;{\u0026#34;spec\u0026#34; : {\u0026#34;suspend\u0026#34; : false }}\u0026#39; The hms-discovery cronjob will run within 5 minutes of being unsuspended and start powering on the chassis enclosures, switches, and compute blades. If components are not being powered back on, then power them on manually:\nncn# cray capmc xname_on create --xnames x[1000-1003]c[0-7]r[0-7],x[1000-1003]c[0-7]s[0-7] --prereq true --continue true The --prereq option ensures all required components are powered on first. The --continue option allows the command to complete in systems without fully populated hardware.\n  After the components have powered on, boot the nodes using the Boot Orchestration Services (BOS).\n   Update Non-Compute Node (NCN) BIOS and BMC Firmware Gigabyte and HPE non-compute nodes (NCNs) firmware can be updated with FAS. This section includes templates for JSON files that can be used to update firmware with the cray fas actions create command.\nAfter creating the JSON file for the device being upgraded, use the following command to run the FAS job:\nncn-w001# cray fas actions create CUSTOM_DEVICE_PARAMETERS.json All of the example JSON files below are set to run a dry-run. Update the overrideDryrun value to True to update the firmware.\nWARNING: Rebooting more than one NCN at a time MAY cause system instability. Be sure to follow the correct process for updating NCNs. Firmware updates have the capacity to harm the system.\nAfter updating the BIOS, the NCN will need to be rebooted. Follow the Reboot NCNs procedure.\nProcedure for updating NCNs:\n For HPE NCNs, check the DNS servers by running the script /opt/cray/csm/scripts/node_management/set-bmc-ntp-dns.sh ilo -H x3000c0s10b0 -s (x3000c0s10b0 is the xname of the NCN BMC)- See Configure DNS and NTP on Each BMC Run a dryrun for all NCNs first to determine which NCNs and targets need updating. For each NCN requiring updates to target BMC or iLO5 (Update of BMC and iLO 5 will not affect the nodes):  Unlock the NCN BMC - See Lock and Unlock Management Nodes Run the FAS action on the NCN Relock the NCN BMC - See Lock and Unlock Management Nodes   For each NCN requiring updates to target BIOS or System ROM:  Unlock the NCN BMC - See Lock and Unlock Management Nodes Run the FAS action on the NCN Reboot the Node - See Reboot NCNs For HPE NCNs, run the script /opt/cray/csm/scripts/node_management/set-bmc-ntp-dns.sh - See Configure DNS and NTP on Each BMC Relock the NCN BMC - See Lock and Unlock Management Nodes    Gigabyte Device Type: NodeBMC | Target: BMC\n{ \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ], \u0026#34;xnames\u0026#34;: [ \u0026#34;x3000c0s1b0\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;gigabyte\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;BMC\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 4000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of Gigabyte node BMCs\u0026#34; } } IMPORTANT: The timeLimit is 4000 because the gigabytes can take a lot longer to update.\nYou may receive a node failed to update with the output: stateHelper = \u0026quot;Firmware Update Information Returned Downloading – See /redfish/v1/UpdateService\u0026quot; FAS has incorrectly marked this node as failed. It most likely will complete the update successfully. You can check the update status by looking at the Redfish FirmwareInventory (/redfish/v1/UpdateService/FirmwareInventory/BMC) or rerunning FAS to verify that the BMC firmware was updated. Make sure you have waited for the current firmware to be updated before starting a new FAS action on the same node.\nDevice Type: NodeBMC | Target: BIOS\n{ \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ], \u0026#34;xnames\u0026#34;: [ \u0026#34;x3000c0s1b0\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;gigabyte\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;BIOS\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 4000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of Gigabyte node BIOS\u0026#34; } } IMPORTANT: The timeLimit is 4000 because the gigabytes can take a lot longer to update.\nHPE Device Type: NodeBMC | Target: iLO 5 aka BMC\n\u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ], \u0026#34;xnames\u0026#34;: [ \u0026#34;x3000c0s1b0\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;hpe\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;iLO 5\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of HPE node iLO 5\u0026#34; } } Device Type: NodeBMC | Target: System ROM aka BIOS\nIMPORTANT: If updating the System ROM of an NCN, the NTP and DNS server values will be lost and must be restored. For NCNs other than ncn-m001 this can be done using the /opt/cray/csm/scripts/node_management/set-bmc-ntp-dns.sh script. Use the -h option to get a list of command line options required to restore the NTP and DNS values. See Configure DNS and NTP on Each BMC\n{ \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;NodeBMC\u0026#34; ], \u0026#34;xnames\u0026#34;: [ \u0026#34;x3000c0s1b0\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;hpe\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;System ROM\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of HPE node system rom\u0026#34; } } The NCN must be rebooted after updating the BIOS firmware. Follow the Reboot NCNs procedure.\n Update Liquid-Cooled Compute Node BIOS Firmware Use this procedure to update compute node BIOS firmware using FAS. There are two nodes that must be updated, which have the Node0.BIOS and Node1.BIOS targets.\nPrerequisites  The Cray nodeBMC device needs to be updated before the nodeBIOS because the nodeBMC adds a new field Redfish (softwareId) that the NodeX.BIOS update will require. See Update Liquid-Cooled Node or Switch Firmware for more information. Compute node BIOS updates require the nodes to be off. If nodes are not off when update command is issued, it will report as a failed update. The Cray command line interface (CLI) tool is initialized and configured on the system.  Example Recipes Manufacturer: Gigabyte | Device Type : NodeBMC | Target : BIOS\n{ \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;gigabyte\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;BIOS\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 4000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of Gigabyte node BIOS\u0026#34; } } IMPORTANT: The timeLimit is 4000 because the gigabytes can take a lot longer to update.\nManufacturer: HPE | Device Type: NodeBMC | Target: System ROM aka BIOS\n{ \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;NodeBMC\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;hpe\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;System ROM\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of HPE node system rom\u0026#34; } } Manufacturer: Cray | Device Type : NodeBMC | Target : NodeBIOS\nIMPORTANT: The Nodes themselves must be powered off in order to update the BIOS on the nodes. The BMC will still have power and will perform the update.\nIMPORTANT: When the BMC is updated or rebooted after updating the Node0.BIOS and/or Node1.BIOS liquid-cooled nodes, the node BIOS version will not report the new version string until the nodes are powered back on. It is recommended that the Node0/1 BIOS be updated in a separate action, either before or after a BMC update and the nodes are powered back on after a BIOS update. The liquid-cooled nodes must be powered off for the BIOS to be updated.\n{ \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;Node0.BIOS\u0026#34;, \u0026#34;Node1.BIOS\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of Node BIOS\u0026#34; } } Procedure   Create a JSON file using an example recipe above with the command parameters required for updating the node BIOS firmware.\n  Initiate a dry-run update to verify that the firmware can be updated.\n  Create the dry-run session.\nThe overrideDryrun = false value returned indicates a dry-run.\nncn-w001# cray fas actions create nodeBIOS.json overrideDryrun = false actionID = \u0026#34;a88d8207-8bca-4ba1-9b80-14772e5f3f34\u0026#34;   Describe the firmware update dry-run job.\nReplace the actionID value with the string returned in the previous step. In this example, \u0026quot;a88d8207-8bca-4ba1-9b80-14772e5f3f34\u0026quot; is used.\nncn-w001# cray fas actions describe {actionID} blockedBy = [] state = \u0026#34;completed\u0026#34; actionID = \u0026#34;a88d8207-8bca-4ba1-9b80-14772e5f3f34\u0026#34; startTime = \u0026#34;2020-08-31 15:49:44.568271843 +0000 UTC\u0026#34; snapshotID = \u0026#34;00000000-0000-0000-0000-000000000000\u0026#34; endTime = \u0026#34;2020-08-31 15:51:35.426714612 +0000 UTC\u0026#34; [command] description = \u0026#34;Update Cray Node BIOS Dryrun\u0026#34; tag = \u0026#34;default\u0026#34; restoreNotPossibleOverride = true timeLimit = 10000 version = \u0026#34;latest\u0026#34; overrideDryrun = false If state = \u0026quot;completed\u0026quot;, the dry-run has found and checked all the nodes. Check the following sections for more information:\n  Lists the nodes that have a valid image for updating:\n[operationSummary.succeeded]   Lists the nodes that do not need updates:\n[operationSummary.noOperation]   Lists the nodes that had an error when attempting to update:\n[operationSummary.failed]   Lists the nodes that do not have a valid image for updating:\n[operationSummary.noSolution]       Update the firmware after verifying that the dry-run worked as expected.\n  Edit the JSON file and update the values so an actual firmware update can be run.\nThe following example shows the nodeBIOS.json file. Update the following values:\n\u0026#34;overrideDryrun\u0026#34;:true, \u0026#34;description\u0026#34;:\u0026#34;Update Cray Node BIOS\u0026#34;   Run the firmware update.\nThe returned overrideDryrun = true indicates that an actual firmware update job was created. A new actionID will also be returned.\nncn-w001# cray fas actions create nodeBIOS.json overrideDryrun = true actionID = \u0026#34;bc40f10a-e50c-4178-9288-8234b336077b\u0026#34; The time it takes for a firmware update will vary. It can take a couple of minutes, or over 20 minutes, depending on how many items must be updated and response time.\n    Retrieve the operationID and verify that the update is complete.\nncn-w001# cray fas actions describe {actionID} [[operationSummary.noOperation.operationKeys]] stateHelper = \u0026#34; fromFirmwareVersion = \u0026#34;wnc.bios-1.2.4\u0026#34; xname = \u0026#34;x1005c4s4b0\u0026#34; target = \u0026#34;Node1.BIOS\u0026#34; operationID = \u0026#34;24ccb221-b95a-4ccc-8c56-2c81a361f824\u0026#34;   View more details on an operation using the operationID from the previous step.\nncn-w001# cray fas operations describe {operationID} The following example shows a Windom Node Card (WNC) for a liquid-cooled AMD EPYC compute blade.\nncn-w001# cray fas operations describe \u0026#34;24ccb221-b95a-4ccc-8c56-2c81a361f824\u0026#34; fromFirmwareVersion = \u0026#34;wnc.bios-1.2.4\u0026#34; fromTag = \u0026#34; fromImageURL = \u0026#34; endTime = \u0026#34;2020-08-31 17:05:11.301977333 +0000 UTC\u0026#34; actionID = \u0026#34;a88d8207-8bca-4ba1-9b80-14772e5f3f34\u0026#34; startTime = \u0026#34;2020-08-31 17:05:11.301957482 +0000 UTC\u0026#34; fromSemanticFirmwareVersion = \u0026#34;1.2.4\u0026#34; toImageURL = \u0026#34; model = \u0026#34;WNC-Rome\u0026#34; operationID = \u0026#34;24ccb221-b95a-4ccc-8c56-2c81a361f824\u0026#34; fromImageID = \u0026#34;b9c332dd-8af5-4a38-a4e7-3d0b324cb2a8\u0026#34; target = \u0026#34;Node1.BIOS\u0026#34; toImageID = \u0026#34;b9c332dd-8af5-4a38-a4e7-3d0b324cb2a8\u0026#34; toSemanticFirmwareVersion = \u0026#34;1.2.4\u0026#34; refreshTime = \u0026#34;2020-08-31 17:05:15.069895134 +0000 UTC\u0026#34; blockedBy = [] toTag = \u0026#34; state = \u0026#34;noOperation\u0026#34; stateHelper = \u0026#34; deviceType = \u0026#34;NodeBMC\u0026#34; expirationTime = \u0026#34;2020-08-31 19:51:51.301959012 +0000 UTC\u0026#34; manufacturer = \u0026#34;cray\u0026#34; xname = \u0026#34;x1005c4s4b0\u0026#34; toFirmwareVersion = \u0026#34;wnc.bios-1.2.4\u0026#34;   The nodes can be powered back on after the BIOS is updated.\n Compute Node BIOS Workaround for HPE CRAY EX425 Correct an issue where the model of the liquid-cooled compute node BIOS is the incorrect name. The name has changed from WNC-ROME to HPE CRAY EX425 or HPE CRAY EX425 (ROME).\nPrerequisites   The system is running HPE Cray EX release v1.4 or higher.\n  The system has completed the Cray System Management (CSM) installation.\n  A firmware upgrade has been done following Update Liquid-Cooled Compute Node BIOS Firmware.\n The result of the upgrade is that the NodeX.BIOS has failed as noSolution and the stateHelper field for the operation states: \u0026quot;No Image Available\u0026quot;. The BIOS in question is running a version less than or equal to 1.2.5 as reported by Redfish or described by the noSolution operation in FAS.    The hardware model reported by Redfish is wnc-rome, which is now designated as HPE CRAY EX425.\nIf the Redfish model is different (ignoring casing) and the blades in question are not Windom, contact customer support. To find the model reported by Redfish:\nncn# cray fas operations describe {operationID} --format json { \u0026#34;operationID\u0026#34;:\u0026#34;102c949f-e662-4019-bc04-9e4b433ab45e\u0026#34;, \u0026#34;actionID\u0026#34;:\u0026#34;9088f9a2-953a-498d-8266-e2013ba2d15d\u0026#34;, \u0026#34;state\u0026#34;:\u0026#34;noSolution\u0026#34;, \u0026#34;stateHelper\u0026#34;:\u0026#34;No Image available\u0026#34;, \u0026#34;startTime\u0026#34;:\u0026#34;2021-03-08 13:13:14.688500503 +0000 UTC\u0026#34;, \u0026#34;endTime\u0026#34;:\u0026#34;2021-03-08 13:13:14.688508333 +0000 UTC\u0026#34;, \u0026#34;refreshTime\u0026#34;:\u0026#34;2021-03-08 13:13:14.722345901 +0000 UTC\u0026#34;, \u0026#34;expirationTime\u0026#34;:\u0026#34;2021-03-08 15:59:54.688500753 +0000 UTC\u0026#34;, \u0026#34;xname\u0026#34;:\u0026#34;x9000c1s0b0\u0026#34;, \u0026#34;deviceType\u0026#34;:\u0026#34;NodeBMC\u0026#34;, \u0026#34;target\u0026#34;:\u0026#34;Node1.BIOS\u0026#34;, \u0026#34;targetName\u0026#34;:\u0026#34;Node1.BIOS\u0026#34;, \u0026#34;manufacturer\u0026#34;:\u0026#34;cray\u0026#34;, \u0026#34;model\u0026#34;:\u0026#34;WNC-Rome\u0026#34;, \u0026#34;softwareId\u0026#34;:\u0026#34;, \u0026#34;fromImageID\u0026#34;:\u0026#34;00000000-0000-0000-0000-000000000000\u0026#34;, \u0026#34;fromSemanticFirmwareVersion\u0026#34;:\u0026#34;, \u0026#34;fromFirmwareVersion\u0026#34;:\u0026#34;wnc.bios-1.2.5\u0026#34;, \u0026#34;fromImageURL\u0026#34;:\u0026#34;, \u0026#34;fromTag\u0026#34;:\u0026#34;, \u0026#34;toImageID\u0026#34;:\u0026#34;00000000-0000-0000-0000-000000000000\u0026#34;, \u0026#34;toSemanticFirmwareVersion\u0026#34;:\u0026#34;, \u0026#34;toFirmwareVersion\u0026#34;:\u0026#34;, \u0026#34;toImageURL\u0026#34;:\u0026#34;, \u0026#34;toTag\u0026#34;:\u0026#34;, \u0026#34;blockedBy\u0026#34;:[ ] } The model in this example is WNC-Rome and the firmware version currently running is wnc.bios-1.2.5.\n  Procedure   Search for a FAS image record with cray as the manufacturer, Node1.BIOS as the target, and HPE CRAY EX425 as the model.\nncn# cray fas images list --format json | jq \u0026#39;.images[] | select(.manufacturer==\u0026#34;cray\u0026#34;) \\ | select(.target==\u0026#34;Node1.BIOS\u0026#34;) | select(any(.models[]; contains(\u0026#34;EX425\u0026#34;)))\u0026#39; { \u0026#34;imageID\u0026#34;: \u0026#34;e23f5465-ed29-4b18-9389-f8cf0580ca60\u0026#34;, \u0026#34;createTime\u0026#34;: \u0026#34;2021-03-04T00:04:05Z\u0026#34;, \u0026#34;deviceType\u0026#34;: \u0026#34;nodeBMC\u0026#34;, \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34;, \u0026#34;models\u0026#34;: [ \u0026#34;HPE CRAY EX425\u0026#34; ], \u0026#34;softwareIds\u0026#34;: [ \u0026#34;bios.ex425..\u0026#34; ], \u0026#34;target\u0026#34;: \u0026#34;Node1.BIOS\u0026#34;, \u0026#34;tags\u0026#34;: [ \u0026#34;default\u0026#34; ], \u0026#34;firmwareVersion\u0026#34;: \u0026#34;ex425.bios-1.4.3\u0026#34;, \u0026#34;semanticFirmwareVersion\u0026#34;: \u0026#34;1.4.3\u0026#34;, \u0026#34;pollingSpeedSeconds\u0026#34;: 30, \u0026#34;s3URL\u0026#34;: \u0026#34;s3:/fw-update/2227040f7c7d11eb9fa00e2f2e08fd5d/ex425.bios-1.4.3.tar.gz\u0026#34; } Take note of the returned imageID value to use in the next step.\n  Create a JSON file to override the existing image with the corrected values.\nIMPORTANT: The imageID must be changed to match the identified image ID in the previous step.\n{ \u0026#34;stateComponentFilter\u0026#34;:{ \u0026#34;deviceTypes\u0026#34;:[ \u0026#34;nodeBMC\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;:{ \u0026#34;manufacturer\u0026#34;:\u0026#34;cray\u0026#34; }, \u0026#34;targetFilter\u0026#34;:{ \u0026#34;targets\u0026#34;:[ \u0026#34;Node0.BIOS\u0026#34;, \u0026#34;Node1.BIOS\u0026#34; ] }, \u0026#34;imageFilter\u0026#34;:{ \u0026#34;imageID\u0026#34;:\u0026#34;e23f5465-ed29-4b18-9389-f8cf0580ca60\u0026#34;, \u0026#34;overrideImage\u0026#34;:true }, \u0026#34;command\u0026#34;:{ \u0026#34;version\u0026#34;:\u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;:\u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;:true, \u0026#34;restoreNotPossibleOverride\u0026#34;:true, \u0026#34;timeLimit\u0026#34;:1000, \u0026#34;description\u0026#34;:\u0026#34; upgrade of Node BIOS\u0026#34; } }   Run a firmware upgrade using the updated parameters defined in the new JSON file.\nncn# cray fas actions create UPDATED_COMMAND.json   Get a high-level summary of the job to verify the changes corrected the issue.\nUse the returned actionID from the cray fas actions create command.\nncn# cray fas actions create UPDATED_COMMAND.json   "
},
{
	"uri": "/docs-csm/en-11/operations/firmware/fas_recipes/",
	"title": "Recipes",
	"tags": [],
	"description": "",
	"content": "Recipes The following example JSON files are useful to reference when updating specific hardware components. In all of these examples, the overrideDryrun field will be set to false; set them to true to perform a live update.\nWhen updating an entire system, walk down the device hierarchy component type by component type, starting first with \u0026lsquo;Routers\u0026rsquo; (switches), proceeding to Chassis, and then finally to Nodes. While this is not strictly necessary, it does help eliminate confusion.\nRefer to FAS Filters for more information on the content used in the example JSON files.\n Manufacturer : Cray Device Type: ChassisBMC | Target: BMC IMPORTANT: Before updating a CMM, make sure all slot and rectifier power is off. The hms-discovery job must also be stopped before updates and restarted after updates are complete.\n Stop hms-discovery job: kubectl -n services patch cronjobs hms-discovery -p '{\u0026quot;spec\u0026quot;:{\u0026quot;suspend\u0026quot;:true}}' Start hms-discovery job: kubectl -n services patch cronjobs hms-discovery -p '{\u0026quot;spec\u0026quot;:{\u0026quot;suspend\u0026quot;:false}}'  { \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34; }, \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;chassisBMC\u0026#34; ] }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;BMC\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of Cray Chassis Controllers\u0026#34; } } Device Type: NodeBMC | Target: BMC { \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;BMC\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of Olympus node BMCs\u0026#34; } } Device Type: NodeBMC | Target: NodeBIOS IMPORTANT:\n The Nodes themselves must be powered off in order to update the BIOS on the nodes. The BMC will still have power and will perform the update. When the BMC is updated or rebooted after updating the Node0.BIOS and/or Node1.BIOS liquid-cooled nodes, the node BIOS version will not report the new version string until the nodes are powered back on. It is recommended that the Node0/1 BIOS be updated in a separate action, either before or after a BMC update and the nodes are powered back on after a BIOS update. The liquid-cooled nodes must be powered off for the BIOS to be updated.  { \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;Node0.BIOS\u0026#34;, \u0026#34;Node1.BIOS\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of Node BIOS\u0026#34; } } NOTE: If this update does not work as expected, follow the Compute Node BIOS Workaround for HPE CRAY EX425 procedure.\nDevice Type: NodeBMC | Target: Redstone FPGA IMPORTANT:\n The Nodes themselves must be powered on in order to update the firmware of the Redstone FPGA on the nodes. If updating FPGAs fails because of \u0026ldquo;No Image available\u0026rdquo;, update using the \u0026ldquo;Override an Image for an Update\u0026rdquo; procedure in FAS Admin Procedures. Find the imageID using the following command: cray fas images list --format json | jq '.[] | .[] | select(.target==\u0026quot;Node0.AccFPGA0\u0026quot;)'  { \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;Node0.AccFPGA0\u0026#34;, \u0026#34;Node1.AccFPGA0\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of Node Redstone FPGA\u0026#34; } }  Manufacturer: HPE Device Type: NodeBMC | Target: iLO 5 aka BMC { \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;hpe\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;iLO 5\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of HPE node iLO 5\u0026#34; } } Device Type: NodeBMC | Target: System ROM aka BIOS IMPORTANT:\n If updating the System ROM of an NCN, the NTP and DNS server values will be lost and must be restored. For NCNs other than ncn-m001 this can be done using the /opt/cray/csm/scripts/node_management/set-bmc-ntp-dns.sh script. Use the -h option to get a list of command line options required to restore the NTP and DNS values. See Configure DNS and NTP on Each BMC Node should be powered on for System ROM update and will need to be rebooted to use the updated BIOS.  { \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;NodeBMC\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;hpe\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;System ROM\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of HPE node system rom\u0026#34; } } NOTE:\n Update of System ROM may report as an error when it actually succeeded because of an incorrect string in the image metadata in FAS. Manually check the update version to get around this error.   Manufacturer: Gigabyte Device Type: NodeBMC | Target: BMC { \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;gigabyte\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;BMC\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 4000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of Gigabyte node BMCs\u0026#34; } } NOTE: The timeLimit is 4000 because the Gigabytes can take a lot longer to update.\nYou may receive a node failed to update with the output: stateHelper = \u0026quot;Firmware Update Information Returned Downloading – See /redfish/v1/UpdateService\u0026quot; FAS has incorrectly marked this node as failed. It most likely will complete the update successfully. You can check the update status by looking at the Redfish FirmwareInventory (/redfish/v1/UpdateService/FirmwareInventory/BMC) or rerunning FAS to verify that the BMC firmware was updated. Make sure you have waited for the current firmware to be updated before starting a new FAS action on the same node.\nDevice Type: NodeBMC | Target: BIOS { \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;gigabyte\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;BIOS\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 4000, \u0026#34;description\u0026#34;: \u0026#34;Dryrun upgrade of Gigabyte node BIOS\u0026#34; } } Update Non-Compute Nodes (NCNs) See Uploading BIOS and BMC Firmware for NCNs in FAS Use Cases\n"
},
{
	"uri": "/docs-csm/en-11/operations/firmware/update_firmware_with_fas/",
	"title": "Update Firmware With FAS",
	"tags": [],
	"description": "",
	"content": "Update Firmware with FAS The Firmware Action Service (FAS) provides an interface for managing firmware versions of Redfish-enabled hardware in the system. FAS interacts with the Hardware State Managers (HSM), device data, and image data in order to update firmware.\nReset Gigabyte node BMC to factory defaults if having problems with ipmitool, using Redfish, or when flashing procedures fail. See Set Gigabyte Node BMC to Factory Defaults.\nFAS images contain the following information that is needed for a hardware device to update firmware versions:\n Hardware-specific information: Contains the allowed device states and how to reboot a device if necessary. Selection criteria: How to link a firmware image to a specific hardware type. Image data: Where the firmware image resides in Simple Storage Service (S3) and what firmwareVersion it will report after it is successfully applied. See Artifact Management for more information.  Topics  Prerequisites Warning Current Capabilities Order Of Operations Hardware Precedence Order FAS Admin Procedures Firmware Actions Firmware Operations Firmware Images  Prerequisites  CSM software has been installed, firmware has been loaded into FAS as part of the HPC Firmware Pack (HFP) install, HSM is running, and nodes have been discovered. All management nodes have been locked. Identify the type and manufacturers of hardware in the system. If Gigabyte nodes are not in use on the system, do not update them!  Warning WARNING: Non-compute nodes (NCNs) should be locked with the HSM locking API to ensure they are not unintentionally updated by FAS. See Lock and Unlock Management Nodes for more information. Failure to lock the NCNs could result in unintentional update of the NCNs if FAS is not used correctly; this will lead to system instability problems.\nFollow the process outlined in FAS CLI to update the system. Use the recipes listed in FAS Recipes to update each supported type.\nNOTE: Each system is different and may not have all hardware options.\nCurrent Capabilities The following table describes the hardware items that can have their firmware updated via FAS. For more information about the upgradable targets, refer to the Firmware product stream repository.\nTable 1. Upgradable Firmware Items\n   Manufacturer Type Target     Cray nodeBMC BMC, Node0.BIOS, Node1.BIOS, Recovery, Node1.AccFPGA0, Node0.AccFPGA0   Cray chassisBMC BMC, Recovery   Cray routerBMC BMC, Recovery   Gigabyte nodeBMC BMC, BIOS   HPE nodeBMC iLO 5 (BMC aka 1 ), System ROM ,Redundant System ROM (BIOS aka 2)    Order Of Operations For each item in the Hardware Precedence Order:\n  Complete a dry-run:\n cray fas actions create {jsonfile} Note the ActionID! Poll the status of the action until the action state is completed:  cray fas actions describe {actionID} --format json      Interpret the outcome of the dry-run. Look at the counts and determine if the dry-run identified any hardware to update:\nFor the steps below, the following returned messages will help determine if a firmware update is needed. The following are end states for operations. The Firmware action itself should be in completed once all operations have finished.\n NoOp: Nothing to do, already at version. NoSol: No viable image is available; this will not be updated. succeeded: IF dryrun: The operation should succeed if performed as a live update. succeeded means that FAS identified that it COULD update an xname + target with the declared strategy. IF live update: the operation succeeded, and has updated the xname + target to the identified version. failed: IF dryrun: There is something that FAS could do, but it likely would fail; most likely because the file is missing. IF live update: the operation failed, the identified version could not be put on the xname + target.    If succeeded count \u0026gt; 0, now perform a live update.\n  Update the JSON file overrideDryrun to true:\n cray fas actions create {jsonfile} Note the ActionID! Poll the status of the action until the action state is completed:  cray fas actions describe {actionID} --format json      Interpret the outcome of the live update; proceed to next type of hardware.\n  Hardware Precedence Order After identifying which hardware is in the system, start with the top most item on this list to update. If any of the following hardware is not in the system, skip it.\nIMPORTANT:\n This process does not communicate the SAFE way to update NCNs. If the NCNs have not been locked, or FAS is blindly used to update NCNs without following the correct process, then THE STABILITY OF THE SYSTEM WILL BE JEOPARDIZED. Read the corresponding recipes before updating. There are sometimes ancillary actions that must be completed in order to ensure update integrity.  NOTE to update Switch Controllers (sC) or RouterBMC refer to the Rosetta Documentation\n Cray  ChassisBMC NodeBMC  BMC NodeBIOS Redstone FPGA     Gigabyte  BMC BIOS   HPE  BMC (iLO5) BIOS (System ROM)    FAS Admin Procedures There are several use cases for using the FAS to update firmware on the system. These use cases are intended to be run by system administrators with a good understanding of firmware. Under no circumstances should non-administrator users attempt to use FAS or perform a firmware update.\n Perform a firmware update: Update the firmware of an xname\u0026rsquo;s target to the latest, earliest, or an explicit version. Determine what hardware can be updated by performing a dry-run: This is the easiest way to determine what can be updated. Take a snapshot of the system: Record the firmware versions present on each target for the identified xnames. If the firmware version corresponds to an image available in the images repository, link the imageID to the record. Restore the snapshot of the system: Take the previously recorded snapshot and use the related imageIDs to put the xname/targets back to the firmware version they were at, at the time of the snapshot. Provide firmware for updating: FAS can only update an xname/target if it has an image record that is applicable. Most administrators will not encounter this use case.  Firmware Actions An action is collection of operations, which are individual firmware update tasks. Only one FAS action can be run at a time. Any other attempted action will be queued. Additionally, only one operation can be run on an xname at a time. For example, if there are 1000 xnames with 5 targets each to be updated, all 1000 xnames can be updating a target, but only 1 target on each xname will be updated at a time.\nThe life cycle of any action can be divided into the static and dynamic portions of the life cycle.\nThe static portion of the life cycle is where the action is created and configured. It begins with a request to create an action through either of the following requests:\n Direct: Request to /actions API. Indirect: Request to restore a snapshot via the /snapshots API.  The dynamic portion of the life cycle is where the action is executed to completion. It begins when the actions is transitioned from the new to configured state. The action will then be ultimately transitioned to an end state of aborted or completed.\nFirmware Operations Operations are individual tasks in a FAS action. FAS will create operations based on the configuration sent through the actions create command. FAS operations will have one of the following states:\n initial - Operation just created. configured - The operation is configured, but nothing has been started. blocked - Only one operation can be performed on a node at a time. If more than one update is required for a xname, operations will be blocked. This will have a message of \u0026ldquo;blocked by sibling\u0026rdquo;. inProgress - Update is in progress, but not completed. verifying - Waiting for update to complete. failed - An update was attempted, but FAS is unable to tell that the update succeeded in the allotted time. noOperation - Firmware is at the correct version according to the images loaded into FAS. noSolution - FAS does not have a suitable image for an update. aborted - the operation was aborted before it could determine if it was successful. If aborted after the update command was sent to the node, the node may still have updated.  Firmware Images FAS requires images in order to update firmware for any device on the system. An image contains the data that allows FAS to establish a link between an admin command, available devices (xname/targets), and available firmware.\nThe following is an example of an image:\n{ \u0026#34;imageID\u0026#34;: \u0026#34;3fa85f64-5717-4562-b3fc-2c963f66afa6\u0026#34;, \u0026#34;createTime\u0026#34;: \u0026#34;2020-05-11T17:11:07.017Z\u0026#34;, \u0026#34;deviceType\u0026#34;: \u0026#34;nodeBMC\u0026#34;, \u0026#34;manufacturer\u0026#34;: \u0026#34;intel\u0026#34;, \u0026#34;model\u0026#34;: [\u0026#34;s2600\u0026#34;,\u0026#34;s2600_REV_a\u0026#34;], \u0026#34;target\u0026#34;: \u0026#34;BIOS\u0026#34;, \u0026#34;tag\u0026#34;: [\u0026#34;recovery\u0026#34;, default\u0026#34;], \u0026#34;firmwareVersion\u0026#34;: \u0026#34;f1.123.24xz\u0026#34;, \u0026#34;semanticFirmwareVersion\u0026#34;: \u0026#34;v1.2.252\u0026#34;, \u0026#34;updateURI\u0026#34;: \u0026#34;/redfish/v1/Systems/UpdateService/BIOS\u0026#34;, \u0026#34;needManualReboot\u0026#34;: true, \u0026#34;waitTimeBeforeManualRebootSeconds\u0026#34;: 600, \u0026#34;waitTimeAfterRebootSeconds\u0026#34;: 180, \u0026#34;pollingSpeedSeconds\u0026#34;: 30, \u0026#34;forceResetType\u0026#34;: \u0026#34;ForceRestart\u0026#34;, \u0026#34;s3URL\u0026#34;: \u0026#34;s3://firmware/f1.1123.24.xz.iso\u0026#34;, \u0026#34;allowableDeviceStates\u0026#34;: [ \u0026#34;On\u0026#34;, \u0026#34;Off\u0026#34; ] } The main components of an image are described below:\n  Key\nThis includes the deviceType, manufacturer, model, target, tag, semanticFirmwareVersion (firmware version) fields.\nThese fields are how administrators assess what firmware is on a device, and if an image is applicable to that device.\n  Process guides\nThis includes the forceResetType, pollingSpeedSeconds, waitTime(s), allowableDeviceStates fields.\nFAS gets information about how to update the firmware from these fields. These values determine if FAS is responsible for rebooting the device, and what communication pattern to use.\n  s3URL\nThe URL that FAS uses to get the firmware binary and the download link that is supplied to Redfish devices. Redfish devices are not able to directly communicate with S3.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/firmware/fas_admin_procedures/",
	"title": "FAS Admin Procedures",
	"tags": [],
	"description": "",
	"content": "FAS Admin Procedures \nProcedures for leveraging the Firmware Action Service (FAS) CLI to manage firmware.\nTopics  FAS Admin Procedures  Topics Warning for Non-Compute Nodes (NCNs) Ignore Nodes within FAS  Procedure   Override an Image for an Update Procedure Check for New Firmware Versions with a Dry-Run  Procedure   Load Firmware from Nexus Load Firmware from RPM or ZIP file     Warning for Non-Compute Nodes (NCNs) WARNING: NCNs should be locked with the HSM locking API to ensure they are not unintentionally updated by FAS. Research Lock and Unlock Management Nodes for more information. Failure to lock the NCNs could result in unintentional update of the NCNs if FAS is not used correctly; this will lead to system instability problems.\n Ignore Nodes within FAS The default configuration of FAS no longer ignores management nodes, which prevents FAS from firmware updating the NCNs. To reconfigure the FAS deployment to exclude non-compute nodes (NCNs) and ensure they cannot have their firmware upgraded, the NODE_BLACKLIST value must be manually enabled\nNodes can also be locked with the Hardware State Manager (HSM) API. Refer to Lock and Unlock Management Nodes for more information.\nProcedure   Check that there are no FAS actions running.\nncn# cray fas actions list   Edit the cray-fas deployment.\nncn# kubectl -n services edit deployment cray-fas   Change the NODE_BLACKLIST value from ignore_ignore_ignore to management.\n  Save and quit the deployment. This will restart FAS.\n   Override an Image for an Update If an update fails because of \u0026quot;No Image available\u0026quot;, it may be caused by FAS unable to match the data on the node to find an image in the image list.\n\nProcedure   Find the available image in FAS.\nChange TARGETNAME to the actual target being searched.\nncn# cray fas images list --format json | jq \u0026#39;.[] | .[] | select(.target==\u0026#34;TARGETNAME\u0026#34;)\u0026#39; To narrow down the selection, update the select field to match multiple items. For example:\nncn# cray fas images list --format json | jq \u0026#39;.[] | .[] | select(.target==\u0026#34;BMC\u0026#34; and .manufacturer==\u0026#34;cray\u0026#34; and .deviceType==\u0026#34;NodeBMC\u0026#34;)\u0026#39; The example command displays one or more images available for updates.\n{ \u0026quot;imageID\u0026quot;: \u0026quot;ff268e8a-8f73-414f-a9c7-737a34bb02fc\u0026quot;, \u0026quot;createTime\u0026quot;: \u0026quot;2021-02-24T02:25:03Z\u0026quot;, \u0026quot;deviceType\u0026quot;: \u0026quot;nodeBMC\u0026quot;, \u0026quot;manufacturer\u0026quot;: \u0026quot;cray\u0026quot;, \u0026quot;models\u0026quot;: [ \u0026quot;HPE Cray EX235n\u0026quot;, \u0026quot;GrizzlyPkNodeCard_REV_B\u0026quot; ], \u0026quot;softwareIds\u0026quot;: [ \u0026quot;fgpa:NVIDIA.HGX.A100.4.GPU:*:*\u0026quot; ], \u0026quot;target\u0026quot;: \u0026quot;Node0.AccFPGA0\u0026quot;, \u0026quot;tags\u0026quot;: [ \u0026quot;default\u0026quot; ], \u0026quot;firmwareVersion\u0026quot;: \u0026quot;2.7\u0026quot;, \u0026quot;semanticFirmwareVersion\u0026quot;: \u0026quot;2.7.0\u0026quot;, \u0026quot;pollingSpeedSeconds\u0026quot;: 30, \u0026quot;s3URL\u0026quot;: \u0026quot;s3:/fw-update/80a62641764711ebabe28e2b78a05899/accfpga_nvidia_2.7.tar.gz\u0026quot; } If the firmwareVersion from the FAS image matches the fromFirmwareVersion from the FAS action, the firmware is at the latest version and no update is needed.\n  Use the imageID from the cray images list in the previous step and add the following line to the action JSON file, replacing IMAGEID with the imageID.\nIn this example, the value would be: ff268e8a-8f73-414f-a9c7-737a34bb02fc.\n\u0026#34;imageFilter\u0026#34;: { \u0026#34;imageID\u0026#34;:\u0026#34;IMAGEID\u0026#34;, \u0026#34;overrideImage\u0026#34;:true } Example actions JSON file with imageFilter added:\n{ \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;:[\u0026#34;nodeBMC\u0026#34;] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;:\u0026#34;cray\u0026#34; }, \u0026#34;imageFilter\u0026#34;: { \u0026#34;imageID\u0026#34;:\u0026#34;ff268e8a-8f73-414f-a9c7-737a34bb02fc\u0026#34;, \u0026#34;overrideImage\u0026#34;:true }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;:[\u0026#34;Node0.AccFPGA0\u0026#34;,\u0026#34;Node1.AccFPGA0\u0026#34;] }, \u0026#34;command\u0026#34;: { \u0026#34;overrideDryrun\u0026#34;:false, \u0026#34;restoreNotPossibleOverride\u0026#34;:true, \u0026#34;overwriteSameImage\u0026#34;:false } }   Verify the correct image ID was found.\nncn# cray fas images describe {imageID} WARNING: FAS will force a flash of the device, using incorrect firmware may make it inoperable.\n  Re-run the FAS actions command using the updated json file. It is strongly recommended to run a dry-run (overrideDryrun=false) first and check the actions output.\n Check for New Firmware Versions with a Dry-Run Use the Firmware Action Service (FAS) dry-run feature to determine what firmware can be updated on the system. Dry-runs are enabled by default, and can be configured with the overrideDryrun parameter. A dry-run will create a query according to the filters requested by the admin. It will initiate an update sequence to determine what firmware is available, but will not actually change the state of the firmware.\nWARNING: It is crucial that an admin is familiar with the release notes of any firmware. The release notes will indicate what new features the firmware provides and if there are any incompatibilities. FAS does not know about incompatibilities or dependencies between versions. The admin assumes full responsibility for this knowledge.\nIt is likely that when performing a firmware update, that the current version of firmware will not be available. This means that after successfully upgrading, the firmware cannot be downgraded.\nThis procedure includes information on how check the firmware versions for the entire system, as well as how to target specific manufacturers, xnames, and targets.\nProcedure   Run a dry-run firmware update.\nThe following command parameters should be included in dry-run JSON files:\n overrideDryrun: The overrideDryrun parameter is set to false by default. FAS will only update the system if this is parameter is set to true. restoreNotPossibleOverride: FAS will not perform an update if the currently running firmware is not available in the images repository. Set to true to allow FAS to update firmware, even if the current firmware is unavailable on the system. description: A brief description that helps administrators distinguish between actions. version: Determine if the firmware should be set to the latest, the earliest semantic version, or set to a specific firmware version.  Use one of the options below to run on a dry-run on every system device or on targeted devices:\nOption 1: Determine the available firmware for every device on the system:\n  Create a JSON file for the command parameters.\n{ \u0026#34;command\u0026#34;: { \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;full system dryrun 2020623_0\u0026#34; } }   Run the dry-run for the full system.\nncn# cray fas actions create COMMAND.json Proceed to the next step to determine if any firmware needs to be updated.\n  Option 2: Determine the available firmware for specific devices:\n  Create a JSON file with the specific device information to target when doing a dry-run.\n{ \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;xnames\u0026#34;: [ \u0026#34;x9000c1s3b1\u0026#34; ] }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;Node1.BIOS\u0026#34;, \u0026#34;Node0.BIOS\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;dryrun upgrade of x9000c1s3b1 Nodex.BIOS to WNC 1.1.2\u0026#34; } }   Run a dry-run on the targeted devices.\nncn# cray fas actions create CUSTOM_DEVICE_PARAMETERS.json Proceed to the next step to determine if any firmware needs to be updated.\n    View the status of the dry-run to determine if any firmware updates can be made.\nThe following returned messages will help determine if a firmware update is needed.\n noOperation: Nothing to do, already at version. noSolution: No image is available or data is missing. succeeded: A firmware version that FAS can update the firmware to is available and it should work when actually updating the firmware. failed: There is something that FAS could do, but it likely would fail; most likely because the file is missing.    Get a high-level summary of the FAS job to determine if there are any upgradable firmware images available.\nUse the returned actionID from the cray fas actions create command.\nIn the example below, there are two operations in the succeeded state, indicating there is an available firmware version that FAS can use to update firmware.\n  ncn# cray fas actions status list {actionID} actionID = \u0026quot;e6dc14cd-5e12-4d36-a97b-0dd372b0930f\u0026quot; snapshotID = \u0026quot;00000000-0000-0000-0000-000000000000\u0026quot; startTime = \u0026quot;2021-09-07 16:43:04.294233199 +0000 UTC\u0026quot; endTime = \u0026quot;2021-09-07 16:53:09.363233482 +0000 UTC\u0026quot; state = \u0026quot;completed\u0026quot; blockedBy = [] [command] overrideDryrun = false restoreNotPossibleOverride = true overwriteSameImage = false timeLimit = 2000 version = \u0026quot;latest\u0026quot; tag = \u0026quot;default\u0026quot; description = \u0026quot;Dryrun upgrade of Gigabyte node BMCs\u0026quot; [operationCounts] total = 14 initial = 0 configured = 0 blocked = 0 needsVerified = 0 verifying = 0 inProgress = 0 failed = 0 succeeded = 8 noOperation = 6 noSolution = 0 aborted = 0 unknown = 0 The action is still in progress if the state field is not completed or aborted.\n View the details of an action to get more information on each operation in the FAS action.\nIn the example below, there is an operation for an xname in the failed state, indicating there is something that FAS could do, but it likely would fail. A common cause for an operation failing is due to a missing firmware image file.\nncn# cray fas actions describe {actionID} --format json { \u0026#34;parameters\u0026#34;: { \u0026#34;stateComponentFilter\u0026#34;: { \u0026#34;deviceTypes\u0026#34;: [ \u0026#34;nodeBMC\u0026#34; ] }, \u0026#34;command\u0026#34;: { \u0026#34;dryrun\u0026#34;: false, \u0026#34;description\u0026#34;: \u0026#34;upgrade of nodeBMCs for cray\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34; }, \u0026#34;inventoryHardwareFilter\u0026#34;: { \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34; }, \u0026#34;imageFilter\u0026#34;: { \u0026#34;imageID\u0026#34;: \u0026#34;00000000-0000-0000-0000-000000000000\u0026#34; }, \u0026#34;targetFilter\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;BMC\u0026#34; ] } }, \u0026#34;blockedBy\u0026#34;: [], \u0026#34;state\u0026#34;: \u0026#34;completed\u0026#34;, \u0026#34;command\u0026#34;: { \u0026#34;dryrun\u0026#34;: false, \u0026#34;description\u0026#34;: \u0026#34;upgrade of nodeBMCs for cray\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34; }, \u0026#34;actionID\u0026#34;: \u0026#34;e0cdd7c2-32b1-4a25-9b2a-8e74217eafa7\u0026#34;, \u0026#34;startTime\u0026#34;: \u0026#34;2020-06-26 20:03:37.316932354 +0000 UTC\u0026#34;, \u0026#34;snapshotID\u0026#34;: \u0026#34;00000000-0000-0000-0000-000000000000\u0026#34;, \u0026#34;endTime\u0026#34;: \u0026#34;2020-06-26 20:04:07.118243184 +0000 UTC\u0026#34;, \u0026#34;operationSummary\u0026#34;: { \u0026#34;succeeded\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;verifying\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;unknown\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;configured\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;initial\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;failed\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [ { \u0026#34;stateHelper\u0026#34;: \u0026#34;unexpected change detected in firmware version. Expected sc.1.4.35-prod- master.arm64.2020-06-26T08:36:42+00:00.0c2bb02 got: sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026#34;, \u0026#34;fromFirmwareVersion\u0026#34;: \u0026#34;, \u0026#34;xname\u0026#34;: \u0026#34;x5000c1r7b0\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;BMC\u0026#34;, \u0026#34;operationID\u0026#34;: \u0026#34;0796eed0-e95d-45ea-bc71-8903d52cffde\u0026#34; }, ] }, \u0026#34;noSolution\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;aborted\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;needsVerified\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;noOperation\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;inProgress\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] }, \u0026#34;blocked\u0026#34;: { \u0026#34;OperationsKeys\u0026#34;: [] } } }     View the details for a specific operation.\nIn this example, there is a device that is available for a firmware upgrade because the operation being viewed is a succeeded operation.\nncn# cray fas operations describe {operationID} --format json { \u0026#34;fromFirmwareVersion\u0026#34;: \u0026#34;, \u0026#34;fromTag\u0026#34;: \u0026#34;, \u0026#34;fromImageURL\u0026#34;: \u0026#34;, \u0026#34;endTime\u0026#34;: \u0026#34;2020-06-24 14:23:37.544814197 +0000 UTC\u0026#34;, \u0026#34;actionID\u0026#34;: \u0026#34;f48aabf1-1616-49ae-9761-a11edb38684d\u0026#34;, \u0026#34;startTime\u0026#34;: \u0026#34;2020-06-24 14:19:15.10128214 +0000 UTC\u0026#34;, \u0026#34;fromSemanticFirmwareVersion\u0026#34;: \u0026#34;, \u0026#34;toImageURL\u0026#34;: \u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;WindomNodeCard_REV_D\u0026#34;, \u0026#34;operationID\u0026#34;: \u0026#34;24a5e5fb-5c4f-4848-bf4e-b071719c1850\u0026#34;, \u0026#34;fromImageID\u0026#34;: \u0026#34;00000000-0000-0000-0000-000000000000\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;BMC\u0026#34;, \u0026#34;toImageID\u0026#34;: \u0026#34;71c41a74-ab84-45b2-95bd-677f763af168\u0026#34;, \u0026#34;toSemanticFirmwareVersion\u0026#34;: \u0026#34;, \u0026#34;refreshTime\u0026#34;: \u0026#34;2020-06-24 14:23:37.544824938 +0000 UTC\u0026#34;, \u0026#34;blockedBy\u0026#34;: [], \u0026#34;toTag\u0026#34;: \u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;succeeded\u0026#34;, \u0026#34;stateHelper\u0026#34;: \u0026#34;unexpected change detected in firmware version. Expected nc.1.3.8-shasta-release.arm.2020-06-15T22:57:31+00:00.b7f0725 got: nc.1.2.25-shasta-release.arm.2020-05-15T17:27:16+00:00.0cf7f51\u0026#34;, \u0026#34;deviceType\u0026#34;: \u0026#34;, \u0026#34;expirationTime\u0026#34;: \u0026#34;, \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34;, \u0026#34;xname\u0026#34;: \u0026#34;x9000c1s3b1\u0026#34;, \u0026#34;toFirmwareVersion\u0026#34;: \u0026#34; } ```   Update the firmware on any devices indicating a new version is needed.\n Load Firmware from Nexus This procedure will read all RPMs in the Nexus repository and upload firmware images to S3 and create image records for firmware not already in FAS.\n  Check the loader status.\nncn# cray fas loader list | grep loaderStatus This will return a ready or busy status.\nloaderStatus = \u0026#34;ready\u0026#34; The loader can only run one job at a time, if the loader is busy, it will return an error on any attempt to create an additional job.\n  Run the loader Nexus command.\nncn# cray fas loader nexus create This will return an ID which will be used to check the status of the run.\nloaderRunID = \u0026#34;7b0ce40f-cd6d-4ff0-9b71-0f3c9686f5ce\u0026#34; NOTE: Depending on how many files are in Nexus and how large those files are, the loader may take several minutes to complete.\n  Check the results of the loader run.\nncn# cray fas loader describe {loaderRunID} --format json NOTE: {loadRunID} is the ID from step #2 above in that case \u0026ldquo;7b0ce40f-cd6d-4ff0-9b71-0f3c9686f5ce\u0026rdquo;. Use the --format json to make it easier to read.\n{ \u0026#34;loaderRunOutput\u0026#34;: [ \u0026#34;2021-07-20T18:17:58Z-FWLoader-INFO-Starting FW Loader, LOG_LEVEL: INFO; value: 20\u0026#34;, \u0026#34;2021-07-20T18:17:58Z-FWLoader-INFO-urls: {\u0026#39;fas\u0026#39;: \u0026#39;http://cray-fas\u0026#39;, \u0026#39;fwloc\u0026#39;: \u0026#39;file://download/\u0026#39;}\u0026#34;, \u0026#34;2021-07-20T18:17:58Z-INFO: LOG_LEVEL: DEBUG; value: 10\u0026#34;, \u0026#34;2021-07-20T18:17:58Z-INFO: NEXUS_ENDPOINT: http://nexus.nexus.svc.cluster.local\u0026#34;, \u0026#34;2021-07-20T18:17:58Z-INFO: NEXUS_REPO: shasta-firmware\u0026#34;, \u0026#34;2021-07-20T18:17:58Z-INFO: Repomd URL: http://nexus.nexus.svc.cluster.local/repository/shasta-firmware/repodata/repomd.xml\u0026#34;, \u0026#34;2021-07-20T18:17:58Z-DEBUG: Starting new HTTP connection (1): nexus.nexus.svc.cluster.local:80\u0026#34;, \u0026#34;2021-07-20T18:17:58Z-DEBUG: http://nexus.nexus.svc.cluster.local:80 \\\u0026#34;GET /repository/shasta-firmware/repodata/repomd.xml HTTP/1.1\\\u0026#34; 200 3080\u0026#34;, \u0026#34;2021-07-20T18:17:58Z-INFO: Packages URL: http://nexus.nexus.svc.cluster.local/repository/shasta-firmware/repodata/7f727fc9c4a8d0df528798dc85f1c5178128f3e00a0820a4d07bf9842ddcb6e1-primary.xml.gz\u0026#34;, \u0026#34;2021-07-20T18:17:58Z-DEBUG: Starting new HTTP connection (1): nexus.nexus.svc.cluster.local:80\u0026#34;, \u0026#34;2021-07-20T18:17:58Z-DEBUG: http://nexus.nexus.svc.cluster.local:80 \\\u0026#34;GET /repository/shasta-firmware/repodata/7f727fc9c4a8d0df528798dc85f1c5178128f3e00a0820a4d07bf9842ddcb6e1-primary.xml.gz HTTP/1.1\\\u0026#34; 200 6137\u0026#34;, ... ... \u0026#34;2021-07-20T18:19:04Z-FWLoader-INFO-update ACL to public-read for e7b20c7ae98611eb880aa2c40cff7c62/nc-1.5.15.itb\u0026#34;, \u0026#34;2021-07-20T18:19:04Z-FWLoader-INFO-update ACL to public-read for ec324d05e98611ebbb9da2c40cff7c62/rom.ima_enc\u0026#34;, \u0026#34;2021-07-20T18:19:04Z-FWLoader-INFO-update ACL to public-read for e74c5977e98611eb8e9aa2c40cff7c62/cc-1.5.15.itb\u0026#34;, \u0026#34;2021-07-20T18:19:04Z-FWLoader-INFO-update ACL to public-read for e1feb6d6e98611eb877aa2c40cff7c62/accfpga_nvidia_2.7.tar.gz\u0026#34;, \u0026#34;2021-07-20T18:19:04Z-FWLoader-INFO-update ACL to public-read for dc830cb2e98611ebb4d2a2c40cff7c62/A48_2.40_02_24_2021.signed.flash\u0026#34;, \u0026#34;2021-07-20T18:19:04Z-FWLoader-INFO-update ACL to public-read for e28626d4e98611ebb0a7a2c40cff7c62/wnc.i210-p2sn01.tar.gz\u0026#34;, \u0026#34;2021-07-20T18:19:04Z-FWLoader-INFO-update ACL to public-read for eee87acde98611eba8f4a2c40cff7c62/image.RBU\u0026#34;, \u0026#34;2021-07-20T18:19:04Z-FWLoader-INFO-update ACL to public-read for de3d01bee98611eb9affa2c40cff7c62/A47_2.40_02_23_2021.signed.flash\u0026#34;, \u0026#34;2021-07-20T18:19:04Z-FWLoader-INFO-update ACL to public-read for e8ab6f61e98611eb913fa2c40cff7c62/ex235n.bios-1.1.1.tar.gz\u0026#34;, \u0026#34;2021-07-20T18:19:04Z-FWLoader-INFO-update ACL to public-read for e8ab6f61e98611eb913fa2c40cff7c62/ex235n.bios-1.1.1.tar.gz\u0026#34;, \u0026#34;2021-07-20T18:19:04Z-FWLoader-INFO-finished updating images ACL\u0026#34;, \u0026#34;2021-07-20T18:19:04Z-FWLoader-INFO-*** Number of Updates: 24 ***\u0026#34; ] } A successful run will end with *** Number of Updates: x ***.\nNOTE: The FAS loader will not overwrite image records already in FAS. Number of Updates will be the number of new images found in Nexus. If the number is 0, all images were already in FAS.\n   Load Firmware from RPM or ZIP file This procedure will read a single local RPM (or ZIP) file and upload firmware images to S3 and create image records for firmware not already in FAS.\n  Copy the file to ncn-m001 or one of the other NCNs.\n  Check the loader status:\nncn# cray fas loader list | grep loaderStatus This will return a ready or busy status.\nloaderStatus = \u0026#34;ready\u0026#34; The loader can only run one job at a time, if the loader is busy, it will return an error on any attempt to create an additional job.\n  Run the loader command.\nfirmware.rpm is the name of the RPM. If the file is not in the current directory, add the path to the filename.\nncn# cray fas loader create --file firmware.RPM This will return an ID which will be used to check the status of the run.\nloaderRunID = \u0026#34;7b0ce40f-cd6d-4ff0-9b71-0f3c9686f5ce\u0026#34;   Check the results of the loader run.\nncn# cray fas loader describe {loaderRunID} --format json NOTE: {loadRunID} is the ID from step #2 above in that case \u0026ldquo;7b0ce40f-cd6d-4ff0-9b71-0f3c9686f5ce\u0026rdquo;. Use the --format json to make it easier to read.\n{ \u0026#34;loaderRunOutput\u0026#34;: [ \u0026#34;2021-04-28T14:40:45Z-FWLoader-INFO-Starting FW Loader, LOG_LEVEL: INFO; value: 20\u0026#34;, \u0026#34;2021-04-28T14:40:45Z-FWLoader-INFO-urls: {\u0026#39;fas\u0026#39;: \u0026#39;http://localhost:28800\u0026#39;, \u0026#39;fwloc\u0026#39;: \u0026#39;file://download/\u0026#39;}\u0026#34;, \u0026#34;2021-04-28T14:40:45Z-FWLoader-INFO-Using local file: /ilo5_241.zip\u0026#34;, \u0026#34;2021-04-28T14:40:45Z-FWLoader-INFO-unzip /ilo5_241.zip\u0026#34;, \u0026#34;Archive: /ilo5_241.zip\u0026#34;, \u0026#34; inflating: ilo5_241.bin\u0026#34;, \u0026#34; inflating: ilo5_241.json\u0026#34;, \u0026#34;2021-04-28T14:40:45Z-FWLoader-INFO-Processing files from file://download/\u0026#34;, \u0026#34;2021-04-28T14:40:45Z-FWLoader-INFO-get_file_list(file://download/)\u0026#34;, \u0026#34;2021-04-28T14:40:45Z-FWLoader-INFO-Processing File: file://download/ ilo5_241.json\u0026#34;, \u0026#34;2021-04-28T14:40:45Z-FWLoader-INFO-Uploading b73a48cea82f11eb8c8a0242c0a81003/ilo5_241.bin\u0026#34;, \u0026#34;2021-04-28T14:40:45Z-FWLoader-INFO-Metadata {\u0026#39;imageData\u0026#39;: \\\u0026#34;{\u0026#39;deviceType\u0026#39;: \u0026#39;nodeBMC\u0026#39;, \u0026#39;manufacturer\u0026#39;: \u0026#39;hpe\u0026#39;, \u0026#39;models\u0026#39;: [\u0026#39;ProLiant XL270d Gen10\u0026#39;, \u0026#39;ProLiant DL325 Gen10\u0026#39;, \u0026#39;ProLiant DL325 Gen10 Plus\u0026#39;, \u0026#39;ProLiant DL385 Gen10\u0026#39;, \u0026#39;ProLiant DL385 Gen10 Plus\u0026#39;, \u0026#39;ProLiant XL645d Gen10 Plus\u0026#39;, \u0026#39;ProLiant XL675d Gen10 Plus\u0026#39;], \u0026#39;targets\u0026#39;: [\u0026#39;iLO 5\u0026#39;], \u0026#39;tags\u0026#39;: [\u0026#39;default\u0026#39;], \u0026#39;firmwareVersion\u0026#39;: \u0026#39;2.41 Mar 08 2021\u0026#39;, \u0026#39;semanticFirmwareVersion\u0026#39;: \u0026#39;2.41.0\u0026#39;, \u0026#39;pollingSpeedSeconds\u0026#39;: 30, \u0026#39;fileName\u0026#39;: \u0026#39;ilo5_241.bin\u0026#39;}\\\u0026#34;}\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-IMAGE: {\\\u0026#34;s3URL\\\u0026#34;: \\\u0026#34;s3:/fw-update/b73a48cea82f11eb8c8a0242c0a81003/ilo5_241.bin\\\u0026#34;, \\\u0026#34;target\\\u0026#34;: \\\u0026#34;iLO 5\\\u0026#34;, \\\u0026#34;deviceType\\\u0026#34;: \\\u0026#34;nodeBMC\\\u0026#34;, \\\u0026#34;manufacturer\\\u0026#34;: \\\u0026#34;hpe\\\u0026#34;, \\\u0026#34;models\\\u0026#34;: [\\\u0026#34;ProLiant XL270d Gen10\\\u0026#34;, \\\u0026#34;ProLiant DL325 Gen10\\\u0026#34;, \\\u0026#34;ProLiant DL325 Gen10 Plus\\\u0026#34;, \\\u0026#34;ProLiant DL385 Gen10\\\u0026#34;, \\\u0026#34;ProLiant DL385 Gen10 Plus\\\u0026#34;, \\\u0026#34;ProLiant XL645d Gen10 Plus\\\u0026#34;, \\\u0026#34;ProLiant XL675d Gen10 Plus\\\u0026#34;], \\\u0026#34;softwareIds\\\u0026#34;: [], \\\u0026#34;tags\\\u0026#34;: [\\\u0026#34;default\\\u0026#34;], \\\u0026#34;firmwareVersion\\\u0026#34;: \\\u0026#34;2.41 Mar 08 2021\\\u0026#34;, \\\u0026#34;semanticFirmwareVersion\\\u0026#34;: \\\u0026#34;2.41.0\\\u0026#34;, \\\u0026#34;allowableDeviceStates\\\u0026#34;: [], \\\u0026#34;needManualReboot\\\u0026#34;: false, \\\u0026#34;pollingSpeedSeconds\\\u0026#34;: 30}\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-Number of Updates: 1\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-Iterate images\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-update ACL to public-read for 5ab9f804a82b11eb8a700242c0a81003/wnc.bios-1.1.2.tar.gz\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-update ACL to public-read for 5ab9f804a82b11eb8a700242c0a81003/wnc.bios-1.1.2.tar.gz\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-update ACL to public-read for 53c060baa82a11eba26c0242c0a81003/controllers-1.3.317.itb\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-update ACL to public-read for b73a48cea82f11eb8c8a0242c0a81003/ilo5_241.bin\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-finished updating images ACL\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-removing local file: /ilo5_241.zip\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-*** Number of Updates: 1 ***\u0026#34; ] } A successful run will end with *** Number of Updates: x ***.\nNOTE: The FAS loader will not overwrite image records already in FAS. Number of Updates will be the number of new images found in the RPM. If the number is 0, all images were already in FAS.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/firmware/fas_cli/",
	"title": "FAS Cli",
	"tags": [],
	"description": "",
	"content": "FAS CLI This section describes the basic capabilities of the Firmware Action Service (FAS) CLI commands. These commands can be used to manage firmware for system hardware supported by FAS. Refer to the prerequisites section before proceeding to any of the sections for the supported operations.\nThe following CLI operations are described:\n Action  Execute an Action Abort an Action Describe an Action   Snapshots  Create a Snapshot View a Snapshot List Snapshots   Update an Image FAS Loader Commands  Prerequisites The Cray command line interface (CLI) tool is initialized and configured on the system.\nExecute an Action Use FAS to execute an action. An action produces a set of firmware operations. Each operation represents an xname + target on that xname that will be targeted for update. There are two of firmware action modes: : dryrun or liveupdate; the parameters used when creating either are completely identical except for the overrideDryrun setting. overrideDryrun will determine if feature to determine what firmware can be updated on the system. Dry-runs are enabled by default, and can be configured with the overrideDryrun parameter. A dry-run will create a query according to the filters requested by the admin. It will initiate an update sequence to determine what firmware is available, but will not actually change the state of the firmware\nWARNING: It is crucial that an admin is familiar with the release notes of any firmware. The release notes will indicate what new features the firmware provides and if there are any incompatibilities. FAS does not know about incompatibilities or dependencies between versions. The admin assumes full responsibility for this knowledge. It is also likely that when performing a firmware update, the current version of firmware will not be available. This means that after successfully upgrading, the firmware cannot be reverted or downgraded to a previous version.\nProcedure This will cover the generic process for executing an action. For more specific examples and detailed explanations of options see the Recipes and Filters sections.\n Identify the selection of filters to apply.  Filters narrow the scope of FAS to target specific xnames, manufacturers, targets, and so on. For this example, FAS will run with no selection filters applied.\n Create a JSON file {whole-system-dryrun.json}; to make this a live update set \u0026quot;overrideDryrun\u0026quot;: true.\n{ \u0026#34;command\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;overrideDryrun\u0026#34;: false, \u0026#34;restoreNotPossibleOverride\u0026#34;: true, \u0026#34;timeLimit\u0026#34;: 1000, \u0026#34;description\u0026#34;: \u0026#34;dryrun of full system\u0026#34; } }   Execute the dry-run.\nncn# cray fas actions create {whole-system-dryrun.json} ... { \u0026#34;actionID\u0026#34;: \u0026#34;e0cdd7c2-32b1-4a25-9b2a-8e74217eafa7\u0026#34;, \u0026#34;overrideDryun\u0026#34;: false }   Note the returned actionID.\nSee Interpreting Output for more information.\nAbort an Action Firmware updates can be stopped if required. This is useful given only one action can be run at a time. This is to protect hardware from multiple actions trying to modify it at the same time.\nIMPORTANT: If a Redfish update is already in progress, the abort will not stop that process on the device. It is likely the device will update. If the device needs to be manually power cycled (needManualReboot), it is possible that the device will update, but not actually apply the update until its next reboot. Administrators must verify the state of the system after an abort. Only perform an abort if truly necessary. The best way to check the state of the system is to do a snapshot or do a dry-run of an update.\nProcedure   Issue the abort command to the action.\nncn# cray fas actions instance delete {actionID}   The action could take up to a minute to fully abort.\nDescribe an Action There are several ways to get more information about a firmware update. An actionID and operationIDs are generated when an live update or dry-run is created. These values can be used to learn more about what is happening on the system during an update.\nInterpreting Output For the steps below, the following returned messages will help determine if a firmware update is needed. The following are end states for operations. The Firmware action itself should be in completed once all operations have finished.\n NoOp: Nothing to do, already at version. NoSol: No image is available. succeeded:  IF dryrun: The operation should succeed if performed as a live update. succeeded means that FAS identified that it COULD update an xname + target with the declared strategy. IF live update: the operation succeeded, and has updated the xname + target to the identified version.   failed:  IF dryrun : There is something that FAS could do, but it likely would fail; most likely because the file is missing. IF live update : the operation failed, the identified version could not be put on the xname + target.    Data can be viewed at several levels of information:\nProcedure Get High Level Summary To view counts of operations, what state they are in, the overall state of the action, and what parameters were used to create the action:\nncn# cray fas actions status list {actionID} actionID = \u0026quot;e6dc14cd-5e12-4d36-a97b-0dd372b0930f\u0026quot; snapshotID = \u0026quot;00000000-0000-0000-0000-000000000000\u0026quot; startTime = \u0026quot;2021-09-07 16:43:04.294233199 +0000 UTC\u0026quot; endTime = \u0026quot;2021-09-07 16:53:09.363233482 +0000 UTC\u0026quot; state = \u0026quot;completed\u0026quot; blockedBy = [] [command] overrideDryrun = false restoreNotPossibleOverride = true overwriteSameImage = false timeLimit = 2000 version = \u0026quot;latest\u0026quot; tag = \u0026quot;default\u0026quot; description = \u0026quot;Dryrun upgrade of Gigabyte node BMCs\u0026quot; [operationCounts] total = 14 initial = 0 configured = 0 blocked = 0 needsVerified = 0 verifying = 0 inProgress = 0 failed = 0 succeeded = 8 noOperation = 6 noSolution = 0 aborted = 0 unknown = 0 IMPORTANT: Unless the action\u0026rsquo;s state is completed or aborted; then this action is still in progress.\nGet Details of Action  ncn# cray fas actions describe {actionID} --format json { \u0026quot;parameters\u0026quot;: { \u0026quot;stateComponentFilter\u0026quot;: { \u0026quot;deviceTypes\u0026quot;: [ \u0026quot;nodeBMC\u0026quot; ] }, \u0026quot;command\u0026quot;: { \u0026quot;dryrun\u0026quot;: false, \u0026quot;description\u0026quot;: \u0026quot;upgrade of nodeBMCs for cray\u0026quot;, \u0026quot;tag\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;restoreNotPossibleOverride\u0026quot;: true, \u0026quot;timeLimit\u0026quot;: 1000, \u0026quot;version\u0026quot;: \u0026quot;latest\u0026quot; }, \u0026quot;inventoryHardwareFilter\u0026quot;: { \u0026quot;manufacturer\u0026quot;: \u0026quot;cray\u0026quot; }, \u0026quot;imageFilter\u0026quot;: { \u0026quot;imageID\u0026quot;: \u0026quot;00000000-0000-0000-0000-000000000000\u0026quot; }, \u0026quot;targetFilter\u0026quot;: { \u0026quot;targets\u0026quot;: [ \u0026quot;BMC\u0026quot; ] } }, \u0026quot;blockedBy\u0026quot;: [], \u0026quot;state\u0026quot;: \u0026quot;completed\u0026quot;, \u0026quot;command\u0026quot;: { \u0026quot;dryrun\u0026quot;: false, \u0026quot;description\u0026quot;: \u0026quot;upgrade of nodeBMCs for cray\u0026quot;, \u0026quot;tag\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;restoreNotPossibleOverride\u0026quot;: true, \u0026quot;timeLimit\u0026quot;: 1000, \u0026quot;version\u0026quot;: \u0026quot;latest\u0026quot; }, \u0026quot;actionID\u0026quot;: \u0026quot;e0cdd7c2-32b1-4a25-9b2a-8e74217eafa7\u0026quot;, \u0026quot;startTime\u0026quot;: \u0026quot;2020-06-26 20:03:37.316932354 +0000 UTC\u0026quot;, \u0026quot;snapshotID\u0026quot;: \u0026quot;00000000-0000-0000-0000-000000000000\u0026quot;, \u0026quot;endTime\u0026quot;: \u0026quot;2020-06-26 20:04:07.118243184 +0000 UTC\u0026quot;, \u0026quot;operationSummary\u0026quot;: { \u0026quot;succeeded\u0026quot;: { \u0026quot;OperationsKeys\u0026quot;: [] }, \u0026quot;verifying\u0026quot;: { \u0026quot;OperationsKeys\u0026quot;: [] }, \u0026quot;unknown\u0026quot;: { \u0026quot;OperationsKeys\u0026quot;: [] }, \u0026quot;configured\u0026quot;: { \u0026quot;OperationsKeys\u0026quot;: [] }, \u0026quot;initial\u0026quot;: { \u0026quot;OperationsKeys\u0026quot;: [] }, \u0026quot;failed\u0026quot;: { \u0026quot;OperationsKeys\u0026quot;: [ { \u0026quot;stateHelper\u0026quot;: \u0026quot;unexpected change detected in firmware version. Expected sc.1.4.35-prod- master.arm64.2020-06-26T08:36:42+00:00.0c2bb02 got: sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026quot;, \u0026quot;fromFirmwareVersion\u0026quot;: \u0026quot;, \u0026quot;xname\u0026quot;: \u0026quot;x5000c1r7b0\u0026quot;, \u0026quot;target\u0026quot;: \u0026quot;BMC\u0026quot;, \u0026quot;operationID\u0026quot;: \u0026quot;0796eed0-e95d-45ea-bc71-8903d52cffde\u0026quot; }, ] }, \u0026quot;noSolution\u0026quot;: { \u0026quot;OperationsKeys\u0026quot;: [] }, \u0026quot;aborted\u0026quot;: { \u0026quot;OperationsKeys\u0026quot;: [] }, \u0026quot;needsVerified\u0026quot;: { \u0026quot;OperationsKeys\u0026quot;: [] }, \u0026quot;noOperation\u0026quot;: { \u0026quot;OperationsKeys\u0026quot;: [] }, \u0026quot;inProgress\u0026quot;: { \u0026quot;OperationsKeys\u0026quot;: [] }, \u0026quot;blocked\u0026quot;: { \u0026quot;OperationsKeys\u0026quot;: [] } } } Get Details of Operation Using the operationID listed in the actions array we can see the full detail of the operation.\nncn# cray fas operations describe {operationID} --format json { \u0026quot;fromFirmwareVersion\u0026quot;: \u0026quot;, \u0026quot;fromTag\u0026quot;: \u0026quot;, \u0026quot;fromImageURL\u0026quot;: \u0026quot;, \u0026quot;endTime\u0026quot;: \u0026quot;2020-06-24 14:23:37.544814197 +0000 UTC\u0026quot;, \u0026quot;actionID\u0026quot;: \u0026quot;f48aabf1-1616-49ae-9761-a11edb38684d\u0026quot;, \u0026quot;startTime\u0026quot;: \u0026quot;2020-06-24 14:19:15.10128214 +0000 UTC\u0026quot;, \u0026quot;fromSemanticFirmwareVersion\u0026quot;: \u0026quot;, \u0026quot;toImageURL\u0026quot;: \u0026quot;, \u0026quot;model\u0026quot;: \u0026quot;WindomNodeCard_REV_D\u0026quot;, \u0026quot;operationID\u0026quot;: \u0026quot;24a5e5fb-5c4f-4848-bf4e-b071719c1850\u0026quot;, \u0026quot;fromImageID\u0026quot;: \u0026quot;00000000-0000-0000-0000-000000000000\u0026quot;, \u0026quot;target\u0026quot;: \u0026quot;BMC\u0026quot;, \u0026quot;toImageID\u0026quot;: \u0026quot;71c41a74-ab84-45b2-95bd-677f763af168\u0026quot;, \u0026quot;toSemanticFirmwareVersion\u0026quot;: \u0026quot;, \u0026quot;refreshTime\u0026quot;: \u0026quot;2020-06-24 14:23:37.544824938 +0000 UTC\u0026quot;, \u0026quot;blockedBy\u0026quot;: [], \u0026quot;toTag\u0026quot;: \u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;succeeded\u0026quot;, \u0026quot;stateHelper\u0026quot;: \u0026quot;unexpected change detected in firmware version. Expected nc.1.3.8-shasta-release.arm.2020-06-15T22:57:31+00:00.b7f0725 got: nc.1.2.25-shasta-release.arm.2020-05-15T17:27:16+00:00.0cf7f51\u0026quot;, \u0026quot;deviceType\u0026quot;: \u0026quot;, \u0026quot;expirationTime\u0026quot;: \u0026quot;, \u0026quot;manufacturer\u0026quot;: \u0026quot;cray\u0026quot;, \u0026quot;xname\u0026quot;: \u0026quot;x9000c1s3b1\u0026quot;, \u0026quot;toFirmwareVersion\u0026quot;: \u0026quot; } Create Snapshots FAS includes a snapshot feature to record the firmware value for each device (type and target) on the system into the FAS database. Similar to the FAS actions described above, FAS provides a lot of flexibility for taking snapshots.\nA snapshot of the system captures the firmware version for every device that is in the Hardware State Manager (HSM) Redfish Inventory.\nProcedure   Determine what part of the system to take a snapshot.\n  Full System: json { \u0026quot;name\u0026quot;:\u0026quot;fullSystem_20200701\u0026quot; }\n  Partial System json { \u0026quot;name\u0026quot;: \u0026quot;20200402_all_xnames\u0026quot;, \u0026quot;expirationTime\u0026quot;: \u0026quot;2020-06-26T16:32:53.275Z\u0026quot;, \u0026quot;stateComponentFilter\u0026quot;: { \u0026quot;partitions\u0026quot;: [ \u0026quot;p1\u0026quot; ], \u0026quot;deviceTypes\u0026quot;: [ \u0026quot;nodeBMC\u0026quot; ] }, \u0026quot;inventoryHardwareFilter\u0026quot;: { \u0026quot;manufacturer\u0026quot;: \u0026quot;gigabyte\u0026quot; }, \u0026quot;targetFilter\u0026quot;: { \u0026quot;targets\u0026quot;: [ \u0026quot;BMC\u0026quot; ] } }\n    Create the snapshot.\nncn# cray fas snapshots create {file.json}   Use the snapshot name to query the snapshot. This is a long running operation, so monitor the state field to determine if the snapshot is complete.\n  List Snapshots A list of all snapshots can be viewed on the system. Any of the snapshots listed can be used to restore the firmware on the system.\nProcedure   List the snapshots.\nncn# cray fas snapshots list --format json { \u0026quot;snapshots\u0026quot;: [ { \u0026quot;ready\u0026quot;: true, \u0026quot;captureTime\u0026quot;: \u0026quot;2020-06-25 22:47:11.072268274 +0000 UTC\u0026quot;, \u0026quot;relatedActions\u0026quot;: [], \u0026quot;name\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;uniqueDeviceCount\u0026quot;: 9 }, { \u0026quot;ready\u0026quot;: true, \u0026quot;captureTime\u0026quot;: \u0026quot;2020-06-25 22:49:13.314876084 +0000 UTC\u0026quot;, \u0026quot;relatedActions\u0026quot;: [], \u0026quot;name\u0026quot;: \u0026quot;3\u0026quot;, \u0026quot;uniqueDeviceCount\u0026quot;: 9 }, { \u0026quot;ready\u0026quot;: true, \u0026quot;captureTime\u0026quot;: \u0026quot;2020-06-26 22:38:12.309979483 +0000 UTC\u0026quot;, \u0026quot;relatedActions\u0026quot;: [], \u0026quot;name\u0026quot;: \u0026quot;adn0\u0026quot;, \u0026quot;uniqueDeviceCount\u0026quot;: 6 } ] }   View Snapshots View a snapshot to see which versions of firmware are set for each target. The command to view the contents of a snapshot is the same command that is used to create a snapshot.\nProcedure   View a snapshot.\nncn# cray fas snapshots describe {snapshot_name} --format json { \u0026quot;relatedActions\u0026quot;: [], \u0026quot;name\u0026quot;: \u0026quot;all\u0026quot;, \u0026quot;parameters\u0026quot;: { \u0026quot;stateComponentFilter\u0026quot;: {}, \u0026quot;targetFilter\u0026quot;: {}, \u0026quot;name\u0026quot;: \u0026quot;all\u0026quot;, \u0026quot;inventoryHardwareFilter\u0026quot;: {} }, \u0026quot;ready\u0026quot;: true, \u0026quot;captureTime\u0026quot;: \u0026quot;2020-06-26 19:13:53.755350771 +0000 UTC\u0026quot;, \u0026quot;devices\u0026quot;: [ { \u0026quot;xname\u0026quot;: \u0026quot;x3000c0s19b4\u0026quot;, \u0026quot;targets\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;BIOS\u0026quot;, \u0026quot;firmwareVersion\u0026quot;: \u0026quot;C12\u0026quot;, \u0026quot;imageID\u0026quot;: \u0026quot;00000000-0000-0000-0000-000000000000\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;BMC\u0026quot;, \u0026quot;firmwareVersion\u0026quot;: \u0026quot;12.03.3\u0026quot;, \u0026quot;imageID\u0026quot;: \u0026quot;00000000-0000-0000-0000-000000000000\u0026quot; } ] }, { \u0026quot;xname\u0026quot;: \u0026quot;x3000c0s1b0\u0026quot;, \u0026quot;targets\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;BPB_CPLD1\u0026quot;, \u0026quot;firmwareVersion\u0026quot;: \u0026quot;10\u0026quot;, \u0026quot;imageID\u0026quot;: \u0026quot;00000000-0000-0000-0000-000000000000\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;BMC\u0026quot;, \u0026quot;firmwareVersion\u0026quot;: \u0026quot;12.03.3\u0026quot;, \u0026quot;imageID\u0026quot;: \u0026quot;00000000-0000-0000-0000-000000000000\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;BIOS\u0026quot;, \u0026quot;firmwareVersion\u0026quot;: \u0026quot;C12\u0026quot;, \u0026quot;imageID\u0026quot;: \u0026quot;00000000-0000-0000-0000-000000000000\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;BPB_CPLD2\u0026quot;, \u0026quot;firmwareVersion\u0026quot;: \u0026quot;10\u0026quot;, \u0026quot;imageID\u0026quot;: \u0026quot;00000000-0000-0000-0000-000000000000\u0026quot; } ] } ] }   Update a Firmware Image If FAS indicates hardware is in a nosolution state as a result of a dry-run or update, it is an indication that there is no matching image available to update firmware. A missing image is highly possible, but the issue could also be that the hardware has inconsistent model names in the image file.\nGiven the nature of the model field and its likelihood to not be standardized, it may be necessary to update the image to include an image that is not currently present.\nProcedure   List the existing firmware images to find the imageID of the desired firmware image.\nncn# cray fas images list   Describe the image file using the imageID.\nncn# cray fas images describe {imageID} { \u0026#34;semanticFirmwareVersion\u0026#34;: \u0026#34;0.2.6\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;Node0.BIOS\u0026#34;, \u0026#34;waitTimeBeforeManualRebootSeconds\u0026#34;: 0, \u0026#34;tags\u0026#34;: [ \u0026#34;default\u0026#34; ], \u0026#34;models\u0026#34;: [ \u0026#34;GrizzlyPeak-Rome\u0026#34; ], \u0026#34;updateURI\u0026#34;: \u0026#34;, \u0026#34;waitTimeAfterRebootSeconds\u0026#34;: 0, \u0026#34;imageID\u0026#34;: \u0026#34;efa4c2bc-06b9-4e88-8098-8d6778c1db52\u0026#34;, \u0026#34;s3URL\u0026#34;: \u0026#34;s3:/fw-update/794c47d1b7e011ea8d20569839947aa5/gprnc.bios-0.2.6.tar.gz\u0026#34;, \u0026#34;forceResetType\u0026#34;: \u0026#34;, \u0026#34;deviceType\u0026#34;: \u0026#34;nodeBMC\u0026#34;, \u0026#34;pollingSpeedSeconds\u0026#34;: 30, \u0026#34;createTime\u0026#34;: \u0026#34;2020-06-26T19:08:52Z\u0026#34;, \u0026#34;firmwareVersion\u0026#34;: \u0026#34;gprnc.bios-0.2.6\u0026#34;, \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34; }   Describe the FAS operation and compare it to the image file from the previous step. Look at the hardware models to see if some of the population is in a noSolution state, while others are in a succeeded state. If that is the case, view the operation data and examine the models.\nncn# cray fas actions describe {actionID} --format json { \u0026quot;parameters\u0026quot;: { \u0026quot;stateComponentFilter\u0026quot;: { \u0026quot;deviceTypes\u0026quot;: [ \u0026quot;nodeBMC\u0026quot; ] }, \u0026quot;command\u0026quot;: { \u0026quot;dryrun\u0026quot;: false, \u0026quot;description\u0026quot;: \u0026quot;upgrade of nodeBMCs for cray\u0026quot;, \u0026quot;tag\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;restoreNotPossibleOverride\u0026quot;: true, \u0026quot;timeLimit\u0026quot;: 1000, \u0026quot;version\u0026quot;: \u0026quot;latest\u0026quot; }, \u0026quot;inventoryHardwareFilter\u0026quot;: { \u0026quot;manufacturer\u0026quot;: \u0026quot;cray\u0026quot; }, \u0026quot;imageFilter\u0026quot;: { \u0026quot;imageID\u0026quot;: \u0026quot;00000000-0000-0000-0000-000000000000\u0026quot; }, \u0026quot;targetFilter\u0026quot;: { \u0026quot;targets\u0026quot;: [ \u0026quot;BMC\u0026quot; ] } }, \u0026quot;blockedBy\u0026quot;: [], \u0026quot;state\u0026quot;: \u0026quot;completed\u0026quot;, \u0026quot;command\u0026quot;: { \u0026quot;dryrun\u0026quot;: false, \u0026quot;description\u0026quot;: \u0026quot;upgrade of nodeBMCs for cray\u0026quot;, \u0026quot;tag\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;restoreNotPossibleOverride\u0026quot;: true, \u0026quot;timeLimit\u0026quot;: 1000, \u0026quot;version\u0026quot;: \u0026quot;latest\u0026quot; }, \u0026quot;actionID\u0026quot;: \u0026quot;e0cdd7c2-32b1-4a25-9b2a-8e74217eafa7\u0026quot;, \u0026quot;startTime\u0026quot;: \u0026quot;2020-06-26 20:03:37.316932354 +0000 UTC\u0026quot;, \u0026quot;snapshotID\u0026quot;: \u0026quot;00000000-0000-0000-0000-000000000000\u0026quot;, \u0026quot;endTime\u0026quot;: \u0026quot;2020-06-26 20:04:07.118243184 +0000 UTC\u0026quot;, \u0026quot;operationSummary\u0026quot;: { \u0026quot;succeeded\u0026quot;: { \u0026quot;OperationsKeys\u0026quot;: [] }, \u0026quot;verifying\u0026quot;: { \u0026quot;OperationsKeys\u0026quot;: [] }, \u0026quot;unknown\u0026quot;: { \u0026quot;OperationsKeys\u0026quot;: [] }, \u0026quot;configured\u0026quot;: { \u0026quot;OperationsKeys\u0026quot;: [] }, \u0026quot;initial\u0026quot;: { \u0026quot;OperationsKeys\u0026quot;: [] }, \u0026quot;failed\u0026quot;: { \u0026quot;OperationsKeys\u0026quot;: [ { \u0026quot;stateHelper\u0026quot;: \u0026quot;unexpected change detected in firmware version. Expected sc.1.4.35-prod- master.arm64.2020-06-26T08:36:42+00:00.0c2bb02 got: sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026quot;, \u0026quot;fromFirmwareVersion\u0026quot;: \u0026quot;, \u0026quot;xname\u0026quot;: \u0026quot;x5000c1r7b0\u0026quot;, \u0026quot;target\u0026quot;: \u0026quot;BMC\u0026quot;, \u0026quot;operationID\u0026quot;: \u0026quot;0796eed0-e95d-45ea-bc71-8903d52cffde\u0026quot; }, { \u0026quot;stateHelper\u0026quot;: \u0026quot;unexpected change detected in firmware version. Expected sc.1.4.35-prod- master.arm64.2020-06-26T08:36:42+00:00.0c2bb02 got: sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026quot;, \u0026quot;fromFirmwareVersion\u0026quot;: \u0026quot;sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026quot;, \u0026quot;xname\u0026quot;: \u0026quot;x5000c3r7b0\u0026quot;, \u0026quot;target\u0026quot;: \u0026quot;BMC\u0026quot;, \u0026quot;operationID\u0026quot;: \u0026quot;11421f0b-1fde-4917-ba56-c42b321fc833\u0026quot; }, { \u0026quot;stateHelper\u0026quot;: \u0026quot;unexpected change detected in firmware version. Expected sc.1.4.35-prod- master.arm64.2020-06-26T08:36:42+00:00.0c2bb02 got: sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026quot;, \u0026quot;fromFirmwareVersion\u0026quot;: \u0026quot;sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026quot;, \u0026quot;xname\u0026quot;: \u0026quot;x5000c3r3b0\u0026quot;, \u0026quot;target\u0026quot;: \u0026quot;BMC\u0026quot;, \u0026quot;operationID\u0026quot;: \u0026quot;21e04403-f89f-4a9f-9fd6-5affc9204689\u0026quot; }, { \u0026quot;stateHelper\u0026quot;: \u0026quot;unexpected change detected in firmware version. Expected sc.1.4.35-prod- master.arm64.2020-06-26T08:36:42+00:00.0c2bb02 got: sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026quot;, \u0026quot;fromFirmwareVersion\u0026quot;: \u0026quot;sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026quot;, \u0026quot;xname\u0026quot;: \u0026quot;x5000c1r5b0\u0026quot;, \u0026quot;target\u0026quot;: \u0026quot;BMC\u0026quot;, \u0026quot;operationID\u0026quot;: \u0026quot;3a13a459-2102-4ee5-b516-62880baa132d\u0026quot; }, { \u0026quot;stateHelper\u0026quot;: \u0026quot;unexpected change detected in firmware version. Expected sc.1.4.35-prod- master.arm64.2020-06-26T08:36:42+00:00.0c2bb02 got: sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026quot;, \u0026quot;fromFirmwareVersion\u0026quot;: \u0026quot;sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026quot;, \u0026quot;xname\u0026quot;: \u0026quot;x5000c1r1b0\u0026quot;, \u0026quot;target\u0026quot;: \u0026quot;BMC\u0026quot;, \u0026quot;operationID\u0026quot;: \u0026quot;80fafbdd-9bac-407d-b28a-ad47c197bbc1\u0026quot; }, { \u0026quot;stateHelper\u0026quot;: \u0026quot;unexpected change detected in firmware version. Expected sc.1.4.35-prod- master.arm64.2020-06-26T08:36:42+00:00.0c2bb02 got: sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026quot;, \u0026quot;fromFirmwareVersion\u0026quot;: \u0026quot;sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026quot;, \u0026quot;xname\u0026quot;: \u0026quot;x5000c3r5b0\u0026quot;, \u0026quot;target\u0026quot;: \u0026quot;BMC\u0026quot;, \u0026quot;operationID\u0026quot;: \u0026quot;a86e8e04-81cc-40ad-ac62-438ae73e033a\u0026quot; }, { \u0026quot;stateHelper\u0026quot;: \u0026quot;unexpected change detected in firmware version. Expected sc.1.4.35-prod- master.arm64.2020-06-26T08:36:42+00:00.0c2bb02 got: sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026quot;, \u0026quot;fromFirmwareVersion\u0026quot;: \u0026quot;sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026quot;, \u0026quot;xname\u0026quot;: \u0026quot;x5000c1r3b0\u0026quot;, \u0026quot;target\u0026quot;: \u0026quot;BMC\u0026quot;, \u0026quot;operationID\u0026quot;: \u0026quot;dd0e8b62-8894-4751-bd22-a45506a2a50a\u0026quot; }, { \u0026quot;stateHelper\u0026quot;: \u0026quot;unexpected change detected in firmware version. Expected sc.1.4.35-prod- master.arm64.2020-06-26T08:36:42+00:00.0c2bb02 got: sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026quot;, \u0026quot;fromFirmwareVersion\u0026quot;: \u0026quot;sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026quot;, \u0026quot;xname\u0026quot;: \u0026quot;x5000c3r1b0\u0026quot;, \u0026quot;target\u0026quot;: \u0026quot;BMC\u0026quot;, \u0026quot;operationID\u0026quot;: \u0026quot;f87bff63-d231-403e-b6b6-fc09e4dc7d11\u0026quot; } ] }, \u0026quot;noSolution\u0026quot;: { \u0026quot;OperationsKeys\u0026quot;: [] }, \u0026quot;aborted\u0026quot;: { \u0026quot;OperationsKeys\u0026quot;: [] }, \u0026quot;needsVerified\u0026quot;: { \u0026quot;OperationsKeys\u0026quot;: [] }, \u0026quot;noOperation\u0026quot;: { \u0026quot;OperationsKeys\u0026quot;: [] }, \u0026quot;inProgress\u0026quot;: { \u0026quot;OperationsKeys\u0026quot;: [] }, \u0026quot;blocked\u0026quot;: { \u0026quot;OperationsKeys\u0026quot;: [] } } } View the operation data. If the model name is different between identical hardware, it may be appropriate to update the image model with the model of the noSolution hardware.\nncn# cray fas operations describe {operationID} --format json { \u0026#34;fromFirmwareVersion\u0026#34;: \u0026#34;sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026#34;, \u0026#34;fromTag\u0026#34;: \u0026#34;, \u0026#34;fromImageURL\u0026#34;: \u0026#34;, \u0026#34;endTime\u0026#34;: \u0026#34;2020-06-26 20:15:38.535719717 +0000 UTC\u0026#34;, \u0026#34;actionID\u0026#34;: \u0026#34;e0cdd7c2-32b1-4a25-9b2a-8e74217eafa7\u0026#34;, \u0026#34;startTime\u0026#34;: \u0026#34;2020-06-26 20:03:39.44911099 +0000 UTC\u0026#34;, \u0026#34;fromSemanticFirmwareVersion\u0026#34;: \u0026#34;, \u0026#34;toImageURL\u0026#34;: \u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;ColoradoSwitchBoard_REV_A\u0026#34;, \u0026#34;operationID\u0026#34;: \u0026#34;f87bff63-d231-403e-b6b6-fc09e4dc7d11\u0026#34;, \u0026#34;fromImageID\u0026#34;: \u0026#34;00000000-0000-0000-0000-000000000000\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;BMC\u0026#34;, \u0026#34;toImageID\u0026#34;: \u0026#34;1540ce48-91db-4bbf-a0cf-5cf936c30fbc\u0026#34;, \u0026#34;toSemanticFirmwareVersion\u0026#34;: \u0026#34;1.4.35\u0026#34;, \u0026#34;refreshTime\u0026#34;: \u0026#34;2020-06-26 20:15:38.535722248 +0000 UTC\u0026#34;, \u0026#34;blockedBy\u0026#34;: [], \u0026#34;toTag\u0026#34;: \u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;failed\u0026#34;, \u0026#34;stateHelper\u0026#34;: \u0026#34;unexpected change detected in firmware version. Expected sc.1.4.35-prod- master.arm64.2020-06-26T08:36:42+00:00.0c2bb02 got: sc.1.3.307-prod-master.arm64.2020-06-13T00:28:26+00:00.f91edff\u0026#34;, \u0026#34;deviceType\u0026#34;: \u0026#34;RouterBMC\u0026#34;, \u0026#34;expirationTime\u0026#34;: \u0026#34;2020-06-26 20:20:19.44911275 +0000 UTC\u0026#34;, \u0026#34;manufacturer\u0026#34;: \u0026#34;cray\u0026#34;, \u0026#34;xname\u0026#34;: \u0026#34;x5000c3r1b0\u0026#34;, \u0026#34;toFirmwareVersion\u0026#34;: \u0026#34;sc.1.4.35-prod-master.arm64.2020-06-26T08:36:42+00:00.0c2bb02\u0026#34; }   Update the firmware image file.\nThis step should be skipped if there is no clear evidence of a missing image or incorrect model name.\nWARNING: The admin needs to be certain the firmware is compatible before proceeding.\n  Dump the content of the firmware image to a JSON file.\nncn# cray fas images describe {imageID} --format json \u0026gt; imagedata.json   Edit the new imagedata.json file. Update any incorrect firmware information, such as the model name.\n  Update the firmware image.\nncn# cray fas images update {imagedata.json} {imageID}      FAS Loader Commands Loader Status To check if the loader is currently busy and receive a list of loader run IDs:\nncn# cray fas loader list loaderStatus = \u0026#34;ready\u0026#34; [[loaderRunList]] loaderRunID = \u0026#34;770af5a4-15bf-4e9f-9983-03069479dc23\u0026#34; [[loaderRunList]] loaderRunID = \u0026#34;8efb19c4-77a2-41da-9a8f-fccbfe06f674\u0026#34; The loader can only run one job at a time, if the loader is busy, it will return an error on any attempt to create an additional job.\nLoad Firmware From Nexus Firmware may be released and placed into the Nexus repository. FAS will return a loaderRunID. Use the loaderRunID to check the results of the loader run. To load the firmware from Nexus into FAS, use the following command:\nncn# cray fas loader nexus create loaderRunID = \u0026#34;c2b7e9bb-f428-4e4c-aa83-d8fd8bcfd820\u0026#34; See Load Firmware from Nexus in FAS Admin Procedures\nLoad Individual RPM or ZIP into FAS To load an RPM or ZIP into FAS on a system, copy the RPM or ZIP file to ncn-m001 or one of the other NCNs. FAS will return a loaderRunID. Use the loaderRunID to check the results of the loader run. Run the following command (RPM is this case is firmware.rpm): NOTE: If firmware is not in the current directory, add the path to the filename.\nncn# cray fas loader create --file firmware.rpm loaderRunID = \u0026#34;dd37dd45-84ec-4bd6-b3c9-7af480048966\u0026#34; See Load Firmware from RPM or ZIP file in FAS Admin Procedures\nDisplay Results of Loader Run Using the loaderRunID returned from the loader upload command, run the following command to get the output from the upload. The --format json parameter makes it easier to read.\nNOTE: dd37dd45-84ec-4bd6-b3c9-7af480048966 is the loaderRunID from previous run command.\nncn# cray fas loader describe dd37dd45-84ec-4bd6-b3c9-7af480048966 --format json { \u0026#34;loaderRunOutput\u0026#34;: [ \u0026#34;2021-04-28T14:40:45Z-FWLoader-INFO-Starting FW Loader, LOG_LEVEL: INFO; value: 20\u0026#34;, \u0026#34;2021-04-28T14:40:45Z-FWLoader-INFO-urls: {\u0026#39;fas\u0026#39;: \u0026#39;http://localhost:28800\u0026#39;, \u0026#39;fwloc\u0026#39;: \u0026#39;file://download/\u0026#39;}\u0026#34;, \u0026#34;2021-04-28T14:40:45Z-FWLoader-INFO-Using local file: /ilo5_241.zip\u0026#34;, \u0026#34;2021-04-28T14:40:45Z-FWLoader-INFO-unzip /ilo5_241.zip\u0026#34;, \u0026#34;Archive: /ilo5_241.zip\u0026#34;, \u0026#34; inflating: ilo5_241.bin\u0026#34;, \u0026#34; inflating: ilo5_241.json\u0026#34;, \u0026#34;2021-04-28T14:40:45Z-FWLoader-INFO-Processing files from file://download/\u0026#34;, \u0026#34;2021-04-28T14:40:45Z-FWLoader-INFO-get_file_list(file://download/)\u0026#34;, \u0026#34;2021-04-28T14:40:45Z-FWLoader-INFO-Processing File: file://download/ ilo5_241.json\u0026#34;, \u0026#34;2021-04-28T14:40:45Z-FWLoader-INFO-Uploading b73a48cea82f11eb8c8a0242c0a81003/ilo5_241.bin\u0026#34;, \u0026#34;2021-04-28T14:40:45Z-FWLoader-INFO-Metadata {\u0026#39;imageData\u0026#39;: \\\u0026#34;{\u0026#39;deviceType\u0026#39;: \u0026#39;nodeBMC\u0026#39;, \u0026#39;manufacturer\u0026#39;: \u0026#39;hpe\u0026#39;, \u0026#39;models\u0026#39;: [\u0026#39;ProLiant XL270d Gen10\u0026#39;, \u0026#39;ProLiant DL325 Gen10\u0026#39;, \u0026#39;ProLiant DL325 Gen10 Plus\u0026#39;, \u0026#39;ProLiant DL385 Gen10\u0026#39;, \u0026#39;ProLiant DL385 Gen10 Plus\u0026#39;, \u0026#39;ProLiant XL645d Gen10 Plus\u0026#39;, \u0026#39;ProLiant XL675d Gen10 Plus\u0026#39;], \u0026#39;targets\u0026#39;: [\u0026#39;iLO 5\u0026#39;], \u0026#39;tags\u0026#39;: [\u0026#39;default\u0026#39;], \u0026#39;firmwareVersion\u0026#39;: \u0026#39;2.41 Mar 08 2021\u0026#39;, \u0026#39;semanticFirmwareVersion\u0026#39;: \u0026#39;2.41.0\u0026#39;, \u0026#39;pollingSpeedSeconds\u0026#39;: 30, \u0026#39;fileName\u0026#39;: \u0026#39;ilo5_241.bin\u0026#39;}\\\u0026#34;}\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-IMAGE: {\\\u0026#34;s3URL\\\u0026#34;: \\\u0026#34;s3:/fw-update/b73a48cea82f11eb8c8a0242c0a81003/ilo5_241.bin\\\u0026#34;, \\\u0026#34;target\\\u0026#34;: \\\u0026#34;iLO 5\\\u0026#34;, \\\u0026#34;deviceType\\\u0026#34;: \\\u0026#34;nodeBMC\\\u0026#34;, \\\u0026#34;manufacturer\\\u0026#34;: \\\u0026#34;hpe\\\u0026#34;, \\\u0026#34;models\\\u0026#34;: [\\\u0026#34;ProLiant XL270d Gen10\\\u0026#34;, \\\u0026#34;ProLiant DL325 Gen10\\\u0026#34;, \\\u0026#34;ProLiant DL325 Gen10 Plus\\\u0026#34;, \\\u0026#34;ProLiant DL385 Gen10\\\u0026#34;, \\\u0026#34;ProLiant DL385 Gen10 Plus\\\u0026#34;, \\\u0026#34;ProLiant XL645d Gen10 Plus\\\u0026#34;, \\\u0026#34;ProLiant XL675d Gen10 Plus\\\u0026#34;], \\\u0026#34;softwareIds\\\u0026#34;: [], \\\u0026#34;tags\\\u0026#34;: [\\\u0026#34;default\\\u0026#34;], \\\u0026#34;firmwareVersion\\\u0026#34;: \\\u0026#34;2.41 Mar 08 2021\\\u0026#34;, \\\u0026#34;semanticFirmwareVersion\\\u0026#34;: \\\u0026#34;2.41.0\\\u0026#34;, \\\u0026#34;allowableDeviceStates\\\u0026#34;: [], \\\u0026#34;needManualReboot\\\u0026#34;: false, \\\u0026#34;pollingSpeedSeconds\\\u0026#34;: 30}\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-Number of Updates: 1\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-Iterate images\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-update ACL to public-read for 5ab9f804a82b11eb8a700242c0a81003/wnc.bios-1.1.2.tar.gz\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-update ACL to public-read for 5ab9f804a82b11eb8a700242c0a81003/wnc.bios-1.1.2.tar.gz\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-update ACL to public-read for 53c060baa82a11eba26c0242c0a81003/controllers-1.3.317.itb\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-update ACL to public-read for b73a48cea82f11eb8c8a0242c0a81003/ilo5_241.bin\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-finished updating images ACL\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-removing local file: /ilo5_241.zip\u0026#34;, \u0026#34;2021-04-28T14:40:46Z-FWLoader-INFO-*** Number of Updates: 1 ***\u0026#34; ] } A successful run will end with *** Number of Updates: x ***.\nNOTE: The FAS loader will not overwrite image records already in FAS. Number of Updates will be the number of new images found in the RPM. If the number is 0, all images were already in FAS.\nDelete Loader Run Data To delete the output from a loader run and remove it from the loader run list: NOTE: dd37dd45-84ec-4bd6-b3c9-7af480048966 is the loaderRunID from previous run command.\nncn# cray fas loader delete dd37dd45-84ec-4bd6-b3c9-7af480048966 The delete command does not return anything if successful.\nNOTE: The loader delete command does not delete any images from FAS, it only deletes the loader run saved status and removes the ID from the loader run list.\n"
},
{
	"uri": "/docs-csm/en-11/operations/conman/troubleshoot_conman_asking_for_password_on_ssh_connection/",
	"title": "Troubleshoot Conman Asking For Password On SSH Connection",
	"tags": [],
	"description": "",
	"content": "Troubleshoot ConMan Asking for Password on SSH Connection If ConMan starts to ask for a password when there is an SSH connection to the node on liquid-cooled hardware, that usually indicates there is a problem with the SSH key that was established on the node BMC. The key may have been replaced or overwritten on the hardware.\nUse this procedure to renew or reinstall the SSH key on the BMCs.\nPrerequisites This procedure requires administrative privileges.\nProcedure   Scale the cray-console-operator pods to 0 replicas.\nncn-m001# kubectl -n services scale --replicas=0 deployment/cray-console-operator deployment.apps/cray-console-operator scaled   Verify that the cray-console-operator service is no longer running.\nncn-m001# kubectl -n services get pods | grep console-operator ncn-m001#   Exec into a cray-console-node pod.\nncn-m001# kubectl -n services exec -it cray-console-node-0 -- /bin/bash cray-console-node-0:/ #   Delete the SSH keys and exit from the pod.\ncray-console-node-0:/ # rm /var/log/console/conman.key cray-console-node-0:/ # rm /var/log/console/conman.key.pub cray-console-node-0:/ # exit ncn-m001#   Restart the cray-console-operator pod.\nncn-m001# kubectl -n services scale --replicas=1 deployment/cray-console-operator deployment.apps/cray-console-operator scaled It may take some time to regenerate the keys and get them deployed to the BMCs, but in a while the console connections using SSH should be reestablished. Note that it may be worthwhile to determine how the SSH key was modified and establish site procedures to coordinate SSH key use or they may be overwritten again at a later time.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/conman/troubleshoot_conman_blocking_access_to_a_node_bmc/",
	"title": "Troubleshoot Conman Blocking Access To A Node BMC",
	"tags": [],
	"description": "",
	"content": "Troubleshoot ConMan Blocking Access to a Node BMC Disable ConMan if it is blocking access to a node by other means. ConMan runs on the system as a containerized service, and it is enabled by default. However, the use of ConMan to connect to a node blocks access to that node by other Serial over LAN (SOL) utilities or by a virtual KVM.\nFor information about how ConMan works, see ConMan.\nPrerequisites The Cray command line interface (CLI) tool is initialized and configured on the system.\nProcedure   Disable the console services.\nBecause the console services are looking for new hardware and continually verifying that console connections are established with all the nodes in the system, these services must be disabled to stop automatic console connections.\nFollow the directions in Disable ConMan After the System Software Installation to disable the automatic console connections.\n  Disable the SOL session.\nEven after the console services are disabled, the conman sol session might need to be directly disabled using ipmitool. Note that this is only required for River nodes as Mountain hardware does not use IPMI.\nncn-m001# export USERNAME=root ncn-m001# export IPMI_PASSWORD=changeme ncn-m001# ipmitool -I lanplus -H BMC_IP -U $USERNAME -E sol deactivate   Restart the console services.\nRefer to the directions in Disable ConMan After the System Software Installation to restart the console services when all work is complete.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/conman/troubleshoot_conman_failing_to_connect_to_a_console/",
	"title": "Troubleshoot Conman Failing To Connect To A Console",
	"tags": [],
	"description": "",
	"content": "Troubleshoot ConMan Failing to Connect to a Console There are many reasons that conman may not be able to connect to a specific console. This procedure outlines several things to check that may impact the connectivity with a console.\nPrerequisites This procedure requires administrative privileges.\nProcedure   Check for general network availability.\nIf the Kubernetes worker node the cray-console-node pod that is attempting to connect with a console cannot access that network address the connection will fail.\n  Find the cray-console-node pod that is connecting with the console of interest.\nncn-m001# CONPOD=$(kubectl get pods -n services \\ -o wide|grep cray-console-operator|awk '{print $1}') ncn-m001# kubectl -n services exec $CONPOD -c cray-console-operator -- sh -c \\ '/app/get-node XNAME' | jq .podname | sed 's/\u0026quot;//g' cray-console-node-2   Find the worker node this pod is running on.\nncn-m001# kubectl -n services get pods -o wide | grep cray-console-pod-2 cray-console-node-2 3/3 Running 0 28h 10.42.0.49 ncn-w003 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   SSH to the worker node that the pod is running on.\nncn-m001# ssh ncn-w003 ncn-w003#   Check that the BMC for this node is accessible from this worker.\nThe xname of the BMC is the same as the node, but with the node designation at the end removed. For example if the node is x3000c0s15b0n0, the BMC is x3000c0s15b0.\nncn-w003# ping BMC_XNAME PING x3000c0s7b0.hmn (10.254.1.7) 56(84) bytes of data. From ncn-m002.hmn (10.254.1.18) icmp_seq=1 Destination Host Unreachable From ncn-m002.hmn (10.254.1.18) icmp_seq=2 Destination Host Unreachable From ncn-m002.hmn (10.254.1.18) icmp_seq=3 Destination Host Unreachable From ncn-m002.hmn (10.254.1.18) icmp_seq=4 Destination Host Unreachable This indicates there is a network issue between the worker node and the node of interest. When the issue is resolved the console connection will be reestablished automatically.\n    Check for something else using the serial console connection.\nFor IPMI-based connections, there can only be one active connection at a time. If something else has taken that connection, ConMan will not be able to connect to it.\n  Find the cray-console-node pod that is connecting with the console of interest.\nncn-m001# CONPOD=$(kubectl get pods -n services \\-o wide|grep cray-console-operator|awk '{print $1}') ncn-m001# kubectl -n services exec $CONPOD -c cray-console-operator -- sh -c '/app/get-node XNAME' \\ | jq .podname | sed 's/\u0026quot;//g' cray-console-node-2   Check the log information for the node.\nncn-m001# kubectl -n services logs cray-console-node-2 cray-console-node | grep XNAME If something else is using the connection, there will be log entries like the following:\n2021/05/20 15:42:43 INFO: Console [x3000c0s15b0n0] disconnected from \u0026lt;x3000c0s15b0\u0026gt; 2021/05/20 15:43:23 INFO: Unable to connect to \u0026lt;x3000c0s15b0\u0026gt; via IPMI for [x3000c0s15b0n0]: connection timeout 2021/05/20 15:44:24 INFO: Unable to connect to \u0026lt;x3000c0s15b0\u0026gt; via IPMI for [x3000c0s15b0n0]: SOL in use 2021/05/20 15:45:23 INFO: Unable to connect to \u0026lt;x3000c0s15b0\u0026gt; via IPMI for [x3000c0s15b0n0]: SOL in use 2021/05/20 16:13:25 INFO: Unable to connect to \u0026lt;x3000c0s15b0\u0026gt; via IPMI for [x3000c0s15b0n0]: SOL in use   Force the connection to become available again.\nThe BMC username and password must be known for this command to work.\nncn-m001# export USERNAME=root ncn-m001# export IPMI_PASSWORD=changeme ncn-m001# ipmitool -H XNAME -U $USERNAME -E -I lanplus sol deactivate   Retry conman to verify the connection has been reestablished.\n    "
},
{
	"uri": "/docs-csm/en-11/operations/conman/conman/",
	"title": "Conman",
	"tags": [],
	"description": "",
	"content": "ConMan ConMan is a tool used for connecting to remote consoles and collecting console logs. These node logs can then be used for various administrative purposes, such as troubleshooting node boot issues.\nConMan runs on the system as a containerized service. It runs in a set of Docker containers within Kubernetes pods named cray-console-operator and cray-console-node. ConMan can be configured to encrypt usernames and passwords. Node console logs are stored locally within the cray-console-operator pod in the /var/log/conman/ directory, as well as being collected by the System Monitoring Framework (SMF).\nNew in Release 1.5 In previous releases, the entire service was contained in the cray-conman pod, and both logs and interactive access was handled through that single pod. Starting with version 1.5, the logs can be accessed through the cray-console-operator pod but interactive console access is handled through one of the cray-console-node pods. There are multiple cray-console-node pods, scaled to the size of the system.\n"
},
{
	"uri": "/docs-csm/en-11/operations/conman/disable_conman_after_system_software_installation/",
	"title": "Disable Conman After The System Software Installation",
	"tags": [],
	"description": "",
	"content": "Disable ConMan After the System Software Installation The ConMan utility is enabled by default. This procedure provides instructions for disabling it after the system software has been installed.\nPrerequisites This procedure requires administrative privileges.\nProcedure   Log on to a non-compute node (NCN) that acts as a Kubernetes master. This procedure assumes that it is being carried out on an NCN acting as a Kubernetes master.\n  Scale the cray-console-operator pods to 0 replicas.\nncn-m001# kubectl -n services scale --replicas=0 deployment/cray-console-operator deployment.apps/cray-console-operator scaled   Verify the cray-console-operator service is no longer running.\nncn-m001# kubectl -n services get pods | grep console-operator ncn-m001#   Scale the cray-console-node pods to 0 replicas.\nncn-m001# kubectl -n services scale --replicas=0 statefulset/cray-console-node statefulset.apps/cray-console-node scaled   Verify the cray-console-node service is no longer running.\nncn-m001# kubectl -n services get pods | grep console-node ncn-m001#   Restore the services.\nScale the cray-console-operator service back to 1 replica to restore the service at a later time. It will scale the cray-console-node pods after it starts operation.\nncn-m001# kubectl -n services scale --replicas=1 deployment/cray-console-operator deployment.apps/cray-console-operator scaled   Verify services are running again.\nncn-m001# kubectl -n services get pods | grep -e console-operator -e console-node cray-console-node-0 3/3 Running 0 8m44s cray-console-node-1 3/3 Running 0 8m18s cray-console-operator-79bf95964-lngpz 2/2 Running 0 9m29s   "
},
{
	"uri": "/docs-csm/en-11/operations/conman/establish_a_serial_connection_to_ncns/",
	"title": "Establish A Serial Connection To NCNs",
	"tags": [],
	"description": "",
	"content": "Establish a Serial Connection to NCNs The ConMan pod can be used to establish a serial console connection with each non-compute node (NCN) in the system. In the scenario of a power down or reboot of an NCN worker, it will be important to check which NCN the conman pod is running on, and to ensure that it is NOT running on the NCN worker that will be impacted. If the NCN worker (where ConMan is running) is powered down or rebooted, the ConMan pod will be unavailable until the node comes back up or until the ConMan pod has terminated on the downed NCN worker node and come up on another NCN worker node.\nPrerequisites The user performing this procedure needs to have access permission to the cray-console-operator and cray-console-node pods.\nProcedure   Check which cray-console-node pod the NCN is connected through using the xname of the NCN.\nIf doing an NCN reboot, the xname can be gathered from the /opt/cray/platform-utils/ncnGetXnames.sh script.\nncn-m001# CONPOD=$(kubectl get pods -n services -o wide|grep cray-console-operator|awk '{print $1}') ncn-m001# NODEPOD=$(kubectl -n services exec $CONPOD -c cray-console-operator -- sh -c '/app/get-node XNAME' \\ | jq .podname | sed 's/\u0026quot;//g') ncn-m001# echo $NODEPOD cray-console-node-2   Check which NCN this pod is running on.\nncn-m001# kubectl -n services get pods -o wide -A | grep $NODEPOD   Optional: Move the cray-console-node pod.\nThe pod can be proactively moved to a different worker if a power or reboot operation is going to be performed on the node where the pod is running.\n  Prevent new pods from being scheduled on the NCN worker node currently running ConMan.\nReplace the NCN_HOSTNAME value before running the command. An example NCN_HOSTNAME is ncn-w002.\nncn-m001# kubectl cordon NCN_HOSTNAME   Delete the pod.\nWhen the pod comes back up, it will be on a different NCN worker node.\nncn-m001# kubectl -n services delete $NODEPOD   Wait for pod to terminate and come back up again.\nIt may take several minutes even after the pod is running for the console connections to be re-established. In the mean time, there is a small chance that the console connection will be moved to a different cray-console-node pod.\n  Check which cray-console-node pod the NCN is connected to.\nncn-m001# NODEPOD=$(kubectl -n services exec $CONPOD -c cray-console-operator -- sh -c '/app/get-node XNAME' \\ | jq .podname | sed 's/\u0026quot;//g') ncn-m001# echo $NODEPOD If the result is \u0026lsquo;cray-console-node-\u0026rsquo; the console connection has not been re-established so wait and try again until a valid pod name is returned.\n    Establish a serial console session (from ncn-m001) with the desired NCN.\nExec into the correct cray-console-node pod.\nncn-m001# kubectl -n services exec -it $NODEPOD -- /bin/bash cray-console-node-1:/ #   Establish a console session for the desired NCN.\ncray-console-node-1:/ # conman -j XNAME \u0026lt;ConMan\u0026gt; Connection to console [XNAME] opened. nid000009 login: The console session log files for each NCN is located in the cray-console-operator and cray-console-node pods in a shared volume at the /var/log/conman/ directory in a file named \u0026lsquo;console.\u0026rsquo;.\nIMPORTANT: If the cray-console-node pod the user is connected through is running on the NCN that the console session is connected to and a reboot is initiated, expect the connection to terminate and there to be a gap in the console log file until the console connection is reestablished through a different cray-console-node pod or for the existing pod to be restarted on a different NCN. If the cray-console-node pod was running on a different NCN or was moved prior to the reboot, the console log and session should persist through the operation.\n  Exit the connection to the console with the \u0026amp;. command.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/conman/log_in_to_a_node_using_conman/",
	"title": "Log In To A Node Using Conman",
	"tags": [],
	"description": "",
	"content": "Log in to a Node Using ConMan This procedure shows how to connect to the node\u0026rsquo;s Serial Over Lan (SOL) via ConMan.\nPrerequisites The user performing this procedure needs to have access permission to the cray-console-operator and cray-console-node pods.\nProcedure   Log in to a non-compute node (NCN) that acts as the Kubernetes master or worker. This procedure assumes that it is being carried out on an NCN acting as a Kubernetes master.\n  Retrieve the cray-console-operator pod ID.\nncn-m001# CONPOD=$(kubectl get pods -n services \\ -o wide|grep cray-console-operator|awk '{print $1}') ncn-m001# echo $CONPOD cray-console-operator-79bf95964-qpcpp   Find the cray-console-node pod that is connected to the node.\nncn-m001: # NODEPOD=$(kubectl -n services exec $CONPOD -c cray-console-operator -- sh -c '/app/get-node XNAME' | jq .podname | sed 's/\u0026quot;//g') ncn-m001: # echo $NODEPOD cray-console-node-1   Log into the correct console-node pod.\nncn-m001# kubectl exec -it -n services $NODEPOD -- /bin/bash cray-console-node-1:/ #   Use the node\u0026rsquo;s ID to connect to the node\u0026rsquo;s SOL via ConMan.\ncray-console-node-1:/ # conman -j NODE_ID \u0026lt;ConMan\u0026gt; Connection to console [x3000c0s25b1] opened. nid000009 login: Using the command above, a user can also attach to an already active SOL session that is being used by another user, so both can access the node\u0026rsquo;s SOL simultaneously.\n  Exit the connection to the console with the \u0026amp;. command.\n  Exit the cray-console-node pod.\ncray-console-node-1:/ # exit ncn-m001: #   "
},
{
	"uri": "/docs-csm/en-11/operations/conman/manage_node_consoles/",
	"title": "Manage Node Consoles",
	"tags": [],
	"description": "",
	"content": "Manage Node Consoles ConMan is used for connecting to remote consoles and collecting console logs. These node logs can then be used for various administrative purposes, such as troubleshooting node boot issues.\nConMan runs on the system in a set of containers within Kubernetes pods named cray-console-operator and cray-console-node.\nThe cray-console-operator and cray-console-node pods determine which nodes they should monitor by checking with the Hardware State Manager (HSM) service. They do this once when they starts. If HSM has not discovered some nodes when they start, then HSM is unaware of them and so are cray-console-operator and cray-console-node pods.\nVerify that all nodes are being monitored for console logging and connect to them if desired.\nSee ConMan for other procedures related to remote consoles and node console logging.\nProcedure This procedure can be run from any member of the Kubernetes cluster to verify node consoles are being managed by ConMan and to connect to a console.\nNote: this procedure has changed since the CSM 0.9 release.\n  Find the cray-console-operator pod ID\nncn# CONPOD=$(kubectl get pods -n services \\-o wide|grep cray-console-operator|awk \u0026#39;{print $1}\u0026#39;) ncn# echo $CONPOD   Find the cray-console-node pod that is connected to the node. Be sure to substitute the actual xname of the node in the command below.\nncn# NODEPOD=$(kubectl -n services exec $CONPOD -c cray-console-operator -- sh -c \u0026#34;/app/get-node \u0026lt;xname\u0026gt;\u0026#34; | jq .podname | sed \u0026#39;s/\u0026#34;//g\u0026#39;) ncn# echo $NODEPOD   Log into the cray-console-node container in this pod:\nncn# kubectl exec -n services -it $NODEPOD -c cray-console-node -- bash cray-console-node#   Check the list of nodes being monitored.\ncray-console-node# conman -q Output looks similar to the following:\nx9000c0s1b0n0 x9000c0s20b0n0 x9000c0s22b0n0 x9000c0s24b0n0 x9000c0s27b1n0 x9000c0s27b2n0 x9000c0s27b3n0   Compute nodes or UANs are automatically added to this list a short time after they are discovered.\n  To access the node\u0026rsquo;s console, run the following command from within the pod. Again, remember to substitute the actual xname of the node.\ncray-console-node# conman -j \u0026lt;xname\u0026gt;  The console session can be exited by entering \u0026amp;.\n   Repeat the previous steps to verify that cray-conman is now managing all nodes that are included in HSM.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/conman/access_compute_node_logs/",
	"title": "Access Compute Node Logs",
	"tags": [],
	"description": "",
	"content": "Access Compute Node Logs This procedure shows how the ConMan utility can be used to retrieve compute node logs.\nPrerequisites The user performing this procedure needs to have access permission to the cray-console-operator pod.\nLimitations Encryption of compute node logs is not enabled, so the passwords may be passed in clear text.\nProcedure   Log on to a Kubernetes master or worker node.\n  Retrieve the cray-console-operator pod ID.\nncn-m001# CONPOD=$(kubectl get pods -n services \\ -o wide|grep cray-console-operator|awk '{print $1}') ncn-m001# echo $CONPOD cray-console-operator-79bf95964-qpcpp   Log on to the pod.\nncn-m001# kubectl exec -it -n services $CONPOD -- sh   Go to the log directory.\n# cd /var/log/conman   List the directory contents to identify node IDs.\n/var/log/conman # ls -la total 44 -rw------- 1 root root 1415 Nov 30 20:00 console.NODE_ID ...   Use the node\u0026rsquo;s ID to retrieve its logs.\n/var/log/conman # tail console.NODE_ID   Exit out of the pod.\n/var/log/conman # exit ncn-m001#   "
},
{
	"uri": "/docs-csm/en-11/operations/conman/access_console_log_data_via_the_system_monitoring_framework_smf/",
	"title": "Access Console Log Data Via The System Monitoring Framework (smf)",
	"tags": [],
	"description": "",
	"content": "Access Console Log Data Via the System Monitoring Framework (SMF) Console log data is collected by SMF and can be queried through the Kibana UI or Elasticsearch. Each line of the console logs are an individual record in the SMF database.\nPrerequisites This procedure requires the Kibana service to be up and running on a non-compute node (NCN).\nProcedure   Determine the external domain name by running the following command on any NCN:\nncn-m001# kubectl get secret site-init -n loftsman -o jsonpath='{.data.customizations.yaml}' \\ | base64 -d | grep \u0026quot;external:\u0026quot; external: SHASTA_EXTERNAL_DOMAIN.com   Navigate to the following URL in a web browser:\nhttps://sma-kibana.SHASTA_EXTERNAL_DOMAIN.com/app/kibana    If this appears: Do this:     the Keycloak login page Supply valid credentials, then wait to be redirected to the Kibana dashboard before continuing to the next step.   the error Kibana did not load properly. Check the server output for more information. Clear browser cookies for https://sma-kibana.SHASTA_EXTERNAL_DOMAIN.com   The Kibana dashboard (see example below) Proceed to next step    When the Kibana dashboard loads, the web UI displays the Discover page by default. Note that even without entering a search pattern, an index pattern shows entries ordered in time.\n  Select the Shasta index for the type of logs desired from the drop-down list to search that data source.\n  Identify the xname for individual consoles to search for specific logs.\nEach line of the log data is prepended with \u0026ldquo;console.hostname: XNAME\u0026rdquo; where XNAME is the name of the node for the console log. This information can be used to identify each individual console.\nFor example, the following is the console log for x3000c0s19b4n0:\nconsole.hostname: x3000c0s19b4n0 \u0026lt;ConMan\u0026gt; Console [x3000c0s19b4n0] joined by \u0026lt;root@localhost\u0026gt; on pts/0 at 10-09 15:11. console.hostname: x3000c0s19b4n0 2020-10-09 15:11:39 Keepalived_vrrp[38]: bogus VRRP packet received on vlan002 !!!   Enter Search terms for the specific console xname using the \u0026ldquo;console.hostname: XNAME\u0026rdquo; string.\n  Click the time range drop-down menu to select the time period for which logs are displayed.\nUsing a time range for these searches is important to limit the scope and number of records returned, as well as limiting the time required to perform the search.\nThe default time range is 15 minutes.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/configure_cray_cli/",
	"title": "Configure The Cray Command Line Interface (`cray` Cli)",
	"tags": [],
	"description": "",
	"content": "Configure the Cray Command Line Interface (cray CLI) The cray command line interface (CLI) is a framework created to integrate all of the system management REST APIs into easily usable commands.\nLater procedures in the installation workflow use the cray CLI to interact with multiple services. The cray CLI configuration needs to be initialized for the Linux account, and the Keycloak user running the procedure needs to be authorized. This section describes how to initialize the cray CLI for use by a user and how to authorize that user.\nThe cray CLI only needs to be initialized once per user on a node.\nProcedure   Unset the CRAY_CREDENTIALS environment variable, if previously set.\nSome of the installation procedures leading up to this point use the CLI with a Kubernetes managed service account that is normally used for internal operations. There is a procedure for extracting the OAUTH token for this service account and assigning it to the CRAY_CREDENTIALS environment variable to permit simple CLI operations.\nncn# unset CRAY_CREDENTIALS   Initialize the cray CLI for the root account.\nThe cray CLI needs to know what host to use to obtain authorization and what user is requesting authorization, so it can obtain an OAUTH token to talk to the API Gateway. This is accomplished by initializing the CLI configuration. In this example, the \u0026lsquo;vers\u0026rsquo; username and its password are used.\nIf LDAP configuration was enabled, then use a valid account in LDAP instead of the example account \u0026lsquo;vers\u0026rsquo;.\nIf LDAP configuration was not enabled, or is not working, then a Keycloak local account could be created. See Configure Keycloak Account to create this local account in Keycloak and then use it instead of the example account \u0026lsquo;vers\u0026rsquo;.\nncn# cray init When prompted, remember to use the correct username instead of \u0026lsquo;vers\u0026rsquo;. Expected output (including the typed input) should look similar to the following:\nCray Hostname: api-gw-service-nmn.local Username: vers Password: Success! Initialization complete.   Verify the cray CLI is operational.\nncn# cray artifacts buckets list -vvv Expected output, if an error occurs see the troubleshooting section below in this topic.\nLoaded token: /root/.config/cray/tokens/api_gw_service_nmn_local.vers REQUEST: PUT to https://api-gw-service-nmn.local/apis/sts/token OPTIONS: {'verify': False} S3 credentials retrieved successfully results = [ \u0026quot;alc\u0026quot;, \u0026quot;badger\u0026quot;, \u0026quot;benji-backups\u0026quot;, \u0026quot;boot-images\u0026quot;, \u0026quot;etcd-backup\u0026quot;, \u0026quot;fw-update\u0026quot;, \u0026quot;ims\u0026quot;, \u0026quot;install-artifacts\u0026quot;, \u0026quot;nmd\u0026quot;, \u0026quot;postgres-backup\u0026quot;, \u0026quot;prs\u0026quot;, \u0026quot;sat\u0026quot;, \u0026quot;sds\u0026quot;, \u0026quot;sls\u0026quot;, \u0026quot;sma\u0026quot;, \u0026quot;ssd\u0026quot;, \u0026quot;ssm\u0026quot;, \u0026quot;vbis\u0026quot;, \u0026quot;velero\u0026quot;, \u0026quot;wlm\u0026quot;,]   Troubleshooting NOTE: While resolving these issues is beyond the scope of this section, more information about what is failing can be found by adding -vvvvv to the cray init ... commands.\nIf initialization fails in the above step, there are several common causes:\n DNS failure looking up api-gw-service-nmn.local may be preventing the CLI from reaching the API Gateway and Keycloak for authorization Network connectivity issues with the NMN may be preventing the CLI from reaching the API Gateway and Keycloak for authorization Certificate mismatch or trust issues may be preventing a secure connection to the API Gateway Istio failures may be preventing traffic from reaching Keycloak Keycloak may not yet be set up to authorize the user  If the initialization fails and the reason output is similar to the following example, restart radosgw on the storage nodes.\nncn-m002# cray artifacts buckets list -vvv Loaded token: /root/.config/cray/tokens/api_gw_service_nmn_local.vers REQUEST: PUT to https://api-gw-service-nmn.local/apis/sts/token OPTIONS: {\u0026#39;verify\u0026#39;: False} ERROR: { \u0026#34;detail\u0026#34;: \u0026#34;The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.\u0026#34;, \u0026#34;status\u0026#34;: 500, \u0026#34;title\u0026#34;: \u0026#34;Internal Server Error\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;about:blank\u0026#34; } Usage: cray artifacts buckets list [OPTIONS] Try \u0026#39;cray artifacts buckets list --help\u0026#39; for help. Error: Internal Server Error: The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.   SSH to ncn-s001/2/3.\n  Restart the Ceph radosgw process.\nThe expected output will be similar to the following, but it will vary based on the number of nodes running radosgw.\nncn-s00(1/2/3)# ceph orch restart rgw.site1.zone1 restart rgw.site1.zone1.ncn-s001.cshvbb from host \u0026#39;ncn-s001\u0026#39; restart rgw.site1.zone1.ncn-s002.tlegbb from host \u0026#39;ncn-s002\u0026#39; restart rgw.site1.zone1.ncn-s003.vwjwew from host \u0026#39;ncn-s003\u0026#39;   Check to see that the processes restarted.\nThe \u0026ldquo;running\u0026rdquo; time should be in seconds. Restarting all of them could require a couple of minutes depending on how many.\nncn-s001# ceph orch ps --daemon_type rgw NAME HOST STATUS REFRESHED AGE VERSION IMAGE NAME IMAGE ID CONTAINER ID rgw.site1.zone1.ncn-s001.cshvbb ncn-s001 running (29s) 23s ago 9h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 2a712824adc1 rgw.site1.zone1.ncn-s002.tlegbb ncn-s002 running (29s) 28s ago 9h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c e423f22d06a5 rgw.site1.zone1.ncn-s003.vwjwew ncn-s003 running (29s) 23s ago 9h 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 1e6ad6bc2c62   In the event that more than 5 minutes has passed and the radosgw services have not restarted, fail the ceph-mgr process to the standby.\nThere are cases where an orchestration task gets stuck and our current remediation is to fail the Ceph manager process.\n# Get active ceph-mgr ncn-s00(1/2/3)#ceph mgr dump | jq -r .active_name ncn-s002.zozbqp # Fail the active ceph-mgr ncn-s00(1/2/3)# ceph mgr fail $(ceph mgr dump | jq -r .active_name) #Confirm ceph-mgr has moved to a different ceph-mgr container ncn-s00(1/2/3)# ceph mgr dump | jq -r .active_name ncn-s001.qucrpr   Verify that the processes restarted using the command from step 3.\nAt this point the processes should restart. If they do not, it is possible that steps 2 and 3 will need to be done again.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/configuration_management/view_configuration_session_logs/",
	"title": "View Configuration Session Logs",
	"tags": [],
	"description": "",
	"content": "View Configuration Session Logs Logs for the individual steps of a session are available via the kubectl log command for each container of a Configuration Framework Service (CFS) session. Refer to Configuration Sessions for more info about these containers.\nTo find the name of the Kubernetes pod that is running the CFS session:\nncn# kubectl get pods --no-headers -o \\ custom-columns=\u0026#34;:metadata.name\u0026#34; -n services -l cfsession=example cfs-f9d18751-e6d1-4326-bf76-434293a7b1c5-q8tsc Store the returned pod name as the CFS_POD_NAME variable for future use:\nncn# export CFS_POD_NAME=cfs-f9d18751-e6d1-4326-bf76-434293a7b1c5-q8tsc Alternatively, if the session is one of many recent sessions and the session name is not known, it is possible to list all CFS pods by start time and pick the desired pod based on status or start time:\nncn# kubectl -n services --sort-by=.metadata.creationTimestamp get pods | grep cfs cfs-47bed8b5-e1b1-4dd7-b71c-40e9750d3183-7msmr 0/7 Completed 0 36m cfs-0675d19f-5bec-424a-b0e1-9d466299aff5-dtwhl 0/7 Error 0 5m25s cfs-f49af8e9-b8ab-4cbb-a4f6-febe519ef65f-nw76v 0/7 Error 0 4m14s cfs-31635b42-6d03-4972-9eba-b011baf9c5c2-jmdjx 6/7 NotReady 0 3m33s cfs-b9f50fbe-04de-4d9a-b5eb-c75d2d561221-dhgg6 6/7 NotReady 0 2m10s To view the logs of the various containers:\nncn# kubectl logs -n services ${CFS_POD_NAME} -c ${CONTAINER_NAME} The ${CONTAINER_NAME} value is one of the containers mentioned in Configuration Sessions. Depending on the number of configuration layers in the session, some sessions will have more containers available. Use the -f option in the previous command to follow the logs if the session is still running.\nTo view the Ansible logs, determine which configuration layer\u0026rsquo;s logs to view from the order of the configuration set in the session. For example, if it is the first layer, the ${CONTAINER_NAME} will be ansible-0.\nncn# kubectl logs -n services ${CFS_POD_NAME} -c ansible-0 The git-clone-# and ansible-# containers may not start at 0 and may not be numbered sequentially if the session was created with the --configuration-limit option.\n"
},
{
	"uri": "/docs-csm/en-11/operations/configuration_management/write_ansible_code_for_cfs/",
	"title": "Write Ansible Code For CFS",
	"tags": [],
	"description": "",
	"content": "Write Ansible Code for CFS Cray provides Ansible plays and roles for software products deemed necessary for the system to function. Customers are free to write their own Ansible plays and roles to augment what Cray provides or implement new features. Basic knowledge of Ansible is needed to write plays and roles. The information below includes recommendations and best practices for writing and running Ansible code on the system successfully with the Configuration Framework Service (CFS).\nHelp with Ansible can be found in the external Ansible documentation:\n Ansible playbook best practices Ansible examples  Ansible Code Structure The Version Control Service (VCS) is setup during the Cray System Management (CSM) product installation, which is the appropriate place to store configuration content. Individual product installations include the Ansible code to properly configure each product.\nThe structure of the individual repository directories matches the recommended directory layout from the external Ansible documentation. The default playbook site.yml is found at the top level, if it exists, and the Ansible roles and variables are in their appropriate directories. Inventory directories like group_vars and host_vars may exist, but they are empty and left for variable overrides and customizations by the user.\nWrite Playbooks for Multiple Node Types Within an Ansible playbook, users can designate which node groups the various tasks and roles will run against. For example, a default site.yml playbook may contain a list of roles that are run on only the Compute nodes, and a list of roles that will run on only Application nodes. This is designated using the hosts parameter.\nFor example, hosts: Compute would be used to target the compute nodes. Users can create additional sections that target other node types, or adjust the hosts that the included roles will run against as necessary. It is also possible to target multiple groups within a section of a playbook, or to specify complex targets, such as nodes that are in one group and not in another group. The syntax for this is available in the external Ansible documentation. Hosts can be in more than one group at a time if there are user-defined groups. In this case, Ansible will run all sections that match the node type against the node.\nSee the Ansible Inventory section for more information about groups that are made automatically available through CFS dynamic inventory.\nPerformance and Scaling Tips CFS will handle scaling up Ansible to run on many hosts, but there are still places where performance can be improved by correctly writing Ansible plays.\n Use image customization when possible to limit how many times a task is run and improve boot times. Configuration that is the same for all nodes of the same type will benefit from image customization. See the next section for how to target specific tasks for running only during image customization. Import roles rather than playbooks. Each time a new playbook starts, Ansible automatically gathers facts for all the systems it is running against. This is not necessary more than once and can slow down Ansible execution. Turn off facts that are not needed in a playbook by setting gather_facts: false. If only a few facts are required, it is also possible to limit fact gathering by setting gather_subset. For more information on gather_subset, see the external Ansible module setup documentation. Use loops rather than individual tasks where modules are called multiple times. Some Ansible modules will optimize the command, such as grouping package installations into a single transaction (Refer to the external Ansible playbook loops documentation).  "
},
{
	"uri": "/docs-csm/en-11/operations/configuration_management/use_a_custom_ansible-cfg_file/",
	"title": "Use A Custom Ansible-cfg File",
	"tags": [],
	"description": "",
	"content": "Use a Custom ansible-cfg File The Configuration Framework Service (CFS) allows for flexibility with the Ansible Execution Environment (AEE) by allowing for changes to included ansible.cfg file. When installed, CFS imports a custom ansible.cfg file into the cfs-default-ansible-cfg Kubernetes ConfigMap in the services namespace.\nAdministrators who want to make changes to the ansible.cfg file on a per-session or system-wide basis can upload a new file to a new ConfigMap and direct CFS to use their file. See Set the ansible.cfg for a Session for more information.\nTo create a custom ansible.cfg that CFS can use, create a new ansible.cfg file and a ConfigMap:\nncn# vi ansible.cfg # create an Ansible config file ncn# kubectl create configmap custom-ansible-cfg -n services --from-file=ansible.cfg To use this Ansible configuration file for a specific session, set --ansible-config custom-ansible-cfg when creating a session, or set --default-ansible-config custom-ansible-cfg when setting global CFS options with the cray cfs options update command.\n"
},
{
	"uri": "/docs-csm/en-11/operations/configuration_management/use_a_specific_inventory_in_a_configuration_session/",
	"title": "Use A Specific Inventory In A Configuration Session",
	"tags": [],
	"description": "",
	"content": "Use a Specific Inventory in a Configuration Session A special repository can be added to a Configuration Framework Service (CFS) configuration to help with certain scenarios, specifically when developing Ansible plays for use on the system. A static inventory often changes along with the Ansible content, and CFS users may need to test different configuration values simultaneously and not be forced to use the global additionalInventoryUrl.\nTherefore, an additional_inventory mapping can be added to the CFS configuration. Similar to a standard configuration layer, the additional inventory only requires a commit and repository clone URL, and it overrides the global additionalInventoryUrl if it is specified in the global CFS options.\nFor example:\nncn-m001# cat configurations-example-additional-inventory.json { \u0026#34;layers\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;configurations-layer-example-1\u0026#34;, \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/example-repo.git\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;\u0026lt;git commit id\u0026gt;\u0026#34; } ], \u0026#34;additional_inventory\u0026#34;: { \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/inventory.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;a7d08b6e1be590ac01711e39c684b6893c1da0a9\u0026#34; } } "
},
{
	"uri": "/docs-csm/en-11/operations/configuration_management/vcs_branching_strategy/",
	"title": "Vcs Branching Strategy",
	"tags": [],
	"description": "",
	"content": "VCS Branching Strategy Individual products import configuration content (Ansible plays, roles, and more) into a repository in the Version Control Service (VCS) through their installation process. Typically, this repository is of the following form and exists in the cray organization in VCS:\n[product name]-config-management The import branch of the product is considered \u0026ldquo;pristine content\u0026rdquo; and is added to VCS in a read-only branch. This step is taken to ensure the future updates of the product\u0026rsquo;s configuration content can be based on a clean branch, and that the upgrade procedure can proceed without merging issues.\nCray products import their content into a branch in the following format:\ncray/[product]/[product version] The version is in SemVer format. Product installation and operational workflows will direct users to create branches from these pristine product branches to begin customizing their configuration from the default provided by the Cray product.\nThe workflow of branching from pristine imported content and handling upgrades to that content is shown in the diagram below (see \u0026ldquo;Branch Workflow\u0026rdquo;). In this scenario, the configuration customizations are made in a customer/main branch, which can be any branch in the repository, such as master, main, integration, or others. As configuration changes are made and the system is configured as required, users can point to the commits (white circles) in their CFS configuration layers to test and run Cray products on the system. Users may also point to the commits on pristine branches (blue circle) in CFS configuration layers if the default configuration is sufficient.\nWhen a product upgrade occurs (cray/product/1.4.0 in the above diagram), the product installer will create a new branch and commit based off the previous pristine branch. If the previous customizations on the customer/main branch also need to be included with the new 1.4.0 content, the user should initiate a Git merge of the new pristine content into their branch (or possibly into a test branch based on customer/main). This process will be the same for subsequent updates of the product as well.\nImportant: If the Cray product has specific instructions for installing and upgrading configuration content, those should take precedence over this generic workflow.\nThe \u0026ldquo;Branch Workflow\u0026rdquo; diagram is an example workflow that can be used to manage new product content being introduced during upgrades. However, CFS and VCS do not require any specific branching strategy. Users are free to manage the branches as they see fit with the exception of the pristine branches imported by individual Cray products. CFS configuration layers (see Configuration Layers) only require a Git commit ID, a Git repository clone URL, and the path to an Ansible playbook to run the configuration content in the repository.\n"
},
{
	"uri": "/docs-csm/en-11/operations/configuration_management/version_control_service_vcs/",
	"title": "Version Control Service (vcs)",
	"tags": [],
	"description": "",
	"content": "Version Control Service (VCS) The Version Control Service (VCS) includes a web interface for repository management, pull requests, and a visual view of all repositories and organizations. The following URL is for the VCS web interface:\nhttps://vcs.SHASTA_CLUSTER_DNS_NAME On cluster nodes, the VCS service can be accessed through the gateway. VCS credentials for the crayvcs user are required before cloning a repository (see the \u0026ldquo;VCS Administrative User\u0026rdquo; section below). To clone a repository in the cray organization, use the following command:\nncn# git clone https://api-gw-service-nmn.local/vcs/cray/REPO_NAME.git VCS Administrative User The Cray System Management (CSM) product installation creates the administrative user crayvcs that is used by CSM and other product installers to import their configuration content into VCS. The initial VCS credentials for the crayvcs user are obtained with the following command:\nncn# kubectl get secret -n services vcs-user-credentials \\ --template={{.data.vcs_password}} | base64 --decode The initial VCS login credentials for the crayvcs user are stored in three places:\n vcs-user-credentials Kubernetes secret: This is used to initialize the other two locations, as well as providing a place where other users can query for the password. VCS (Gitea): These credentials are used when pushing to Git using the default username and password. The password should be changed through the Gitea UI. Keycloak: These credentials are used to access the VCS UI. They must be changed through Keycloak.  WARNING: These three sources of credentials are not synced by any mechanism. Changing the default password requires that is it changed in all three places. Changing only one may result in difficulty determining the password at a later date, or may result in losing access to VCS altogether.\nTo change the password in the vcs-user-credentials Kubernetes secret, use the following command:\nncn# kubectl create secret generic vcs-user-credentials --save-config \\ --from-literal=vcs_username=\u0026#34;crayvcs\u0026#34; \\ --from-literal=vcs_password=\u0026#34;NEW_PASSWORD\u0026#34; \\ --dry-run=client -o yaml | kubectl apply -f - The NEW_PASSWORD value must be replaced with the updated password.\nAccess the cray Gitea Organization The VCS UI uses Keycloak to authenticate users on the system. However, users from external authentication sources are not automatically associated with permissions in the cray Gitea organization. As a result, users configured via Keycloak can log in and create organizations and repositories of their own, but they cannot modify the cray organization that is created during system installation unless they are given permissions to do so.\nThe crayvcs Gitea admin user that is created during CSM installation can log in to the UI via Keycloak. To allow users other than crayvcs to have access to repositories in the cray organization, use the following procedure:\n  Log in to VCS as the crayvcs user on the system:\nhttps://vcs.SHASTA_CLUSTER_DNS_NAME   Navigate to the cray organization owners page at\nhttps://vcs.SHASTA_CLUSTER_DNS_NAME/vcs/cray/teams/owners   Enter the username of the user who should have access to the organization in the Search user\u0026hellip; text field, and click the Add Team Member button.\n  IMPORTANT The \u0026ldquo;Owner\u0026rdquo; role has full access to all repositories in the organization, as well as administrative access to the organization, including the ability to create and delete repositories.\nFor granting non-administrative access to the organization and its repositories, create a new team at the following URL:\nhttps://vcs.SHASTA_CLUSTER_DNS_NAME/vcs/org/cray/teams/new Select the permissions appropriately, and then navigate to the following URL to add members to the newly created team:\nhttps://vcs.SHASTA_CLUSTER_DNS_NAME/vcs/org/cray/teams Backup and Restore Data Data for gitea is stored in two places. Git content is stored directly in a PVC, while structural data, such as gitea users and the list and attributes of repos, is stored in a Postgres database. Because of this, both sources must be backed up and restored together.\nBackup Postgres Data   Determine which Postgres member is the leader and exec into the leader pod to dump the data to a local file:\nncn-w001# kubectl exec gitea-vcs-postgres-0 -n services -c postgres -it -- patronictl list + Cluster: gitea-vcs-postgres (6995618180238446669) -----+----+-----------+ | Member | Host | Role | State | TL | Lag in MB | +----------------------+--------------+--------+---------+----+-----------+ | gitea-vcs-postgres-0 | 10.45.0.21 | Leader | running | 1 | | | gitea-vcs-postgres-1 | 10.46.128.19 | | running | 1 | 0 | | gitea-vcs-postgres-2 | 10.47.0.21 | | running | 1 | 0 | +----------------------+--------------+--------+---------+----+-----------+ ncn-w001# POSTGRES_LEADER=gitea-vcs-postgres-0 ncn-w001# kubectl exec -it ${POSTGRES_LEADER} -n services -c postgres -- pg_dumpall -c -U postgres \u0026gt; gitea-vcs-postgres.sql   Determine what secrets are associated with the postgresql credentials:\nncn-w001# kubectl get secrets -n services | grep gitea-vcs-postgres.credentials postgres.gitea-vcs-postgres.credentials Opaque 2 13d service-account.gitea-vcs-postgres.credentials Opaque 2 13d standby.gitea-vcs-postgres.credentials Opaque 2 13d Export each secret to a manifest file:\nncn-w001:~ # SECRETS=\u0026quot;postgres service-account standby\u0026quot; ncn-w001:~ # echo \u0026quot;---\u0026quot; \u0026gt; gitea-vcs-postgres.manifest ncn-w001:~ # for secret in $SECRETS; do \u0026gt; kubectl get secret \u0026quot;${secret}.gitea-vcs-postgres.credentials\u0026quot; -n services -o yaml \u0026gt;\u0026gt; gitea-vcs-postgres.manifest \u0026gt; echo \u0026quot;---\u0026quot; \u0026gt;\u0026gt; gitea-vcs-postgres.manifest \u0026gt; done Edit the manifest file created above to remove creationTimestamp, resourceVersion, selfLink, uid for each entry Then copy all files to a safe location.\n  Backup PVC Data The VCS postgres backups should be accompanied by backups of the VCS PVC. The export process can be run at any time while the service is running using the following commands:\nBackup (save the resulting tar file to a safe location):\nPOD=$(kubectl -n services get pod -l app.kubernetes.io/instance=gitea -o json | jq -r '.items[] | .metadata.name') kubectl -n services exec ${POD} -- tar -cvf vcs.tar /data/ kubectl -n services cp ${POD}:vcs.tar ./vcs.tar Restore Postgres Data Restoring VCS from Postgres is documented here: Restore_Postgres.md\nRestore PVC Data When restoring the VCS postgres database, the PVC should also be restored to the same point in time. The restore process can be run at any time while the service is running using the following commands:\nRestore:\nPOD=$(kubectl -n services get pod -l app.kubernetes.io/instance=gitea -o json | jq -r '.items[] | .metadata.name') kubectl -n services cp ./vcs.tar ${POD}:vcs.tar kubectl -n services exec ${POD} -- tar -xvf vcs.tar kubectl -n services rollout restart deployment gitea-vcs Alternative Backup/Restore Strategy An alternative to the separate backups of the postgres and pvc data is to backup the git data. This has the advantage that only one backup is needed and that the git backups can be imported into any git server, not just gitea, but has the disadvantage that some information about the gitea deployment is lost (such as user/org information) and may need to be recreated manually if the VCS deployment is lost.\nThe following scripts create/use a vcs-content directory that contains all git data. This should be copied to a safe location after export, and moved back to the system before import.\nExport:\nRESULTS=vcs-content mkdir $RESULTS VCS_USER=$(kubectl get secret -n services vcs-user-credentials --template={{.data.vcs_username}} | base64 --decode) VCS_PASSWORD=$(kubectl get secret -n services vcs-user-credentials --template={{.data.vcs_password}} | base64 --decode) git config --global credential.helper store echo \u0026quot;https://${VCS_USER}:${VCS_PASSWORD}@api-gw-service-nmn.local\u0026quot; \u0026gt; ~/.git-credentials for repo in $(curl -s https://api-gw-service-nmn.local/vcs/api/v1/orgs/cray/repos -u ${VCS_USER}:${VCS_PASSWORD}| jq -r '.[] | .name') do git clone --mirror https://api-gw-service-nmn.local/vcs/cray/${repo}.git cd ${repo}.git git bundle create ${repo}.bundle --all cp ${repo}.bundle ../$RESULTS cd .. rm -r $repo.git done Import:\nSOURCE=vcs-content VCS_USER=$(kubectl get secret -n services vcs-user-credentials --template={{.data.vcs_username}} | base64 --decode) VCS_PASSWORD=$(kubectl get secret -n services vcs-user-credentials --template={{.data.vcs_password}} | base64 --decode) git config --global credential.helper store echo \u0026quot;https://${VCS_USER}:${VCS_PASSWORD}@api-gw-service-nmn.local\u0026quot; \u0026gt; ~/.git-credentials for file in $(ls $SOURCE) do repo=$(echo $file | sed 's/.bundle$//') git clone --mirror ${SOURCE}/${repo}.bundle cd ${repo}.git git remote set-url origin https://api-gw-service-nmn.local/vcs/cray/${repo}.git git push cd .. rm -r ${repo}.git done Prior to import, the repo structure may need to be recreated if it has not already been by an install. (Adjust the repo list as necessary if any additional are present. Repo settings such as public/private will also need to be manually set if this is used.)\nVCS_USER=$(kubectl get secret -n services vcs-user-credentials --template={{.data.vcs_username}} | base64 --decode) VCS_PASSWORD=$(kubectl get secret -n services vcs-user-credentials --template={{.data.vcs_password}} | base64 --decode) REPOS=\u0026quot;analytics-config-management cos-config-management cpe-config-management slurm-config-management sma-config-management uan-config-management csm-config-management\u0026quot; for repo in $REPOS do curl -X POST https://api-gw-service-nmn.local/vcs/api/v1/orgs/cray/repos -u ${VCS_USER}:${VCS_PASSWORD} -d name=${repo} done "
},
{
	"uri": "/docs-csm/en-11/operations/configuration_management/troubleshoot_ansible_play_failures_in_cfs_sessions/",
	"title": "Troubleshoot Ansible Play Failures In CFS Sessions",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Ansible Play Failures in CFS Sessions View the Kubernetes logs for a Configuration Framework Service (CFS) pod in an error state to determine whether the error resulted from the CFS infrastructure or from an Ansible play that was run by a specific configuration layer in a CFS session.\nUse this procedure to obtain important triage information for Ansible plays being called by CFS.\nPrerequisites A configuration session exists for CFS.\nProcedure   Find the CFS pod that is in an error state.\nIn the example below, the $CFS_POD_NAME is cfs-e8e48c2a-448f-4e6b-86fa-dae534b1702e-pnxmn.\nncn# kubectl get pods -n services $CFS_POD_NAME NAME READY STATUS RESTARTS AGE cfs-e8e48c2a-448f-4e6b-86fa-dae534b1702e-pnxmn 0/3 Error 0 25h   Check to see what containers are in the pod.\nncn# kubectl logs -n services $CFS_POD_NAME Error from server (BadRequest): a container name must be specified for pod cfs-e8e48c2a-448f-4e6b-86fa-dae534b1702e-pnxmn, choose one of: [inventory ansible-0 istio-proxy] or one of the init containers: [git-clone-0 istio-init] Issues rarely occur in the istio-init and istio-proxy containers. These containers can be ignored for now.\n  Check the git-clone-0, inventory, ansible-0 containers in that order.\n  Check the git-clone-0 container.\nncn# kubectl logs -n services CFS_POD_NAME git-clone-0 Cloning into \u0026#39;/inventory\u0026#39;...   Check the inventory container.\n# kubectl logs -n services CFS_POD_NAME inventory % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0curl: (7) Failed to connect to localhost port 15000: Connection refused Waiting for Sidecar % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 HTTP/1.1 200 OK content-type: text/html; charset=UTF-8 cache-control: no-cache, max-age=0 x-content-type-options: nosniff date: Thu, 05 Dec 2019 15:00:11 GMT server: envoy transfer-encoding: chunked Sidecar available 2019-12-05 15:00:12,160 - INFO - cray.cfs.inventory - Starting CFS Inventory version=0.4.3, namespace=services 2019-12-05 15:00:12,171 - INFO - cray.cfs.inventory - Inventory target=dynamic for cfsession=boa-2878e4c0-39c2-4df0-989e-053bb1edee0c 2019-12-05 15:00:12,227 - INFO - cray.cfs.inventory.dynamic - Dynamic inventory found a total of 2 groups 2019-12-05 15:00:12,227 - INFO - cray.cfs.inventory - Writing out the inventory to /inventory/hosts   Check the ansible-0 container.\nLook towards the end of the Ansible log in the PLAY RECAP section to see if any have failed. If it failed, look above at the immediately preceding play. In the example below, the ncmp_hsn_cns role has an issue when being run against the compute nodes.\nncn# kubectl logs -n services CFS_POD_NAME ansible-0 Waiting for Inventory Waiting for Inventory Inventory available % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 ... TASK [ncmp_hsn_cns : SLES Compute Nodes (HSN): Create/Update ifcfg-hsnx File(s)] *** fatal: [x3000c0s19b1n0]: FAILED! =\u0026gt; {\u0026#34;msg\u0026#34;: \u0026#34;\u0026#39;interfaces\u0026#39; is undefined\u0026#34;} fatal: [x3000c0s19b2n0]: FAILED! =\u0026gt; {\u0026#34;msg\u0026#34;: \u0026#34;\u0026#39;interfaces\u0026#39; is undefined\u0026#34;} fatal: [x3000c0s19b3n0]: FAILED! =\u0026gt; {\u0026#34;msg\u0026#34;: \u0026#34;\u0026#39;interfaces\u0026#39; is undefined\u0026#34;} fatal: [x3000c0s19b4n0]: FAILED! =\u0026gt; {\u0026#34;msg\u0026#34;: \u0026#34;\u0026#39;interfaces\u0026#39; is undefined\u0026#34;} NO MORE HOSTS LEFT ************************************************************* PLAY RECAP ********************************************************************* x3000c0s19b1n0 : ok=28 changed=20 unreachable=0 failed=1 skipped=77 rescued=0 ignored=1 x3000c0s19b2n0 : ok=27 changed=19 unreachable=0 failed=1 skipped=63 rescued=0 ignored=1 x3000c0s19b3n0 : ok=27 changed=19 unreachable=0 failed=1 skipped=63 rescued=0 ignored=1 x3000c0s19b4n0 : ok=27 changed=19 unreachable=0 failed=1 skipped=63 rescued=0 ignored=1     Run the Ansible play again once the underlying issue has been resolved.\n"
},
{
	"uri": "/docs-csm/en-11/operations/configuration_management/troubleshoot_cfs_session_failing_to_complete/",
	"title": "Troubleshoot CFS Session Failing To Complete",
	"tags": [],
	"description": "",
	"content": "Troubleshoot CFS Session Failing to Complete Troubleshoot issues where Configuration Framework Service (CFS) sessions/pods fail and Ansible hangs. These issues can be resolved by modifying Ansible to produce less output.\nPrerequisites A CFS session or pod is failing to complete, and the Ansible logs are not showing progress or completion.\nThe following is an example of the error causing Ansible to hang:\nPLAY [Compute] ***************************************************************** META: ran handlers META: ran handlers META: ran handlers PLAY [Compute] ***************************************************************** Using module file /usr/lib/python3.6/site-packages/ansible/modules/system/setup.py Pipelining is enabled. \u0026lt;x5000c3s3b0n1\u0026gt; ESTABLISH SSH CONNECTION FOR USER: root \u0026lt;x5000c3s3b0n1\u0026gt; SSH: EXEC ssh -vvv -o ServerAliveInterval=30 -o ControlMaster=auto -o ControlPersist=60s -o StrictHostKeyChecking=no -o \u0026#39;IdentityFile=\u0026#34;/secret/key\u0026#34;\u0026#39; -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o \u0026#39;User=\u0026#34;root\u0026#34;\u0026#39; -o ConnectTimeout=10 -o ControlPath=/root/.ansible/cp/f6f378183d x5000c3s3b0n1 \u0026#39;/bin/sh -c \u0026#39;\u0026#34;\u0026#39;\u0026#34;\u0026#39;/usr/bin/python3 \u0026amp;\u0026amp; sleep 0\u0026#39;\u0026#34;\u0026#39;\u0026#34;\u0026#39;\u0026#39; Procedure The steps in this procedure are independent from each other and are used to troubleshoot different underlying problems that both present a similar symptom.\n  Reduce the amount of output produced by Ansible.\nAny of the following steps can be taken to help reduce the output generated by Ansible:\n  Reduce the verbosity when running Ansible commands.\nIf the session was created with a higher value of \u0026ndash;ansible-verbosity (three or higher), Ansible can generate a lot of output that can cause the pod to hang. Reducing the verbosity by one or more may resolve this issue.\n  Update the Ansible configuration to produce less output.\nSee Enable Ansible Profiling for an example of modifying the configuration.\n  Adjust the use of flags used when running Ansible commands.\nThe display_ok_hosts and display_skipped_hosts are examples of settings that can be disabled to reduce output. Refer to the Ansible documentation for more information on what flags can be used.\n    "
},
{
	"uri": "/docs-csm/en-11/operations/configuration_management/update_a_cfs_configuration/",
	"title": "Update A CFS Configuration",
	"tags": [],
	"description": "",
	"content": "Update a CFS Configuration Modify a Configuration Framework Service (CFS) configuration by specifying the JSON of the configuration and its layers. Use the cray cfs configurations update command, similar to creating a configuration.\nPrerequisites  A CFS configuration has been created. The Cray command line interface (CLI) tool is initialized and configured on the system.  Procedure   Add and/or remove the configuration layers from an existing JSON configuration file.\nDo not include the name of the configuration in the JSON file. This is specified on the command line in the next step.\nncn# cat configurations-example.json { \u0026#34;layers\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;configurations-layer-example-1\u0026#34;, \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/example-repo.git\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;\u0026lt;git commit id\u0026gt;\u0026#34; } ] }   Update the configuration in CFS.\nncn# cray cfs configurations update configurations-example \\ --file ./configurations-example.json --format json { \u0026#34;lastUpdated\u0026#34;: \u0026#34;2021-07-28T03:26:30:37Z\u0026#34;, \u0026#34;layers\u0026#34;: [ { \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/example-repo.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;\u0026lt;git commit id\u0026gt;\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;configurations-layer-example-1\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;configurations-example\u0026#34; }   "
},
{
	"uri": "/docs-csm/en-11/operations/configuration_management/update_the_privacy_settings_for_gitea_configuration_content_repositories/",
	"title": "Update The Privacy Settings For Gitea Configuration Content Repositories",
	"tags": [],
	"description": "",
	"content": "Update the Privacy Settings for Gitea Configuration Content Repositories Change the visibility of Gitea configuration content repositories from public to private. All Cray-provided repositories are created as private by default.\nProcedure   Log in to the Version Control Service (VCS) as the crayvcs user.\nUse the following URL to access the VCS web interface:\nhttps://vcs.SYSTEM-NAME.DOMAIN-NAME   Navigate to the cray organization.\nhttps://vcs.SYSTEM-NAME.DOMAIN-NAME/vcs/cray   Select the repository title for each repository listed on the page.\n  Click the Settings button in the repository header section.\n  Update the visibility settings for the repository.\n  Click the Visibility check box to make the repository private.\n  Click the Update Settings button to save the change for the repository.\n    "
},
{
	"uri": "/docs-csm/en-11/operations/configuration_management/set_the_ansible-cfg_for_a_session/",
	"title": "Set The `ansible.cfg` For A Session",
	"tags": [],
	"description": "",
	"content": "Set the ansible.cfg for a Session View and update the Ansible configuration used by CFS.\nAnsible configuration is available through the ansible.cfg file. See the Configuring Ansible external documentation for more information about what values can be set.\nThe Configuration Framework Service (CFS) provides a default ansible.cfg file in the cfs-default-ansible-cfg Kubernetes ConfigMap in the services namespace.\nTo view the ansible.cfg file:\nncn# kubectl get cm -n services cfs-default-ansible-cfg \\ -o json | jq -r \u0026#39;.data.\u0026#34;ansible.cfg\u0026#34;\u0026#39; .... WARNING: Much of the configuration in this file is required by CFS to function properly. Particularly the cfs_aggregator callback plug-in, which is used for reporting configuration state to the CFS APIs, and the cfs_* strategy plug-ins. Exercise extreme caution when making changes to this ConfigMap\u0026rsquo;s contents. See Ansible Execution Environments for more information\nThe default ansible.cfg file ConfigMap can be changed to a custom ConfigMap (within the services Kubernetes namespace) by updating it in the CFS service options. This will update all CFS sessions to use this file for ansible.cfg.\nTo use a different ansible.cfg on a per-session basis, use the --ansible-config option when creating a CFS session. Refer to Use a Custom ansible-cfg File for more information.\n"
},
{
	"uri": "/docs-csm/en-11/operations/configuration_management/target_ansible_tasks_for_image_customization/",
	"title": "Target Ansible Tasks For Image Customization",
	"tags": [],
	"description": "",
	"content": "Target Ansible Tasks for Image Customization The Configuration Framework Service (CFS) enables Ansible playbooks to run against both running nodes and images. See the \u0026ldquo;Use Cases\u0026rdquo; header in the Configuration Management section for more information about image customization and when it should be used.\nCFS uses the cray_cfs_image variable to distinguish between node personalization (running on live nodes) and image customization (configuring an image prior to boot). When this variable is set to true, it indicates that the CFS session is an image customization type and the playbook is targeting an image.\nCFS automatically sets this variable in the hosts/01-cfs-generated.yaml file for all sessions. When the session target is image customization, it sets cray_cfs_image to true; otherwise, it is false.\ncray_cfs_image can be set with Ansible playbook conditionals to selectively run individual tasks with when: cray_cfs_image, or to ignore individual tasks with when: not cray_cfs_image.\nConditionals can also be applied to entire roles if desired (see the external apply Ansible conditionals to roles). In instances where the same playbook may be run in both modes, it is best practice to include a conditional on all parts of the playbook, or at least those that take significant time so that work is not duplicated.\nIt is also best practice to include a default in Ansible roles for playbook and role portability because CFS injects this variable at runtime. This can be done in the defaults section of the role, or where the variable is called. For example:\nwhen: \u0026#34;{{ cray_cfs_image | default(false) }}\u0026#34; If a default is not provided, any playbooks or roles will not be runnable outside of the CFS Ansible Execution Environment (AEE) without the user specifying cray_cfs_image in the vars files or with the Ansible extra-vars options.\nWhen running Ansible against an IMS-hosted image root during an image customization CFS session, there are no special requirements for the paths when copying or syncing files. The image root directories will appears as if Ansible is connecting to a regular, live node. However, the image is not a running node, so actions that require a running system, such as starting/reloading a service, will not work properly and will cause the Ansible play to fail. Actions like these should be done only during live-node configuration modes such as node personalization.\n"
},
{
	"uri": "/docs-csm/en-11/operations/configuration_management/track_the_status_of_a_session/",
	"title": "Track The Status Of A Session",
	"tags": [],
	"description": "",
	"content": "Track the Status of a Session A configuration session can be a long-running process, and depends on many system factors, as well as the number of configuration layers and Ansible tasks that are run in each layer. The Configuration Framework Service (CFS) provides the session status through the session metadata to allow for tracking progress and session state.\nTo view the session status of a session named example, use the following command:\n# cray cfs sessions describe example --format json { \u0026#34;ansible\u0026#34;: { \u0026#34;config\u0026#34;: \u0026#34;cfs-default-ansible-cfg\u0026#34;, \u0026#34;limit\u0026#34;: \u0026#34;, \u0026#34;verbosity\u0026#34;: 0 }, \u0026#34;configuration\u0026#34;: { \u0026#34;limit\u0026#34;: \u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;configurations-example\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;example\u0026#34;, \u0026#34;status\u0026#34;: { \u0026#34;artifacts\u0026#34;: [], \u0026#34;session\u0026#34;: { \u0026#34;completionTime\u0026#34;: \u0026#34;2020-07-28T03:26:30\u0026#34;, \u0026#34;job\u0026#34;: \u0026#34;cfs-8c8d628b-ebac-4946-a8b7-f1f167b35b0d\u0026#34;, \u0026#34;startTime\u0026#34;: \u0026#34;2020-07-28T03:26:00\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;complete\u0026#34;, \u0026#34;succeeded\u0026#34;: \u0026#34;true\u0026#34; } }, \u0026#34;tags\u0026#34;: {}, \u0026#34;target\u0026#34;: { \u0026#34;definition\u0026#34;: \u0026#34;dynamic\u0026#34;, \u0026#34;groups\u0026#34;: null } } The jq tool, along with the --format json output option of the CLI, are helpful for filtering the session data to view just the session status:\n# cray cfs sessions describe example \\ --format json | jq .status.session { \u0026#34;completionTime\u0026#34;: \u0026#34;2020-07-28T03:26:30\u0026#34;, \u0026#34;job\u0026#34;: \u0026#34;cfs-8c8d628b-ebac-4946-a8b7-f1f167b35b0d\u0026#34;, \u0026#34;startTime\u0026#34;: \u0026#34;2020-07-28T03:26:00\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;complete\u0026#34;, \u0026#34;succeeded\u0026#34;: \u0026#34;true\u0026#34; } The status section of the cray cfs session describe command output will not be populated until the CFS session Kubernetes job has started.\nThe .status.session mapping shows the overall status of the configuration session. The .succeeded key within this mapping is a string with values of either \u0026quot;true\u0026quot;, \u0026quot;false\u0026quot;, \u0026quot;unknown\u0026quot;, or \u0026quot;none\u0026quot;.\n\u0026quot;none\u0026quot; occurs if the session has not yet completed, and \u0026quot;unknown\u0026quot; occurs when the session is deleted mid-run, there is an error creating the session and it never starts, or any similar case where checking the session status would fail to find the underlying Kubernetes job running the CFS session.\nValues of .status can be \u0026quot;pending\u0026quot;, \u0026quot;running\u0026quot;, or \u0026quot;complete\u0026quot;.\n"
},
{
	"uri": "/docs-csm/en-11/operations/configuration_management/enable_ansible_profiling/",
	"title": "Enable Ansible Profiling",
	"tags": [],
	"description": "",
	"content": "Enable Ansible Profiling Ansible tasks and playbooks can be profiled in order to determine execution times and single out poor performance in runtime. The default Configuration Framework Service (CFS) ansible.cfg in the cfs-default-ansible-cfg ConfigMap does not enable these profiling tools. If profiling tools are desired, modify the default Ansible configuration file to enable them.\nProcedure   Edit the cfs-default-ansible-cfg ConfigMap.\nncn# kubectl edit cm cfs-default-ansible-cfg -n services   Uncomment the indicated line by removing the # character from the beginning of the line.\n#callback_whitelist = cfs_aggregator, timer, profile_tasks, profile_roles   Comment out the indicated line by adding a # character to the beginning of the line.\ncallback_whitelist = cfs_aggregator   New sessions will be created with profiling information available in the Ansible logs of the session pods. Alternatively, if editing the default ansible.cfg file that CFS uses, is not desired, a new Ansible configuration can also be created to enable profiling and direct CFS to use it. See Use a Custom ansible-cfg File for more information.\n"
},
{
	"uri": "/docs-csm/en-11/operations/configuration_management/git_operations/",
	"title": "Git Operations",
	"tags": [],
	"description": "",
	"content": "Git Operations Use the git command to manage repository content in the Version Control Service (VCS).\nOnce a repository is cloned, the git command line tool is available to interact with a repository from the Version Control Service (VCS). The git command is used for making commits, creating new branches, and pushing new branches, tags, and commits to the remote repository stored in VCS.\nWhen pushing changes to the VCS server using the crayvcs user, input the password retrieved from the Kubernetes secret as the credentials. See the \u0026ldquo;VCS Administrative User\u0026rdquo; heading in Version Control Service (VCS) for more information.\nncn# git push Username for \u0026#39;https://api-gw-service-nmn.local\u0026#39;: crayvcs Password for \u0026#39;https://crayvcs@api-gw-service-nmn.local\u0026#39;: \u0026lt;input password here\u0026gt; ... For more information on how to use the Git command line tools, refer to the external Git User Manual.\n"
},
{
	"uri": "/docs-csm/en-11/operations/configuration_management/manage_multiple_inventories_in_a_single_location/",
	"title": "Manage Multiple Inventories In A Single Location",
	"tags": [],
	"description": "",
	"content": "Manage Multiple Inventories in a Single Location Many configuration layers may be present in a single configuration for larger systems that configure multiple Cray products. When values for each of these layers need to be customized, it can be tedious to override values in each of the respective repositories. The CFS additionalInventoryUrl option allows for static inventory files to be automatically added to the hosts directory of each configuration layer before it is applied by Ansible. It is then possible to add this additional Ansible inventory information to all configuration sessions so it can be used simultaneously with other inventory types, including the CFS dynamic inventory type, across all configuration layers in a session.\nThe additionalInventoryUrl option is optional and is set on a global CFS level. If provided, it must be set to the URL of a Git repository containing inventory files in the base directory of the repository. For ordering purposes, any inventory generated by CFS will also be placed in this directory with the name 01-cfs-generated.yaml. See the \u0026ldquo;Dynamic Inventory and Host Groups\u0026rdquo; section in Ansible Inventory.\nThe following is an example of an inventory repository:\n02-static-inventory.ini 03-my-dynamic-inventory.py group_vars/... host_vars/... CFS will provide the following inventory to Ansible when running a configuration session:\nhosts/01-cfs-generated.yaml hosts/02-static-inventory.ini hosts/03-my-dynamic-inventory.py hosts/group_vars/... hosts/host_vars/... CFS will clone the additional inventory Git repository and use the default branch (usually master) to populate the hosts directory. Only one inventory repository can be specified, and it will apply to all CFS sessions.\nUse the following command to set the additionalInventoryUrl value:\nncn# cray cfs options update \\ --additional-inventory-url https://api-gw-service-nmn.local/vcs/cray/inventory.git Use the following command to unset the additionalInventoryUrl value:\nncn# cray cfs options update --additional-inventory-url \u0026#34; "
},
{
	"uri": "/docs-csm/en-11/operations/configuration_management/set_limits_for_a_configuration_session/",
	"title": "Set Limits For A Configuration Session",
	"tags": [],
	"description": "",
	"content": "Set Limits for a Configuration Session The configuration layers and session hosts can be limited when running a Configuration Framework Service (CFS) session.\nLimit CFS Session Hosts Subsets of nodes can be targeted in the inventory when running CFS sessions, which is useful specifically when running a session with dynamic inventory. Use the CFS --ansible-limit option when creating a session to apply the limits. The option directly corresponds to the --limit option offered by ansible-playbook, and can be used to specify hosts, groups, or combinations of them with patterns. CFS passes the value of this option directly to the ansible-playbook command for each configuration layer in the session. See the Ansible documentation on Patterns: targeting hosts and groups.\nIMPORTANT: The --limit option is useful for temporarily limiting the scope of targets for a configuration session. For example, it could be used to target a subset of the Compute group that has been separated for development use. However it should not be used to limit an Ansible playbook to target only the nodes that the playbook is intended to use. If a playbook should only be run on a specific group, target the proper group(s) with the hosts: section of the Ansible playbook.\nSee Using Ansible Limits for more information about limiting hosts and groups in playbooks.\nUse the following command to create a CFS session to run on all hosts in the Compute group, but not a previously defined dev group:\nncn# cray cfs sessions create --name example \\ --configuration-name configurations-example \\ --ansible-limit \u0026#39;Compute:!dev\u0026#39; { \u0026#34;ansible\u0026#34;: { \u0026#34;config\u0026#34;: \u0026#34;cfs-default-ansible-cfg\u0026#34;, \u0026#34;limit\u0026#34;: \u0026#34;Compute:!dev\u0026#34;, \u0026#34;verbosity\u0026#34;: 0 }, \u0026#34;configuration\u0026#34;: { \u0026#34;limit\u0026#34;: \u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;configurations-example\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;example\u0026#34;, \u0026#34;status\u0026#34;: { \u0026#34;artifacts\u0026#34;: [], \u0026#34;session\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;pending\u0026#34;, \u0026#34;succeeded\u0026#34;: \u0026#34;none\u0026#34; } }, \u0026#34;tags\u0026#34;: {}, \u0026#34;target\u0026#34;: { \u0026#34;definition\u0026#34;: \u0026#34;dynamic\u0026#34;, \u0026#34;groups\u0026#34;: null } } Limit CFS Session Configuration Layers It is possible to limit the session to only specific layers of the configuration that is specified. This is useful when re-applying configuration of a specific layer and applying the other layers is not necessary or desired. This option may also reduce the number of configurations that need to be created and stored by CFS because sessions can specify layers from a master configuration layer list.\nUse the --configuration-limit option when creating a CFS session to apply configuration layer limits. Multiple layers to limit the session are specified as a comma-separated list either by name (if layers were given names when created) or by zero-based index as defined in the configuration submitted to CFS.\nUse the following command to create a CFS session to run only on example-layer1, and then example-layer5 of a previously created configurations-example configuration:\nWARNING: If the configuration\u0026rsquo;s layers do not have names, then indices must be specified. Do not mix layer names and layer indices when using limits.\nncn# cray cfs sessions create --name example \\ --configuration-name configurations-example \\ --configuration-limit \u0026#39;example-layer1,example-layer5\u0026#39; { \u0026#34;ansible\u0026#34;: { \u0026#34;config\u0026#34;: \u0026#34;cfs-default-ansible-cfg\u0026#34;, \u0026#34;limit\u0026#34;: \u0026#34;, \u0026#34;verbosity\u0026#34;: 0 }, \u0026#34;configuration\u0026#34;: { \u0026#34;limit\u0026#34;: \u0026#34;example-layer1,example-layer5\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;configurations-example\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;example\u0026#34;, \u0026#34;status\u0026#34;: { \u0026#34;artifacts\u0026#34;: [], \u0026#34;session\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;pending\u0026#34;, \u0026#34;succeeded\u0026#34;: \u0026#34;none\u0026#34; } }, \u0026#34;tags\u0026#34;: {}, \u0026#34;target\u0026#34;: { \u0026#34;definition\u0026#34;: \u0026#34;dynamic\u0026#34;, \u0026#34;groups\u0026#34;: null } } "
},
{
	"uri": "/docs-csm/en-11/operations/configuration_management/create_an_image_customization_cfs_session/",
	"title": "Create An Image Customization CFS Session",
	"tags": [],
	"description": "",
	"content": "Create an Image Customization CFS Session A configuration session that is meant to customize image roots tracked by the Image Management Service (IMS) can be created using the --target-definition image option. This option will instruct the Configuration Framework Service (CFS) to prepare the image IDs specified and assign them to the groups specified in Ansible inventory. IMS will then provide SSH connection information to each image root that CFS will use to configure Ansible.\nUsers can expect that staging the image and generating an inventory will be a longer process than creating a session with other target definitions (for example, inventories). Tearing down the configuration session will also require additional time while IMS packages up the image build artifacts and uploads them to the artifact repository.\nIn order to use the image target definition, an image must be registered with the IMS. For example, if the image ID is 5d64c8b2-4f0e-4b2e-b334-51daba16b7fb, use jq along with the CLI --format json output option to determine if the image ID is known to IMS:\nncn# cray ims images list --format json | jq -r \u0026#39;any(.[]; .id == \u0026#34;5d64c8b2-4f0e-4b2e-b334-51daba16b7fb\u0026#34;)\u0026#39; true To create a CFS session for image customization, provide a session name, the name of the configuration to apply, and the group/image ID mapping:\nWARNING: If a CFS session is created with an ID that is not known to IMS, CFS will not fail and will instead wait for the image ID to become available in IMS.\nncn# cray cfs sessions create --name example \\ --configuration-name configurations-example \\ --target-definition image \\ --target-group Compute IMS_IMAGE_ID { \u0026#34;ansible\u0026#34;: { \u0026#34;config\u0026#34;: \u0026#34;cfs-default-ansible-cfg\u0026#34;, \u0026#34;limit\u0026#34;: null, \u0026#34;verbosity\u0026#34;: 0 }, \u0026#34;configuration\u0026#34;: { \u0026#34;limit\u0026#34;: \u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;configurations-example\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;example\u0026#34;, \u0026#34;status\u0026#34;: { \u0026#34;artifacts\u0026#34;: [], \u0026#34;session\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;pending\u0026#34;, \u0026#34;succeeded\u0026#34;: \u0026#34;none\u0026#34; } }, \u0026#34;tags\u0026#34;: {}, \u0026#34;target\u0026#34;: { \u0026#34;definition\u0026#34;: \u0026#34;image\u0026#34;, \u0026#34;groups\u0026#34;: [ { \u0026#34;members\u0026#34;: [ \u0026#34;\u0026lt;IMS IMAGE ID\u0026gt;\u0026#34; ], \u0026#34;name\u0026#34;: \u0026#34;Compute\u0026#34; } ] } } Retrieve the Resultant Image ID When an image customization CFS session is complete, use the CFS describe command to show the IMS image ID that results from the applied configuration:\nncn# cray cfs sessions describe example --format json | jq .status.artifacts [ { \u0026#34;image_id\u0026#34;: \u0026#34;\u0026lt;IMS IMAGE ID\u0026gt;\u0026#34;, \u0026#34;result_id\u0026#34;: \u0026#34;\u0026lt;RESULTANT IMS IMAGE ID\u0026gt;\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;ims_customized_image\u0026#34; } ] This resultant image ID can be used to be further customized pre-boot, or if it is ready, in a Boot Orchestration Service (BOS) boot session template.\n"
},
{
	"uri": "/docs-csm/en-11/operations/configuration_management/create_and_populate_a_vcs_configuration_repository/",
	"title": "Create And Populate A Vcs Configuration Repository",
	"tags": [],
	"description": "",
	"content": "Create and Populate a VCS Configuration Repository Create a new repository in the VCS and populate it with content for site customizations in a custom Configuration Framework Service (CFS) configuration layer.\nPrerequisites  The Version Control Service (VCS) login credentials for the crayvcs user are set up. See the \u0026ldquo;VCS Administrative User\u0026rdquo; heading in Version Control Service (VCS) for more information.  Procedure   Create the empty repository in VCS.\nReplace the CRAYVCS_PASSWORD value in the following command before running it.\nncn# curl -X POST https://api-gw-service-nmn.local/vcs/api/v1/org/cray/repos \\ -d \u0026#39;{\u0026#34;name\u0026#34;: \u0026#34;NEW_REPO\u0026#34;, \u0026#34;private\u0026#34;: true}\u0026#39; -u crayvcs:CRAYVCS_PASSWORD \\ -H \u0026#34;Content-Type: application/json\u0026#34;   Clone the empty VCS repository.\nncn# git clone https://api-gw-service-nmn.local/vcs/cray/NEW_REPO.git   Change to the directory of the empty Git repository and populate it with content.\nncn# cd NEW_REPO ncn# cp -a ~/user/EXAMPLE-config-management/* .   Add the new content, commit it, and push it to VCS.\nThe following command will move the content to the master branch of the repository.\nncn# git add --all \u0026amp;\u0026amp; git commit -m \u0026#34;Initial config\u0026#34; \u0026amp;\u0026amp; git push   Retrieve the Git hash for the Configuration Framework Service (CFS) layer definition.\nncn# git rev-parse --verify HEAD 63ed2b8c02fb71975a05003a7ef2f84db581786b   "
},
{
	"uri": "/docs-csm/en-11/operations/configuration_management/customize_configuration_values/",
	"title": "Customize Configuration Values",
	"tags": [],
	"description": "",
	"content": "Customize Configuration Values In general, most systems will require some customization from the default values provided by Cray products. As stated in the previous section, these changes cannot be made on the pristine product branches that are imported during product installation and upgrades. Changes can only be made in Git branches that are based on the pristine branches.\nChanging or overriding default values should be done in accordance with Ansible best practices (see the external Ansible best practices guide) and variable precedence (see the external Ansible variable guide) in mind.\nThe following practices should also be followed:\n When it is necessary to add more functionality beyond an Ansible playbook provided by a Cray product, include the product playbook in a new playbook instead of modifying it directly. Any modifications to a product playbook will result in a merge being required during a product upgrade. Do not modify default Ansible role variables, override all values using inventory (group_vars and host_vars directories). Cray products do not import any content to inventory locations, so merges of new product content will not cause conflicts if variables are located in inventory. Do not put any sensitive or secret information, such as passwords, in the Git repository. These values should be pulled during runtime from an external key management system.  Handling Sensitive Information Passwords and other sensitive content should not be stored in a Version Control Service (VCS) repository. Instead, consider writing Ansible tasks/roles that pull the value in dynamically while the playbook is running from an external secrets management system.\nAt this time, the Ansible Vault is not supported by the Configuration Framework Service (CFS).\nExample: Override a Role Default Value To override a value that is defined in an Ansible role in a configuration repository, set the value in the Ansible inventory. If the override pertains to an entire Ansible group of nodes, create a file as follows:\n  Clone the configuration repository.\n  Checkout the branch that will include the change, or create a new branch.\n  Capture the variable name in the roles/[role name]/defaults/main.yml file.\n  Create a new directory and edit a file with the role name.\nncn# mkdir -p group_vars/all \u0026amp;\u0026amp; touch group_vars/all/[role name].yml   Set the variable to a new value in the file.\nncn# echo \u0026#39;[variable name]: [new variable value]\u0026#39; \u0026gt;\u0026gt; group_vars/all/[role name].yml   Stage the file in the Git branch, commit it, and promote the change.\n  This change will be applied to all nodes by using the group name all. To narrow the variable scope to a specific group (Compute for example), use the group name instead of all as follows:\nncn# mkdir -p group_vars/Compute \u0026amp;\u0026amp; touch group_vars/Compute/[role name].yml ncn# echo \u0026#39;[variable name]: [new variable value]\u0026#39; \u0026gt;\u0026gt; group_vars/Compute/[role name].yml To narrow the variable scope to a single node create the file in the host_vars/[node xname]/[role name].yaml and override the value.\nThe name of the created file is largely inconsequential. The identified best practice is to include the name of the role in the created file for reasons of maintainability and discoverability. However, be aware that the file name matters when multiple files in the same directory contain the same variable with different values. In that case, use a single all.yml file rather than a directory, or use files names with import ordering in mind. See the external Ansible documentation for more information on how variables precedence is handled.\nTo override role variables for roles that exist across multiple repositories, consider using the CFS Additional Inventory feature. See Manage Multiple Inventories in a Single Location.\nExample: Add or Remove Functionality to a Provided Playbook To add more functionality to a playbook provided by a configuration repository, it is considered best practice to leave the existing playbook unmodified (if possible) to not have merge conflicts when new versions of the playbook are installed. For instance, if a site.yml playbook needs to be extended with a custom site-custom.yml playbook, consider creating a new playbook that imports and runs them both. For example, the site-all.yml playbook.\nncn# cat site-all.yml - import_playbook: site.yml - import_playbook: site-custom.yml See the external Ansible documentation on re-using playbooks.\nTo remove a role from a provided playbook, the best practice is to determine if the role provides a variable to skip the roles tasks altogether. See the roles/[role name]/README file for a listing of the role variables.\nIf changes to role variables are not able to skip the role, the role may be commented out in the playbook. However, note that an upgrade to the configuration will result in a merge conflict because of the changes made in the playbook if the upgrade pristine branch is merged with the branch containing the commented playbook.\n"
},
{
	"uri": "/docs-csm/en-11/operations/configuration_management/delete_cfs_sessions/",
	"title": "Delete CFS Sessions",
	"tags": [],
	"description": "",
	"content": "Delete CFS Sessions Delete an existing Configuration Framework Service (CFS) configuration session with the CFS delete command.\nUse the session name to delete the session:\nncn# cray cfs sessions delete example \u0026lt;no output expected\u0026gt; To delete all completed CFS sessions, use the deleteall command. This command can also filter the sessions to delete based on tags, name, status, age, and success or failure. By default, if no other filter is specified, this command only deletes completed sessions.\nCompleted CFS sessions can also be automatically deleted based on age. See the Automatic Session Deletion with sessionTTL section.\n"
},
{
	"uri": "/docs-csm/en-11/operations/configuration_management/configuration_sessions/",
	"title": "Configuration Sessions",
	"tags": [],
	"description": "",
	"content": "Configuration Sessions Once configurations have been created with the required layers and values set in the configuration repositories (or the additional inventory repository), create a Configuration Framework Session (CFS) session to apply the configuration to the targets.\nSessions are created via the Cray CLI or through the CFS REST API. A session stages Ansible inventory (whether dynamic, static, or image customization), launches Ansible Execution Environments (AEE) in order for each configuration layer in the service mesh, tears down the environments as required, and reports the session status to the CFS API.\nWhen a session target is an Image Management Service (IMS) image ID for the purposes of pre-boot image customization, the CFS session workflow varies slightly. The inventory staging instead calls IMS to expose the requested image root(s) via SSH. After the AEE(s) finish applying the configuration layers, CFS then instructs IMS to tear down the image root environment and package up the resultant image and records the new image ID in the session metadata.\nSession Naming Conventions CFS follows the same naming conventions for session names as Kubernetes does for jobs. Session names must follow the Kubernetes naming conventions and are limited to 45 characters.\nRefer to the external Kubernetes naming conventions documentation for more information.\nConfiguration Session Filters CFS provides several filters for use when listing sessions or using the bulk delete option. These following filters are available:\n  --status - Session status options include pending, running, and complete.\n  --succeeded - If the session has not yet completed, this will be set to none. Otherwise, this will be set to true, false, or unknown in the event that CFS was unable to find the Kubernetes job associated with the session.\n  --min-age/--max-age - Returns only the sessions that fall within the given age. For example, --max-age could be used to list only the recent sessions, or --min-age could be used to find old sessions for cleanup. Age is given in the format \u0026ldquo;1d\u0026rdquo; for days, or \u0026ldquo;6h\u0026rdquo; for hours.\n  --tags - Sessions can be created with searchable tags. By default, this includes the bos_session tag when CFS is triggered by BOS. This can be searched using the following command:\nncn-m001# cray cfs sessions list --tags bos_session=BOS_SESSION_NAME   Configuration Session Workflow CFS progresses through a session by running a series of commands in containers located in a Kubernetes job pod. Four container types are present in the job pod which pertain to CFS session setup, execution, and teardown:\n  git-clone-*\nThese init containers are responsible for cloning the configuration repository and checking out the branch/commit specified in each configuration layer.\nThese containers are run in the same order as the layers are specified, and their names are indexed appropriately: git-clone-0, git-clone-1, and so on.\n  git-clone-hosts\nThis init container clones the repository specified in the parameter additionalInventoryUrl, if specified.\n  inventory\nThis container is responsible for generating the dynamic inventory or for communicating to IMS to stage boot image roots that need to be made available via SSH when the session --target-definition is image.\n  ansible-*\nThese containers run the AEE after CFS injects the inventory and Git repository content from previous containers. One container is executed for each configuration layer specified.\nThese containers are run in the same order as the layers are specified, and their names are indexed appropriately: ansible-0, ansible-1, and so on.\n  teardown\nThis container waits for the last ansible-* to complete and subsequently calls IMS to package up customized image roots. The container only exists when the session --target-definition is image.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/configuration_management/create_a_cfs_configuration/",
	"title": "Create A CFS Configuration",
	"tags": [],
	"description": "",
	"content": "Create a CFS Configuration Create a Configuration Framework Service (CFS) configuration, which contains an ordered list of layers. Each layer is defined by a Git repository clone URL, a Git commit, a name, and the path in the repository to an Ansible playbook to execute.\nPrerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system.  Procedure   Create a JSON file to hold data about the CFS configuration.\nncn# cat configurations-example.json { \u0026#34;layers\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;configurations-layer-example-1\u0026#34;, \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/example-repo.git\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;\u0026lt;git commit id\u0026gt;\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;configurations-layer-example-2\u0026#34;, \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/example-repo2.git\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;\u0026lt;git commit id\u0026gt;\u0026#34; }, # { ... add more configuration layers here, if needed ... } ] }   Add the configuration to CFS with the JSON file.\nncn# cray cfs configurations update configurations-example \\ --file ./configurations-example.json --format json { \u0026#34;lastUpdated\u0026#34;: \u0026#34;2021-07-28T03:26:00:37Z\u0026#34;, \u0026#34;layers\u0026#34;: [ ... ], \u0026#34;name\u0026#34;: \u0026#34;configurations-example\u0026#34; }   "
},
{
	"uri": "/docs-csm/en-11/operations/configuration_management/create_a_cfs_session_with_dynamic_inventory/",
	"title": "Create A CFS Session With Dynamic Inventory",
	"tags": [],
	"description": "",
	"content": "Create a CFS Session with Dynamic Inventory A Configuration Framework Service (CFS) session using dynamic inventory is used to configure live nodes. To create a CFS session using the default dynamic inventory, simply provide a session name and the name of the configuration to apply:\nncn# cray cfs sessions create --name example \\ --configuration-name configurations-example { \u0026#34;ansible\u0026#34;: { \u0026#34;config\u0026#34;: \u0026#34;cfs-default-ansible-cfg\u0026#34;, \u0026#34;limit\u0026#34;: \u0026#34;, \u0026#34;verbosity\u0026#34;: 0 }, \u0026#34;configuration\u0026#34;: { \u0026#34;limit\u0026#34;: \u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;configurations-example\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;example\u0026#34;, \u0026#34;status\u0026#34;: { \u0026#34;artifacts\u0026#34;: [], \u0026#34;session\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;pending\u0026#34;, \u0026#34;succeeded\u0026#34;: \u0026#34;none\u0026#34; } }, \u0026#34;tags\u0026#34;: {}, \u0026#34;target\u0026#34;: { \u0026#34;definition\u0026#34;: \u0026#34;dynamic\u0026#34;, \u0026#34;groups\u0026#34;: null } } Add the --target-definition dynamic parameter to the create command to explicitly define the inventory type to be dynamic. This will enable CFS to provide the Ansible host groups via its dynamic inventory. The individual Ansible playbooks specified in the configuration layers will decide which hosts and/or groups will have configuration applied to them.\n"
},
{
	"uri": "/docs-csm/en-11/operations/configuration_management/configuration_layers/",
	"title": "Configuration Layers",
	"tags": [],
	"description": "",
	"content": "Configuration Layers The Configuration Framework Service (CFS) uses configuration layers to specify the location of configuration content that will be applied. Configurations may include one or more layers. Each layer is defined by a Git repository clone URL, a Git commit, a name (optional), and the path in the repository to an Ansible playbook to execute.\nConfigurations with a single layer are useful when testing out a new configuration on targets, or when configuring system components with one product at a time. To fully configure a node or boot image component with all of the software products required, multiple layers can be used to apply all configurations in a single CFS session. When applying layers in a session, CFS runs through the configuration layers serially in the order specified.\nExample Configuration (Single Layer) The following is an example configuration with a single layer. This can be used as a template to create a new configuration JSON file to input to CFS.\nncn# cat configuration-single.json { \u0026#34;layers\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;configurations-layer-example-1\u0026#34;, \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/example-repo.git\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;43ecfa8236bed625b54325ebb70916f55884b3a4\u0026#34; } ] } Example Configuration (Multiple Layers) The following is an example configuration with multiple layers from one or more different configuration repositories. This can be used as a template to create a new configuration JSON file to input to CFS.\nncn# cat configuration-multiple.json { \u0026#34;layers\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;configurations-layer-example-1\u0026#34;, \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/example-repo.git\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;43ecfa8236bed625b54325ebb70916f55884b3a4\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;configurations-layer-example-2\u0026#34;, \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/example-repo.git\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site-custom.yml\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;43ecfa8236bed625b54325ebb70916f55884b3a4\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;configurations-layer-example-3\u0026#34;, \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/second-example-repo.git\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;8236bed625b4b3a443ecfa54325ebb70916f5588\u0026#34; } ] } Use Branches in Configuration Layers When defining a configuration layer, the branch or commit values can be used to reference a Git commit. The commit value is the recommended way to reference a Git commit. In the following example, when the configuration is created or updated, CFS will automatically check with VCS to get the commit at the head of the branch. Both the commit and the branch are then stored. The commit acts as normal, and the branch is stored to make future updates to the commit easier.\nncn-m001# cat configurations-example.json { \u0026#34;layers\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;configurations-layer-example-1\u0026#34;, \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/example-repo.git\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34;, \u0026#34;branch\u0026#34;: \u0026#34;main\u0026#34; } ] } ncn-m001# cray cfs configurations update configurations-example \\ --file ./configurations-example.json \\ --format json { \u0026#34;lastUpdated\u0026#34;: \u0026#34;2021-07-28T03:26:30:37Z\u0026#34;, \u0026#34;layers\u0026#34;: [ { \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/example-repo.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;\u0026lt;git commit id\u0026gt;\u0026#34;, \u0026#34;branch\u0026#34;: \u0026#34;main\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;configurations-layer-example-1\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;configurations-example\u0026#34; } If changes are made to a repository and branches are specified in the configuration, users can then use the --update-branches flag to update a configuration so that all commits reflect the latest commit on the branches specified.\nncn-m001# cray cfs configurations update configurations-example --update-branches { \u0026#34;lastUpdated\u0026#34;: \u0026#34;2021-07-28T03:26:30:37Z\u0026#34;, \u0026#34;layers\u0026#34;: [ { \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/example-repo.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;\u0026lt;latest git commit id\u0026gt;\u0026#34;, \u0026#34;branch\u0026#34;: \u0026#34;main\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;configurations-layer-example-1\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;configurations-example\u0026#34; } Manage Configurations Use the cray cfs configurations --help command to manage CFS configurations on the system. The following operations are available:\n list: List all configurations. describe: Display info about a single configuration and its layer(s). update: Create a new configuration or modify an existing configuration. delete: Delete an existing configuration.  "
},
{
	"uri": "/docs-csm/en-11/operations/configuration_management/configuration_management/",
	"title": "Configuration Management",
	"tags": [],
	"description": "",
	"content": "Configuration Management The Configuration Framework Service (CFS) is available on systems for remote execution and configuration management of nodes and boot images. This includes nodes available in the Hardware State Manager (HSM) inventory (compute, non-compute, application, and user access nodes), and boot images hosted by the Image Management Service (IMS).\nCFS configures nodes and images via a gitops methodology. All configuration content is stored in a version control service (VCS), and is managed by authorized system administrators. CFS provides a scalable Ansible Execution Environment (AEE) for the configuration to be applied with flexible inventory and node targeting options.\nUse Cases CFS is available for the following use cases on systems:\n Image customization: Pre-configure bootable images available via IMS. This use case enables partitioning a full configuration of a target node. Non-node-specific settings are applied pre-boot, which reduces the amount of configuration required after a node boots, and therefore reduces the bring-up time for nodes. Post-boot configuration: Fully configure or reconfigure booted nodes in a scalable, performant way to add the required settings. \u0026ldquo;Push-based\u0026rdquo; deployment: When using post-boot configuration with only node-specific configuration data, the target undergoes node personalization. The two-step process of pre-boot image customization and post-boot node personalization results in a fully configured node, optimized for minimal bring-up times. \u0026ldquo;Pull-based\u0026rdquo; deployment: Provide configuration management to nodes by prescribing a desired configuration state and ensuring the current node configuration state matches the desired state automatically. This is achieved via the CFS Hardware Synchronization Agent and the CFS Batcher implementation.  CFS Components CFS is comprised of a group of services and components interacting within the Cray System Management (CSM) service mesh, and provides a means for system administrators to configure nodes and boot images via Ansible. CFS includes the following components:\n A REST API service. A command-line interface (CLI) to the API (via the cray cfs command). Pre-packaged AEE(s) with values tuned for performant configuration for executing Ansible playbooks, and reporting plug-ins for communication with CFS. The CFS Hardware Sync Agent, which pulls in node information from the system inventory to the CFS database to track the node configuration state. The CFS Batcher, which manages the configuration state of system components (nodes).  Although it is not a formal part of the service, CFS integrates with a Gitea instance (VCS) running in the CSM service mesh for management of the configuration content life-cycle.\nHigh-Level Configuration Workflow CFS remotely executes Ansible configuration content on nodes or boot images with the following workflow:\n Creating a configuration with one or more layers within a specific Git repository, and committing it to be executed by Ansible. Targeting a node, boot image, or group of nodes to apply the configuration. Creating a configuration session to apply and track the status of Ansible, applying each configuration layer to the targets specified in the session metadata.  Additionally, configuration management of specific components (nodes) can also be achieved by doing the following:\n Creating a configuration with one or more layers within a specific Git repository, and committing it to be executed by Ansible. Setting the desired configuration state of a node to the prescribed layers. Enabling the CFS Batcher to automatically configure nodes by creating one or more configuration sessions to apply the configuration layer(s).  "
},
{
	"uri": "/docs-csm/en-11/operations/configuration_management/configuration_management_of_system_components/",
	"title": "Configuration Management Of System Components",
	"tags": [],
	"description": "",
	"content": "Configuration Management of System Components The configuration of individual system components is managed with the cray cfs components command. The Configuration Framework Service (CFS) contains a database of the configuration state of available hardware known to the Hardware State Manager (HSM). When new nodes are added to the HSM database, a CFS Hardware Sync Agent enters the component into the CFS database with a null state of configuration.\nAdministrators are able to set a desired CFS configuration for each component, and the CFS Batcher ensures the desired configuration state and the current configuration state match.\nAutomatic Configuration Whenever CFS detects that the desired configuration does not match the current configuration state, CFS Batcher will automatically start a CFS session to apply the necessary configuration. See Configuration Management with CFS Batcher for more information.\nThere are several situations that will cause automatic configuration:\n When rebooted, components that have the cfs-state-reporter package installed will register a null current configuration, resulting in a full configuration. When a configuration is updated, all components with that desired configuration will automatically get updates for the layers of the configuration that have changed. If a configuration is only partially applied because of a previous failed configuration session and the component has not exceeded its maximum retries, it will be configured with any layers of the configurations that have not yet been successfully applied. When a user manually resets the configuration state of a component, it will force reconfiguration without rebooting a node. If a manual CFS session applies a version of a playbook that conflicts with the version in the desired configuration, CFS will re-apply the desired version after the manual session is completed. Any other situation that causes the desired state to not match with the current state of a component will trigger automatic configuration. CFS only tracks the current state of components as they are configured by CFS sessions. It does not track configuration state created or modified by other tooling on the system.  View Component Configuration Configuration status of a given component (using the xname) is available through the cray cfs components describe command. The following fields are provided to determine the status and state of the component:\n  configurationStatus\nThe status of the component\u0026rsquo;s configuration. Valid status values are unconfigured, failed, pending, and configured.\n  desiredConfig\nThe CFS configurations entry assigned to this component.\n  enabled\nIndicates whether the component will be configured by CFS or not.\n  errorCount\nThe number of times configuration sessions have failed to configure this component.\n  retryPolicy\nThe number of times the configuration will be attempted if it fails. If errorCount \u0026gt;= retryPolicy, CFS will not continue attempts to apply the desiredConfig.\n  state\nThe list of configuration layers that have been applied to the component from the desiredConfig.\n  To view the configuration state of a given component, use the describe command for a given xname:\nncn# cray cfs components describe XNAME --format json { \u0026#34;configurationStatus\u0026#34;: \u0026#34;configured\u0026#34;, \u0026#34;desiredConfig\u0026#34;: \u0026#34;configurations-example\u0026#34;, \u0026#34;enabled\u0026#34;: true, \u0026#34;errorCount\u0026#34;: 0, \u0026#34;id\u0026#34;: \u0026#34;x3000c0s13b0n0\u0026#34;, \u0026#34;retryPolicy\u0026#34;: 3, \u0026#34;state\u0026#34;: [ { \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/example.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;f6a2727a70fdd6d95df6ad9c883188e694d5b37f\u0026#34;, \u0026#34;lastUpdated\u0026#34;: \u0026#34;2021-07-28T03:26:00Z\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34;, \u0026#34;sessionName\u0026#34;: \u0026#34;batcher-6c95df62-2fe3-451b-8cc5-21d3cf748f83\u0026#34; }, { \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/another-example.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;282a9bfbf802d7b5c4d9bb5549b6e77957ec37f0\u0026#34;, \u0026#34;lastUpdated\u0026#34;: \u0026#34;2021-07-28T03:26:10Z\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;ncn.yml\u0026#34;, \u0026#34;sessionName\u0026#34;: \u0026#34;batcher-6c95df62-2fe3-451b-8cc5-21d3cf748f83\u0026#34; } ] } When a layer fails to configure, CFS will append a _failed status to the commit field. CFS Batcher will continue to attempt to configure this component with this configuration layer unless the errorCount has reached the retryPolicy limit.\n{ \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/another-example.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;282a9bfbf802d7b5c4d9bb5549b6e77957ec37f0_failed\u0026#34;, \u0026#34;lastUpdated\u0026#34;: \u0026#34;2021-07-28T03:26:20\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;ncn.yml\u0026#34;, \u0026#34;sessionName\u0026#34;: \u0026#34;batcher-74f83dad-9f90-4f5e-bf45-0498ffde8795\u0026#34; } In the event that a playbook is specified in the configuration that does not apply to the specific component, CFS will append _skipped to the commit field.\n{ \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/another-example.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;a8b132fa5ca04cbe1716501d7be38d9b34532a44_skipped\u0026#34;, \u0026#34;lastUpdated\u0026#34;: \u0026#34;2021-07-28T03:26:30\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34;, \u0026#34;sessionName\u0026#34;: \u0026#34;batcher-e3152e08-77df-4719-9b15-4fd5ad696730\u0026#34; } If a playbook exits early because of the Ansible any_errors_fatal setting, CFS will append _incomplete to the commit field for all components that did not cause the failure. This situation would most likely occur only when using an Ansible linear playbook execution strategy.\n{ \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/another-example.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;282a9bfbf802d7b5c4d9bb5549b6e77957ec37f0_incomplete\u0026#34;, \u0026#34;lastUpdated\u0026#34;: \u0026#34;2021-07-28T03:26:40\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34;, \u0026#34;sessionName\u0026#34;: \u0026#34;batcher-6c95df62-2fe3-451b-8cc5-21d3cf748f83\u0026#34; } Force Component Reconfiguration To force a component which has a specific desiredConfig to a different configuration, use the update subcommand to change the configuration:\n# cray cfs components update XNAME --desired-config new-config IMPORTANT: Ensure that the new configuration has been created with the cray cfs configurations update new-config command before assigning the configuration to any components.\nTo force a component to retry its configuration again after it failed, change the errorCount to less than the retryPolicy, or raise the retryPolicy. If the errorCount has not reached the retry limit, CFS will automatically keep attempting the configuration and no action is required.\n# cray cfs components update XNAME --error-count 0 Disable Component Configuration To disable CFS configuration of a component, use the --enabled option:\nWARNING: When a node reboots and the state-reporter reports in to CFS, it will automatically enable configuration. The following command only disables configuration until a node reboots.\n# cray cfs components update XNAME --enabled false Use --enabled true to re-enable the component.\n"
},
{
	"uri": "/docs-csm/en-11/operations/configuration_management/configuration_management_with_the_cfs_batcher/",
	"title": "Configuration Management With The CFS Batcher",
	"tags": [],
	"description": "",
	"content": "Configuration Management with the CFS Batcher Creating configuration sessions with the Configuration Framework Service (CFS) enables remote execution for configuring live nodes and boot images prior to booting. CFS also provides its Batcher component for configuration management of registered system components. The CFS Batcher periodically examines the aggregated configuration state of registered components and schedules CFS sessions against those that have not been configured to their desired state. The frequency of scheduling, the maximum number of components to schedule in the same CFS session, and the expiration time for scheduling less than full sessions are configurable.\nThe CFS-Batcher schedules CFS sessions according to the following rules:\n  Components are assigned to a batch if they need configuration, are not disabled, and are currently not assigned to a batch.\n Components are grouped according to their desired state information. A new batch is created if no partial batches match the desired state, and all similar batches are full.    Batches are scheduled as CFS sessions when the batch is full or the batch window time has been exceeded.\n The timer for the batch window is started when the first component is added, and is never reset. Nodes should never wait more than the window period between being ready for configuration and being scheduled in a CFS session.    CFS cannot guarantee that jobs for similar batches will start at the same time, even if all CFS sessions are created at the same time. This variability is due to the nature of Kubernetes scheduling.\n Checking the start time for the CFS session is more accurate than checking the pod start time when determining when a batch was scheduled.    There are two safety mechanisms built into the Batcher scheduling that can delay batches more than the usual amount of time. Both mechanisms are indicated in the logs:\n  CFS Batcher will not schedule multiple sessions to configure the same component. Batcher monitors on-going sessions that it started so that if one session is started and the desired configuration changes, Batcher can wait until the initial session is completed before scheduling the component with the new configuration to a new session.\n If Batcher is restarted, it will attempt to rebuild its state based on sessions with the \u0026ldquo;batcher-\u0026rdquo; naming scheme that are still in progress. This ensures that scheduling conflicts will not occur even if Batcher is restarted. On restart, some information on the in-flight sessions is lost, so this wait ensures that the Batcher does not schedule multiple configuration sessions for the same component at the same time.    If several CFS sessions that are created by the Batcher Agent fail in a row (the most recent 20 sessions), Batcher will start throttling the creation of new sessions.\n The throttling is automatically reset if a single session succeeds. Users can also manually reset this by restarting Batcher. The back-off is increased if new sessions continue to fail. This helps protect against cases where high numbers of retries are allowed so that Batcher cannot flood Kubernetes with new jobs in a short period of time.    Configure Batcher Several Batcher behaviors are configurable. All of the Batcher configuration is available through the CFS options:\n# cray cfs options list | grep -i batch batchSize = 25 batchWindow = 60 batcherCheckInterval = 10 defaultBatcherRetryPolicy = 3 See CFS Global Options for more information. Use the cray cfs options update command to change these values as needed.\nReview the following information about CFS Batcher options before changing the defaults. Setting these to non-optimal values may affect system performance. The optimal values will depend on system size and the specifics of the configuration layers that will be applied in the sessions created by CFS Batcher.\n  batchSize\nThis option determines the maximum number of components that will be included in each session created by CFS Batcher.\nThe default value is 25 components per session.\nWARNING: Increasing this value will result in fewer batcher-created sessions, but will also require more resources for Ansible Execution Environment (AEE) containers to do the configuration.\n  batchWindow\nThis option sets the number of seconds that CFS batcher will wait before scheduling a CFS session when the number of components needing configuration has not reached the batchSize limit. CFS Batcher will immediately create a session when the batchSize limit is reached. However, in the case where there are few components or long periods of time between components notifying CFS Batcher of the need for configuration, the batchWindow will time-box the creation of sessions so no component needs to wait for the queue to fill.\nThe default value is 60 seconds.\nWARNING: Lower values will cause CFS Batcher to be more responsive to creating sessions, but values too low may result in degraded performance of both the CFS APIs as well as the overall system.\n  batcherCheckInterval\nThis option sets how often CFS batcher checks for components waiting to be configured. This value must be lower than batchWindow value.\nThe default value is 10 seconds.\nWARNING: Lower values will cause CFS Batcher to be more responsive to creating sessions, but values too low may result in degraded performance of the CFS APIs on larger systems.\n  defaultBatcherRetryPolicy\nWhen a component (node) requiring configuration fails to configure from a previous configuration session launched by CFS Batcher, the error is logged. defaultBatcherRetryPolicy is the maximum number of failed configurations allowed per component before CFS Batcher will stop attempts to configure the component.\nThis value can be overridden on a per component basis.\n  List CFS Batcher Sessions The CFS Batcher prepends all CFS session names it creates with batcher-. Sessions that have be created by CFS Batcher are found by using the following command with the --name-contains option:\n# cray cfs sessions list --name-contains batcher- To list the batcher sessions that are currently running, filter with the cray cfs sessions list command options:\n# cray cfs sessions list --name-contains batcher- --status running Use the cray cfs sessions list --help command output for all filtering options, including session age, tags, status, and success.\nMap CFS Batcher Sessions to BOS Sessions To find all of the sessions created by the CFS Batcher because of configuration requests made by a specific Boot Orchestration Service (BOS) session, filter the sessions by the name of the BOS session, which is added as a tag on the sessions. The BOS session ID is required to run the following command.\n# cray cfs sessions list --tags bos_session=BOS_SESSION_ID "
},
{
	"uri": "/docs-csm/en-11/operations/configuration_management/ansible_inventory/",
	"title": "Ansible Inventory",
	"tags": [],
	"description": "",
	"content": "Ansible Inventory The Configuration Framework Service (CFS) provides several options for targeting nodes or boot images for configuration by Ansible. The contents of the Ansible inventory determine which nodes are available for configuration in each CFS session and how default configuration values can be customized.\nThe following are the inventory options provided by CFS:\n Dynamic inventory Static inventory Image Customization  Dynamic Inventory and Host Groups Dynamic inventory is the default inventory when creating a CFS session. CFS automatically generates an Ansible hosts file with data provided by the Hardware State Manager (HSM) when using a dynamic inventory. CFS automatically generates Ansible hosts groups for each group defined in HSM and creates Ansible host groups for nodes based on hardware roles and sub-roles.\nRetrieve a list of HSM groups with the following command:\nncn# cray hsm groups list --format json | jq .[].label \u0026#34;example-group1\u0026#34; \u0026#34;example-group2\u0026#34; These groups can be referenced in Ansible plays or when creating a CFS session directly.\nHardware roles and sub-roles are available as [Role] and [Role]_[Subrole] Ansible host groups. For instance, if targeting just the nodes with the \u0026ldquo;Application\u0026rdquo; role, the host group name is Application. If targeting just the sub-role \u0026ldquo;UAN\u0026rdquo;, which is a sub-role of the \u0026ldquo;Application\u0026rdquo; role, the host group name provided by CFS is Application_UAN. See HSM Roles and Subroles for more information.\nConsult the cray-hms-base-config Kubernetes ConfigMap in the services namespace for a listing of the available roles and sub-roles on the system.\nDuring a CFS session, the dynamic inventory is generated and placed in the hosts/01-cfs-generated.yaml file, relative to the configuration management repository root defined in the current configuration layer. Refer to the external Ansible Inventory documentation for more information about managing inventory, as well as variable precedence within multiple inventory files.\nCFS prefixes its dynamic inventory file with 01- so that its variables can be easily overridden as needed because Ansible reads inventory files in lexicographic order.\nStatic Inventory Even though CFS uses dynamic inventory and the HSM groups, roles, and sub-roles to enable specific targeting of groups of nodes, it is also possible to target nodes within the system using a static inventory file. To do so, each node must be listed in an inventory file itself. This approach is useful for testing configuration changes on a small scale.\nCreate a static inventory file in a hosts directory at the root of the configuration management repository in Ansible INI format. For example, create it in a branch and persist the changes.\nIn the following example, this is done for a single node in static inventory:\nncn# mkdir -p hosts; cd hosts; cat \u0026gt; static \u0026lt;\u0026lt;EOF [test_nodes] x3000c0s25b0n0 EOF ncn# cd ..; git add hosts/static ncn# git commit -m \u0026#34;Added a single test node to static inventory\u0026#34; ncn# git push The process can be used to include any nodes in the system reachable over the Node Management Network (NMN), which contains the public SSH key pair provisioned by the install process. This inventory information will only be located in the repository to which it is added. If the desired configuration contains multiple layers, use the additionalInventoryUrl option in CFS to provide inventory information on a per-session level instead of a per-repository level. Refer to Manage Multiple Inventories in a Single Location for more information.\nImage Customization CFS handles inventory for image customization differently because this type of configuration session does not target live nodes. When creating a configuration session meant to customize a boot image, the Image Management Service (IMS) image IDs are used as hosts and grouped according to input to the session creation.\nSee Create an Image Customization CFS Session for more information.\n"
},
{
	"uri": "/docs-csm/en-11/operations/configuration_management/automatic_session_deletion_with_sessionttl/",
	"title": "Automatic Session Deletion With `sessionttl`",
	"tags": [],
	"description": "",
	"content": "Automatic Session Deletion with sessionTTL By default, when a session completes, the Configuration Framework Service (CFS) will delete sessions and Kubernetes jobs associated with the session when the start date was more than seven days prior. This is done to ensure CFS sessions do not accumulate and eventually adversely affect the performance of the Kubernetes cluster.\nFor larger systems or systems that do frequent reboots of nodes that are configured with CFS sessions, this setting may need to be reduced.\nUpdate the sessionTTL using the following command:\nncn# cray cfs options update --session-ttl 24h [ ... output removed ...] sessionTTL = \u0026#34;24h\u0026#34; To disable the sessionTTL feature, use \u0026ndash;session-ttl \u0026quot; in the command above.\nImportant: The sessionTTL option deletes all completed sessions that meet the TTL criteria, regardless of if they were successful.\n"
},
{
	"uri": "/docs-csm/en-11/operations/configuration_management/cfs_global_options/",
	"title": "CFS Global Options",
	"tags": [],
	"description": "",
	"content": "CFS Global Options The Configuration Framework Service (CFS) provides a global service options endpoint for modifying the base configuration of the service itself.\nView the options with the following command:\nncn# cray cfs options list --format json { \u0026#34;additionalInventoryUrl\u0026#34;: \u0026#34;, \u0026#34;batchSize\u0026#34;: 25, \u0026#34;batchWindow\u0026#34;: 60, \u0026#34;batcherCheckInterval\u0026#34;: 10, \u0026#34;defaultAnsibleConfig\u0026#34;: \u0026#34;cfs-default-ansible-cfg\u0026#34;, \u0026#34;defaultBatcherRetryPolicy\u0026#34;: 1, \u0026#34;defaultPlaybook\u0026#34;: \u0026#34;site.yml\u0026#34;, \u0026#34;hardwareSyncInterval\u0026#34;: 10, \u0026#34;sessionTTL\u0026#34;: \u0026#34;7d\u0026#34; } The following are the CFS global options:\n  additionalInventoryUrl\nA Git clone URL to supply additional inventory content to all CFS sessions.\nSee Manage Multiple Inventories in a Single Location for more information.\n  batchSize\nThis option determines the maximum number of components that will be included in each session created by CFS Batcher.\nSee Configuration Management with the CFS Batcher for more information.\n  batchWindow\nThis option sets the number of seconds that CFS batcher will wait before scheduling a CFS session when the number of components needing configuration has not reached the batchSize limit.\nSee Configuration Management with the CFS Batcher for more information.\n  batcherCheckInterval\nThis option sets how often CFS batcher checks for components waiting to be configured. This value must be lower than batchWindow.\nSee Configuration Management with the CFS Batcher for more information.\n  defaultBatcherRetryPolicy\nWhen a component (node) requiring configuration fails to configure from a previous configuration session launched by CFS Batcher, the error is logged. defaultBatcherRetryPolicy is the maximum number of failed configurations allowed per component before CFS Batcher will stop attempts to configure the component.\nSee Configuration Management with the CFS Batcher for more information.\n  defaultAnsibleConfig\nSee Set the ansible.cfg for a Session for more information.\n  defaultPlaybook\nUse this value when no playbook is specified in a configuration layer.\n  hardwareSyncInterval\nThe number of seconds between checks to the Hardware State Manager (HSM) for new hardware additions to the system. When new hardware is registered with HSM, CFS will add it as a component.\nSee Configuration Management of System Components for more information.\n  The default values for all CFS global options can be modified with the cray cfs options update command.\n"
},
{
	"uri": "/docs-csm/en-11/operations/configuration_management/change_the_ansible_verbosity_logs/",
	"title": "Change The Ansible VerBOSity Logs",
	"tags": [],
	"description": "",
	"content": "Change the Ansible Verbosity Logs It is useful to view the Ansible logs in a Configuration Framework Session (CFS) session with greater verbosity than the default. CFS sessions are able to set the Ansible verbosity from the command line when the session is created. The verbosity will apply to all configuration layers in the session.\nSpecify an integer using the \u0026ndash;ansible-verbosity option, where 1 = -v, 2 = -vv, and so on. Valid values range from 0 (default) to 4. See the ansible-playbook help for more information.\nIt is not recommended to use level 3 or 4 with sessions that target large numbers of hosts. When using \u0026ndash;ansible-verbosity to debug Ansible plays or roles, consider also limiting the session targets with \u0026ndash;ansible-limit to reduce log output.\nWarning: Setting the \u0026ndash;ansible-verbosity to 4 can cause CFS sessions to hang for unknown reasons. To correct this issue, reduce the verbosity to 3 or lower, or adjust the usage of the display_ok_hosts and display_skipped_hosts settings in the ansible.cfg file the session is using. Consider also reviewing the Ansible tasks being run and reducing the amount log output from these individual tasks.\n"
},
{
	"uri": "/docs-csm/en-11/operations/configuration_management/ansible_execution_environments/",
	"title": "Ansible Execution Environments",
	"tags": [],
	"description": "",
	"content": "Ansible Execution Environments Configuration Framework Service (CFS) sessions are comprised of a single Kubernetes pod with several containers. Inventory and Git clone setup containers run first, and a teardown container runs last (if the session is running an image customization).\nThe containers that run the Ansible code cloned from the Git repositories in the configuration layers are Ansible Execution Environments (AEE). The AEE is provided as a SLES-based docker image, which includes Ansible version 2.9.11 installed using Python 3.6. In addition to the base Ansible installation, CFS also includes several Ansible modules and plug-ins that are required for CFS and Ansible to work properly on the system.\nThe following modules and plug-ins are available:\n  cfs_aggregator.py Callback Plug-in\nThis callback plug-in is included to relay playbook execution results back to CFS for the purpose of tracking session status and component state.\nWarning: This plug-in is required for CFS to function properly and must not be removed from the ansible.cfg file.\n  cfs_linear and cfs_free Strategy Plug-ins\nCFS provides two strategy plug-ins, cfs_linear and cfs_free, which should be used in place of the stock Ansible free and linear playbook execution strategies.\nFor more information about Ansible strategies, see the external Ansible playbook strategies documentation.\n  shasta_s3_cred.py Module\nThis module is provided to obtain access to S3 credentials stored in Kubernetes secrets in the cluster, specifically secrets with names such as \u0026lt;service\u0026gt;-s3-credentials.\nAn example of using this module is as follows:\n- name: Retrieve credentials from abc-s3-credentials k8s secret shasta_s3_creds: k8s_secret: abc-s3-credentials k8s_namespace: services register: creds no_log: true The access key is available at \u0026ldquo;{{ creds.access_key }}\u0026rdquo; and the secret key is at \u0026ldquo;{{ creds.secret_key }}\u0026rdquo;.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/compute_rolling_upgrades/troubleshoot_a_failed_crus_session_due_to_unmet_conditions/",
	"title": "Troubleshoot A Failed CRUS Session Because Of Unmet Conditions",
	"tags": [],
	"description": "",
	"content": "Troubleshoot a Failed CRUS Session Because of Unmet Conditions If a CRUS session has any unmet conditions, adding or fixing them will cause the session to continue from wherever it got stuck. Updating other parts of the system to meet the required conditions of a CRUS session will unblock the upgrade session.\nThe following are examples of unmet conditions:\n Undefined groups in the Hardware State Manager (HSM). No predefined Boot Orchestration Service (BOS) session template exists that describes the desired states of the nodes being upgraded.  Prerequisites  A Compute Rolling Upgrade Service (CRUS) session was run and failed to complete. The Cray command line interface (CLI) tool is initialized and configured on the system.  Procedure   View the details for the CRUS session that failed.\nncn# cray crus session describe CRUS_UPGRADE_ID api_version = \u0026#34;1.0.0\u0026#34; completed = false failed_label = \u0026#34;failed-node-group\u0026#34; kind = \u0026#34;ComputeUpgradeSession\u0026#34; messages = [ \u0026#34;Processing step 0 in stage STARTING failed - failed to obtain Node Group named \u0026#39;failed-node-group\u0026#39; - {\u0026#34;type\u0026#34;:\u0026#34;about:blank\u0026#34;,\u0026#34;title\u0026#34;:\u0026#34;Not Found\u0026#34;,\u0026#34;detail\u0026#34;:\u0026#34;No such group: failed-node-group\u0026#34;,\u0026#34;status\u0026#34;:404}\\n[404]\u0026#34;,] starting_label = \u0026#34;slurm-nodes\u0026#34; state = \u0026#34;UPDATING\u0026#34; upgrade_id = \u0026#34;2c7fdce6-0047-4421-9676-4301d411d14e\u0026#34; upgrade_step_size = 50 upgrade_template_id = \u0026#34;dummy-boot-template\u0026#34; upgrading_label = \u0026#34;dummy-node-group\u0026#34; workload_manager_type = \u0026#34;slurm\u0026#34; The messages value returned in the output will provide details explaining where the job failed. In this example, there is a note stating the failed node group could not be obtained. This implies that the user forgot to create the failed node group before starting the job.\n  Create a new node group for the missing group.\nFollowing the example in the previous step, the failed node group needs to be created.\nncn# cray hsm groups create --label failed-node-group [[results]] URI = \u0026#34;/hsm/v2/groups/failed-node-group\u0026#34;   View the details for the CRUS session again to see if the job started.\nncn-w001# cray crus session describe CRUS_UPGRADE_ID api_version = \u0026#34;1.0.0\u0026#34; completed = false failed_label = \u0026#34;failed-node-group\u0026#34; kind = \u0026#34;ComputeUpgradeSession\u0026#34; messages = [ \u0026#34;Processing step 0 in stage STARTING failed - failed to obtain Node Group named \u0026#39;failed-node-group\u0026#39; - {\u0026#34;type\u0026#34;:\u0026#34;about:blank\u0026#34;,\u0026#34;title\u0026#34;:\u0026#34;Not Found\u0026#34;,\u0026#34;detail\u0026#34;:\u0026#34;No such group: failed-node-group\u0026#34;,\u0026#34;status\u0026#34;:404}\\n[404]\u0026#34;, \u0026#34;Quiesce requested in step 0: moving to QUIESCING\u0026#34;, \u0026#34;All nodes quiesced in step 0: moving to QUIESCED\u0026#34;, \u0026#34;Began the boot session for step 0: moving to BOOTING\u0026#34;,] starting_label = \u0026#34;slurm-nodes\u0026#34; state = \u0026#34;UPDATING\u0026#34; upgrade_id = \u0026#34;2c7fdce6-0047-4421-9676-4301d411d14e\u0026#34; upgrade_step_size = 50 upgrade_template_id = \u0026#34;dummy-boot-template\u0026#34; upgrading_label = \u0026#34;dummy-node-group\u0026#34; workload_manager_type = \u0026#34;slurm\u0026#34; The messages value states that the job has resumed now that the error has been fixed.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/compute_rolling_upgrades/upgrade_compute_nodes_with_crus/",
	"title": "Upgrade Compute Nodes With CRUS",
	"tags": [],
	"description": "",
	"content": "Upgrade Compute Nodes with CRUS Upgrade a set of compute nodes with the Compute Rolling Upgrade Service (CRUS). Manage the workload management status of nodes and quiesce each node before taking the node out of service and upgrading it. Then reboot it into the upgraded state and return it to service within the workload manager (WLM).\nPrerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system. A Boot Orchestration Service (BOS) session template describing the desired states of the nodes being upgraded must exist.  Procedure   Create and populate the starting node group.\nThis will be the group of nodes that will be upgraded.\n  Create a starting node group (starting label).\nLabel names are defined by the user and the names used in this procedure are only examples. The label name used in this example is slurm-nodes.\nncn# cray hsm groups create --label slurm-nodes \\ --description \u0026#39;Starting Node Group for my Compute Node upgrade\u0026#39;   Add members to the group.\nAdd compute nodes to the group by using the xname for each node being added. The following is an example:\nncn# cray hsm groups members create slurm-nodes --id XNAME [[results]] URI = \u0026#34;/hsm/v2/groups/slurm-nodes/members/x0c0s28b0n0\u0026#34;     Create a group for upgrading nodes (upgrading label).\nThe label name used in this example is upgrading-nodes.\nncn-w001# cray hsm groups create --label upgrading-nodes \\ --description \u0026#39;Upgrading Node Group for my Compute Node upgrade\u0026#39; There is no need to add members to this group because it should be empty when the compute rolling upgrade process begins.\n  Create a group for failed nodes (failed label).\nThe label name used in this example is failed-nodes.\nncn# cray hsm groups create --label failed-nodes \\ --description \u0026#39;Failed Node Group for my Compute Node upgrade\u0026#39; There is no need to add members to this group because it should be empty when the compute rolling upgrade process begins.\n  Create an upgrade session with CRUS.\nThe following example is upgrading 50 nodes at a step. The \u0026ndash;upgrade-template-id value should be the name of the Boot Orchestration Service (BOS) session template being used.\nncn# cray crus session create \\ --starting-label slurm-nodes \\ --upgrading-label upgrading-nodes \\ --failed-label failed-nodes \\ --upgrade-step-size 50 \\ --workload-manager-type slurm \\ --upgrade-template-id=BOS_SESSION_TEMPLATE_NAME api_version = \u0026#34;1.0.0\u0026#34; completed = false failed_label = \u0026#34;failed-nodes\u0026#34; kind = \u0026#34;ComputeUpgradeSession\u0026#34; messages = [] starting_label = \u0026#34;slurm-nodes\u0026#34; state = \u0026#34;UPDATING\u0026#34; upgrade_id = \u0026#34;e0131663-dbee-47c2-aa5c-13fe9b110242\u0026#34; upgrade_step_size = 50 upgrade_template_id = \u0026#34;boot-template\u0026#34; upgrading_label = \u0026#34;upgrading-nodes\u0026#34; workload_manager_type = \u0026#34;slurm\u0026#34; If successful, note the upgrade_id in the returned data.\nncn-w001# export UPGRADE_ID=e0131663-dbee-47c2-aa5c-13fe9b110242   Monitor the status of the upgrade session.\nThe progress of the session through the upgrade process is described in the messages field of the session. This is a list of messages, in chronological order, containing information about stage transitions, step transitions, and other conditions of interest encountered by the session as it progresses. It is cleared once the session completes.\nncn-w001# cray crus session describe $UPGRADE_ID api_version = \u0026#34;1.0.0\u0026#34; completed = false failed_label = \u0026#34;failed-nodes\u0026#34; kind = \u0026#34;ComputeUpgradeSession\u0026#34; messages = [ \u0026#34;Quiesce requested in step 0: moving to QUIESCING\u0026#34;, \u0026#34;All nodes quiesced in step 0: moving to QUIESCED\u0026#34;, \u0026#34;Began the boot session for step 0: moving to BOOTING\u0026#34;,] starting_label = \u0026#34;slurm-nodes\u0026#34; state = \u0026#34;UPDATING\u0026#34; upgrade_id = \u0026#34;e0131663-dbee-47c2-aa5c-13fe9b110242\u0026#34; upgrade_step_size = 50 upgrade_template_id = \u0026#34;boot-template\u0026#34; upgrading_label = \u0026#34;upgrading-nodes\u0026#34; workload_manager_type = \u0026#34;slurm\u0026#34; A CRUS session goes through a number of steps (approximately the number of nodes to be upgraded divided by the requested step size) to complete an upgrade. Each step moves through the following stages, unless the boot session is interrupted by being deleted.\n Starting - Preparation for the step and CRUS initiates WLM quiescing of nodes. Quiescing - Waits for all WLM nodes in the step to reach a quiesced (not busy) state. Quiesced - The nodes in the step are all quiesced and CRUS initiates booting the nodes into the upgraded environment. Booting - Waits for the boot session to complete. Booted - The boot session has completed. Check the success or failure of the boot session. Mark all nodes in the step as failed if the boot session failed. WLM Waiting - The boot session succeeded. Wait for nodes to reach a ready state in the WLM. All nodes in the step that fail to reach a ready state within 10 minutes of entering this stage are marked as failed. Cleanup - The upgrade step has finished. Clean up resources to prepare for the next step. When a step moves from one stage to the next, CRUS adds a message to the messages field of the upgrade session to mark the progress.    Optional: Delete the CRUS upgrade session.\nOnce a CRUS upgrade session it is complete, it can no longer be used. It can be kept for historical purposes if desired, or it can be deleted.\nncn-w001# cray crus session delete $UPGRADE_ID api_version = \u0026#34;1.0.0\u0026#34; completed = true failed_label = \u0026#34;failed-nodes\u0026#34; kind = \u0026#34;ComputeUpgradeSession\u0026#34; messages = [ \u0026#34;Upgrade Session Completed\u0026#34;,] starting_label = \u0026#34;slurm-nodes\u0026#34; state = \u0026#34;DELETING\u0026#34; upgrade_id = \u0026#34;e0131663-dbee-47c2-aa5c-13fe9b110242\u0026#34; upgrade_step_size = 50 upgrade_template_id = \u0026#34;boot-template\u0026#34; upgrading_label = \u0026#34;upgrading-nodes\u0026#34; workload_manager_type = \u0026#34;slurm\u0026#34; The session may be visible briefly after it is deleted. This allows for orderly cleanup of the session.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/compute_rolling_upgrades/compute_rolling_upgrades/",
	"title": "Compute Rolling Upgrades",
	"tags": [],
	"description": "",
	"content": "Compute Rolling Upgrades The Compute Rolling Upgrade Service (CRUS) upgrades sets of compute nodes without requiring an entire set of nodes to be out of service at once. CRUS manages the workload management status of nodes, handling each of the following steps required to upgrade compute nodes:\n Quiesce each node before taking the node out of service. Upgrade the node. Reboot the node into the upgraded state. Return the node to service within its respective workload manager.  CRUS enables administrators to limit the impact on production caused from upgrading compute nodes by working through one step of the upgrade process at a time. The nodes in each step are taken out of service in the workload manager to prevent work from being scheduled, upgraded, rebooted, and put back into service in the workload manager.\nCRUS is built upon a few basic features of the system:\n The grouping of nodes by label provided by the Hardware State Manager (HSM) groups mechanism. Workload management that can gracefully take nodes out of service (quiesce nodes), declare nodes as failed, and return nodes to service. The Boot Orchestration Service (BOS) and boot session templates.  "
},
{
	"uri": "/docs-csm/en-11/operations/compute_rolling_upgrades/crus_workflow/",
	"title": "CRUS Workflow",
	"tags": [],
	"description": "",
	"content": "CRUS Workflow The following workflow is intended to be a high-level overview of how to upgrade compute nodes. This workflow depicts how services interact with each other during the compute node upgrade process, and helps to provide a quicker and deeper understanding of how the system functions.\nUpgrade Compute Nodes Use Cases: Administrator upgrades select compute nodes (around 500) to a newer compute image by using Compute Rolling Upgrade Service (CRUS).\nRequirement The compute nodes are up and running and their workloads are managed by Slurm.\nComponents: This workflow is based on the interaction of CRUS with Boot Orchestration Service (BOS) and Slurm (Workload Manager).\nMentioned in this workflow:\n Compute Rolling Upgrade Service (CRUS) allows an administrator to modify the boot image and/or configuration on a set of compute nodes without the need to take the entire set of nodes out of service at once. It manages the workload management status of nodes, quiescing each node before taking the node out of service, upgrading the node, rebooting the node into the upgraded state and then returning the node to service within the workload manager. Boot Orchestration Service (BOS) is responsible for booting, configuring, and shutting down collections of nodes. The Boot Orchestration Service has the following components:  Boot Orchestration Session Template is a collection of one or more boot set objects. A boot set defines a collection of nodes and the information about the boot artifacts and parameters. Boot Orchestration Session carries out an operation. The possible operations in a session are boot, shutdown, reboot, and configure. Boot Orchestration Agent (BOA) is automatically launched to execute the session. A BOA executes the given operation, and if the operation is a boot or a reboot, it also configures the nodes post-boot (if configure is enabled).   Slurm has a component called slurmctld that runs on a non-compute node in a container. The Slurm control daemon or slurmctld is the central management daemon of Slurm. It monitors all other Slurm daemons and resources, accepts jobs, and allocates resources to those jobs. Slurm also has a component called slurmd that runs on all compute nodes. The Slurm daemon or slurmd monitors all tasks running on the compute node, accepts tasks, launches tasks, and kills running tasks upon request.  Workflow Overview: The following sequence of steps occur during this workflow.\n  Administrator creates HSM groups and populates the starting group\nCreate three HSM groups with starting, failed, and upgrading labels.\nFor example: crusfailed, crusupgrading, and crus_starting.\nAdd all of the 500 compute nodes to be updated to the crus_starting group. Leave the failed and upgrading groups empty.\n  Administrator creates a session template\nCreate a BOS session template which points to the new image, the desired CFS configuration, and with a boot set which includes all the compute nodes to be updated. The boot set can include additional nodes, but it must contain all the nodes that need to be updated. The BOS session template you use should specify \u0026ldquo;upgrading_label\u0026rdquo; in the \u0026ldquo;node_groups\u0026rdquo; field of one of its boot sets.\nFor example: newcomputetemplate.\n  Administrator creates a CRUS session\nA new upgrade session is launched as a result of this call.\nSpecify the following parameters:\n  failed_label: An empty Hardware State Manager (HSM) group which CRUS will populate with any nodes that fail their upgrades.\n  starting_label: An HSM group which contains the total set of nodes to be upgraded. Example: 500.\n  upgrading_label: An empty HSM group which CRUS will use to boot and configure the discrete sets of nodes.\n  upgradestepsize: The number of nodes to include in each discrete upgrade step.\nThe upgrade steps will never exceed this quantity, although in some cases they may be smaller. Example: 50\n  upgradetemplateid: The name of the BOS session template to use for the upgrades. A session template is a collection of metadata for a group of nodes and their desired configuration.\n  workloadmanagertype: Currently only slurm is supported.\n    CRUS to HSM\nCRUS calls HSM to find the nodes in the starting_label group.\n  CRUS to HSM\nIt then takes a number of these nodes equal to the step size (50), and calls HSM to put them into the upgrading_label group.\n  CRUS to Slurm\nCRUS tells Slurm to quiesce these nodes. As each node is quiesced, Slurm puts the node offline.\n  Slurm to CRUS\nSlurm reports back to CRUS that all of the nodes as offline.\n  CRUS to BOS\nCRUS calls BOS to create a session with the following arguments:\n operation: reboot templateUuid: upgradetemplateid limit: upgrading_label    CRUS retrieves the BOA job details from BOS\nCRUS retrieves the BOS session to get the BOA job name. CRUS waits for the BOA job to finish.\nCRUS looks at the exit code of the BOA job to determine whether or not there were errors.\nIf there were errors, CRUS adds the nodes from the upgrading_label group into the failed_label group.\n  CRUS to HSM\nAfter the BOA job is complete, CRUS calls HSM to empty the upgrading_label group.\n  CRUS repeats steps for remaining nodes, then updates status\nCRUS repeats steps 5-10 until all of the nodes from the starting_label group have gone through these steps. After this, CRUS marks the session status as \u0026ldquo;complete\u0026rdquo;.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/compute_rolling_upgrades/troubleshoot_a_failed_crus_session_due_to_bad_parameters/",
	"title": "Troubleshoot A Failed CRUS Session Because Of Bad Parameters",
	"tags": [],
	"description": "",
	"content": "Troubleshoot a Failed CRUS Session Because of Bad Parameters A CRUS session must be deleted and recreated if it does not start or complete because of parameters having incorrect values.\nThe following are examples of incorrect parameters:\n Choosing the wrong Boot Orchestration Service (BOS) session template. Choosing the wrong group labels. Improperly defined BOS session template. For example, specifying nodes in the template instead of using the label of the upgrading group.  Prerequisites  A Compute Rolling Upgrade Service (CRUS) session was run and failed to complete. The Cray command line interface (CLI) tool is initialized and configured on the system.  Procedure   Delete the failed session.\nDeleting a CRUS session that is in progress will terminate the session and move all of the unfinished nodes into the group said up for failed nodes. The time frame for recognizing a delete request, cleaning up, and deleting the session is roughly a minute. A session being deleted will move to a DELETING status immediately upon receiving a delete request, which will prevent further processing of the upgrade in that session.\nncn# cray crus session delete CRUS_UPGRADE_ID api_version = \u0026#34;1.0.0\u0026#34; completed = false failed_label = \u0026#34;failed-node-group\u0026#34; kind = \u0026#34;ComputeUpgradeSession\u0026#34; messages = [ \u0026#34;Processing step 0 in stage STARTING failed - failed to obtain Node Group named \u0026#39;slurm-node-group\u0026#39; - {\u0026#34;type\u0026#34;:\u0026#34;about:blank\u0026#34;,\u0026#34;title\u0026#34;:\u0026#34;Not Found\u0026#34;,\u0026#34;detail\u0026#34;:\u0026#34;No such group: slurm-node-group\u0026#34;,\u0026#34;status\u0026#34;:404}\\n[404]\u0026#34;,] starting_label = \u0026#34;slurm-node-group\u0026#34; state = \u0026#34;DELETING\u0026#34; upgrade_id = \u0026#34;d388c6f5-be67-4a31-87a9-819bb4fa804c\u0026#34; upgrade_step_size = 50 upgrade_template_id = \u0026#34;boot-template\u0026#34; upgrading_label = \u0026#34;node-group\u0026#34; workload_manager_type = \u0026#34;slurm\u0026#34;   Recreate the session that failed.\nEnsure the correct parameters are used when restarting the session.\nncn# cray crus session create \\ --starting-label slurm-nodes \\ --upgrading-label node-group \\ --failed-label failed-node-group \\ --upgrade-step-size 50 \\ --workload-manager-type slurm \\ --upgrade-template-id boot-template api_version = \u0026#34;1.0.0\u0026#34; completed = false failed_label = \u0026#34;failed-node-group\u0026#34; kind = \u0026#34;ComputeUpgradeSession\u0026#34; messages = [] starting_label = \u0026#34;slurm-nodes\u0026#34; state = \u0026#34;UPDATING\u0026#34; upgrade_id = \u0026#34;135f9667-6d33-45d4-87c8-9b09c203174e\u0026#34; upgrade_step_size = 50 upgrade_template_id = \u0026#34;boot-template\u0026#34; upgrading_label = \u0026#34;node-group\u0026#34; workload_manager_type = \u0026#34;slurm\u0026#34;   "
},
{
	"uri": "/docs-csm/en-11/operations/compute_rolling_upgrades/troubleshoot_nodes_failing_to_upgrade_in_a_crus_session/",
	"title": "Troubleshoot Nodes Failing To Upgrade In A CRUS Session",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Nodes Failing to Upgrade in a CRUS Session Troubleshoot compute nodes failing to upgrade during a Compute Rolling Upgrade Service (CRUS) session and rerun the session on the failed nodes.\nWhen a nodes are marked as failed they are added to the failed node group associated with the upgrade session, and the nodes are marked as down in the workload manager (WLM). If the WLM supports some kind of reason string, that string contains the cause of the down status.\nComplete a CRUS session that did not successfully upgrade all of the intended compute nodes.\nPrerequisites  A CRUS upgrade session has completed with a group of nodes that failed to upgrade. The Cray command line interface (CLI) tool is initialized and configured on the system.  Procedure   Determine which nodes failed the upgrade by listing the contents of the Hardware State Manager (HSM) group that was set up for failed nodes.\nncn# cray hsm groups describe FAILED_NODES_GROUP label = \u0026#34;failed-nodes\u0026#34; description = \u0026#34; [members] ids = [ \u0026#34;x0c0s28b0n0\u0026#34;,]   Determine the cause of the failed nodes and fix it.\nFailed nodes result from the following:\n Failure of the BOS upgrade session for a given step of the upgrade causes all of the nodes in that step to be marked as failed. Failure of any given node in a step to reach a ready state in the workload manager within 10 minutes of detecting that the BOS boot session has completed causes that node to be marked as failed. Deletion of a CRUS session while the current step is at or beyond the \u0026lsquo;Booting\u0026rsquo; stage causes all of the nodes in that step that have not reached a ready state in the workload manager to be marked as failed.    Create a new CRUS session on the failed nodes.\n  Create a new failed node group with a different name.\nThis group should be empty.\nncn# cray hsm groups create --label NEW_FAILED_NODES_GROUP \\ --description \u0026#39;Failed Node Group for my Compute Node upgrade\u0026#39;   Create a new CRUS session.\nUse the label of the failed node group from the original upgrade session as the starting label, and use the new failed node group as the failed label. The rest of the parameters need to be the same ones that were used in the original upgrade.\nncn# cray crus session create \\ --starting-label OLD_FAILED_NODES_GROUP \\ --upgrading-label node-group \\ --failed-label NEW_FAILED_NODES_GROUP \\ --upgrade-step-size 50 \\ --workload-manager-type slurm \\ --upgrade-template-id boot-template api_version = \u0026#34;1.0.0\u0026#34; completed = false failed_label = \u0026#34;NEW_FAILED_NODES_GROUP\u0026#34; kind = \u0026#34;ComputeUpgradeSession\u0026#34; messages = [] starting_label = \u0026#34;OLD_FAILED_NODES_GROUP\u0026#34; state = \u0026#34;UPDATING\u0026#34; upgrade_id = \u0026#34;135f9667-6d33-45d4-87c8-9b09c203174e\u0026#34; upgrade_step_size = 50 upgrade_template_id = \u0026#34;boot-template\u0026#34; upgrading_label = \u0026#34;node-group\u0026#34; workload_manager_type = \u0026#34;slurm\u0026#34;     "
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/troubleshoot_uan_boot_issues/",
	"title": "Troubleshoot UAN Boot Issues",
	"tags": [],
	"description": "",
	"content": "Troubleshoot UAN Boot Issues Use this topic to guide troubleshooting of UAN boot issues.\nThe UAN boot process BOS boots UANs. BOS uses session templates to define various parameters such as:\n Which nodes to boot Which image to boot Kernel parameters Whether to perform post-boot configuration (Node Personalization) of the nodes by CFS. Which CFS configuration to use if Node Personalization is enabled.  UAN boots are performed in three phases:\n PXE booting an iPXE binary that will load the initrd of the wanted UAN image to boot. Booting the initrd (dracut) image which configures the UAN for booting the UAN image. This process consists of two phases.  Configuring the UAN node to use the Content Projection Service (CPS) and Data Virtualization Service (DVS). These services manage the UAN image rootfs mounting and make that image available to the UAN nodes. Mounting the rootfs   Booting the UAN image rootfs.  PXE Issues Most failures to PXE are the result of misconfigured network switches and/or BIOS settings. The UAN must PXE boot over the Node Management Network (NMN) and the switches must be configured to allow connectivity to the NMN. The cable for the NMN must be connected to the first port of the OCP card on HPE DL325 and DL385 servers or to the first port of the built-in LAN-On-Motherboard (LOM) on Gigabyte servers.\nInitrd (Dracut) Issues Dracut failures are often caused by the wrong interface being named nmn0, or to multiple entries for the UAN xname in DNS. The latter is a result of multiple interfaces making DHCP requests. Either condition can cause IP address mismatches in the dvs_node_map. DNS configures entries based on DHCP leases.\nWhen dracut starts, it renames the network device named by the ifmap=netX:nmn0 kernel parameter to nmn0. This interface is the only one dracut will enable DHCP on. The ip=nmn0:dhcp kernel parameter limits dracut to DHCP only nmn0. The ifmap value must be set correctly in the kernel_parameters field of the BOS session template.\nFor UAN nodes that have more than one PCI card installed, ifmap=net2:nmn0 is the correct setting. If only one PCI card is installed, ifmap=net0:nmn0 is normally the correct setting.\nUANs require CPS and DVS to boot from images. These services are configured in dracut to retrieve the rootfs and mount it. If the image fails to download, check that DVS and CPS are both healthy, and DVS is running on all worker nodes. Run the following commands to check DVS and CPS:\nncn-m001# kubectl get nodes -l cps-pm-node=True -o custom-columns=\u0026#34;:metadata.name\u0026#34; --no-headers ncn-w001 ncn-w002 ncn-m001# for node in `kubectl get nodes -l cps-pm-node=True -o custom-columns=\u0026#34;:metadata.name\u0026#34; --no-headers`; do ssh $node \u0026#34;lsmod | grep \u0026#39;^dvs \u0026#39;\u0026#34; done ncn-w001 ncn-w002 If DVS and CPS are both healthy, then both of these commands will return all the worker NCNs in the HPE Cray EX system.\nImage Boot Issues Once dracut exits, the UAN will boot the rootfs image. Failures seen in this phase tend to be failures of spire-agent, cfs-state-reporter, or both, to start. The cfs-state-reporter tells BOA that the node is ready and allows BOA to start CFS for Node Personalization. If cfs-state-reporter does not start, check if the spire-agent has started. The cfs-state-reporter depends on the spire-agent. Running systemctl status spire-agent will show that that service is enabled and running if there are no issues with that service. Similarly, running systemctl status cfs-state-reporter will show a status of SUCCESS.\nuan# systemctl status spire-agent ● spire-agent.service - SPIRE Agent Loaded: loaded (/usr/lib/systemd/system/spire-agent.service; enabled; vendor preset: enabled) Active: active (running) since Wed 2021-02-24 14:27:33 CST; 19h ago Main PID: 3581 (spire-agent) Tasks: 57 CGroup: /system.slice/spire-agent.service └─3581 /usr/bin/spire-agent run -expandEnv -config /root/spire/conf/spire-agent.conf uan# systemctl status cfs-state-reporter ● cfs-state-reporter.service - cfs-state-reporter reports configuration level of the system Loaded: loaded (/usr/lib/systemd/system/cfs-state-reporter.service; enabled; vendor preset: enabled) Active: inactive (dead) since Wed 2021-02-24 14:29:51 CST; 19h ago Main PID: 3827 (code=exited, status=0/SUCCESS) "
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/upload_node_boot_information_to_boot_script_service_bss/",
	"title": "Upload Node Boot Information To Boot Script Service (bss)",
	"tags": [],
	"description": "",
	"content": "Upload Node Boot Information to Boot Script Service (BSS) The following information must be uploaded to BSS as a prerequisite to booting a node via iPXE:\n The location of an initrd image in the artifact repository The location of a kernel image in the artifact repository Kernel boot parameters The node(s) associated with that information, using either host name or NID  BSS manages the iPXE boot scripts that coordinate the boot process for nodes, and it enables basic association of boot scripts with nodes. The boot scripts supply a booting node with a pointer to the necessary images (kernel and initrd) and a set of boot-time parameters.\nPrerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system. Boot Script Service (BSS) is running in containers on a non-compute node (NCN). An initrd image and kernel image for one or more nodes have been uploaded to the artifact repository (see Manage Artifacts with the Cray CLI).  Procedure Because the parameters that must be specified in the PUT command are lengthy, this procedure shows a simple bash script (not to be confused with iPXE boot scripts) to enter the boot information into BSS. The first step creates a script that can use either node ID (NID) or host name to identify the node(s) with which to associate the boot information.\n  Create a bash script to enter the following boot information into BSS in preparation for booting one or more nodes identified by NID or host name.\n  NCN = the host name of a non-compute node (NCN) that is a Kubernetes master node. This procedure uses api-gw-service-nmn.local, the API service name on the Node Management Network (NMN). For more information, see Access to System Management Services.\n  KERNEL = the download URL of the kernel image artifact that was uploaded to S3, which is in the s3://s3_BUCKET/S3_OBJECT_KEY/kernel format.\n  INITRD = the download URL of the initrd image artifact that was uploaded to S3, which is in the s3://s3_BUCKET/S3_OBJECT_KEY/initrd format.\n  PARAMS = the boot kernel parameters.\nIMPORTANT: The PARAMS line must always include the substring crashkernel=360M. This enables node dumps, which are needed to troubleshoot node crashes.\n  NIDS = a list of node IDs of the nodes to be booted.\n  HOSTS = a list of strings identifying by host name the nodes to be booted.\n  The following script is generic. A script with specific values is below this one.\n#!/bin/bash NCN=api-gw-service-nmn.local KERNEL=s3://S3_BUCKET/S3_OBJECT_KEY/initrd INITRD=s3://S3_BUCKET/S3_OBJECT_KEY/kernel PARAMS=\u0026quot;STRING_WITH_BOOT_PARAMETERS crashkernel=360M\u0026quot; # # By NID NIDS=NID1,NID2,NID3 cray bss bootparameters create --nids $NIDS --kernel $KERNEL --initrd $INITRD --params $PARAMS # # By host name #HOSTS=\u0026quot;STRING_IDENTIFYING_HOST1\u0026quot;,\u0026quot;STRING_IDENTIFYING_HOST2\u0026quot; #cray bss bootparameters create --hosts $HOSTS --kernel $KERNEL --initrd $INITRD --params $PARAMS BSS supports a mechanism that allows for a default boot setup, rather than needing to specify boot details for each specific node. The HOSTS value should be set to \u0026ldquo;Default\u0026rdquo; in order to utilize the default boot setup. This feature is particular useful with larger systems.\nThe following script has specific values for the kernel/initrd image names, the kernel parameters, and the list of NIDS and hosts.\n#!/bin/bash NCN=api-gw-service-nmn.local KERNEL=s3://boot-images/97b548b9-2ea9-45c9-95ba-dfc77e5522eb/kernel INITRD=s3://boot-images/97b548b9-2ea9-45c9-95ba-dfc77e5522eb/initrd PARAMS=\u0026quot;console=ttyS0,115200n8 console=tty0 initrd=97b548b9-2ea9-45c9-95ba-dfc77e5522eb root=nfs:$NCN:/var/lib/nfsroot/cmp000001_image rw nofb selinux=0 rd.net.timeout.carrier=20 crashkernel=360M\u0026quot; PARAMS=\u0026quot;console=ttyS0,115200n8 console=tty0 initrd=${INITRD##*/} \\ root=nfs:10.2.0.1:$NFS_IMAGE_ROOT_DIR rw nofb selinux=0 rd.shell crashkernel=360M \\ ip=dhcp rd.neednet=1 htburl=https://10.2.100.50/apis/hbtd/hmi/v1/heartbeat\u0026quot; # # By NID NIDS=1 cray bss bootparameters create --nids $NIDS --kernel $KERNEL --initrd $INITRD --params $PARAMS # # By host name #HOSTS=\u0026quot;nid000001-nmn\u0026quot; #cray bss bootparameters create --hosts $HOSTS --kernel $KERNEL --initrd $INITRD --params $PARAMS   Run the bash script to upload the boot information to BSS for the identified nodes.\nlocalhost# chmod +x script.sh \u0026amp;\u0026amp; ./script.sh   View the boot script.\nThis will show the specific boot script that will be passed to a given node when requesting a boot script. This is useful for debugging boot problems and to verify that BSS is configured correctly.\nlocalhost# cray bss bootscript list --nid NODE_ID   Confirm that the information has been uploaded to BSS.\n  If nodes identified by host name:\nlocalhost# cray bss bootparameters list --hosts HOST_NAME For example:\nlocalhost# cray bss bootparameters list --hosts Default   If nodes identified by NID:\nlocalhost# cray bss bootparameters list --nids NODE_ID For example:\nlocalhost# cray bss bootparameters list --nids 1     View entire contents of BSS, if desired.\nlocalhost# cray bss dumpstate list To view the information retrieved from the HSM:\nlocalhost# cray bss hosts list To view the view the configured boot parameter information:\nlocalhost# cray bss bootparameters list   Boot information has been added to BSS in preparation for iPXE booting all nodes in the list of host names or NIDs.\nAs part of power up the nodes in the host name or NID list, the next step is to reboot the nodes.\n"
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/view_the_status_of_a_bos_session/",
	"title": "View The Status Of A BOS Session",
	"tags": [],
	"description": "",
	"content": "View the Status of a BOS Session The Boot Orchestration Service (BOS) supports a status endpoint that reports the status for individual BOS sessions. The status can be retrieved for each boot set within the session, as well as the individual items within a boot set.\nBOS sessions contain one or more boot sets. Each boot set contains one or more phases, depending upon the operation for that session. For example, a reboot operation would have a shutdown, boot, and possibly configuration phase, but a shutdown operation would only have a shutdown phase. Each phase contains the following categories not_started, in_progress, succeeded, failed, and excluded.\nMetadata Each session, boot set, and phase contains similar metadata. The following is a list of useful attributes to look for in the metadata:\n  start_time\nThe time a session, boot set, or phase started work.\n  in_progress\nThis flag means that the session, boot set, or phase has started and still has work going on.\n  complete\nThe complete flag means the session, boot set, or phase has finished.\n  error_count\nThe number of errors encountered in the boot sets or phases.\n  stop_time\nThe time a session, boot set, or phase ended work.\n  The following table summarizes how to interpret the various combinations of values for the in_progress and complete flags.\n   in_progress Flag complete Flag Meaning     False False Item has not started   True False Item is in progress   False True Item has completed   True True Invalid state (should not occur)    The in_progress flags, complete flags, and error_count flags are cumulative, meaning that they summarize the state of the sub-items.\nPhase: The in_progress flag indicates that there are nodes in the in_progress category. The complete flag means there are no nodes in the in_progress or not_started categories.\nBoot set: The in_progress flag means there is one or more phases that are in_progress. The complete flag means all phases in the boot set are complete.\nSession: The in_progress flag means one or more of the boot sets are in_progress. The complete flag means all boot sets are complete.\nView the Status of a Session The BOS session ID is required to view the status of a session. To list the available sessions, use the following command:\nncn-m001# cray bos session list --format json [ \u0026#34;99a192c2-050e-41bc-a576-548610851742\u0026#34;, \u0026#34;4374f3e6-e8ed-4e66-bf63-3ebe0e618db2\u0026#34;, \u0026#34;fb14932a-a9b7-41b2-ad21-b4bc632cf1ef\u0026#34;, \u0026#34;9321ab7a-bf7f-42fd-8103-94a296552856\u0026#34;, \u0026#34;50aaaa85-6807-45c7-b6de-f984a930e2eb\u0026#34;, \u0026#34;972cfd09-3403-4282-ab93-b41992f7c0d8\u0026#34;, \u0026#34;2c86c1b9-5281-4610-b044-479f1536727a\u0026#34;, \u0026#34;7719385a-e462-4bb6-8fd8-55caa0836528\u0026#34;, \u0026#34;0aac0252-4637-4198-919f-6bafda7fafef\u0026#34;, \u0026#34;13207c87-0b9f-410c-88c1-6e26ff63cb34\u0026#34;, \u0026#34;bd18e7e3-978f-4699-b8f2-8a4ce2d46f75\u0026#34;, \u0026#34;b741e4de-2064-4de4-9f23-20b6c1d0dc1a\u0026#34;, \u0026#34;f4eebe51-a217-46d0-8733-b9499a092042\u0026#34; ] It is recommended to describe the session using the session ID above to verify the desired selection was selected:\nncn-m001# cray bos session describe SESSION_ID status_link = \u0026#34;/v1/session/f4eebe51-a217-46d0-8733-b9499a092042/status\u0026#34; complete = false start_time = \u0026#34;2020-07-22 13:39:07.706774\u0026#34; templateUuid = \u0026#34;cle-1.3.0\u0026#34; error_count = 4 boa_job_name = \u0026#34;boa-f4eebe51-a217-46d0-8733-b9499a092042\u0026#34; in_progress = false operation = \u0026#34;reboot\u0026#34; The status for the session will show the session ID, the boot sets in the session, the metadata, and some links. In the following example, there is only one boot set named computes, and the session ID being used is f4eebe51-a217-46d0-8733-b9499a092042.\nTo display the status for the session:\nncn-m001# cray bos session status list SESSION_ID -–format json { \u0026#34;boot_sets\u0026#34;: [ \u0026#34;computes\u0026#34; ], \u0026#34;id\u0026#34;: \u0026#34;f4eebe51-a217-46d0-8733-b9499a092042\u0026#34;, \u0026#34;links\u0026#34;: [ { \u0026#34;href\u0026#34;: \u0026#34;/v1/session/f4eebe51-a217-46d0-8733-b9499a092042/status\u0026#34;, \u0026#34;rel\u0026#34;: \u0026#34;self\u0026#34; }, { \u0026#34;href\u0026#34;: \u0026#34;/v1/session/f4eebe51-a217-46d0-8733-b9499a092042/status/computes\u0026#34; , \u0026#34;rel\u0026#34;: \u0026#34;Boot Set\u0026#34; } ], \u0026#34;metadata\u0026#34;: { \u0026#34;in_progress\u0026#34;: false, \u0026#34;start_time\u0026#34;: \u0026#34;2020-07-22 13:39:07.706774\u0026#34;, \u0026#34;complete\u0026#34;: false, \u0026#34;error_count\u0026#34;: 4 } } View the Status of a Boot Set Run the following command to view the status for a specific boot set in a session. For more information about retrieving the session ID and boot set name, refer to the \u0026ldquo;View the Status of a Session\u0026rdquo; section above. Descriptions of the different status sections are described below.\n  Boot set\nThe id parameter identifies which session this status belongs to.\nThe name parameter is the name of the boot set.\nThe links section displays links that enable administrators to drill down into each phase of the boot set.\nThere is metadata section for the boot set as a whole.\n  Phases\nThe name parameter is the name of the phase.\nThere is a metadata section for each phase.\nEach phase contains the following categories: not_started, in_progress, succeeded, failed, and excluded. The nodes are listed in the category they are currently occupying.\n  ncn-m001# cray bos session status describe BOOT_SET_NAME SESSION_ID --format json { \u0026#34;phases\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;shutdown\u0026#34;, \u0026#34;categories\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;not_started\u0026#34;, \u0026#34;node_list\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;succeeded\u0026#34;, \u0026#34;node_list\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;failed\u0026#34;, \u0026#34;node_list\u0026#34;: [ \u0026#34;x3000c0s19b4n0\u0026#34;, \u0026#34;x3000c0s19b1n0\u0026#34;, \u0026#34;x3000c0s19b3n0\u0026#34;, \u0026#34;x3000c0s19b2n0\u0026#34; ] }, { \u0026#34;name\u0026#34;: \u0026#34;excluded\u0026#34;, \u0026#34;node_list\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;in_progress\u0026#34;, \u0026#34;node_list\u0026#34;: [] } ], \u0026#34;metadata\u0026#34;: { \u0026#34;stop_time\u0026#34;: \u0026#34;2020-07-22 13:53:19.842705\u0026#34;, \u0026#34;in_progress\u0026#34;: false, \u0026#34;start_time\u0026#34;: \u0026#34;2020-07-22 13:39:08.276530\u0026#34;, \u0026#34;complete\u0026#34;: true, \u0026#34;error_count\u0026#34;: 4 } }, { \u0026#34;name\u0026#34;: \u0026#34;boot\u0026#34;, \u0026#34;categories\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;not_started\u0026#34;, \u0026#34;node_list\u0026#34;: [ \u0026#34;x3000c0s19b4n0\u0026#34;, \u0026#34;x3000c0s19b3n0\u0026#34;, \u0026#34;x3000c0s19b1n0\u0026#34;, \u0026#34;x3000c0s19b2n0\u0026#34; ] }, { \u0026#34;name\u0026#34;: \u0026#34;succeeded\u0026#34;, \u0026#34;node_list\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;failed\u0026#34;, \u0026#34;node_list\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;excluded\u0026#34;, \u0026#34;node_list\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;in_progress\u0026#34;, \u0026#34;node_list\u0026#34;: [] } ], \u0026#34;metadata\u0026#34;: { \u0026#34;in_progress\u0026#34;: false, \u0026#34;start_time\u0026#34;: \u0026#34;2020-07-22 13:39:08.276542\u0026#34;, \u0026#34;complete\u0026#34;: false, \u0026#34;error_count\u0026#34;: 0 } }, { \u0026#34;name\u0026#34;: \u0026#34;configure\u0026#34;, \u0026#34;categories\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;not_started\u0026#34;, \u0026#34;node_list\u0026#34;: [ \u0026#34;x3000c0s19b4n0\u0026#34;, \u0026#34;x3000c0s19b3n0\u0026#34;, \u0026#34;x3000c0s19b1n0\u0026#34;, \u0026#34;x3000c0s19b2n0\u0026#34; ] }, { \u0026#34;name\u0026#34;: \u0026#34;succeeded\u0026#34;, \u0026#34;node_list\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;failed\u0026#34;, \u0026#34;node_list\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;excluded\u0026#34;, \u0026#34;node_list\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;in_progress\u0026#34;, \u0026#34;node_list\u0026#34;: [] } ], \u0026#34;metadata\u0026#34;: { \u0026#34;in_progress\u0026#34;: false, \u0026#34;start_time\u0026#34;: \u0026#34;2020-07-22 13:39:08.276552\u0026#34;, \u0026#34;complete\u0026#34;: false, \u0026#34;error_count\u0026#34;: 0 } } ], \u0026#34;session\u0026#34;: \u0026#34;f4eebe51-a217-46d0-8733-b9499a092042\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;computes\u0026#34;, \u0026#34;links\u0026#34;: [ { \u0026#34;href\u0026#34;: \u0026#34;/v1/session/f4eebe51-a217-46d0-8733-b9499a092042/status/computes\u0026#34;, \u0026#34;rel\u0026#34;: \u0026#34;self\u0026#34; }, { \u0026#34;href\u0026#34;: \u0026#34;/v1/session/f4eebe51-a217-46d0-8733-b9499a092042/status/computes/shutdown\u0026#34;, \u0026#34;rel\u0026#34;: \u0026#34;Phase\u0026#34; }, { \u0026#34;href\u0026#34;: \u0026#34;/v1/session/f4eebe51-a217-46d0-8733-b9499a092042/status/computes/boot\u0026#34;, \u0026#34;rel\u0026#34;: \u0026#34;Phase\u0026#34; }, { \u0026#34;href\u0026#34;: \u0026#34;/v1/session/f4eebe51-a217-46d0-8733-b9499a092042/status/computes/configure\u0026#34;, \u0026#34;rel\u0026#34;: \u0026#34;Phase\u0026#34; } ], \u0026#34;metadata\u0026#34;: { \u0026#34;in_progress\u0026#34;: false, \u0026#34;start_time\u0026#34;: \u0026#34;2020-07-22 13:39:08.276519\u0026#34;, \u0026#34;complete\u0026#34;: false, \u0026#34;error_count\u0026#34;: 4 } } View the Status for an Individual Phase Direct calls to the API are needed to retrieve the status for an individual phase. Support for the Cray CLI is not currently available. The following command is used to view the status of a phase:\nncn-m001# curl -H \u0026#34;Authorization: Bearer BEARER_TOKEN\u0026#34; -X GET \\ https://api-gw-service-nmn.local/apis/bos/v1/session/SESSION_ID/status/BOOT_SET_NAME/PHASE In the following example, the session ID is f89eb554-c733-4197-b2f2-4e1e5ba0c0ec, the boot set name is computes, and the individual phase is shutdown.\nncn-m001# curl -H \u0026#34;Authorization: Bearer BEARER_TOKEN\u0026#34; -X GET \\ https://api-gw-service-nmn.local/apis/bos/v1/session/f89eb554-c733-4197-b2f2-4e1e5ba0c0ec/status/computes/shutdown { \u0026#34;categories\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;not_started\u0026#34;, \u0026#34;node_list\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;succeeded\u0026#34;, \u0026#34;node_list\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;failed\u0026#34;, \u0026#34;node_list\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;excluded\u0026#34;, \u0026#34;node_list\u0026#34;: [] }, { \u0026#34;name\u0026#34;: \u0026#34;in_progress\u0026#34;, \u0026#34;node_list\u0026#34;: [ \u0026#34;x5000c1s2b0n1\u0026#34;, \u0026#34;x5000c1s0b0n0\u0026#34;, \u0026#34;x3000c0s19b4n0\u0026#34;, \u0026#34;x5000c1s0b1n0\u0026#34;, \u0026#34;x5000c1s0b1n1\u0026#34;, \u0026#34;x5000c1s1b1n1\u0026#34;, \u0026#34;x5000c1s2b0n0\u0026#34;, \u0026#34;x3000c0s19b3n0\u0026#34;, \u0026#34;x5000c1s0b0n1\u0026#34;, \u0026#34;x5000c1s2b1n1\u0026#34;, \u0026#34;x3000c0s19b1n0\u0026#34;, \u0026#34;x5000c1s1b1n0\u0026#34;, \u0026#34;x5000c1s2b1n0\u0026#34;, \u0026#34;x3000c0s19b2n0\u0026#34;, \u0026#34;x5000c1s1b0n1\u0026#34;, \u0026#34;x5000c1s1b0n0\u0026#34; ] } ], \u0026#34;metadata\u0026#34;: { \u0026#34;complete\u0026#34;: false, \u0026#34;error_count\u0026#34;: 0, \u0026#34;in_progress\u0026#34;: true, \u0026#34;start_time\u0026#34;: \u0026#34;2020-06-30 21:42:39.355423\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;shutdown\u0026#34; } View the Status for an Individual Category Direct calls to the API are needed to retrieve the status for an individual category. Support for the Cray CLI is not currently available. The following command is used to view the status of a phase:\nncn-m001# curl -H \u0026#34;Authorization: Bearer BEARER_TOKEN\u0026#34; -X GET \\ https://api-gw-service-nmn.local/apis/bos/v1/session/SESSION_ID/status/BOOT_SET_NAME/PHASE/CATEGORY In the following example, the session ID is f89eb554-c733-4197-b2f2-4e1e5ba0c0ec, the boot set name is computes, the phase is shutdown, and the category is in_progress.\nncn-m001# curl -H \u0026#34;Authorization: Bearer BEARER_TOKEN\u0026#34; -X GET \\ https://api-gw-service-nmn.local/apis/bos/v1/session/f89eb554-c733-4197-b2f2-4e1e5ba0c0ec/status/computes/shutdown/in_progress { \u0026#34;name\u0026#34;: \u0026#34;in_progress\u0026#34;, \u0026#34;node_list\u0026#34;: [ \u0026#34;x5000c1s2b0n1\u0026#34;, \u0026#34;x5000c1s0b0n0\u0026#34;, \u0026#34;x3000c0s19b4n0\u0026#34;, \u0026#34;x5000c1s0b1n0\u0026#34;, \u0026#34;x5000c1s0b1n1\u0026#34;, \u0026#34;x5000c1s1b1n1\u0026#34;, \u0026#34;x5000c1s2b0n0\u0026#34;, \u0026#34;x3000c0s19b3n0\u0026#34;, \u0026#34;x5000c1s0b0n1\u0026#34;, \u0026#34;x5000c1s2b1n1\u0026#34;, \u0026#34;x3000c0s19b1n0\u0026#34;, \u0026#34;x5000c1s1b1n0\u0026#34;, \u0026#34;x5000c1s2b1n0\u0026#34;, \u0026#34;x3000c0s19b2n0\u0026#34;, \u0026#34;x5000c1s1b0n1\u0026#34;, \u0026#34;x5000c1s1b0n0\u0026#34; ] } "
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_the_boot_script_service_bss/",
	"title": "Troubleshoot Compute Node Boot Issues Related To The Boot Script Service (bss)",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Compute Node Boot Issues Related to the Boot Script Service (BSS) Boot Script Service (BSS) delivers a boot script to a node based on its MAC address. This boot script tells the node where to obtain its boot artifacts, which include:\n kernel initrd  In addition, the boot script also contains the kernel boot parameters. This procedure helps resolve issues related to missing boot artifacts.\nPrerequisites This procedure requires administrative privileges.\nLimitations Encryption of compute node logs is not enabled, so the passwords may be passed in clear text.\nProcedure   Log onto a non-compute node (NCN) as root.\n  Check that BSS is running.\nncn-m001# kubectl get pods -n services -o wide | grep cray-bss | grep -v -etcd- cray-bss-fd888bd54-gvpxq 2/2 Running 0 2d3h 10.32.0.16 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   Check that the boot script of the node that is failing to boot contains the correct boot artifacts.\n  If nodes are identified by their host names, execute the following:\nncn-m001# cray bss bootparameters list --hosts HOST_NAME   If nodes are identified by their node IDs, execute the following:\nncn-m001# cray bss bootparameters list --nids NODE_ID     View the entire BSS contents.\nncn-m001# cray bss dumpstate list   View the actual boot script.\nUsing hosts:\nncn-m001# cray bss bootscript list --host HOST_NAME Using the MAC address to get the actual boot script:\nncn-m001# cray bss bootscript list --mac MAC_ADDRESS Using node IDs to retrieve boot parameters:\nncn-m001# cray bss bootscript list --nid NODE_ID   "
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_unified_extensible_firmware_interface_uefi/",
	"title": "Troubleshoot Compute Node Boot Issues Related To Unified Extensible Firmware Interface (uefi)",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Compute Node Boot Issues Related to Unified Extensible Firmware Interface (UEFI) If a node is stuck in the UEFI shell, ConMan will be able to connect to it, but nothing else will appear in its logs. The node\u0026rsquo;s logs will look similar to the following, indicating that ConMan is updating its log hourly:\n\u0026lt;ConMan\u0026gt; Console [86] log at 2018-09-07 20:00:00 CDT. \u0026lt;ConMan\u0026gt; Console [86] log at 2018-09-07 21:00:00 CDT. \u0026lt;ConMan\u0026gt; Console [86] log at 2018-09-07 22:00:00 CDT. \u0026lt;ConMan\u0026gt; Console [86] log at 2018-09-07 23:00:00 CDT. \u0026lt;ConMan\u0026gt; Console [86] log at 2018-09-08 00:00:00 CDT. This procedure helps resolve this issue.\nPrerequisites This procedure requires administrative privileges.\nLimitations Encryption of compute node logs is not enabled, so the passwords may be passed in clear text.\nProcedure   Log onto the node via ipmitool or ConMan.\n  Select Enter.\nThe system will present one of the following prompts, both of which indicate that the user has entered the UEFI shell.\n  Use one of the following options to resolve the issue:\n Contact the system administrator or someone who is knowledgeable about UEFI if the node is stuck in the UEFI shell. Reseat the node if it is suspected that this may be a hardware related issue. To reseat a node, pull the power cable off the back end of the box. Wait a few seconds, then reconnect the power cable and push the power button on the front of the box. Reseating a node must be done on site. Contact customer support for more information.    "
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_using_kuberentes/",
	"title": "Troubleshoot Compute Node Boot Issues Using Kubernetes",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Compute Node Boot Issues Using Kubernetes A number of Kubernetes commands can be used to debug issues related to the node boot process. All of the traffic bound for the DHCP server, TFTP server, and Boot Script Service (BSS) is sent on the Node Management Network (NMN).\nIn the current arrangement, all three services are located on a non-compute node (NCN). Thus, traffic must first travel through the NCN to reach these services inside their pods. When attempting to track down missing requests for either DHCP or TFTP, it is helpful to set up tcpdump on the NCN where the pod is resident to ensure that the request got that far. The NODE column in the output of kubectl get pods -o wide shows which node the pod is running on.\nTroubleshooting Tips   Retrieve logs for a specific Kubernetes pod:\nSyntax:\nncn-m001# kubectl logs -n NAMESPACE pod_name Example:\nncn-m001# kubectl get pods -n services -o wide|grep -E \u0026#34;NAME|kea\u0026#34; NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES cray-dhcp-kea-6b78789fc4-lzmff 3/3 Running 0 5d12h 10.42.0.30 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ncn-m001# kubectl logs -n services DHCP_KEA_POD_ID -c CONTAINER DHCPDISCOVER from a4:bf:01:23:1a:f4 via vlan100 ICMP Echo reply while lease 10.100.160.199 valid. Abandoning IP address 10.100.160.199: pinged before offer Reclaiming abandoned lease 10.100.160.195. ...   Check if a Kubernetes pod is running:\nSyntax:\nncn-m001# kubectl get pods -A -o wide | grep pod_name Example:\nncn-m001# kubectl get pods -A -o wide |grep -E \u0026#34;NAME|kea\u0026#34; NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES services cray-dhcp-kea-6b78789fc4-lzmff 3/3 Running 0 5d12h 10.42.0.30 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   Gain access to a Kubernetes pod:\nUse the following command to enter a shell inside a container within the pod. Once inside the shell, execute commands as needed.\nncn-m001# kubectl exec -A -it pod_name /bin/sh For example, if the pod\u0026rsquo;s name is cray-dhcp-kea-6b78789fc4-lzmff, execute:\nncn-m001# kubectl exec -A -it cray-dhcp-kea-6b78789fc4-lzmff /bin/sh   "
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_using_kubernetes/",
	"title": "Troubleshoot Compute Node Boot Issues Using Kubernetes",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Compute Node Boot Issues Using Kubernetes A number of Kubernetes commands can be used to debug issues related to the node boot process. All of the traffic bound for the DHCP server, TFTP server, and Boot Script Service (BSS) is sent on the Node Management Network (NMN).\nIn the current arrangement, all three services are located on a non-compute node (NCN). Thus, traffic must first travel through the NCN to reach these services inside their pods. When attempting to track down missing requests for either DHCP or TFTP, it is helpful to set up tcpdump on the NCN where the pod is resident to ensure that the request got that far. The NODE column in the output of kubectl get pods -o wide shows which node the pod is running on.\nTroubleshooting Tips   Retrieve logs for a specific Kubernetes pod:\nSyntax:\nncn-m001# kubectl logs -n NAMESPACE pod_name Example:\nncn-m001# kubectl get pods -n services -o wide|grep -E \u0026#34;NAME|kea\u0026#34; NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES cray-dhcp-kea-6b78789fc4-lzmff 3/3 Running 0 5d12h 10.42.0.30 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ncn-m001# kubectl logs -n services DHCP_KEA_POD_ID -c CONTAINER DHCPDISCOVER from a4:bf:01:23:1a:f4 via vlan100 ICMP Echo reply while lease 10.100.160.199 valid. Abandoning IP address 10.100.160.199: pinged before offer Reclaiming abandoned lease 10.100.160.195. ...   Check if a Kubernetes pod is running:\nSyntax:\nncn-m001# kubectl get pods -A -o wide | grep pod_name Example:\nncn-m001# kubectl get pods -A -o wide |grep -E \u0026#34;NAME|kea\u0026#34; NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES services cray-dhcp-kea-6b78789fc4-lzmff 3/3 Running 0 5d12h 10.42.0.30 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   Gain access to a Kubernetes pod:\nUse the following command to enter a shell inside a container within the pod. Once inside the shell, execute commands as needed.\nncn-m001# kubectl exec -A -it pod_name /bin/sh For example, if the pod\u0026rsquo;s name is cray-dhcp-kea-6b78789fc4-lzmff, execute:\nncn-m001# kubectl exec -A -it cray-dhcp-kea-6b78789fc4-lzmff /bin/sh   "
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/troubleshoot_booting_nodes_with_hardware_issues/",
	"title": "Troubleshoot Booting Nodes With Hardware Issues",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Booting Nodes with Hardware Issues How to identify a node with hardware issues and how to disable is via the HSM.\nIf a node included in a Boot Orchestration Service (BOS) session template is having hardware issues, it can prevent the node from powering back up correctly. The entire BOS session will fail with a timeout error waiting for the node to become ready.\nThe following is example log output from a node with hardware issues, resulting in a failed BOS session:\nncn-m001# kubectl logs BOS_POD_ID ... 2020-10-03 17:47:30,053 - ERROR - cray.boa.smd.wait_for_nodes - Number of retries: 361 exceeded allowed amount: 360; 2 nodes were not in the state: Ready 2020-10-03 17:47:30,054 - DEBUG - cray.boa.smd.wait_for_nodes - These nodes were not in the state: Ready x1003c0s1b1n1 x1001c0s2b1n1 Disabling nodes that have underlying hardware issues preventing them from booting will help resolve this issue. This can be done via the Hardware State Manager (HSM). This method does not return the node with hardware issues to a healthy state, but it does enable a BOS session that was encountering issues to complete successfully.\n"
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_dynamic_host_configuration_protocol_dhcp/",
	"title": "Troubleshoot Compute Node Boot Issues Related To Dynamic Host Configuration Protocol (DHCP)",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Compute Node Boot Issues Related to Dynamic Host Configuration Protocol (DHCP) DHCP issues can result in node boot failures. This procedure helps investigate and resolve such issues.\nPrerequisites  This procedure requires administrative privileges. kubectl is installed.  Limitations Encryption of compute node logs is not enabled, so the passwords may be passed in clear text.\nProcedure   Log in to a non-compute node (NCN) as root.\n  Check that the DHCP service is running.\nncn-m001# kubectl get pods -A | grep kea services cray-dhcp-kea-554698bb69-r9wwt 3/3 Running 0 13h services cray-dhcp-kea-postgres-0 2/2 Running 0 10d services cray-dhcp-kea-postgres-1 2/2 Running 0 3d18h services cray-dhcp-kea-postgres-2 2/2 Running 0 10d services cray-dhcp-kea-wait-for-postgres-3-7gqvg 0/3 Completed 0 10d   Start a tcpdump session on the NCN.\nThe following example sends tcpdump data to stdout.\ncray-dhcp# tcpdump   Obtain the DHCP pod\u0026rsquo;s ID.\nncn-m001# PODID=$(kubectl get pods --no-headers -o wide | grep cray-dhcp | awk \u0026#39;{print $1}\u0026#39;)   Enter the DHCP pod using its ID.\nncn-m001# kubectl exec -it $PODID /bin/sh   Start a tcpdump session from within the DHCP pod.\n  Open another terminal to perform the following tasks:\n  Issue a DHCP discover request from the NCN using nmap.\n  Analyze the NCN tcpdump data in order to ensure that the DHCP discover request is visible.\n    Go back to the original terminal to analyze the DHCP pod\u0026rsquo;s tcpdump data in order to ensure that the DHCP discover request is visible inside the pod.\nTroubleshooting Information:\nIf the DHCP Discover request is not visible on the NCN, it may be due to a firewall issue. If the DHCP Discover request is not visible inside the pod, double check if the request was issued over the correct interface for the Node Management Network (NMN). If it was, it could indicate a firewall issue.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_slow_boot_times/",
	"title": "Troubleshoot Compute Node Boot Issues Related To Slow Boot Times",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Compute Node Boot Issues Related to Slow Boot Times Inspect BOS, the Boot Orchestration Agent (BOA) job logs, and the Configuration Framework Service (CFS) job logs to obtain information that is critical for boot troubleshooting. Use this procedure to determine why compute nodes are booting slower than expected.\nPrerequisites A boot session has been created with the Boot Orchestration Service (BOS).\nProcedure   View the BOA logs.\n  Find the BOA job from the boot session.\nThe output of the command below is organized by the creation time of the BOA job with the most recent one listed last.\nncn-m001# kubectl -nservices --sort-by=.metadata.creationTimestamp get pods | grep boa boa-e3c845be-3092-4807-a0c9-272bf0e15896-7pnl4 0/2 Completed 0 3d boa-c740f74d-f5af-41f3-a71b-1a3fc00cbe7a-k5hdw 0/2 Completed 0 2d12h boa-a365b6a2-3614-4b53-9b6b-df0f4485e25d-nbcdb 0/2 Completed 0 2m43s   Watch the log from BOA job.\nncn-m001# kubectl logs -n services -f -c boa BOA_JOB_ID 2019-11-12 02:14:27,771 - DEBUG - cray.boa - BOA starting 2019-11-12 02:14:28,786 - DEBUG - cray.boa - Boot Agent Image: acad2b43-dff5-483d-a392-8b1b1f91a60c Nodes: x5000c1s1b1n0, x3000c0s35b2n0, x5000c1s3b1n1, x3000c0s35b3n0, x5000c1s0b1n0, x5000c1s3b0n1, x5000c1s2b0n0, x5000c1s3b1n0, x5000c1s1b1n1, x5000c1s2b0n1, x3000c0s35b1n0, x5000c1s3b0n0, x5000c1s0b1n1, x5000c1s1b0n0, x5000c1s2b1n0, x5000c1s1b0n1, x5000c1s2b1n1 created. 2019-11-12 02:14:29,118 - INFO - cray.boa - Boot Session: 88df3fc3-6697-41cc-9f63-7076d78a9110 2019-11-12 02:14:29,505 - DEBUG - cray.boa.logutil - cray.boa.agent.reboot called with args: (Boot Agent Image: acad2b43-dff5-483d-a392-8b1b1f91a60c Nodes: x5000c1s1b1n0, x3000c0s35b2n0, x5000c1s3b1n1, x3000c0s35b3n0, x5000c1s0b1n0, x5000c1s3b0n1, x5000c1s2b0n0, x5000c1s3b1n0, x5000c1s1b1n1, x5000c1s2b0n1, x3000c0s35b1n0, x5000c1s3b0n0, x5000c1s0b1n1, x5000c1s1b0n0, x5000c1s2b1n0, x5000c1s1b0n1, x5000c1s2b1n1,) 2019-11-12 02:14:29,505 - INFO - cray.boa.agent - Rebooting the Session: 88df3fc3-6697-41cc-9f63-7076d78a9110 Set: computes 2019-11-12 02:15:15,898 - DEBUG - cray.boa.logutil - cray.boa.dbclient.db_write_session called with args: (\u0026lt;etcd3.client.Etcd3Client object at 0x7f822db68dd8\u0026gt;, \u0026#39;88df3fc3-6697-41cc-9f63-7076d78a9110\u0026#39;, \u0026#39;computes\u0026#39;, \u0026#39;status\u0026#39;, \u0026#39;boot_capmc_finished\u0026#39;) 2019-11-12 02:15:15,898 - DEBUG - cray.boa.dbclient - Key: /session/88df3fc3-6697-41cc-9f63-7076d78a9110/computes/status/ Value: boot_capmc_finished 2019-11-12 02:15:15,938 - INFO - cray.boa.smd.wait_for_nodes - Standby: 17 entries 2019-11-12 02:15:15,938 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:15:20,979 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:15:26,021 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:15:31,077 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:15:36,120 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:15:41,161 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:15:46,204 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:15:51,244 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:15:56,288 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:16:01,329 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:16:06,372 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:16:11,413 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:16:16,455 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:16:21,501 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:16:26,544 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:16:31,594 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:16:36,638 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:16:41,681 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:16:46,724 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:16:51,766 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:16:56,810 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:17:01,853 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:17:06,899 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:17:11,940 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:17:16,981 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:17:22,023 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:17:27,065 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:17:32,119 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:17:37,161 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:17:42,203 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:17:47,244 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:17:52,289 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:17:57,332 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:18:02,373 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:18:07,417 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:18:12,459 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:18:17,501 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:18:22,544 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:18:27,586 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 17 nodes to be in state: Ready 2019-11-12 02:18:32,629 - INFO - cray.boa.smd.wait_for_nodes - Ready: [x3000c0s35b1n0] Standby: 16 entries 2019-11-12 02:18:32,629 - INFO - cray.boa.smd.wait_for_nodes - Waiting 5 seconds for 16 nodes to be in state: Ready 2019-11-12 02:18:37,673 - INFO - cray.boa.smd.wait_for_nodes - ...     View the CFS logs related to the boot job.\n  Find the most recent CFS jobs.\nThere may be more than one job if multiple components are being configured. If there are multiple different BOA jobs running, check the BOA logs first to find the timestamp value when CFS was updated. Expect a delay of a couple minutes after the CFS session starts depending on the cfs-batcher settings.\nncn-m001# kubectl -n services get cfs NAME JOB STATUS SUCCEEDED REPOSITORY CLONE URL BRANCH COMMIT PLAYBOOK AGE 066bc062-7fc3-11ea-970e-a4bf0138f2ba cfs-1628cf85-e847-49af-891c-1b7655d8056d complete true https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git master site.yml 4d10h 3c3758a8-7fd9-11ea-a365-a4bf0138f2ba cfs-05420ebf-fbbc-4d3a-a0af-a840e379fe12 complete true https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git master site.yml 4d8h batcher-65f94609-0599-4d86-a8ad-9555d2a9ab9d cfs-b10975a0-fdde-4d00-98f8-0a2895b32d57 complete true https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git 0a38dc0d61f94eb43bf32c8bad801c4d41bf52d9 site.yml 3d17h batcher-6d823363-c7cd-4616-afb8-416156a83522 cfs-78726a81-eb72-4e2f-80e8-a8d08a7c031c complete true https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git 0a38dc0d61f94eb43bf32c8bad801c4d41bf52d9 site.yml 29h batcher-ef877321-922c-4517-bc05-e4f2a5141b2b cfs-a6c6f707-c7ca-48d7-aff7-71b26398c216 complete true https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git a371a11a5cf139ba67cee5823c8ce0e5b61d7a3f site.yml 4d5h ed684272-7fe8-11ea-a0fd-a4bf0138f2ba cfs-6ed992a3-7f63-4187-a2bf-fc4451ead997 complete true https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git master site.yml 4d6h ncn-customization-ncn-w001-uai-hosts-load cfs-acb9f57f-e390-49c6-8028-842550f3d73e complete false https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git master cray-ncn-customization-load.yml 4d6h ncn-customization-ncn-w002-uai-hosts-load cfs-37aebeb8-1c3f-45a4-a9b1-d25ad4e10a91 complete false https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git master cray-ncn-customization-load.yml 4d6h ncn-customization-ncn-w002-uai-hosts-unload cfs-095cce88-1925-4625-a611-ae19d9976a60 complete false https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git master cray-ncn-customization-unload.yml 4d6h ncn-customization-ncn-w003-uai-hosts-load cfs-6b3fdebd-ab2b-4751-b29f-436ff2893569 complete false https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git master cray-ncn-customization-load.yml 4d6h ncn-customization-ncn-w003-uai-hosts-unload cfs-d94ebbe6-6b61-4f78-9dc4-fd24576d32dd complete false https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git master cray-ncn-customization-unload.yml 4d6h If multiple BOA jobs exist, describe the CFS sessions and look at the configuration, as well as which components are included. It is unlikely, but a single session may contain components from multiple separate BOS sessions if they both request the same configuration for different components at around the same time.\nncn-m001# kubectl -n services describe cfs SESSION_NAME   Find the pods for the CFS job.\nncn-m001# kubectl -n services get pods | grep JOB_NAME cfs-1628cf85-e847-49af-891c-1b7655d8056d-29ntt 0/4 Completed 0 4d11h   View the log from the CFS job.\nncn-m001# kubectl -n services logs POD_NAME ansible Inventory available % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 HTTP/1.1 200 OK content-type: text/html; charset=UTF-8 cache-control: no-cache, max-age=0 x-content-type-options: nosniff date: Thu, 16 Apr 2020 09:20:15 GMT server: envoy transfer-encoding: chunked Sidecar available [WARNING]: Ignoring invalid path provided to plugin path: \u0026#39;/opt/cray/crayctl/files\u0026#39; is not a directory PLAY [Compute] ***************************************************************** TASK [rsyslog : Add rsyslog.d config] ****************************************** changed: [cle_default_rootfs_cfs_066bc062-7fc3-11ea-970e-a4bf0138f2ba] TASK [localtime : Create /etc/localtime symlink] ******************************* changed: [cle_default_rootfs_cfs_066bc062-7fc3-11ea-970e-a4bf0138f2ba] TASK [ntp : Install stock /etc/chrony.conf] ************************************ changed: [cle_default_rootfs_cfs_066bc062-7fc3-11ea-970e-a4bf0138f2ba] TASK [cle-hosts-cf : create temporary workarea] ******************************** changed: [cle_default_rootfs_cfs_066bc062-7fc3-11ea-970e-a4bf0138f2ba] TASK [cle-hosts-cf : copy /etc/hosts from NCN host OS into compute image] ****** changed: [cle_default_rootfs_cfs_066bc062-7fc3-11ea-970e-a4bf0138f2ba] TASK [cle-hosts-cf : copy /etc/hosts into place] ******************************* changed: [cle_default_rootfs_cfs_066bc062-7fc3-11ea-970e-a4bf0138f2ba] TASK [cle-hosts-cf : remove temporary workarea] ******************************** changed: [cle_default_rootfs_cfs_066bc062-7fc3-11ea-970e-a4bf0138f2ba] TASK [limits : Ensure our file limits are set] ********************************* changed: [cle_default_rootfs_cfs_066bc062-7fc3-11ea-970e-a4bf0138f2ba] TASK [shadow : Change root password in /etc/shadow] **************************** changed: [cle_default_rootfs_cfs_066bc062-7fc3-11ea-970e-a4bf0138f2ba] TASK [kdump : Install stock /etc/sysconfig/kdump] ****************************** changed: [cle_default_rootfs_cfs_066bc062-7fc3-11ea-970e-a4bf0138f2ba] TASK [kdump : copy kdump initrd script] **************************************** changed: [cle_default_rootfs_cfs_066bc062-7fc3-11ea-970e-a4bf0138f2ba] TASK [kdump : create kdump initrd] ********************************************* changed: [cle_default_rootfs_cfs_066bc062-7fc3-11ea-970e-a4bf0138f2ba] TASK [kdump : remove kdump initrd script] ************************************** changed: [cle_default_rootfs_cfs_066bc062-7fc3-11ea-970e-a4bf0138f2ba] ...     Use the data returned in the BOA and CFS logs to determine the underlying issue for slow boot times.\n"
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_trivial_file_transfer_protocol_tftp/",
	"title": "Troubleshoot Compute Node Boot Issues Related To Trivial File Transfer Protocol (tftp)",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Compute Node Boot Issues Related to Trivial File Transfer Protocol (TFTP) TFTP issues can result in node boot failures. Use this procedure to investigate and resolve such issues.\nPrerequisites This procedure requires administrative privileges.\nLimitations Encryption of compute node logs is not enabled, so the passwords may be passed in clear text.\n  Log onto a non-compute node (NCN) as root.\n  Check that the TFTP service is running.\nncn-m001# kubectl get pods -n services -o wide | grep cray-tftp   Start a tcpdump session on the NCN.\n  Obtain the TFTP pod\u0026rsquo;s ID.\nncn-m001# PODID=$(kubectl get pods -n services --no-headers -o wide | grep cray-tftp | awk \u0026#39;{print $1}\u0026#39;) ncn-m001# echo $PODID cray-tftp-85f8dbf699-fqgfh   Enter the TFTP pod using the pod ID.\nDouble check that PODID contains only one ID. If there are multiple TFTP pods listed, just choose one as the ID.\nncn-m001# kubectl exec -n services -it $PODID /bin/sh   Start a tcpdump session from within the TFTP pod.\n  Open another terminal to perform the following tasks:\n  Use a TFTP client to issue a TFTP request from either the NCN or a laptop.\n  Analyze the NCN tcpdump data to ensure that the TFTP discover request is visible.\n    Go back to the original terminal to analyze the TFTP pod\u0026rsquo;s tcpdump data in order to ensure that the TFTP request is visible inside the pod.\nTroubleshooting Information:\nIf the TFTP request is not visible on the NCN, it may be due to a firewall issue. If the TFTP request is not visible inside the pod, double check that the request was issued over the correct interface for the Node Management Network (NMN). If it was, the underlying issue could be related to the firewall.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/sessions/",
	"title": "BOS Sessions",
	"tags": [],
	"description": "",
	"content": "BOS Sessions Overview of BOS Session operations and limitations.\nThe Boot Orchestration Service (BOS) creates a session when it is asked to perform one of these operations:\n Boot - Boot a designated collection of nodes. Shutdown - Shutdown a designated collection of nodes. Reboot - Reboot a designated collection of nodes. Configure - Configure a designated collection of booted nodes.  BOS sessions can be used to boot compute nodes with customized image roots.\nThe Boot Orchestration Agent (BOA) implements each session and sees it through to completion. A BOA is a Kubernetes job. It runs once to completion. If there are transient failures, BOA will exit and Kubernetes will reschedule it so that it can re-execute its session.\nA session requires two parameters, a session template ID and an operation to perform on that template. The BOS API\u0026rsquo;s session endpoint can display a list of all of the sessions that have been created, including previous and currently running sessions. The endpoint can also display the details of a given session when the specific session ID is provided as a parameter. Sessions can also be deleted through the API.\nBOS supports a RESTful API. This API can be interacted with directly using tools like cURL. It can also be interacted with through the Cray Command Line Interface (CLI). See Manage a BOS Session for more information.\nBOA Functionality in Release 1.5 In release 1.5, BOA moves nodes towards the requested state, but if a node fails during any of the immediate steps, it takes note of it. BOA will then provide a command in the output of the BOA log that can be used to retry the action. This behavior impacts all BOS operations.\nA good example would be if there is a 6,000 node system and 3 nodes fail to power off during a BOS operation. BOA will continue and attempt to re-provision the remaining 5,997 nodes. After the command is finished, it will provide output information about what the administrator needs to do to retry the last 3 nodes that failed.\nCurrent BOS Session Limitations The following limitations currently exist with BOS sessions:\n No checking is done to prevent the launch of multiple sessions with overlapping lists of nodes. Concurrently running sessions may conflict with each other. The boot ordinal and shutdown ordinal are not honored. The partition parameter is not honored. The Configuration Framework Service (CFS) has its own limitations. Refer to the CFS documentation for more information.  "
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/stage_changes_without_bos/",
	"title": "Stage Changes Without BOS",
	"tags": [],
	"description": "",
	"content": "Stage Changes Without BOS Sometimes there is a need to stages changes to take place on a reboot, without immediately rebooting a node. When this is called for, users can bypass BOS, and set boot artifacts or configuration that will only take place when a node is later booted, whether that occurs manually, or triggered by a task manager.\nStage Boot Artifacts For information on staging boot artifacts, see the section Upload Node Boot Information to Boot Script Service (BSS).\nStage a Configuration   Disable CFS for all nodes receiving the staged configuration. Nodes will automatically re-enable configuration when they are rebooted and will be configured with any staged changes.\nncn-m001# cray cfs components update \u0026lt;xname\u0026gt; --enabled false   Either set the new desired configuration, or update the existing configuration.\nIf an entirely new configuration is being used, or if no configuration was previously set for a component, update the configuration name with the following:\nncn-m001# cray cfs components update \u0026lt;xname\u0026gt; --configuration-name \u0026lt;configuration_name\u0026gt; If all nodes that share a configuration are being staged with an update, updating the shared configuration will stage the change for all relevant nodes. Be aware that if this step is taken and not all nodes that use the configuration are disabled in CFS, the configuration will automatically and immediately apply to all enabled nodes that are using it.\nncn-m001# cray cfs configurations update \u0026lt;configuration_name\u0026gt; --file \u0026lt;file_path\u0026gt; Users also have the option of specifying branches rather than commits in configurations. If this feature is used, the configuration can also be updated by telling CFS to update the commits for all layers of a configuration that specify branches. Like with updating the configuration from a file, this will automatically start configuration on any enabled nodes that are using this configuration. For information on using branches, see the section (Use Branches in Configuration Layers\n  "
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/tools_for_resolving_boot_issues/",
	"title": "Tools For Resolving Compute Node Boot Issues",
	"tags": [],
	"description": "",
	"content": "Tools for Resolving Compute Node Boot Issues A number of tools can be used to analyze and debug issues encountered during the compute node boot process. The underlying issue and symptoms dictate the type of tool required.\nnmap Use nmap to send out DHCP Discover requests to test DHCP. nmap can be installed using the following command:\nncn-m001# zypper install nmap To reach the DHCP server, the request generally needs to be sent over the Node Management network (NMN) from the non-compute node (NCN).\nIn the following example, nmap is used to send a broadcast request over the eth1 interface:\nncn-m001# nmap --script broadcast-dhcp-discover -e eth1 Wireshark Use Wireshark to display network traffic. It has powerful display filters that help find information that can be used for debugging. To learn more, visit https://www.wireshark.org\ntcpdump Use tcpdump to capture network traffic, such as DHCP or TFTP requests. It can be installed using the following mechanisms:\n  Install tcpdump inside an Alpine-based pod:\nalpine_pod# apk add --no-cache tcpdump   Install tcpdump on an NCN or some other node that is running SUSE:\nncn-m001# zypper install tcpdump   Invoking tcpdump without any arguments will write all of its output to stdout. This is reasonable for some tasks, but the volume of traffic that tcpdump can capture is large, so it is often better to write the output to a file.\nUse the following command to send tcpdump output to stdout:\npod# tcpdump Use the following command to send tcpdump output to a file, such as a filed named tcpdump.output in the following example:\npod# tcpdump -w /tmp/tcpdump.output Use either tcpdump or Wireshark to read from the tcpdump file. Here is how to read the file using tcpdump:\npod# tcpdump -r /tmp/tcpdump.output Filtering the traffic using tcpdump filters is not recommended because when a TFTP server answers a client they will usually use an ephemeral port that the user may not be able to identify and will not be captured by tcpdump. It is better to capture everything with tcpdump and then filter with Wireshark when looking at the resulting output. Filtering on DHCP traffic can be performed because that uses ports 67 and 68 specifically.\nTFTP Client Use a TFTP client to send TFTP requests to the TFTP server. This will test that the server is functional. TFTP requests can be sent from the NCN, remote node, or laptop, as long as it targets the NMN.\nInstall the TFTP client using the following command:\nncn-m001# zypper install atftp The atftp TFTP client can be used to request files from the TFTP server. The TFTP server is on the NMN and listens on port 69. The TFTP server sends the ipxe.efi file as the response in this example.\nncn-m001# atftp ncn-m001# atftp tftp\u0026gt; connect 10.100.160.2 69 tftp\u0026gt; get ipxe.efi test-ipxe.efi tftp\u0026gt; quit ncn-m001# ls -l test-ipxe.efi -rw-r--r-- 1 root root 951904 Sep 11 10:44 test-ipxe.efi Serial Over Lan (SOL) Sessions There are two tools that can be used to access a BMC\u0026rsquo;s console via SOL:\n  ipmitool - ipmitool is a utility for controlling IPMI-enabled devices.\nUse the following command to access a node\u0026rsquo;s SOL via ipmitool:\nncn-m001# export USERNAME=root ncn-m001# export IPMI_PASSWORD=changeme ncn-m001# ipmitool -I lanplus -U $USERNAME -E \\ -H node_management_network_IP_address_of_node sol activate Example:\nncn-m001# ipmitool -I lanplus -U $USERNAME -E -H 10.100.165.2 sol activate   ConMan - The ConMan tool is used to collect logs from nodes. It is also used to attach to the node\u0026rsquo;s SOL console. For more information, refer to ConMan and Access Compute Node Logs.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/session_templates/",
	"title": "BOS Session Templates",
	"tags": [],
	"description": "",
	"content": "BOS Session Templates Describes the contents of a BOS session template.\nA session template can be created by specifying parameters as part of the call to the Boot Orchestration Service (BOS). When calling BOS directly, JSON is passed as part of the call.\nSession templates can be used to boot images that are customized with the Image Management Service (IMS). A session template has a collection of one or more boot set objects. A boot set defines a collection of nodes and the information about the boot artifacts and kernel parameters used to boot them. This information is written to the Boot Script Service (BSS) and sent to each node over the specified network, enabling these nodes to boot.\nThe Simple Storage Service (S3) is used to store the manifest.json file that is created by IMS. This file contains links to all of the boot artifacts. The following S3 parameters are used in a BOS session template:\n type: This is the type of storage used. Currently, the only allowable value is s3. path: This is the path to the manifest.json file in S3. The path will follow the s3://\u0026lt;BUCKET_NAME\u0026gt;/\u0026lt;KEY_NAME\u0026gt; format. etag: This entity tag helps identify the version of the manifest.json file. Currently not used but cannot be left blank.  The following is an example BOS session template:\n{ \u0026#34;cfs_url\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git\u0026#34;, \u0026lt;\u0026lt;-- Configuration manifest API endpoint \u0026#34;enable_cfs\u0026#34;: true, \u0026lt;\u0026lt;-- Invokes CFS \u0026#34;name\u0026#34;: \u0026#34;session-template-example\u0026#34;, \u0026#34;boot_sets\u0026#34;: { \u0026#34;boot_set1\u0026#34;: { \u0026#34;network\u0026#34;: \u0026#34;nmn\u0026#34;, \u0026#34;boot_ordinal\u0026#34;: 1, \u0026#34;kernel_parameters\u0026#34;: \u0026#34;console=ttyS0,115200 bad_page=panic crashkernel=360M hugepagelist=2m-2g intel_iommu=off intel_pstate=disable iommu=pt ip=dhcp numa_interleave_omit=headless numa_zonelist_order=node oops=panic pageblock_order=14 pcie_ports=native printk.synchronous=y rd.neednet=1 rd.retry=10 rd.shell k8s_gw=api-gw-service-nmn.local quiet turbo_boost_limit=999\u0026#34;, \u0026#34;rootfs_provider\u0026#34;: \u0026#34;cpss3\u0026#34;, \u0026#34;node_list\u0026#34;: [ \u0026lt;\u0026lt;-- List of individual nodes \u0026#34;x3000c0s19b1n0\u0026#34; ], \u0026#34;etag\u0026#34;: \u0026#34;foo\u0026#34;, \u0026lt;\u0026lt;-- Used to identify the version of the manifest.json file \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/e06530f1-fde2-4ca5-9148-7e84f4857d17/manifest_sans_boot_parameters.json\u0026#34;, \u0026lt;\u0026lt;-- The path to the manifest.json file in S3 \u0026#34;rootfs_provider_passthrough\u0026#34;: \u0026#34;66666666:dvs:api-gw-service-nmn.local:300:eth0\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; \u0026lt;\u0026lt;-- Type of storage } }, When multiple boot sets are used in a session template, the boot_ordinal and shutdown_ordinal values indicate the order in which boot sets need to be acted upon. Boot sets sharing the same ordinal number will be addressed at the same time.\nEach boot set needs its own set of S3 parameters (path, type, and optionally etag).\nSpecify Nodes in a BOS Session Template There are three different ways to specify the nodes inside a boot set in a BOS session template. The node list, node groups, or node role groups values can be used. Each can be specified as a comma separated list.\n  Node list\nThe \u0026quot;node_list\u0026quot; value is a list of nodes identified by xnames.\nFor example:\n\u0026#34;node_list\u0026#34;: [\u0026#34;x3000c0s19b1n0\u0026#34;, \u0026#34;x3000c0s19b1n1\u0026#34;, \u0026#34;x3000c0s19b2n0\u0026#34;]   Node groups\nThe \u0026quot;node_groups\u0026quot; value is a list of groups defined by the Hardware State Manager (HSM). Each group may contain zero or more nodes. Groups can be arbitrarily defined.\nFor example:\n\u0026#34;node_groups\u0026#34;: [\u0026#34;green\u0026#34;, \u0026#34;white\u0026#34;, \u0026#34;pink\u0026#34;]   Node roles groups\nThe node role groups is a list of groups based on a node\u0026rsquo;s designated role. Each node\u0026rsquo;s role is specified in the HSM database. For example, to target all of the nodes with a \u0026ldquo;Compute\u0026rdquo; role, \u0026ldquo;Compute\u0026rdquo; would need to be specified in the \u0026quot;node_role_groups\u0026quot; value.\nFor example:\n\u0026#34;node_roles_groups\u0026#34;: [\u0026#34;Compute\u0026#34;] The following roles are defined in the HSM database:\n Compute Service System Application Storage Management    "
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/manage_a_session_template/",
	"title": "Manage A Session Template",
	"tags": [],
	"description": "",
	"content": "Manage a Session Template A session template must be created before starting a session with the Boot Orchestration Service (BOS). Session templates are managed via the Cray CLI with the cray bos sessiontemplate commands.\nGet the Framework for a Session Template When creating a new BOS session template, it can be helpful to start with a framework and then edit it as needed. Use the following command to retrieve the BOS session template framework:\nncn-m001# cray bos sessiontemplatetemplate list --format json { \u0026#34;boot_sets\u0026#34;: { \u0026#34;name_your_boot_set\u0026#34;: { \u0026#34;boot_ordinal\u0026#34;: 1, \u0026#34;etag\u0026#34;: \u0026#34;your_boot_image_etag\u0026#34;, \u0026#34;kernel_parameters\u0026#34;: \u0026#34;your-kernel-parameters\u0026#34;, \u0026#34;network\u0026#34;: \u0026#34;nmn\u0026#34;, \u0026#34;node_list\u0026#34;: [ \u0026#34;xname1\u0026#34;, \u0026#34;xname2\u0026#34;, \u0026#34;xname3\u0026#34; ], \u0026#34;path\u0026#34;: \u0026#34;your-boot-path\u0026#34;, \u0026#34;rootfs_provider\u0026#34;: \u0026#34;your-rootfs-provider\u0026#34;, \u0026#34;rootfs_provider_passthrough\u0026#34;: \u0026#34;your-rootfs-provider-passthrough\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;your-boot-type\u0026#34; } }, \u0026#34;cfs\u0026#34;: { \u0026#34;configuration\u0026#34;: \u0026#34;desired-cfs-config\u0026#34; }, \u0026#34;enable_cfs\u0026#34;: true, \u0026#34;name\u0026#34;: \u0026#34;name-your-template\u0026#34; } Create a Session Template with the Cray CLI The following command takes a JSON input file that contains the information required to create a new BOS session template. It reads it in and creates an actual BOS session template using the BOS API.\nncn-m001# cray bos sessiontemplate create --file INPUT_FILE --name NEW_TEMPLATE_NAME The following is an example of an input file:\n{ \u0026#34;cfs_url\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git\u0026#34;, \u0026#34;enable_cfs\u0026#34;: true, \u0026#34;name\u0026#34;: \u0026#34;cle-1.2.0\u0026#34;, \u0026#34;boot_sets\u0026#34;: { \u0026#34;boot_set1\u0026#34;: { \u0026#34;network\u0026#34;: \u0026#34;nmn\u0026#34;, \u0026#34;boot_ordinal\u0026#34;: 1, \u0026#34;kernel_parameters\u0026#34;: \u0026#34;console=ttyS0,115200 bad_page=panic crashkernel=360M hugepagelist=2m-2g intel_iommu=off intel_pstate=disable iommu=pt ip=dhcp numa_interleave_omit=headless numa_zonelist_order=node oops=panic pageblock_order=14 pcie_ports=native printk.synchronous=y rd.neednet=1 rd.retry=10 rd.shell k8s_gw=api-gwservice-nmn.local quiet turbo_boost_limit=999\u0026#34;, \u0026#34;rootfs_provider\u0026#34;: \u0026#34;cpss3\u0026#34;, \u0026#34;node_list\u0026#34;: [ \u0026#34;x3000c0s19b1n0\u0026#34; ], \u0026#34;etag\u0026#34;: \u0026#34;90b2466ae8081c9a604fd6121f4c08b7\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/06901f40-f2a6-4a64-bc26-772a5cc9d321/manifest.json\u0026#34;, \u0026#34;rootfs_provider_passthrough\u0026#34;: \u0026#34;dvs:api-gw-service-nmn.local:300:eth0\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; } }, \u0026#34;partition\u0026#34;: \u0026#34;, \u0026#34;cfs_branch\u0026#34;: \u0026#34;master\u0026#34; } Create a Session Template with a Bash Script A BOS session template can also be generated with a shell script, which directly uses the BOS API. The following is an example script for creating a session template. The get_token function retrieves a token that validates the request to the API gateway. The values in the body section of the script can be customized when creating a new session template.\n#!/bin/bash # Up to date as of 2020-02-05 ADMIN_SECRET=$(kubectl get secrets admin-client-auth -ojsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d) TOKEN=$(curl -s -d grant_type=client_credentials \\  -d client_id=admin-client \\  -d client_secret=$ADMIN_SECRET \\  https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | python -c \u0026#39;import sys, json; print json.load(sys.stdin)[\u0026#34;access_token\u0026#34;]\u0026#39;) kernel_parameters=\u0026#34;console=ttyS0,115200 bad_page=panic crashkernel=360M hugepagelist=2m-2g \\ intel_iommu=off intel_pstate=disable iommu=pt ip=dhcp numa_interleave_omit=headless \\ numa_zonelist_order=node oops=panic pageblock_order=14 pcie_ports=native printk.synchronous=y \\ rd.neednet=1 rd.retry=10 rd.shell k8s_gw=api-gw-service-nmn.local quiet turbo_boost_limit=999\u0026#34; body=\u0026#39;{ \u0026#34;name\u0026#34;: \u0026#34;cle-1.2.0\u0026#34;, \u0026#34;boot_sets\u0026#34;: { \u0026#34;boot_set1\u0026#34;: { \u0026#34;boot_ordinal\u0026#34;: 1, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/06901f40-f2a6-4a64-bc26-772a5cc9d321/manifest.json\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34;, \u0026#34;etag\u0026#34;: \u0026#34;90b2466ae8081c9a604fd6121f4c08b7\u0026#34;, \u0026#34;node_list\u0026#34;: [\u0026#34;x3000c0s19b1n0\u0026#34;], \u0026#34;rootfs_provider\u0026#34;: \u0026#34;cpss3\u0026#34;, \u0026#34;rootfs_provider_passthrough\u0026#34;: \u0026#34;dvs:api-gw-service-nmn.local:300:eth0\u0026#34;, \u0026#34;kernel_parameters\u0026#34;: \u0026#34;\u0026#39;\u0026#34;$kernel_parameters\u0026#34;\u0026#39;\u0026#34;, \u0026#34;network\u0026#34;: \u0026#34;nmn\u0026#34; }}, \u0026#34;cfs_branch\u0026#34;: \u0026#34;master\u0026#34;, \u0026#34;cfs_url\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git\u0026#34;, \u0026#34;enable_cfs\u0026#34;: true, \u0026#34;partition\u0026#34;: \u0026#34; }\u0026#39; curl -i -X POST -s https://api-gw-service-nmn.local/apis/bos/v1/sessiontemplate \\  -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; \\  -H \u0026#34;Content-Type: application/json\u0026#34; \\  -d \u0026#34;$body\u0026#34; Either script above will generate the following session template:\nncn-m001# cray bos sessiontemplate describe session_template1 --format json { \u0026#34;cfs_url\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git\u0026#34;, \u0026#34;enable_cfs\u0026#34;: true, \u0026#34;name\u0026#34;: \u0026#34;cle-1.2.0\u0026#34;, \u0026#34;boot_sets\u0026#34;: { \u0026#34;boot_set1\u0026#34;: { \u0026#34;network\u0026#34;: \u0026#34;nmn\u0026#34;, \u0026#34;boot_ordinal\u0026#34;: 1, \u0026#34;kernel_parameters\u0026#34;: \u0026#34;console=ttyS0,115200 bad_page=panic crashkernel=360M hugepagelist=2m-2g intel_iommu=off intel_pstate=disable iommu=pt ip=dhcp numa_interleave_omit=headless numa_zonelist_order=node oops=panic pageblock_order=14 pcie_ports=native printk.synchr \u0026#34;rootfs_provider\u0026#34;: \u0026#34;cpss3\u0026#34;, \u0026#34;node_list\u0026#34;: [ \u0026#34;x3000c0s19b1n0\u0026#34; ], \u0026#34;etag\u0026#34;: \u0026#34;90b2466ae8081c9a604fd6121f4c08b7\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/06901f40-f2a6-4a64-bc26-772a5cc9d321/manifest.json\u0026#34;, \u0026#34;rootfs_provider_passthrough\u0026#34;: \u0026#34;dvs:api-gw-service-nmn.local:300:eth0\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; } }, \u0026#34;partition\u0026#34;: \u0026#34;, \u0026#34;cfs_branch\u0026#34;: \u0026#34;master\u0026#34; } List All Session Templates Use the following command to view all of the available session templates:\nncn-m001# cray bos sessiontemplate list --format json [ { \u0026#34;enable_cfs\u0026#34;: true, \u0026#34;description\u0026#34;: \u0026#34;Template for booting compute nodes, generated by the installation\u0026#34;, \u0026#34;boot_sets\u0026#34;: { \u0026#34;computes\u0026#34;: { \u0026#34;network\u0026#34;: \u0026#34;nmn\u0026#34;, \u0026#34;rootfs_provider\u0026#34;: \u0026#34;cpss3\u0026#34;, \u0026#34;boot_ordinal\u0026#34;: 1, \u0026#34;kernel_parameters\u0026#34;: \u0026#34;console=ttyS0,115200 bad_page=panic crashkernel=360M hugepagelist=2m-2g intel_iommu=off intel_pstate=disable iommu=pt ip=dhcp numa_interleave_omit=headless numa_zonelist_order=node oops=panic pageblock_order=14 pcie_ports=native printk.synchronous=y rd.neednet=1 rd.retry=10 rd.shell k8s_gw=api-gw-service-nmn.local quiet turbo_boost_limit=999\u0026#34;, \u0026#34;node_roles_groups\u0026#34;: [ \u0026#34;Compute\u0026#34; ], \u0026#34;etag\u0026#34;: \u0026#34;b0ace28163302e18b68cf04dd64f2e01\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/ef97d3c4-6f10-4d58-b4aa-7b70fcaf41ba/manifest.json\u0026#34;, \u0026#34;rootfs_provider_passthrough\u0026#34;: \u0026#34;dvs:api-gw-service-nmn.local:300:eth0\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; } }, \u0026#34;name\u0026#34;: \u0026#34;cle-1.2.0\u0026#34;, \u0026#34;cfs_branch\u0026#34;: \u0026#34;master\u0026#34;, \u0026#34;cfs_url\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git\u0026#34; } ] ... Show Details for a Session Template View the details for a specific session template. In the following example, the session template name is cle-1.2.0.\nncn-m001# cray bos sessiontemplate describe SESSION_TEMPLATE_NAME --format json { \u0026#34;cfs_url\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git\u0026#34;, \u0026#34;enable_cfs\u0026#34;: true, \u0026#34;description\u0026#34;: \u0026#34;Template for booting compute nodes, generated by the installation\u0026#34;, \u0026#34;boot_sets\u0026#34;: { \u0026#34;computes\u0026#34;: { \u0026#34;network\u0026#34;: \u0026#34;nmn\u0026#34;, \u0026#34;rootfs_provider\u0026#34;: \u0026#34;cpss3\u0026#34;, \u0026#34;boot_ordinal\u0026#34;: 1, \u0026#34;kernel_parameters\u0026#34;: \u0026#34;console=ttyS0,115200 bad_page=panic crashkernel=360M hugepagelist=2m-2g intel_iommu=off intel_pstate=disable iommu=pt ip=dhcp numa_interleave_omit=headless numa_zonelist_order=node oops=panic pageblock_order=14 pcie_ports=native printk.synchronous=y rd.neednet=1 rd.retry=10 rd.shell k8s_gw=api-gw-service-nmn.local quiet turbo_boost_limit=999\u0026#34;, \u0026#34;node_roles_groups\u0026#34;: [ \u0026#34;Compute\u0026#34; ], \u0026#34;etag\u0026#34;: \u0026#34;b0ace28163302e18b68cf04dd64f2e01\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/ef97d3c4-6f10-4d58-b4aa-7b70fcaf41ba/manifest.json\u0026#34;, \u0026#34;rootfs_provider_passthrough\u0026#34;: \u0026#34;dvs:api-gw-service-nmn.local:300:eth0\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; } }, \u0026#34;cfs_branch\u0026#34;: \u0026#34;master\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;cle-1.2.0\u0026#34; } Delete a Session Template Remove an existing session template with the following command:\nncn-m001# cray bos sessiontemplate delete SESSIONTEMPLATE_NAME "
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/node_boot_root_cause_analysis/",
	"title": "Node Boot Root Cause Analysis",
	"tags": [],
	"description": "",
	"content": "Node Boot Root Cause Analysis The first step in debugging compute node boot-related issues is to determine the underlying cause, and the stage that the issue was encountered at.\nThe ConMan tool collects compute node logs. To learn more about ConMan, refer to ConMan.\nA node\u0026rsquo;s console data can be accessed through its log file, as described in Access Compute Node Logs). This information can also be accessed by connecting to the node\u0026rsquo;s console with ipmitool. Refer to online documentation to learn more about using ipmitool.\n"
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/redeploy_the_ipxe_and_tftp_services/",
	"title": "Redeploy The IPXE And Tftp Services",
	"tags": [],
	"description": "",
	"content": "Redeploy the iPXE and TFTP Services Redeploy the iPXE and TFTP services if a pod with a ceph-fs Process Virtualization Service (PVS) on a Kubernetes worker node is causing a HEALTH_WARN error.\nResolve issues with ceph-fs and ceph-mds by restarting the iPXE and TFTP services. The Ceph cluster will return to a healthy state after this procedure.\nPrerequisites This procedure requires administrative privileges.\nProcedure   Find the iPXE and TFTP deployments.\nncn-m001# kubectl get deployments -n services|egrep \u0026#39;tftp|ipxe\u0026#39; cray-ipxe 1/1 1 1 22m cray-tftp 3/3 3 3 28m   Delete the deployments for the iPXE and TFTP services.\nncn-m001# kubectl -n services delete deployment cray-tftp ncn-m001# kubectl -n services delete deployment cray-ipxe   Check the status of Ceph.\nCeph commands need to be run on ncn-m001. If a health warning is shown after checking the status, the ceph-mds daemons will need to be restarted on the manager nodes.\n  Check the health of the Ceph cluster.\nncn-m001# ceph -s cluster: id: bac74735-d804-49f3-b920-cd615b18316b health: HEALTH_WARN 1 filesystem is degraded services: mon: 3 daemons, quorum ncn-m001,ncn-m002,ncn-m003 (age 13d) mgr: ncn-m001(active, since 24h), standbys: ncn-m002, ncn-m003 mds: cephfs:1/1 {0=ncn-m002=up:reconnect} 2 up:standby osd: 60 osds: 60 up (since 4d), 60 in (since 4d) rgw: 5 daemons active (ncn-s001.rgw0, ncn-s002.rgw0, ncn-s003.rgw0, ncn-s004.rgw0, ncn-s005.rgw0) data: pools: 13 pools, 1664 pgs objects: 2.47M objects, 9.3 TiB usage: 26 TiB used, 78 TiB / 105 TiB avail pgs: 1664 active+clean io: client: 990 MiB/s rd, 111 MiB/s wr, 2.76k op/s rd, 1.03k op/s wr   Obtain more information on the health of the cluster.\nncn-m001# ceph health detail HEALTH_WARN 1 filesystem is degraded FS_DEGRADED 1 filesystem is degraded fs cephfs is degraded   Show the status of all CephFS components.\nncn-m001# ceph fs status cephfs - 9 clients ====== +------+-----------+----------+----------+-------+-------+ | Rank | State | MDS | Activity | dns | inos | +------+-----------+----------+----------+-------+-------+ | 0 | reconnect | ncn-m002 | | 11.0k | 74 | +------+-----------+----------+----------+-------+-------+ +-----------------+----------+-------+-------+ | Pool | type | used | avail | +-----------------+----------+-------+-------+ | cephfs_metadata | metadata | 780M | 20.7T | | cephfs_data | data | 150M | 20.7T | +-----------------+----------+-------+-------+ +-------------+ | Standby MDS | +-------------+ | ncn-m003 | | ncn-m001 |   Restart the ceph-mds service.\nThis step should only be done if a health warning is shown in the previous substeps.\nncn-m001# for i in 1 2 3 ; do ansible ncn-m00$i -m shell -a \u0026#34;systemctl restart ceph-mds@ncn-m00$i\u0026#34;; done     Failover the ceph-mds daemon.\nThis step should only be done if a health warning still exists after restarting the ceph-mds service.\nncn-m001# ceph mds fail ncn-m002 The initial output will display the following:\ncephfs - 0 clients ====== +------+--------+----------+----------+-------+-------+ | Rank | State | MDS | Activity | dns | inos | +------+--------+----------+----------+-------+-------+ | 0 | **rejoin** | ncn-m003 | | 0 | 0 | +------+--------+----------+----------+-------+-------+ +-----------------+----------+-------+-------+ | Pool | type | used | avail | +-----------------+----------+-------+-------+ | cephfs_metadata | metadata | 781M | 20.7T | | cephfs_data | data | 117M | 20.7T | +-----------------+----------+-------+-------+ +-------------+ | Standby MDS | +-------------+ | ncn-m002 | | ncn-m001 | +-------------+ The rejoin status should turn to active:\ncephfs - 7 clients ====== +------+--------+----------+---------------+-------+-------+ | Rank | State | MDS | Activity | dns | inos | +------+--------+----------+---------------+-------+-------+ | 0 | **active** | ncn-m003 | Reqs: 0 /s | 11.1k | 193 | +------+--------+----------+---------------+-------+-------+ +-----------------+----------+-------+-------+ | Pool | type | used | avail | +-----------------+----------+-------+-------+ | cephfs_metadata | metadata | 781M | 20.7T | | cephfs_data | data | 117M | 20.7T | +-----------------+----------+-------+-------+ +-------------+ | Standby MDS | +-------------+ | ncn-m002 | | ncn-m001 | +-------------+   Ensure the service is deleted along with the associated PVC.\nThe output for the command below should empty. If an output is displayed, such as in the example below, then the resources have not been deleted.\nncn-m001# kubectl get pvc -n services|grep tftp cray-tftp-shared-pvc Bound pvc-315d08b0-4d00-11ea-ad9d-b42e993b7096 5Gi RWX ceph-cephfs-external 29m Optional: Use the command below to delete the associated PVC.\nncn-m001# kubectl -n services delete pvc PVC_NAME   Deploy the TFTP service.\nWait for the TFTP pods to come online and verify the PVC was created.\nncn-m001# loftsman helm upgrade cray-tftp loftsman/cray-tftp   Deploy the iPXE service.\nThis may take a couple of minutes and may show up in error state. Wait a couple minutes and it will go to running.\nncn-m001# loftsman helm upgrade cms-ipxe loftsman/cms-ipxe   Log into the iPXE pod and verify the iPXE file was created.\nThis may take another couple of minutes while it is creating the files.\n  Find the iPXE pod ID.\nncn-m001# kubectl get pods -n services --no-headers -o wide | grep cray-ipxe | awk \u0026#39;{print $1}\u0026#39; cray-ipxe-759f95fcf5-mdbv8   Log into the pod using the iPXE pod ID.\nncn-m001# kubectl exec -n services -it IPXE_POD_ID /bin/sh To see the containers in the pod:\nncn-m001# kubectl describe pod/CRAY-IPXE_POD_NAME -n services     Log into the TFTP pods and verify it is seeing the correct file size.\n  Find the TFTP pod ID.\nncn-m001# kubectl get pods -n services --no-headers -o wide | grep cray-tftp | awk \u0026#39;{print $1}\u0026#39; cray-tftp-7dc77f9cdc-bn6ml cray-tftp-7dc77f9cdc-ffgnh cray-tftp-7dc77f9cdc-mr6zd cray-tftp-modprobe-42648 cray-tftp-modprobe-4kmqg cray-tftp-modprobe-4sqsk cray-tftp-modprobe-hlfcc cray-tftp-modprobe-r6bvb cray-tftp-modprobe-v2txr   Log into the pod using the TFTP pod ID.\nncn-m001# kubectl exec -n services -it TFTP_POD_ID /bin/sh   Change to the /var/lib/tftpboot directory.\n# cd /var/lib/tftpboot   Check the ipxe.efi size on the TFTP servers.\nIf there are any issues, the file will have a size of 0 bytes.\n# ls -l total 1919 -rw-r--r-- 1 root root 980768 May 15 16:49 debug.efi -rw-r--r-- 1 root root 983776 May 15 16:50 ipxe.efi     "
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/limitations_for_gigabyte_bmc_hardware/",
	"title": "BOS Limitations For Gigabyte BMC Hardware",
	"tags": [],
	"description": "",
	"content": "BOS Limitations for Gigabyte BMC Hardware Special steps need to be taken when using the Boot Orchestration Service (BOS) to boot, reboot, shutdown, or configure Gigabyte hardware. Gigabyte hardware treats power off and power on requests as successful, regardless of if actually successfully completed. The power on/off requests are ignored by Cray Advanced Platform Monitoring and Control (CAPMC) if they are received within a short period of time, which is typically around 60 seconds per operation.\nThe work around for customers with Gigabyte BMC hardware is to manually serialize power off events. This is done to prevent frequent power actions from being attempted and ignored by CAPMC. From a boot orchestration perspective, this can be effectively worked around by issuing CAPMC power off commands before issuing BOS reboot commands.\n"
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/log_file_locations_and_ports_used_in_compute_node_boot_troubleshooting/",
	"title": "Log File Locations And Ports Used In Compute Node Boot Troubleshooting",
	"tags": [],
	"description": "",
	"content": "Log File Locations and Ports Used in Compute Node Boot Troubleshooting This section includes the port IDs and log file locations of components associated with the node boot process.\nLog File Locations The log file locations for ConMan, DHCP, and TFTP.\n  ConMan logs are located within the conman pod at /var/log/conman.log.\n  DHCP:\nncn-m001# kubectl logs DHCP_POD_ID   TFTP:\nncn-m001# kubectl logs -n services TFTP_POD_ID   Port IDs The following table includes the port IDs for DHCP and TFTP.\n   Component Port     DHCP server 67   DHCP client 68   TFTP server 69    "
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/manage_a_bos_session/",
	"title": "Manage A BOS Session",
	"tags": [],
	"description": "",
	"content": "Manage a BOS Session Once there is a Boot Orchestration Service (BOS) session template created, users can perform operations on nodes, such as boot, reboot, configure, and shutdown. Managing sessions through the Cray CLI can be accomplished using the cray bos session commands.\nCreate a New Session Creating a new BOS session requires the following command-line options:\n --template-uuid: Use this option to specify the name value returned in the cray bos sessiontemplate list command. --operation: Use this option to indicate if a boot, reboot, configure, or shutdown action is being taken.  The following is an example of a boot operation:\nncn-m001# cray bos session create --template-uuid SESSIONTEMPLATE_NAME --operation Boot operation = \u0026#34;Boot\u0026#34; templateUuid = \u0026#34;TEMPLATE_UUID\u0026#34; [[links]] href = \u0026#34;foo-c7faa704-3f98-4c91-bdfb-e377a184ab4f\u0026#34; jobId = \u0026#34;boa-a939bd32-9d27-433f-afc2-735e77ec8e58\u0026#34; rel = \u0026#34;session\u0026#34; type = \u0026#34;GET\u0026#34; List all Sessions List all existing BOS sessions with the following command:\nncn-m001# cray bos session list results = [ \u0026#34;fc469e41-6419-4367-a571-d5fd92893398\u0026#34;, \u0026#34;st3-d6730dd5-f0f8-4229-b224-24df005cae52\u0026#34;,] Show Details for a Session Get details for a BOS session using the session ID returned in the cray bos session list command output.\nncn-m001# cray bos session describe BOS_SESSION_JOB_ID computes = \u0026quot;boot_finished\u0026quot; boa_finish = \u0026quot;2019-12-13 17:07:23.501674\u0026quot; bos_launch = \u0026quot;2019-12-13 17:02:24.000324\u0026quot; operation = \u0026quot;reboot\u0026quot; session_template_id = \u0026quot;cle-1.1.0\u0026quot; boa_launch = \u0026quot;2019-12-13 17:02:29.703310\u0026quot; stage = \u0026quot;Done\u0026quot; Troubleshooting: There is a known issue in BOS where some sessions cannot be described using the cray bos session describe command. The issue with the describe action results in a 404 error, despite the session existing in the output of cray bos session list command.\nDelete a Session Delete a specific BOS session:\nncn-m001# cray bos session delete BOS_SESSION_JOB_ID "
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/edit_the_ipxe_embedded_boot_script/",
	"title": "Edit The IPXE Embedded Boot Script",
	"tags": [],
	"description": "",
	"content": "Edit the iPXE Embedded Boot Script Manually adjust the iPXE embedded boot script to change the order of network interfaces for DHCP request. Changing the order of network interfaces for DHCP requests helps improve boot time performance.\nPrerequisites This procedure requires administrative privileges.\nProcedure   Edit the ConfigMap using one of the following options.\nNOTE: Save a backup of the ConfigMap before making any changes.\nThe following is an example of creating a backup:\nncn-m001# kubectl get configmap -n services cray-ipxe-bss-ipxe \\ -o yaml \u0026gt; /root/k8s/cray-ipxe-bss-ipxe-backup.yaml Administrators can add, remove, or reorder sections in the ConfigMap related to the interface being used.\nIn the following example, the net2 section is located before the net0 section. If an administrator wants net0 to be run first, they could move the net0 section to be located before the net2 section.\n:net2 dhcp net2 || goto net2_stop echo net2 IPv4 lease: ${ip} mac: ${net2/mac} chain --timeout 10000 https://api-gw-service-nmn.local/apis/bss/boot/v1/bootscript?mac=${net2/mac} || echo Failed to retrieve next chain from Boot Script Service over net2 (https://api-gw-service-nmn.local/apis/bss/boot/v1/bootscript?mac=${net2/mac} \u0026amp;\u0026amp; goto net2_stop :net2_stop ifclose net2 || echo No routes to drop. :net0 dhcp net0 || goto net0_stop echo net0 IPv4 lease: ${ip} mac: ${net0/mac} chain --timeout 10000 https://api-gw-service-nmn.local/apis/bss/boot/v1/bootscript?mac=${net0/mac} || echo Failed to retrieve next chain from Boot Script Service over net0 (https://api-gw-service-nmn.local/apis/bss/boot/v1/bootscript?mac=${net0/mac} \u0026amp;\u0026amp; goto net0_stop :net0_stop ifclose net0 || echo No routes to drop.   Option 1: Edit the cray-ipxe-bss-ipxe ConfigMap directly.\nncn-m001# kubectl edit configmap -n services cray-ipxe-bss-ipxe   Option 2: Edit the ConfigMap by saving the file, editing it, and reloading the ConfigMap.\n  Save the file.\nncn-m001# kubectl get configmap -n services cray-ipxe-bss-ipxe \\ -o yaml \u0026gt; /root/k8s/cray-ipxe-bss-ipxe.yaml   Edit the cray-ipxe-bss-ipxe.yaml file.\nncn-m001# vi /root/k8s/cray-ipxe-bss-ipxe.yaml   Reload the ConfigMap.\nDeleting and recreating the ConfigMap will reload it.\nncn-m001# kubectl delete configmap -n services cray-ipxe-bss-ipxe ncn-m001# kubectl create configmap -n services cray-ipxe-bss-ipxe \\ --from-file=/root/k8s/cray-ipxe-bss-ipxe.yaml       Delete the iPXE pod to ensure the updated ConfigMap will be used.\n  Find the pod ID.\nncn-m001# kubectl -n services get pods|grep cray-ipxe cray-ipxe-5dddfc65f-qfmrr 2/2 Running 2 39h   Delete the pod.\nReplace CRAY-IPXE_POD_ID with the value returned in the previous step. In this example, the pod ID is cray-ipxe-5dddfc65f-qfmrr.\nncn-m001# kubectl -n services delete pod CRAY-IPXE_POD_ID     Wait about 30 seconds for the iPXE binary to be regenerated, and then the nodes will pick up the new ipxe.efi binary.\n"
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/healthy_compute_node_boot_process/",
	"title": "Healthy Compute Node Boot Process",
	"tags": [],
	"description": "",
	"content": "Healthy Compute Node Boot Process In order to investigate node boot-related issues, it is important to understand the flow of a healthy boot process and the associated components. This section outlines the normal flow of components that play a role in booting compute nodes, including DHCP, BSS, and TPTP.\nDHCP A healthy DHCP exchange between server and client looks like the following:\n   Traffic Description Sender     DHCP Discover A broadcast request from the client requesting an IP address. The request contains the client\u0026rsquo;s MAC address. Client   DHCP Offer The server offers an IP address to the client. Server   DHCP Request After testing the IP address to see that it is not in use, the client requests the proffered IP address. Client   DHCP ACK The server acknowledges that the client owns the lease on the IP address. Server    The following figure shows what a healthy DHCP discover process looks like via Wireshark, which is a packet analyzer:\nThe DHCP client uses port 68, whereas the DHCP server uses port 67. Unlike most Kubernetes pods, the DHCP pod is located on the host network.\nTFTP A healthy TFTP exchange between server and client looks like the following.\n   Traffic Description Sender     Read Request File: filename tsize=0 The client requests a file with a tsize equal to zero. Client   Option Acknowledgement The server acknowledges the request and provides the file\u0026rsquo;s size and block transfer size. Server   Error Code, Code: Option negotiation failed, Message: User aborted the transfer The client aborts the transfer once it determines the file size. Client   Read Request File: filename The client requests the file again. Client   Option Acknowledgement The server acknowledges the request and provides the block transfer size. Server   Acknowledgement, Block: 0 The client acknowledges the server. Client   Data Packet, Block: 1 The server sends the first data packet. Server   Acknowledgement, Block: 1 The client acknowledges reception of block 1. Client    The last two steps repeat until the file transfer is complete. The last block from the server will be labeled as (Last). The TFTP server listens on port 69. Kubernetes forwards port 69 on every node in the Kubernetes cluster to the TFTP pod.\nBoot Script Service (BSS) A healthy transaction with the Boot Script Service (BSS) looks similar to the following:\nncn-m001# cray bss bootscript list --mac a4:bf:01:3e:c0:a2 #!ipxe kernel --name kernel http://rgw.local:8080/boot-images/29c2cc23-a9d6-4e9a-ab1a-b5fa9270c975/kernel?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026amp;X-Amz-Credential=5RN45WD0L8KY8W4317WP%2F20200326%2Fdefault%2Fs3%2Faws4_request\u0026amp;X-Amz-Date=20200326T185958Z\u0026amp;X-Amz-Expires=86400\u0026amp;X-Amz-SignedHeaders=host\u0026amp;X-Amz-Signature=43f5b0c5909ee51dabc564d2b72401983ff8fd03cc6fc309b04cb16e67f1989d initrd=initrd console=ttyS0,115200 bad_page=panic crashkernel=360M hugepagelist=2m-2g intel_iommu=off intel_pstate=disable iommu=pt ip=dhcp numa_interleave_omit=headless numa_zonelist_order=node oops=panic pageblock_order=14 pcie_ports=native printk.synchronous=y rd.neednet=1 rd.retry=10 rd.shell k8s_gw=api-gw-service-nmn.local quiet turbo_boost_limit=999 root=craycps-s3:s3://boot-images/29c2cc23-a9d6-4e9a-ab1a-b5fa9270c975/rootfs:8c274aecef9e1668a8a44e8cfc2b24b5-165:dvs:api-gw-service-nmn.local:300:eth0 xname=x3000c0s17b4n0 nid=4 || goto boot_retry initrd --name initrd http://rgw.local:8080/boot-images/29c2cc23-a9d6-4e9a-ab1a-b5fa9270c975/initrd?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026amp;X-Amz-Credential=5RN45WD0L8KY8W4317WP%2F20200326%2Fdefault%2Fs3%2Faws4_request\u0026amp;X-Amz-Date=20200326T185958Z\u0026amp;X-Amz-Expires=86400\u0026amp;X-Amz-SignedHeaders=host\u0026amp;X-Amz-Signature=d18f8da89108b9f2e659d7bbefcd106d5f13703a59f8ca837bcbc5938a9f9cc5 || goto boot_retry boot || goto boot_retry :boot_retry sleep 30 chain https://api-gw-service-nmn.local/apis/bss/boot/v1/bootscript?mac=a4:bf:01:3e:f9:28\u0026amp;retry=1 "
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/kernel_boot_parameters/",
	"title": "Kernel Boot Parameters",
	"tags": [],
	"description": "",
	"content": "Kernel Boot Parameters The Image Management Service (IMS) extracts kernel boot parameters from the /boot/kernel-parameters file in the image, if that file exists, and stores them in S3. IMS already stores the other boot artifacts (kernel, initrd, and rootfs) in S3. When told to boot an image, the Boot Orchestration Service (BOS) will extract these parameters and deliver them to the Boot Script Service (BSS) so they can be used during the next boot of a node.\nThere are two benefits to having kernel boot parameters extracted from the image. First, these parameters can be tightly coupled to the image. Second, these parameters do not need to be specified in the BOS session template, making the template shorter, cleaner, and less error prone.\nThe kernel boot parameters obtained from the image can be overridden by specifying the same parameters in the BOS session template. BOS supplies these parameters to the kernel in a deliberate order that causes the parameters obtained from the image to be overridden by those obtained from the session template.\nThe following is a simplified kernel boot parameter ordering:\n\u0026lt;Image parameters\u0026gt; \u0026lt;Session template parameters\u0026gt; If there are competing values, the ones earlier in the boot string are superseded by the ones appearing later in the string.\nThe actual contents of the boot parameters are not as simple as previously described. For completeness, the following is the entire kernel boot parameter ordering:\n\u0026lt;Image parameters\u0026gt; \u0026lt;Session template parameters\u0026gt; \u0026lt;rootfs parameters\u0026gt; \u0026lt;rootfs passthrough parameters\u0026gt; \u0026lt;BOS session id\u0026gt; "
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/limit_the_scope_of_a_bos_session/",
	"title": "Limit The Scope Of A BOS Session",
	"tags": [],
	"description": "",
	"content": "Limit the Scope of a BOS Session The Boot Orchestration Service (BOS) supports an optional \u0026ndash;limit parameter when creating a session. This parameter can be used to further limit the nodes that BOS runs against, and is applied to all boot sets.\nThe --limit parameter takes a comma-separated list of nodes, groups, or roles in any combination. The BOS session will be limited to run against components that match both the boot set information and one or more of the nodes, groups, or roles listed in the limit.\nThe table below describes the operations that can be used to further limit the scope of a BOS session. Components are treated as OR operations unless preceded by one of the operations listed in the following table.\n   Operation Description     \u0026amp; Added to the beginning of a group or role to specify an intersection of groups.   ! Added to the beginning of a node, group, or role to exclude it.   all When only trying to exclude a node or group, the limit must start with \u0026ldquo;all\u0026rdquo;.    The table below helps demonstrate the logic used with the \u0026ndash;limit parameter and includes examples of how to limit against different nodes, groups, and roles.\n   Description Pattern Targets     All nodes all (or leave empty) All nodes   One node node1 node1   Multiple nodes node1,node2 node1 and node2   Excluding a node all,!node1 All nodes except node1   One group group1 Nodes in group1   Multiple groups group1,group2 Nodes in group1 or group2   Excluding groups group1,!group2 Nodes in group1 but not in group2   Intersection of groups group1,\u0026amp;group2 Nodes in both group1 and group2    The --limit parameter for BOS works similarly to the --ansible-limit parameter for CFS, as well as the --limit parameter for Ansible. Some limitations do apply for those familiar with the Ansible syntax. BOS accepts only a comma-separated list, not colons, and does not support regex in the patterns.\n"
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/compute_node_boot_sequence/",
	"title": "Compute Node Boot Sequence",
	"tags": [],
	"description": "",
	"content": "Compute Node Boot Sequence Provides an overview of the compute node boot process and touches upon the fact that issues can be encountered during this process.\nThe following is a high-level overview of the boot sequence for compute nodes:\n  The compute node is powered on.\n  The BIOS issues a DHCP discover request.\n  DHCP responds with the following:\n next-server, which is the IP address of the TFTP server. The name of the file to download from the TFTP server.    The node\u0026rsquo;s PXE sends a TFTP request to the TFTP server.\n  If the TFTP server has the requested file, it sends it to the node\u0026rsquo;s PXE. In this case, the file name is ipxe.efi, which is a Cray-crafted iPXE binary that points at the Boot Script Service (BSS). The BSS will then serve up another iPXE boot script.\n  The ipxe.efi file downloads another ipxe boot script from BSS.\nThis script provides information for downloading:\n The location of kernel. The location of initrd. A string containing the kernel parameters.    The node attempts to download kernel and initrd boot artifacts. If successful, it will boot using these and the kernel parameters. Otherwise, it will retry to download these boot artifacts indefinitely.\n  There may be times when certain issues may be encountered during the compute node boot up process. In order to resolve these issues, it is important to understand the underlying cause, symptoms, and stage at which the issue has occurred. The exact process and tools required to resolve the issue depends on this information.\n"
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/configure_the_bos_timeout_when_booting_nodes/",
	"title": "Configure The BOS Timeout When Booting Compute Nodes",
	"tags": [],
	"description": "",
	"content": "Configure the BOS Timeout When Booting Compute Nodes Manually update the boa-job-template ConfigMap to tune the timeout and sleep intervals for the Boot Orchestration Agent (BOA). Correcting the timeout value is a good troubleshooting option for when BOS sessions hang waiting for nodes to be in a Ready state.\nIf the BOS timeout occurs when booting compute nodes, the system will be unable to boot via BOS.\nPrerequisites A Boot Orchestration Service (BOS) session was run and compute nodes are failing to move to a Ready state.\nProcedure   Edit the boa-job-template ConfigMap to add the new timeout values.\nncn-m001# kubectl edit configmap -n services boa-job-template Node boots can be set to time out faster by adding the following environment variables to the boa-job-template. These variables do not appear in the ConfigMap by default.\n  NODE_STATE_CHECK_NUMBER_OF_RETRIES\nBOA will check on the expected state of nodes this many times before giving up. This number can be set to a very low number to make BOA time-out quickly.\n  NODE_STATE_CHECK_SLEEP_INTERVAL\nThis is how long BOA will sleep between checks. This number can be set to a very low number to make BOA time-out quickly.\n  The current default behavior in the absence of these parameters is 5 seconds (sleep interval) x 120 (retries), which has a timeout of 600 seconds or 10 minutes. The default values are shown below:\n- name: \u0026#34;NODE_STATE_CHECK_NUMBER_OF_RETRIES\u0026#34; value: \u0026#34;120\u0026#34; - name: \u0026#34;NODE_STATE_CHECK_SLEEP_INTERVAL\u0026#34; value: \u0026#34;5\u0026#34; The example below increases the number of retries to 360, which results in a timeout of 1800 seconds or 30 minutes if the sleep interval is not changed from the default value of 5 seconds. Different values might be needed depending on system size.\nAdd the following values to the ConfigMap:\n- name: \u0026#34;NODE_STATE_CHECK_NUMBER_OF_RETRIES\u0026#34; value: \u0026#34;360\u0026#34; The new variables need to be placed under the environment (env:) section in the ConfigMap. As an example, the env section in the ConfigMap looks as below.\nenv: - name: OPERATION value: \u0026#34;{{ operation }}\u0026#34; - name: SESSION_ID value: \u0026#34;{{ session_id }}\u0026#34; - name: SESSION_TEMPLATE_ID value: \u0026#34;{{ session_template_id }}\u0026#34; - name: SESSION_LIMIT value: \u0026#34;{{ session_limit }}\u0026#34; - name: DATABASE_NAME value: \u0026#34;{{ DATABASE_NAME }}\u0026#34; - name: DATABASE_PORT value: \u0026#34;{{ DATABASE_PORT }}\u0026#34; - name: LOG_LEVEL value: \u0026#34;{{ log_level }}\u0026#34; - name: SINGLE_THREAD_MODE value: \u0026#34;{{ single_thread_mode }}\u0026#34; - name: S3_ACCESS_KEY valueFrom: secretKeyRef: name: {{ s3_credentials }} key: access_key - name: S3_SECRET_KEY valueFrom: secretKeyRef: name: {{ s3_credentials }} key: secret_key - name: GIT_SSL_CAINFO value: /etc/cray/ca/certificate_authority.crt - name: S3_PROTOCOL value: \u0026#34;{{ S3_PROTOCOL }}\u0026#34; - name: S3_GATEWAY value: \u0026#34;{{ S3_GATEWAY }}\u0026#34; **- name: \u0026#34;NODE_STATE_CHECK_NUMBER_OF_RETRIES\u0026#34; value: \u0026#34;360\u0026#34;**   Restart BOA.\nRestarting BOA will allow the new timeout values to take effect.\nncn-m001# kubectl scale deployment -n services cray-bos --replicas=0 ncn-m001# kubectl scale deployment -n services cray-bos --replicas=1   "
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/create_a_session_template_to_boot_compute_nodes_with_cps/",
	"title": "Create A Session Template To Boot Compute Nodes With Cps",
	"tags": [],
	"description": "",
	"content": "Create a Session Template to Boot Compute Nodes with CPS When compute nodes are booted, the Content Projection Service (CPS) and Data Virtualization Service (DVS) project the root file system (rootfs) over the network to the compute nodes by default.\nAnother option when compute nodes are booted is to download their rootfs into RAM.\nProcedure   Use either the cray bos session create CLI command or the bash script to create a session template.\nRefer to Manage a Session Template for more information about creating a session template.\nThe Simple Storage Service (S3) parameters that are used in the scripts are shown below:\n  type: Set to s3\n  path: Set to s3://\u0026lt;BUCKET_NAME\u0026gt;/\u0026lt;KEY_NAME\u0026gt;\n  etag: Set to \u0026lt;etag\\\u0026gt; The following values need to be set below to make CPS the rootfs provider:\n  \u0026quot;rootfs_provider\u0026quot;: is set to \u0026quot;cpss3\u0026quot;\n  \u0026quot;rootfs_provider_passthrough\u0026quot;: is set to \u0026quot;dvs:api-gw-service-nmn.local:300:eth0\u0026quot; The Content Projection Service (CPS) is an optional provider for rootfs on compute nodes. The rootfs_provider_passthrough parameter is customized according to the following format:\n  rootfs_provider_passthrough=\u0026lt;transport\u0026gt;:\u0026lt;api_gateway\u0026gt;:\u0026lt;timeout\u0026gt;:interface[,\u0026lt;interface\u0026gt;[,\u0026lt;interface\u0026gt;]...]:\u0026lt;ramroot\u0026gt; The variables used in this parameter represent the following:\n  \u0026lt;transport\u0026gt;\nFile system network transport (For example, NFS and DVS).\nCan be left as an empty string to use the default value dvs.\n  \u0026lt;api_gateway\u0026gt;\nName or address of the Kubernetes API gateway.\nCan be left as an empty string to use the default value api-gw-service-nmn.local.\n  \u0026lt;timeout\u0026gt;\nThe timeout, in seconds, for attempting to mount the netroot via CPS.\nCan be left as an empty string to use the default value of 300 seconds.\n  \u0026lt;etag\u0026gt;\nLists the syntax in use. BOS fills in the s3-path and etag values, so the user does not need to fill in any data.\n  interface[,\u0026lt;interface\u0026gt;[,\u0026lt;interface\u0026gt;]\u0026hellip;]\nA comma separated list of interfaces to support. A minimum of one interface must be specified.\nThe first interface specified must exist on the node or the module will exit with an error. Any other specified interface that is not found on the node will be ignored. The module will wait until all specified and existing interfaces are up before proceeding with boot. The first interface specified will be passed to the CPS mount command to identify the interface to be used for mounting.\n  \u0026lt;ramroot\u0026gt;\nIndicates that the specified S3 path should be copied to RAM (tmpfs) and mounted locally instead of persisting as a remote file system mount.\nCan be left empty. Any string except \u0026ldquo;0\u0026rdquo; is interpreted as True.\n  For example:\nrootfs_provider_passthrough=dvs:api-gw-service-nmn.local:300:eth0 BOS will construct the root= kernel parameter, which will be used by the node when it boots, based on the rootfs_provider and rootfs_provider_passthrough values.\nFor CPS, BOS supplies a protocol craycps-s3, the S3 path to the rootfs, and the etag value (if it exists). The rest of the parameters are supplied from the rootfs_provider_passthrough values as specified above.\nBOS will construct it in the following format:\nroot=craycps-s3:s3-path:\u0026lt;etag\u0026gt;:\u0026lt;transport\u0026gt;:\u0026lt;api_gateway\u0026gt;:\u0026lt;timeout\u0026gt;:interface[,\u0026lt;interface\u0026gt;[,\u0026lt;interface\u0026gt;]...]:\u0026lt;ramroot\u0026gt; The following is an example of an input file to use with the Cray CLI:\n{ \u0026#34;enable_cfs\u0026#34;: true, \u0026#34;description\u0026#34;: \u0026#34;Template for booting compute nodes, generated by the installation\u0026#34;, \u0026#34;boot_sets\u0026#34;: { \u0026#34;computes\u0026#34;: { \u0026#34;network\u0026#34;: \u0026#34;nmn\u0026#34;, \u0026#34;rootfs_provider\u0026#34;: \u0026#34;cpss3\u0026#34;, \u0026#34;boot_ordinal\u0026#34;: 1, \u0026#34;kernel_parameters\u0026#34;: \u0026#34;console=ttyS0,115200 bad_page=panic crashkernel=360M hugepagelist=2m-2g intel_iommu=off intel_pstate=disable iommu=pt ip=dhcp numa_interleave_omit=headless numa_zonelist_order=node oops=panic pageblock_order=14 pcie_ports=native printk.synchronous=y rd.neednet=1 rd.retry=10 rd.shell k8s_gw=api-gw-service-nmn.local quiet turbo_boost_limit=999\u0026#34;, \u0026#34;node_roles_groups\u0026#34;: [ \u0026#34;Compute\u0026#34; ], \u0026#34;etag\u0026#34;: \u0026#34;b0ace28163302e18b68cf04dd64f2e01\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/ef97d3c4-6f10-4d58-b4aa-7b70fcaf41ba/manifest.json\u0026#34;, \u0026#34;rootfs_provider_passthrough\u0026#34;: \u0026#34;dvs:api-gw-service-nmn.local:300:eth0\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34; } }, \u0026#34;name\u0026#34;: \u0026#34;cps_rootfs_template\u0026#34;, \u0026#34;cfs_branch\u0026#34;: \u0026#34;master\u0026#34;, \u0026#34;cfs_url\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git\u0026#34; } Or use a bash script to setup a session template using the BOS API.\n#!/bin/bash function get_token () { ADMIN_SECRET=$(kubectl get secrets admin-client-auth -ojsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d) curl -s -d grant_type=client_credentials -d client_id=admin-client -d client_secret=$ADMIN_SECRET \\  https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | python3 -c \u0026#39;import sys, json; print(json.load(sys.stdin)[\u0026#34;access_token\u0026#34;])\u0026#39; } body=\u0026#39; { \u0026#34;name\u0026#34;: \u0026#34;st1\u0026#34;, \u0026#34;boot_sets\u0026#34;: { \u0026#34;boot_set1\u0026#34;: { \u0026#34;boot_ordinal\u0026#34;: 1, \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34;, \u0026#34;etag\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;s3://boot-images/ef97d3c4-6f10-4d58-b4aa-7b70fcaf41ba/manifest.json\u0026#34;, \u0026#34;node_roles_groups\u0026#34;: [\u0026#34;Compute\u0026#34;], \u0026#34;node_list\u0026#34;: [\u0026#34;], \u0026#34;rootfs_provider\u0026#34;: \u0026#34;cps\u0026#34;, \u0026#34;rootfs_provider_passthrough\u0026#34;: \u0026#34;dvs:api-gw-service-nmn.local:300:eth0\u0026#34;, \u0026#34;kernel_parameters\u0026#34;: \u0026#34;console=ttyS0,115200 bad_page=panic crashkernel=360M hugepagelist=2m-2g intel_iommu=off intel_pstate=disable iommu=pt ip=dhcp numa_interleave_omit=headless numa_zonelist_order=node oops=panic pageblock_order=14 pcie_ports=native printk.synchronous=y rd.neednet=1 rd.retry=10 rd.shell k8s_gw=api-gw-service-nmn.local quiet turbo_boost_limit=999\u0026#34;, \u0026#34;network\u0026#34;: \u0026#34;nmn\u0026#34; } }, \u0026#34;cfs_branch\u0026#34;: \u0026#34;master\u0026#34;, \u0026#34;cfs_url\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/config-management.git\u0026#34;, \u0026#34;enable_cfs\u0026#34;: true, \u0026#34;partition\u0026#34;: \u0026#34; }\u0026#39; curl -i -X POST -s https://api-gw-service-nmn.local/apis/bos/v1/sessiontemplate \\  -H \u0026#34;Authorization: Bearer $(get_token)\u0026#34; \\  -H \u0026#34;Content-Type: application/json\u0026#34; \\  -d \u0026#34;$body\u0026#34;   The new CPS-based session template can be used when creating a BOS session. The following is an example of creating a reboot session using the CLI:\nncn# cray bos session create --template-uuid cps_rootfs_template --operation Reboot "
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/clean_up_logs_after_a_boa_kubernetes_job/",
	"title": "Clean Up Logs After A Boa Kubernetes Job",
	"tags": [],
	"description": "",
	"content": "Clean Up Logs After a BOA Kubernetes Job Delete log entries from previous boot orchestration jobs. The Boot Orchestration Service (BOS) launches a Boot Orchestration Agent (BOA) Kubernetes job. BOA then launches a Configuration Framework Service (CFS) session, resulting in a CFS-BOA Kubernetes job. Thus, there are two separate sets of jobs that can be removed.\nDeleting log entries creates more space and helps improve the usability of viewing logs.\nPrerequisites  A Boot Orchestration Service (BOS) session has finished.  Procedure   View the existing list of jobs.\nThe following command will list the BOA jobs.\nncn-m001# kubectl get jobs -n services | grep boa boa-2c2211aa-9876-4aa7-92e2-c8a64d9bd9a6 1/1 6m58s 13d boa-51918dbd-bde2-4836-9500-2a7bad93787c 1/1 65s 9d boa-6fc198cc-486b-4340-81e0-f17c199a1ec6 1/1 97s 9d boa-8656f64d-baa9-43ea-9e11-2a0b27e89037 1/1 17m 13d boa-86b78489-1d76-4957-9c0e-a7b1d6665c35 1/1 15m 13d boa-a939bd32-9d27-433f-afc2-735e77ec8e58 1/1 13m 13d boa-e9adfa63-24dc-4da6-b870-b3535adf0bcc 1/1 7m53s 13d   Delete any jobs that are no longer needed.\nDo not delete any jobs that are currently running.\nncn-m001# kubectl delete jobs BOA_JOB_ID   "
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/compute_node_boot_issue_symptom_duplicate_address_warnings_and_declined_dhcp_offers_in_logs/",
	"title": "Compute Node Boot Issue Symptom Duplicate Address Warnings And Declined DHCP Offers In Logs",
	"tags": [],
	"description": "",
	"content": "Compute Node Boot Issue Symptom: Duplicate Address Warnings and Declined DHCP Offers in Logs If the DHCP and node logs show duplicate address warnings and indicate declined DHCP offers, it may be because another component owns the IP address that DHCP is trying to assign to a node. If this happens, the node will not accept the IP address and will repeatedly submit a DHCP discover request. As a result, the node and DHCP become entangled in a loop of requesting and rejecting. This often happens when DHCP is statically assigning IP addresses to nodes, but the assigned IP address for a node has already been assigned to another component.\nSymptoms This scenario results in node logs similar to the following:\nNode log:\n[ 97.946332] dracut Warning: Duplicate address detected for 10.100.160.195 while doing dhcp. retrying [ 97.789015] dracut-initqueue[604]: dracut Warning: Duplicate address detected for 10.100.160.195 while doing dhcp. \\ retrying [ 108.007243] dracut Warning: Duplicate address detected for 10.100.160.195 while doing dhcp. retrying [ 107.873650] dracut-initqueue[604]: dracut Warning: Duplicate address detected for 10.100.160.195 while doing dhcp. \\ retrying [ 110.082877] dracut Warning: Duplicate address detected for 10.100.160.195 while doing dhcp. retrying DHCP log:\nAbandoning IP address 10.100.160.195: declined. DHCPDECLINE of 10.100.160.195 from a4:bf:01:2e:81:4c (undefined) via eth0: abandoned DHCPOFFER on 10.100.160.195 to \u0026#34; (undefined) via eth0 DHCPREQUEST for 10.100.160.195 (10.100.160.2) from a4:bf:01:29:92:be via eth0: unknown lease 10.100.160.195. DHCPREQUEST for 10.100.160.195 (10.100.160.2) from a4:bf:01:29:92:eb via eth0: unknown lease 10.100.160.195. DHCPOFFER on 10.100.160.195 to a4:bf:01:2e:81:4c via eth0 DHCPREQUEST for 10.100.160.195 (10.100.160.2) from a4:bf:01:2e:81:4c via eth0 DHCPACK on 10.100.160.195 to a4:bf:01:2e:81:4c via eth0 Abandoning IP address 10.100.160.195: declined. Notice that two different components (identifiable by the two different MAC addresses a4:bf:01:29:92:eb and a4:bf:01:2e:81:4c) have made DHCP requests for the IP address 10.100.160.195.\na4:bf:01:29:92:eb is the component that owns the IP address 10.100.160.195, while a4:bf:01:2e:81:4c has been statically assigned the IP address 10.100.160.195 in the DHCP configuration file. As such, DHCP keeps trying to assign it that address, but after being offered the address, a4:bf:01:2e:81:4c declines it because it realizes that a4:bf:01:29:92:eb already owns it.\nProblem Detection There are multiple ways to check if this problem exists:\n  Ping the IP address and see if another component responds. Log into the component and determine its IP address. If it is the same as the IP address that DHCP is attempting to assign, then this issue does exist.\n  Check the Address Resolution Protocol (ARP) cache using the arp command. Because it is a cache, it is possible that IP addresses can age out of the cache, so the IP address may not be present. If the address that is failing to be assigned is in the ARP cache, and it is assigned to a node with a different MAC address, then that is confirmation that this problem has occurred.\nncn-m001# arp Address HWtype HWaddress Flags Mask Iface ncn-w002.local ether 98:03:9b:b4:f1:fe C vlan002 10.46.11.201 ether ca:d3:dc:33:29:e7 C weave 10.46.12.7 ether 7e:7e:7f:f0:0d:2d C weave 10.46.11.197 ether 62:4c:91:91:ec:9f C weave 10.46.11.193 ether 52:dd:02:01:34:ab C weave 10.32.0.5 ether ba:ff:65:af:a7:4e C weave 10.46.11.191 ether be:36:79:07:84:08 C weave 10.45.1.121 ether fe:93:50:63:9a:fd C weave 10.46.11.187 ether e6:2e:8c:ed:f8:78 C weave 10.46.11.250 ether c6:73:6d:c4:b9:77 C weave 10.48.15.0 ether da:c2:40:ed:f4:ec CM flannel.2 10.46.11.183 ether a2:f2:0d:34:cc:8b C weave 10.46.11.246 ether 7e:42:c4:0f:59:97 C weave nid000003-nmn.local ether a4:bf:01:3e:f9:cd C vlan002 10.46.11.242 ether 4e:06:ef:eb:5c:ba C weave 10.46.11.234 ether f2:76:9d:1a:68:00 C weave sw-spine01-nmn.local ether b8:59:9f:68:97:48 C vlan002 10.46.11.230 ether 6a:a3:65:5c:37:ba C weave 10.46.12.28 ether 06:0e:54:02:a7:c4 C weave 10.46.11.226 ether 5a:5b:55:77:97:b7 C weave 10.46.12.24 ether ba:4c:84:e3:53:2b C weave 10.38.1.89 ether 5a:c7:15:17:78:dc C weave 10.46.11.222 ether 52:61:38:42:7c:00 C weave 10.46.11.218 ether 66:81:68:6e:2e:38 C weave 10.46.11.214 ether ea:91:c2:b7:de:a2 C weave 10.46.12.12 ether 5a:dd:d2:8b:20:66 C weave ncn-m002.local ether b8:59:9f:1d:da:26 C vlan002 10.46.11.210 ether 2e:9f:17:a6:d7:d4 C weave 10.48.52.0 ether e2:9e:26:a4:d3:ba CM flannel.2 ncn-s003.local ether 98:03:9b:bb:a9:48 C vlan002 10.38.1.73 ether 46:67:08:21:f1:fc C weave 10.46.11.206 ether 36:b9:47:ad:69:8d C weave ncn-s002.local ether b8:59:9f:34:88:ca C vlan002 10.46.12.0 ether 36:cb:f8:3c:b4:05 C weave 10.46.11.194 ether e2:7d:57:7e:9c:9e C weave 10.32.0.6 ether 46:e2:fe:29:1d:02 C weave 10.46.11.188 ether aa:3c:5a:60:45:af C weave 10.46.11.255 ether 3a:e6:e4:91:7a:ec C weave 10.48.16.0 ether 1a:fa:2e:8f:80:5c CM flannel.2 10.46.11.247 ether c2:af:c4:53:ba:ff C weave nid000002-nmn.local ether a4:bf:01:3e:ef:d7 C vlan002 10.46.11.243 ether de:5c:cc:47:db:55 C weave 10.46.11.235 ether c6:bb:07:55:8a:4d C weave 10.46.11.231 ether 6a:73:3c:fe:3f:95 C weave 10.46.12.29 ether a6:d6:6f:71:d1:50 C weave 10.46.11.227 ether b6:fe:d9:9d:41:0c C weave 10.46.12.21 ether 42:cd:2c:a3:d2:97 C weave 10.46.11.219 ether 8a:47:b1:71:05:f9 C weave 10.46.11.215 ether ce:b1:cb:48:c7:e3 C weave 10.46.12.13 ether e2:34:38:f3:ce:3f C weave ncn-m001.local ether b8:59:9f:1d:d8:4a C vlan002 10.46.11.211 ether 7e:ad:22:04:9b:32 C weave 10.46.11.203 ether 16:23:a2:96:d4:d2 C weave 10.46.12.1 ether 8e:51:2a:b6:f1:cc C weave 10.46.11.199 ether e6:f4:ee:92:b8:a8 C weave 10.46.11.195 ether ce:0e:d2:e8:d5:fa C weave 10.46.11.189 ether 92:ad:7c:93:ec:d8 C weave 10.46.11.252 ether a2:92:18:0f:54:a8 C weave 10.48.57.0 ether 4a:ac:84:c4:76:b1 CM flannel.2 10.46.11.248 ether ea:ff:53:cd:27:36 C weave 10.46.11.240 ether 6a:33:63:3a:50:0a C weave cfgw-48-vrrp.us.cray.com ether 00:00:5e:00:01:30 C em1 10.252.120.2 ether b8:59:9f:1d:da:26 C vlan002 sw-leaf-001-can.local ether 3c:2c:30:5e:6d:b5 C vlan007 nid000001-nmn.local ether a4:bf:01:3e:e0:93 C vlan002 10.46.11.232 ether 3a:69:e4:d9:7f:1f C weave 10.45.1.166 ether 9a:86:9c:c0:53:c5 C weave 10.46.11.224 ether d6:ec:f3:eb:6a:cb C weave 10.46.12.30 ether a2:e1:8e:f5:65:64 C weave 10.46.11.220 ether 1e:37:ab:42:15:24 C weave 10.46.11.212 ether 16:82:52:f6:ca:de C weave sw-leaf-001-hmn.local ether 3c:2c:30:5e:6d:b5 C vlan004 sw-leaf-001-mtl.local ether 3c:2c:30:5e:6d:b5 C p1p1 10.46.11.208 ether 0e:cf:3d:df:ea:21 C weave 10.46.12.14 ether 92:73:ff:8d:8a:07 C weave 10.46.12.10 ether 32:19:b3:75:3c:f7 C weave ncn-w003.local ether 98:03:9b:bb:a8:8c C vlan002 10.46.11.200 ether 1a:c3:46:0f:a0:b9 C weave 10.48.3.0 ether 16:78:89:dd:ae:7c CM flannel.2 10.46.12.6 ether c6:3d:da:ef:d8:a5 C weave ncn-s001.local ether b8:59:9f:1d:d9:1e C vlan002 10.46.11.196 ether 96:88:fc:f4:15:ac C weave 10.46.12.2 ether d2:ac:dd:44:2c:0a C weave 10.46.11.192 ether e6:aa:51:79:ee:83 C weave 10.46.11.253 ether a6:dc:f1:57:24:84 C weave 10.46.11.190 ether 62:a8:e0:3c:13:be C weave 10.46.11.249 ether fa:37:7f:0a:ed:73 C weave 10.46.11.186 ether aa:d9:05:39:cd:77 C weave 10.46.11.241 ether 36:a7:00:8a:8a:9e C weave nid000004-nmn.local ether a4:bf:01:3e:ca:61 C vlan002 172.17.0.2 ether 02:42:ac:11:00:02 C docker0 10.46.11.233 ether 3a:5f:0f:9f:ce:e1 C weave 10.46.12.35 ether 9e:6d:fa:63:5e:2c C weave 10.45.1.100 ether 32:65:00:82:9b:90 C weave 10.46.12.31 ether ee:bd:3e:ee:ab:80 C weave 169.254.255.254 ether 74:83:ef:2a:e7:83 C hsn0 10.46.12.27 ether 1a:42:25:07:ee:0d C weave 10.46.11.213 ether 06:b8:de:1d:b2:99 C weave 10.46.12.19 ether 82:61:83:18:fc:b7 C weave sw-spine-001-hmn.local ether b8:59:9f:68:97:48 C vlan004 10.46.12.15 ether ce:92:74:c6:4b:b3 C weave ncn-m003.local ether b8:59:9f:1d:d9:f2 C vlan002 10.46.11.205 ether 16:e7:2e:6d:a3:e2 C weave 10.46.12.11 ether 22:33:f7:4f:be:80 C weave   Resolution Force the component that has been assigned an incorrect IP address to request another one. This may involve powering that component down and then back up.\n"
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/compute_node_boot_issue_symptom_message_about_invalid_eeprom_checksum_in_node_console_or_log/",
	"title": "Compute Node Boot Issue Symptom Message About Invalid Eeprom Checksum In Node Console Or Log",
	"tags": [],
	"description": "",
	"content": "Compute Node Boot Issue Symptom: Message About Invalid EEPROM Checksum in Node Console or Log On rare occasions, the processor hardware may lose the Serial Over Lan (SOL) connections and may need to be reseated to allow the node to successfully boot.\nSymptoms This issue can be identified if the following is displayed in the node\u0026rsquo;s console or log:\nconsole.38:2018-09-08 04:54:51 [ 16.721165] ixgbe 0000:18:00.0: The EEPROM Checksum Is Not Valid console.38:2018-09-08 04:55:00 [ 25.768872] ixgbe 0000:18:00.1: The EEPROM Checksum Is Not Valid The following figure shows that the EEPROM checksum errors lead to a dracut-initqueue timeout, and eventually cause the node to drop into the dracut emergency shell.\nProblem Detection Run dmidecode from the compute node to identify its model. H87926-500 is the silver model that may exhibit this issue, whereas the production model, H87926-550, does not exhibit SOL connection issues.\ncmp4:~ # dmidecode|grep H87926 Version: H87926-550 Resolution One way to resolve this issue is to ensure that the latest ixgbe network driver is installed on the nodes.\n"
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/compute_node_boot_issue_symptom_node_is_not_able_to_download_the_required_artifacts/",
	"title": "Compute Node Boot Issue Symptom Node Is Not Able To Download The Required Artifacts",
	"tags": [],
	"description": "",
	"content": "Compute Node Boot Issue Symptom: Node is Not Able to Download the Required Artifacts If either or both of the kernel or the initrd boot artifacts are missing from the artifact repository, Boot Script Service (BSS), or both, the node will not be able to download the required boot artifacts and will fail to boot.\nSymptoms The node\u0026rsquo;s console or log will display lines beginning with, \u0026lsquo;'Could not start download'\u0026rsquo;. Refer to the image below for an example of this error message.\nProblem Detection Use the following command from a non-compute node (NCN) to see which boot artifacts BSS assumes as those used for booting the node.\nncn-m001# cray bss bootparameters list Each boot artifact has a download URL, as shown in the following examples:\nncn-m001# cray bss bootparameters list [[results]] kernel = \u0026#34;s3://boot-images/dc87a741-f7cc-4167-afae-592c5a8ca7ec/vmlinuz-4.12.14-197.29_9.1.14-cray_shasta_c\u0026#34; [[results]] kernel = \u0026#34;s3://boot-images/89e5a1dc-0caa-418a-9742-a832829db0ab/kernel\u0026#34; [[results]] kernel = \u0026#34;s3://boot-images/97b548b9-2ea9-45c9-95ba-dfc77e5522eb/kernel\u0026#34; [[results]] kernel = \u0026#34;s3://boot-images/29c2cc23-a9d6-4e9a-ab1a-b5fa9270c975/kernel\u0026#34; [[results]] initrd = \u0026#34;s3://boot-images/dc87a741-f7cc-4167-afae-592c5a8ca7ec/initrd-4.12.14-197.29_9.1.14-cray_shasta_c\u0026#34; [[results]] initrd = \u0026#34;s3://boot-images/97b548b9-2ea9-45c9-95ba-dfc77e5522eb/initrd\u0026#34; [[results]] initrd = \u0026#34;s3://boot-images/89e5a1dc-0caa-418a-9742-a832829db0ab/initrd\u0026#34; [[results]] initrd = \u0026#34;s3://boot-images/29c2cc23-a9d6-4e9a-ab1a-b5fa9270c975/initrd\u0026#34; [[results]] params = \u0026#34;console=ttyS0,115200n8 console=tty0 initrd=37103ceb-3813-45ba-85b0-a8fc53edd5da rw selinux=0 nofb rd.shell rd.net.timeout.carrier=20 ip=dhcp rd.neednet=1 rd.retry=60 crashkernel=360M reds=use_server api_gw_ip=api-gw-service-nmn.local\u0026#34; initrd = \u0026#34;s3://boot-images/97b548b9-2ea9-45c9-95ba-dfc77e5522eb/initrd\u0026#34; hosts = [ \u0026#34;Unknown-x86_64\u0026#34;,] kernel = \u0026#34;s3://boot-images/97b548b9-2ea9-45c9-95ba-dfc77e5522eb/kernel\u0026#34; Use the artifact\u0026rsquo;s S3 key to download it:\nncn-m001# cray artifacts get S3_BUCKET S3_OBJECT_KEY DOWNLOAD_FILE_PATH For example, if s3://boot-images/97b548b9-2ea9-45c9-95ba-dfc77e5522eb/initrd is the S3 URI for the file initrd, run the following command:\nncn-m001# cray artifacts get boot-images \\ 97b548b9-2ea9-45c9-95ba-dfc77e5522eb/initrd initrd This command will return 404 errors if the specified object does not exist in the S3 bucket.\nResolution Ensure that the required boot artifacts are stored in the artifact repository and/or BSS. If the artifact\u0026rsquo;s name is different than what is already in BSS, then BSS needs to be updated to match.\n"
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/boot_orchestration/",
	"title": "Boot Orchestration",
	"tags": [],
	"description": "",
	"content": "Boot Orchestration The Boot Orchestration Service (BOS) is responsible for booting, configuring, and shutting down collections of nodes. This is accomplished using BOS components, such as boot orchestration session templates and sessions, as well as launching a Boot Orchestration Agent (BOA) that fulfills boot requests.\nBOS users create a BOS session template via the REST API. A session template is a collection of metadata for a group of nodes and their desired boot artifacts and configuration. A BOS session can then be created by applying an action to a session template. The available actions are boot, reboot, shutdown, and configure. BOS will create a Kubernetes BOA job to apply an action. BOA coordinates with the underlying subsystems to complete the action requested. The session can be monitored to determine the status of the request.\nBOS depends on each of the following services to complete its tasks:\n BOA - Handles any action type submitted to the BOS API. BOA jobs are created and launched by BOS. Boot Script Service (BSS) - Stores the configuration information that is used to boot each hardware component. Nodes consult BSS for their boot artifacts and boot parameters when nodes boot or reboot. Configuration Framework Service (CFS) - BOA launches CFS to apply configuration to the nodes in its boot sets (node personalization). Cray Advanced Platform Monitoring and Control (CAPMC) - Used to power on and off the nodes. Hardware State Manager (HSM) - Tracks the state of each node and what groups and roles nodes are included in.  Use the BOS Cray CLI Commands BOS utilizes the Cray CLI commands. The latest API information can be found with the following command:\nncn-m001# cray bos list [[results]] major = \u0026#34;1\u0026#34; minor = \u0026#34;0\u0026#34; patch = \u0026#34;0\u0026#34; [[results.links]] href = \u0026#34;https://api-gw-service-nmn.local/apis/bos/v1\u0026#34; rel = \u0026#34;self\u0026#34; BOS API Changes in Upcoming CSM-1.2.0 Release This is a forewarning of changes that will be made to the BOS API in the upcoming CSM-1.2.0 release. The following changes will be made:\n The --template-body option for the Cray CLI bos command will be deprecated. Performing a GET on the session status for a boot set (i.e. /v1/session/{session_id}/status/{boot_set_name}) currently returns a status code of 201, but instead it should return a status code of 200. This will be corrected to return 200.  "
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/boot_uans/",
	"title": "Boot UANs",
	"tags": [],
	"description": "",
	"content": "Boot UANs Boot UANs with an image so that they are ready for user logins.\nPrerequisites UAN boot images and a BOS session template have been created. See Create UAN Boot Images.\nProcedure   Create a BOS session to boot the UAN nodes.\nncn-m001# cray bos session create --template-uuid uan-sessiontemplate-PRODUCT_VERSION \\ --operation reboot --format json | tee session.json { \u0026#34;links\u0026#34;: [ { \u0026#34;href\u0026#34;: \u0026#34;/v1/session/89680d0a-3a6b-4569-a1a1-e275b71fce7d\u0026#34;, \u0026#34;jobId\u0026#34;: \u0026#34;boa-89680d0a-3a6b-4569-a1a1-e275b71fce7d\u0026#34;, \u0026#34;rel\u0026#34;: \u0026#34;session\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;GET\u0026#34; }, { \u0026#34;href\u0026#34;: \u0026#34;/v1/session/89680d0a-3a6b-4569-a1a1-e275b71fce7d/status\u0026#34;, \u0026#34;rel\u0026#34;: \u0026#34;status\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;GET\u0026#34; } ], \u0026#34;operation\u0026#34;: \u0026#34;reboot\u0026#34;, \u0026#34;templateUuid\u0026#34;: \u0026#34;uan-sessiontemplate-PRODUCT_VERSION\u0026#34; } The first attempt to reboot the UANs will most likely fail. The UAN boot may hang and the UAN console will look similar to the following:\n2021-03-19 01:32:41 dracut-initqueue[420]: DVS: node map generated. 2021-03-19 01:32:41 katlas: init_module: katlas loaded, currently disabled 2021-03-19 01:32:41 2021-03-19 01:32:41 DVS: Revision: kbuild Built: Mar 17 2021 @ 15:14:05 against LNet 2.12.4 2021-03-19 01:32:41 DVS debugfs: Revision: kbuild Built: Mar 17 2021 @ 15:14:05 against LNet 2.12.4 2021-03-19 01:32:41 dracut-initqueue[420]: DVS: loadDVS: successfully added 10 new nodes into map. 2021-03-19 01:32:41 ed dvsproc module. 2021-03-19 01:32:41 DVS: message size checks complete. 2021-03-19 01:32:41 dracut-initqueuedvs_thread_generator: Watching pool DVS-IPC_msg (id 0) 2021-03-19 01:32:41 [420]: DVS: loaded dvs module. 2021-03-19 01:32:41 dracut-initqueue[420]: mount is: /opt/cray/cps-utils/bin/cpsmount.sh -a api-gw-service-nmn.local -t dvs -T 300 -i nmn0 -e 3116cf653e84d265cf8da94956f34d9e-181 s3://boot-images/763213c7-3d5f-4f2f-9d8a-ac6086583f43/rootfs /tmp/cps 2021-03-19 01:32:41 dracut-initqueue[420]: 2021/03/19 01:31:01 cpsmount_helper Version: 1.0.0 2021-03-19 01:32:47 dracut-initqueue[420]: 2021/03/19 01:31:07 Adding content: s3://boot-images/763213c7-3d5f-4f2f-9d8a-ac6086583f43/rootfs 3116cf653e84d265cf8da94956f34d9e-181 dvs 2021-03-19 01:33:02 dracut-initqueue[420]: 2021/03/19 01:31:22 WARN: readyForMount=false type=dvs ready=0 total=2 2021-03-19 01:33:18 dracut-initqueue[420]: 2021/03/19 01:31:38 WARN: readyForMount=false type=dvs ready=0 total=2 2021-03-19 01:33:28 dracut-initqueue[420]: 2021/03/19 01:31:48 2 dvs servers [10.252.1.7 10.252.1.8] If this occurs, repeat the BOS command.\n  Verify that the Day Zero patch was applied correctly during Create UAN Boot Images. Skip this step if the patch has already been verified.\n  SSH into a newly booted UAN.\nncn-m001# ssh uan01-nmn Last login: Wed Mar 17 19:10:12 2021 from 10.252.1.12 uan01#   Verify that the DVS RPM versions match what exists in the 1.4.0-p2/rpms directory.\nuan01# rpm -qa | grep \u0026#39;cray-dvs.*2.12\u0026#39; | sort cray-dvs-compute-2.12_4.0.102-7.0.1.0_8.1__g30d29e7a.x86_64 cray-dvs-devel-2.12_4.0.102-7.0.1.0_8.1__g30d29e7a.x86_64 cray-dvs-kmp-cray_shasta_c-2.12_4.0.102_k4.12.14_197.78_9.1.58-7.0.1.0_8.1__g30d29e7a.x86_64   Log out of the UAN.\nuan01# exit     Retrieve the BOS session ID from the output of the previous command.\nncn-m001# export BOS_SESSION=$(jq -r \u0026#39;.links[] | select(.rel==\u0026#34;session\u0026#34;) | .href\u0026#39; session.json | cut -d \u0026#39;/\u0026#39; -f4) ncn-m001# echo $BOS_SESSION 89680d0a-3a6b-4569-a1a1-e275b71fce7d   Retrieve the Boot Orchestration Agent (BOA) Kubernetes job name for the BOS session.\nncn-m001# BOA_JOB_NAME=$(cray bos session describe $BOS_SESSION --format json | jq -r .boa_job_name)   Retrieve the Kubernetes pod name for the BOA assigned to run this session.\nncn-m001# BOA_POD=$(kubectl get pods -n services -l job-name=$BOA_JOB_NAME \\ --no-headers -o custom-columns=\u0026#34;:metadata.name\u0026#34;)   View the logs for the BOA to track session progress.\nncn-m001# kubectl logs -f -n services $BOA_POD -c boa   List the CFS sessions started by the BOA. Skip this step if CFS was not enabled in the boot session template used to boot the UANs.\nIf CFS was enabled in the boot session template, the BOA will initiate a CFS session.\nIn the following command, pending and complete are also valid statuses to filter on.\nncn-m001# cray cfs sessions list --tags bos_session=$BOS_SESSION --status running --format json   "
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/check_the_progress_of_bos_session_operations/",
	"title": "Check The Progress Of BOS Session Operations",
	"tags": [],
	"description": "",
	"content": "Check the Progress of BOS Session Operations Describes how to view the logs of BOS operations with Kubernetes.\nWhen a Boot Orchestration Service (BOS) session is created, it will return a job ID. This ID can be used to locate the Boot Orchestration Agent (BOA) Kubernetes job that executes the session. For example:\nncn-m001# cray bos session create --template-uuid SESSIONTEMPLATE_NAME --operation Boot operation = \u0026#34;Boot\u0026#34; templateUuid = \u0026#34;TEMPLATE_UUID\u0026#34; [[links]] href = \u0026#34;foo-c7faa704-3f98-4c91-bdfb-e377a184ab4f\u0026#34; jobId = \u0026#34;boa-a939bd32-9d27-433f-afc2-735e77ec8e58\u0026#34; rel = \u0026#34;session\u0026#34; type = \u0026#34;GET\u0026#34; All BOS Kubernetes pods operate in the services namespace.\nFind the BOA Kubernetes Job Use the following command to locate the Kubernetes BOA pod.\nncn-m001# kubectl get pods -n services | grep -E \u0026#34;NAME | BOS_SESSION_JOB_ID\u0026#34; For example:\nncn-m001# kubectl get pods -n services | grep -E \u0026#34;NAME | boa-a939bd32-9d27-433f-afc2-735e77ec8e58\u0026#34; NAME READY STATUS RESTARTS AGE boa-a939bd32-9d27-433f-afc2-735e77ec8e58-ztscd 0/2 Completed 0 16m Use the following command to locate the Kubernetes BOA job.\nbash# kubectl get jobs -n services BOS_SESSION_JOB_ID For example:\n# kubectl get jobs -n services boa-a939bd32-9d27-433f-afc2-735e77ec8e58 NAME COMPLETIONS DURATION AGE boa-a939bd32-9d27-433f-afc2-735e77ec8e58 1/1 13m 15m The Kubernetes BOA pod name is not a one-to-one match with the BOA job name. The pod name has -XXXX appended to it, where \u0026lsquo;X\u0026rsquo; is a hexadecimal digit.\nView the BOA Log Use the following command to look at the BOA pod\u0026rsquo;s logs.\nncn-m001# kubectl logs -n services KUBERNETES_BOA_POD_ID -c boa For example:\nncn-m001# kubectl logs -n services boa-a939bd32-9d27-433f-afc2-735e77ec8e58 -c boa View the Configuration Framework Service (CFS) Log If a session template has CFS enabled, then BOA will attempt to configure the nodes during a boot, reboot, or configure operation. Use the BOA job ID to find the CFS job that BOA launched to configure the nodes.\nncn-m001# cray cfs sessions describe BOA_JOB_ID For example:\n# cray cfs sessions describe boa-86b78489-1d76-4957-9c0e-a7b1d6665c35 --format json { \u0026#34;ansible\u0026#34;: { \u0026#34;limit\u0026#34;: \u0026#34;x3000c0s19b4n0,x3000c0s19b3n0,x3000c0s19b2n0,x3000c0s19b1n0\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34; }, \u0026#34;id\u0026#34;: \u0026#34;ffdda2c6-2277-11ea-8db8-b42e993b706a\u0026#34;, \u0026#34;links\u0026#34;: [ { \u0026#34;href\u0026#34;: \u0026#34;/apis/cfs/sessions/boa-86b78489-1d76-4957-9c0e-a7b1d6665c35\u0026#34;, \u0026#34;rel\u0026#34;: \u0026#34;self\u0026#34; }, { \u0026#34;href\u0026#34;: \u0026#34;/apis/cms.cray.com/v1/namespaces/services/cfsessions/boa-86b78489-1d76-4957-9c0e-a7b1d6665c35\u0026#34;, \u0026#34;rel\u0026#34;: \u0026#34;k8s\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;boa-86b78489-1d76-4957-9c0e-a7b1d6665c35\u0026#34;, \u0026#34;repo\u0026#34;: { \u0026#34;branch\u0026#34;: \u0026#34;master\u0026#34;, \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;artifacts\u0026#34;: [], \u0026#34;session\u0026#34;: { \u0026#34;completionTime\u0026#34;: \u0026#34;2019-12-19T16:05:11+00:00\u0026#34;, \u0026#34;job\u0026#34;: \u0026#34;cfs-85e3e48f-6795-4570-b379-347b05b39dbe\u0026#34;, \u0026lt;\u0026lt;-- Kubernetes CFS job ID \u0026#34;startTime\u0026#34;: \u0026#34;2019-12-19T15:55:37+00:00\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;complete\u0026#34;, \u0026#34;succeeded\u0026#34;: \u0026#34;true\u0026#34; }, \u0026#34;targets\u0026#34;: { \u0026#34;failed\u0026#34;: 0, \u0026#34;running\u0026#34;: 0, \u0026#34;success\u0026#34;: 0 } }, \u0026#34;target\u0026#34;: { \u0026#34;definition\u0026#34;: \u0026#34;dynamic\u0026#34;, \u0026#34;groups\u0026#34;: [] } } Use the Kubernetes CFS job ID in the returned output above to find the CFS pod ID. It is the pod with three containers listed, not two.\nncn-m001# kubectl -n services get pods|grep KUBERNETES_CFS_JOB_ID cfs-85e3e48f-6795-4570-b379-347b05b39dbe-59645667b-ffznt 2/2 Running 0 3h57m cfs-85e3e48f-6795-4570-b379-347b05b39dbe-cvr54 0/3 Completed 0 3h57m View the pod\u0026rsquo;s logs for the Ansible container:\nncn-m001# kubectl -n services logs -f -c ansible KUBERNETES_CFS_POD_ID View the BOS log The BOS log shows when a session was launched. It also logs any errors encountered while attempting to launch a session.\nThe BOS Kubernetes pod ID can be found with the following command:\nncn-m001# kubectl get pods -n services | grep bos | grep -v etcd cray-bos-d97cf465c-klcrw 2/2 Running 0 90s Examine the logs:\nncn-m001# kubectl logs BOS_POD_ID BOS uses an etcd database. Looking at the etcd logs is typically not necessary.\n"
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/clean_up_after_a_bos-boa_job_is_completed_or_cancelled/",
	"title": "Clean Up After A BOS/boa Job Is Completed Or CANcelled",
	"tags": [],
	"description": "",
	"content": "Clean Up After a BOS/BOA Job is Completed or Cancelled When a BOS session is created, there are a number of items created on the system. When a session is cancelled or completed, these items need to be cleaned up to ensure there is not lingering content from the session on the system.\nWhen a session is launched, the items below are created:\n Boot Orchestration Agent (BOA) job: The Kubernetes job that runs and handles the BOS session. ConfigMap for BOA: This ConfigMap contains the configuration information that the BOA job uses. The BOA pod mounts a ConfigMap named boot-session at /mnt/boot_session inside the pod. This ConfigMap has a random UUID name, such as e786def5-37a6-40db-b36b-6b67ebe174ee. This name does not obviously connect it to the BOA job. etcd entries: BOS makes an entry for the session in its etcd key/value store. If the BOA job has run for long enough, it will also have written a status entry into etcd for this session. Configuration Framework Service (CFS) session: If configuration is enabled, and the session is doing a boot, reboot, or configure operation, then BOA will have instructed CFS to configure the nodes once they boot. There is not an easy way to link a BOA session to the CFS sessions that are spawned.  Prerequisites  A Boot Orchestration Service (BOS) session has been completed or cancelled. The Cray command line interface (CLI) tool is initialized and configured on the system.  Procedure   Identify the BOA job that needs to be deleted.\nDescribe the BOS session to find the name of the BOA job under the attribute boa_job_name.\nncn-m001# cray bos session describe --format json BOS_SESSION_ID { \u0026#34;status_link\u0026#34;: \u0026#34;/v1/session/d200f7e4-1a9f-4466-9ef4-30add3bd87dd/status\u0026#34;, \u0026#34;complete\u0026#34;: \u0026#34;, \u0026#34;start_time\u0026#34;: \u0026#34;2020-08-11 21:02:09.137917\u0026#34;, \u0026#34;templateUuid\u0026#34;: \u0026#34;cle-1.3.0-nid1\u0026#34;, \u0026#34;error_count\u0026#34;: \u0026#34;, \u0026#34;boa_job_name\u0026#34;: \u0026#34;boa-d200f7e4-1a9f-4466-9ef4-30add3bd87dd\u0026#34;, \u0026#34;in_progress\u0026#34;: \u0026#34;, \u0026#34;operation\u0026#34;: \u0026#34;boot\u0026#34;, \u0026#34;stop_time\u0026#34;: null }   Find the ConfigMap for the BOA job.\nThe ConfigMap is listed as boot-session under the Volumes section. Retrieve the Name value from the returned output.\nncn-m001# kubectl -n services describe job BOA_JOB_NAME ... Volumes: boot-session: Type: ConfigMap (a volume populated by a ConfigMap) **Name: e786def5-37a6-40db-b36b-6b67ebe174ee** Optional: false   Delete the ConfigMap.\nncn-m001# kubectl -n services delete cm CONFIGMAP_NAME   Delete the etcd entry for the BOS session.\nncn-m001# cray bos session delete BOS_SESSION_ID   Stop CFS from configuring nodes.\nThere are several different use cases covered in this step. The process varies depending on whether a job is being cancelled, or if the CFS content is simply being cleaned up.\nIn the BOS session template, if configuration is enabled, BOA instructs the nodes to configure on boot when doing a boot or reboot operation. When only doing a configure operation, the configuration happens right away. If configuration is disabled or the operation is shutdown, then BOA will not instruct CFS to configure the nodes, and nothing further needs to be done. The remainder of this step may be skipped.\nIf BOA instructs CFS to configure the nodes, then CFS will set the desired configuration for the nodes in its database. Once BOA tells CFS to configure the nodes, which happens early in the BOA job, then CFS will configure the nodes immediately if the operation is configure, or upon the node booting if the operation is boot or reboot.\nAttempting to prevent CFS from configuring the nodes is a multi-step, tedious process. It may be simpler to allow CFS to configure the nodes. If the nodes are going to be immediately rebooted, then the CFS configuration will be rerun once the nodes have booted, thus undoing any previous configuration. Alternately, if the nodes are going to be immediately shutdown, this will remove any need to prevent CFS from configuring the nodes.\nFollow the procedures for one of the following use cases:\n  Configuration has completed and the sessions need to be cleaned up the to reduce clutter:\n  Find the old sessions that needs to be deleted.\nncn-m001# cray cfs sessions list   Delete the sessions.\nncn-m001# cray cfs sessions delete CFS_SESSION_NAME     Configuration has completed and the desired state needs to be cleaned up so that configuration does not happen on restart:\n  Unset the desired state for all components affected.\nTo find the impacted xnames for the components with the desired configuration matching what was applied:\nncn-m001# cray cfs components list Prevent the configuration from running:\nncn-m001# cray cfs components update XNAME --desired-state-commit This needs to be done for each component. It is enough to prevent configuration from running, and it does not revert to the previous desired state. The previous desired state has already been overwritten at this point, so if the user is trying to completely revert, they will either need to know and apply the previous desired state manually, or return BOS with the previous template using the configure operation (which may also trigger a configure operation).\n    Configuration was set/started and needs to be cancelled:\n  Unset the desired state for all components affected.\nTo find the impacted xnames for the components with the desired configuration matching what was applied:\nncn-m001# cray cfs components list Prevent the configuration from running:\nncn-m001# cray cfs components update XNAME --desired-state-commit This needs to be done for each component. It is enough to prevent configuration from running, and it does not revert to the previous desired state. The previous desired state has already been overwritten at this point, so if the user is trying to completely revert, they will either need to know and apply the previous desired state manually, or return BOS with the previous template using the configure operation (which may also trigger a configure operation).\n  Restart the batcher.\nThis will purge any information that CFS cached in relation to the BOA job that it was intending to act upon.\nTo get the cfs-batcher pod id:\nncn-m001# kubectl -n services get pods|grep cray-cfs-batcher cray-cfs-batcher-644599c6cc-rwl8f 2/2 Running 0 6d17h To restart the pod, scale the replicas to 0 to stop it, and back up to 1 to restart it:\nncn-m001# kubectl -n services scale CFS-BATCHER_POD_ID --replicas=0 ncn-m001# kubectl -n services scale CFS-BATCHER_POD_ID --replicas=1   Find the existing session that needs to be deleted.\nncn-m001# cray cfs sessions list   Delete the sessions.\nThis step must be done after restarting cfs-batcher. If the cached information is not purged from Batcher first, then Batcher may start additional CFS sessions in response to them being killed. The Batcher agent would fight against the user if it is not restarted.\nUnfortunately, it is hard to link a specific BOA session to a CFS session. At this time, they are identified by comparing the CFS timestamps with those of the BOA job, and associating them based on proximity. Additionally, examine the components in the CFS job to see that they match the components in the BOA job.\nncn-m001# cray cfs sessions delete CFS_SESSION_NAME       Delete the BOA job.\nThe BOA job is not deleted right away because it is needed to find the ConfigMap name.\nncn-m001# kubectl -n services delete job BOA_JOB_NAME   "
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/boot_issue_symptom_node_hsn_interface_does_not_appear_or_shows_no_link_detected/",
	"title": "Boot Issue Symptom Node HSN Interface Does Not Appear Or Show Detected Links Detected",
	"tags": [],
	"description": "",
	"content": "Boot Issue Symptom: Node HSN Interface Does Not Appear or Show Detected Links Detected A node may fail to boot if the HSN interface is experiencing issues, or if it is not able to detect any links.\nSymptom The node\u0026rsquo;s HSN interface does not appear in the output of the ip addr command or the output of the ethtool interface command shows no link detected.\nResolution Reseat the node\u0026rsquo;s PCIe card.\n"
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/bos_workflows/",
	"title": "BOS Workflows",
	"tags": [],
	"description": "",
	"content": "BOS Workflows The following workflows present a high-level overview of common Boot Orchestration Service (BOS) operations. These workflows depict how services interact with each other when booting, configuring, or shutting down nodes. They also help provide a quicker and deeper understanding of how the system functions.\nThe following workflows are included in this section:\n Boot and Configure Nodes Reconfigure Nodes Power Off Nodes  Boot and Configure Nodes Use Case: Administrator powers on and configures select compute nodes.\nComponents: This workflow is based on the interaction of the BOS with other services during the boot process:\nMentioned in this workflow:\n Boot Orchestration Service (BOS) is responsible for booting, configuring, and shutting down collections of nodes. The Boot Orchestration Service has the following components:  Boot Orchestration Session Template is a collection of one or more boot set objects. A boot set defines a collection of nodes and the information about the boot artifacts and parameters. Boot Orchestration Session carries out an operation. The possible operations in a session are boot, shutdown, reboot, and configure. Boot Orchestration Agent (BOA) is automatically launched to execute the session. A BOA executes the given operation, and if the operation is a boot or a reboot, it also configures the nodes post-boot (if configure is enabled).   Cray Advanced Platform and Monitoring Control (CAPMC) service provides system-level power control for nodes in the system. CAPMC interfaces directly with the Redfish APIs to the controller infrastructure to effect power and environmental changes on the system. Hardware State Manager (HSM) tracks the state of each node and their group and role associations. Boot Script Service (BSS) stores per-node information about iPXE boot script. Nodes consult BSS for boot artifacts (kernel, initrd, image root) and boot parameters when nodes boot or reboot. The Simple Storage Service (Ceph S3) is an artifact repository that stores boot artifacts. Configuration Framework Service (CFS) configures nodes using configuration framework. Launches and aggregates the status from one or more Ansible instances against nodes (node personalization) or images (image customization).  Workflow Overview: The following sequence of steps occur during this workflow.\n  Administrator creates a configuration\nAdd a configuration to CFS.\n# cray cfs configurations update sample-config --file configuration.json { \u0026quot;lastUpdated\u0026quot;: \u0026quot;2020-09-22T19:56:32Z\u0026quot;, \u0026quot;layers\u0026quot;: [ { \u0026quot;cloneUrl\u0026quot;: \u0026quot;https://api-gw-service-nmn.local/vcs/cray/configmanagement. git\u0026quot;, \u0026quot;commit\u0026quot;: \u0026quot;01b8083dd89c394675f3a6955914f344b90581e2\u0026quot;, \u0026quot;playbook\u0026quot;: \u0026quot;site.yaml\u0026quot; } ], \u0026quot;name\u0026quot;: \u0026quot;sample-config\u0026quot; }   Administrator creates a session template\nA session template is a collection of metadata for a group of nodes and their desired configuration. A session template can be created from a JSON structure. It returns a SessionTemplate ID if successful.\nSee Manage a Session Template for more information.\n  Administrator creates a session\nCreate a session to perform the operation specified in the operation request parameter on the boot set defined in the session template. For this use case, Administrator creates a session with operation as Boot and specifies the session template ID. The set of allowed operations are:\n Boot – Boot nodes that are powered off Configure – Reconfigure the nodes using the Configuration Framework Service (CFS) Reboot – Gracefully power down nodes that are on and then power them back up Shutdown – Gracefully power down nodes that are on  # cray bos session create \\ --template-uuid SESSIONTEMPLATE_NAME \\ --operation Boot   Launch BOA\nThe creation of a session results in the creation of a Kubernetes BOA job to complete the operation. BOA coordinates with other services to complete the requested operation.\n  BOA to HSM\nBOA coordinates with HSM to validate node group and node status.\n  BOA to S3\nBOA coordinates with S3 to verify boot artifacts like kernel, initrd, and root file system.\n  BOA to BSS\nBOA updates BSS with boot artifacts and kernel parameters for each node.\n  BOA to CAPMC\nBOA coordinates with CAPMC to power-on the nodes.\n  CAPMC boots nodes\nCAPMC interfaces directly with the Redfish APIs and powers on the selected nodes.\n  BSS interacts with the nodes\nBSS generates iPXE boot scripts based on the image content and boot parameters that have been assigned to a node. Nodes download the iPXE boot script from BSS.\n  S3 interacts with the nodes\nNodes download the boot artifacts. The nodes boot using the boot artifacts pulled from S3.\n  BOA to HSM\nBOA waits for the nodes to boot up and be accessible via SSH. This can take up to 30 minutes. BOA coordinates with HSM to ensures that nodes are booted and Ansible can SSH to them.\n  BOA to CFS\nBOA directs CFS to apply post-boot configuration.\n  CFS applies configuration\nCFS runs Ansible on the nodes and applies post-boot configuration (also called node personalization). CFS then communicates the results back to BOA.\n  Reconfigure Nodes Use Case: Administrator reconfigures compute nodes that are already booted and configured.\nComponents: This workflow is based on the interaction of the BOS with other services during the reconfiguration process.\nMentioned in this workflow:\n Boot Orchestration Service (BOS) is responsible for booting, configuring, and shutting down collections of nodes. The Boot Orchestration Service has the following components:  Boot Orchestration Session Template is a collection of one or more boot set objects. A boot set defines a collection of nodes and the information about the boot artifacts and parameters. Boot Orchestration Session carries out an operation. The possible operations in a session are boot, shutdown, reboot, and configure. Boot Orchestration Agent (BOA) is automatically launched to execute the session. A BOA executes the given operation, and if the operation is a boot or a reboot, it also configures the nodes post-boot (if configure is enabled).   Configuration Framework Service (CFS) configures nodes using configuration framework. Launches and aggregates the status from one or more Ansible instances against nodes (node personalization) or images (image customization). Hardware State Manager (HSM) tracks the state of each node and their group and role associations.  Workflow Overview: The following sequence of steps occur during this workflow.\n  Administrator creates a session template\nA session template is a collection of metadata for a group of nodes and their desired configuration. A session template can be created from a JSON structure. It returns a SessionTemplate ID if successful.\nSee Manage a Session Template for more information.\n  Administrator creates a session\nCreate a session to perform the operation specified in the operation request parameter on the boot set defined in the session template. For this use case, Administrator creates a session with operation as Boot and specifies the session template ID. The set of allowed operations are:\n Boot – Boot nodes that are powered off Configure – Reconfigure the nodes using the Configuration Framework Service (CFS) Reboot – Gracefully power down nodes that are on and then power them back up Shutdown – Gracefully power down nodes that are on  # cray bos session create \\ --template-uuid SESSIONTEMPLATE_NAME \\ --operation Configure   Launch BOA\nThe creation of a session results in the creation of a Kubernetes BOA job to complete the operation. BOA coordinates with the underlying subsystem to complete the requested operation.\n  BOA to HSM\nBOA coordinates with HSM to validate node group and node status.\n  BOA to CFS\nBOA directs CFS to apply post-boot configuration.\n  CFS applies configuration\nCFS runs Ansible on the nodes and applies post-boot configuration (also called node personalization).\n  CFS to BOA\nCFS then communicates the results back to BOA.\n  Power Off Nodes Use Cases: Administrator powers off selected compute nodes.\nComponents: This workflow is based on the interaction of the Boot Orchestration Service (BOS) with other services during the node shutdown process:\nMentioned in this workflow:\n Boot Orchestration Service (BOS) is responsible for booting, configuring, and shutting down collections of nodes. The Boot Orchestration Service has the following components:  Boot Orchestration Session Template is a collection of one or more boot set objects. A boot set defines a collection of nodes and the information about the boot artifacts and parameters. Boot Orchestration Session carries out an operation. The possible operations in a session are boot, shutdown, reboot, and configure. Boot Orchestration Agent (BOA) is automatically launched to execute the session. A BOA executes the given operation, and if the operation is a boot or a reboot, it also configures the nodes post-boot (if configure is enabled).   Cray Advanced Platform and Monitoring Control (CAPMC) service provides system-level power control for nodes in the system. CAPMC interfaces directly with the Redfish APIs to the controller infrastructure to effect power and environmental changes on the system. Hardware State Manager (HSM) tracks the state of each node and their group and role associations.  Workflow Overview: The following sequence of steps occur during this workflow.\n  Administrator creates a session template\nA session template is a collection of metadata for a group of nodes and their desired configuration. A session template can be created from a JSON structure. It returns a SessionTemplate ID if successful.\nSee Manage a Session Template for more information.\n  Administrator creates a session\nCreate a session to perform the operation specified in the operation request parameter on the boot set defined in the session template. For this use case, Administrator creates a session with operation as Boot and specifies the session template ID. The set of allowed operations are:\n Boot – Boot nodes that are powered off Configure – Reconfigure the nodes using the Configuration Framework Service (CFS) Reboot – Gracefully power down nodes that are on and then power them back up Shutdown – Gracefully power down nodes that are on  # cray bos session create \\ --template-uuid SESSIONTEMPLATE_NAME \\ --operation Shutdown   Launch BOA\nThe creation of a session results in the creation of a Kubernetes BOA job to complete the operation. BOA coordinates with the underlying subsystem to complete the requested operation.\n  BOA to HSM\nBOA coordinates with HSM to validate node group and node status.\n  BOA to CAPMC\nBOA directs CAPMC to power off the nodes.\n  CAPMC to the nodes\nCAPMC interfaces directly with the Redfish APIs and powers off the selected nodes.\n  CAPMC to BOA\nCAPMC communicates the results back to BOA.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/boot_orchestration/boot_issue_symptom_node_console_or_logs_indicate_that_the_server_response_has_timed_out/",
	"title": "Compute Node Boot Issue Symptom Node Console Or Logs Indicate That The Server Response Has Timed Out",
	"tags": [],
	"description": "",
	"content": "Compute Node Boot Issue Symptom: Node Console or Logs Indicate that the Server Response has Timed Out If the TFTP request is able to access the TFTP service pod but is unable to find its way back to the node, it may be because the kernel is not tracking established TFTP connections.\nSymptoms The following image, which is tcpdump data from within the TFTP pod, shows what happens when the TFTP request cannot find a route back to the node that sent the request. The node IP address is 10.32.0.1, which is the IP address of the Kubernetes weave network. It is forwarding the node\u0026rsquo;s TFTP to this pod. The server IP address is 10.32.0.13.\n Line 1: The read request from the node arrives. Line 2: The TFTP server attempts to acknowledge the read request. Line 3: ICMP complains about an unreachable destination/port. Lines 4-6: The TFTP server cannot locate the route to the node. It issues an ARP request for 10.32.0.1, but that does not have any effect. Lines 8 and 11: After waiting, the client resends a read request and eventually times out. This repeated request causes a repeat of lines 2-6 as seen in lines 9,12, and 14-17.  Problem Detection Check if the nf_nat_tftp kernel module has been loaded. The kernel module is loaded on all ingress points in the Kubernetes cluster, so there will likely be no missing kernel modules.\nResolution Load nf_nat_tftp if it has not been loaded already by executing modprobe nf_nat_tftp from the non-compute node (NCN), and then restarting the cray-tftp service.\nncn-m001# kubectl get pods -n services -o wide|grep -E \u0026#34;NAME|tftp\u0026#34; NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES cray-tftp-885cc65c4-fk8bm 2/2 Running 0 56s 10.32.0.63 ncn-w002 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ncn-m001# kubectl delete pod cray-tftp-885cc65c4-fk8bm pod \u0026#34;cray-tftp-885cc65c4-fk8bm\u0026#34; deleted "
},
{
	"uri": "/docs-csm/en-11/operations/artifact_management/artifact_management/",
	"title": "Artifact Management",
	"tags": [],
	"description": "",
	"content": "Artifact Management The Ceph Object Gateway Simple Storage Service (S3) API is used for artifact management. The RESTful API that Ceph provides via the gateway is compatible with the basic data access model of the Amazon S3 API. See the https://docs.ceph.com/docs/mimic/radosgw/s3/ for more information about compatibility. The object gateway is also referred to as the RADOS gateway or simply RGW.\nS3 is an object storage service that provides high-level performance, scalability, security, and data availability. S3 exposes a rudimentary data model, similar to a file system, where buckets (directories) store objects (files). Bucket- and object-level Access Control Lists (ACL) can be provided for flexible access authorization to artifacts stored in S3.\nRGW on HPE Cray EX Systems RGW is installed as a part of the HPE Cray EX Stage 3 deployment. The S3 API is available on systems at the following location:\nhttps://rgw-vip.local The RGW administrative interface (radosgw-admin) is available on non-compute nodes (NCNs).\n"
},
{
	"uri": "/docs-csm/en-11/operations/artifact_management/generate_temporary_s3_credentials/",
	"title": "Generate Temporary S3 Credentials",
	"tags": [],
	"description": "",
	"content": "Generate Temporary S3 Credentials Cray provides a simple token service (STS) via the API gateway for administrators to generate temporary Simple Storage Service (S3) credentials for use with S3 buckets. Temporary S3 credentials are generated using either cURL or Python.\nThe generated S3 credentials will expire after one hour.\nProcedure   Retrieve temporary S3 credentials with cURL.\n  Obtain a JWT token.\nSee Retrieve an Authentication Token for more information.\n  Generate temporary S3 credentials.\nThe following command to call STS assumes the environment variable $TOKEN contains the JWT.\nncn# curl -X PUT -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; \\ https://api-gw-service-nmn.local/apis/sts/token { \u0026#34;Credentials\u0026#34;: { \u0026#34;AccessKeyId\u0026#34;: \u0026#34;KtSRFzmAkoDfgCnBLYt\u0026#34;, \u0026#34;EndpointURL\u0026#34;: \u0026#34;http://rgw.local:8080\u0026#34;, \u0026#34;Expiration\u0026#34;: \u0026#34;2019-10-14T15:15:43.480741+00:00\u0026#34;, \u0026#34;SecretAccessKey\u0026#34;: \u0026#34;6CD15EIY6DQOD3DMN0VZPV1XP3W9N4FFPRI0300\u0026#34;, \u0026#34;SessionToken\u0026#34;: \u0026#34;qbwVvv6w1ec/NwI0VzzOXuzFVczjdVICcij0s7kmqKvyZ59RrHJWjLKvmUhGeBATMtkEK72s+qL7Tdn06tPMCQr04MEOpyeUOLmfFyKN3Awm0/7Rlx7rKVaOejpeYaRzO2kWDu3llrpZOONSMPYfck6KjAfvqg/ZJPGEJ5Mzb9YfeSCBq0ghj3G51o9V4DhjjL0YoA/XARMnN0NTHav+OIUHBkXcxZIfT+ti9bSjmz6ExKsJj8zPLvGMK2TIo/Xp\u0026#34; } }     Retrieve temporary S3 credentials with Python (s3creds.py).\n# /usr/bin/env python3 # s3creds.py - Generate a temporary s3 token from the Cray Simple Token Service import os import oauthlib.oauth2 import requests_oauthlib realm = \u0026#39;shasta\u0026#39; client_id = \u0026#39;shasta\u0026#39; username = \u0026#39;testuser\u0026#39; # Provide a user here password = os.environ.get(\u0026#39;TESTUSER_PASSWORD\u0026#39;) # Obtain the password from the env, or elsewhere token_url = \u0026#39;https://api-gw-service-nmn.local/keycloak/realms/%s/protocol/openid-connect/token\u0026#39; % realm sts_url = \u0026#39;https://api-gw-service-nmn.local/apis/sts/token\u0026#39; # Create an OAuth2Session and request a token oauth_client = oauthlib.oauth2.LegacyApplicationClient(client_id=client_id) session = requests_oauthlib.OAuth2Session( client=oauth_client, token_updater=lambda t: None, auto_refresh_url=token_url, auto_refresh_kwargs={\u0026#39;client_id\u0026#39;: client_id} ) session.fetch_token( token_url=token_url, client_id=client_id, username=username, password=password ) # Retrieve S3 credentials from STS sts_response = session.put(sts_url) sts_response.raise_for_status() if sts_response.ok: creds = sts_response.json()[\u0026#39;Credentials\u0026#39;] creds_kwargs = { \u0026#39;aws_access_key_id\u0026#39;: creds[\u0026#39;AccessKeyId\u0026#39;], \u0026#39;aws_secret_access_key\u0026#39;: creds[\u0026#39;SecretAccessKey\u0026#39;], \u0026#39;aws_session_token\u0026#39;: creds[\u0026#39;SessionToken\u0026#39;], \u0026#39;endpoint_url\u0026#39;: creds[\u0026#39;EndpointURL\u0026#39;], } The mapping creds_kwargs can now be used for further interaction with S3 in Python.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/artifact_management/manage_artifacts_with_the_cray_cli/",
	"title": "Manage Artifacts With The Cray Cli",
	"tags": [],
	"description": "",
	"content": "Manage Artifacts with the Cray CLI The artifacts (objects) available for use on the system are created and managed with the Cray CLI. The cray artifacts command provides the ability to manage any given artifact. The Cray CLI automatically authenticates users and provides Simple Storage Service (S3) credentials.\nAll operations with the cray artifacts command assume that the user has already been authenticated. If the user has not been authenticated with the Cray CLI, run the following command and enter the appropriate credentials:\nncn# cray auth login Username: adminuser Password: Success! Authorization Is Local to a Host: whenever you are using the CLI (cray command) on a host (e.g. a workstation or NCN) where it has not been used before, it is necessary to authenticate on that host using cray auth login. There is no mechanism to distribute CLI authorization amongst hosts.\nView S3 Buckets There are several S3 buckets available that can be used to upload and download files with the cray artifacts command. To see the list of available S3 buckets:\nncn# cray artifacts buckets list results = [ \u0026quot;alc\u0026quot;, \u0026quot;badger\u0026quot;, \u0026quot;benji-backups\u0026quot;, \u0026quot;boot-images\u0026quot;, \u0026quot;etcd-backup\u0026quot;, \u0026quot;fw-update\u0026quot;, \u0026quot;ims\u0026quot;, \u0026quot;nmd\u0026quot;, \u0026quot;sds\u0026quot;, \u0026quot;ssm\u0026quot;, \u0026quot;vbis\u0026quot;, \u0026quot;wlm\u0026quot;,] Create and Upload Artifacts Use the cray artifacts create command to create an object and upload it to S3.\nIn the example below, S3_BUCKET is a placeholder for the bucket name, site/repos/repo.tgz is the object name, and /path/to/repo.tgz is the location of the file to be uploaded to S3 on the local file system.\nncn# cray artifacts create S3_BUCKET site/repos/repo.tgz /path/to/repo.tgz artifact = \u0026#34;5c5b6ae5-64da-4212-887a-301087a17099\u0026#34; Key = \u0026#34;site/repos/repo.tgz\u0026#34; In S3, the object name can be path-like and include slashes to resemble files in directories. This is useful for organizing objects within a bucket, but S3 treats it as a name only. No directory structure exists.\nWhen interacting with Cray services, use the artifact value returned by the cray artifacts create command. This will ensure that Cray services can access the uploaded object.\nDownload Artifacts Artifacts are downloaded with the cray artifacts get command. Provide the object name, the bucket, and a file path to download the artifact in order to use this command.\nncn# cray artifacts get S3_BUCKET S3_OBJECT_KEY DOWNLOAD_FILEPATH For example:\nncn# cray artifacts get boot-images 5c5b6ae5-64da-4212-887a-301087a17099 /path/to/downloads/dl-repo.tgz No output is shown unless an error occurs.\nDelete Artifacts Artifacts are removed from buckets with the cray artifacts delete command. Provide the object name and the bucket to delete it.\nncn# cray artifacts delete S3_BUCKET S3_OBJECT_KEY No output is shown unless an error occurs.\nList Artifacts Use the cray artifacts list command to list all artifacts in a bucket.\nncn# cray artifacts list S3_BUCKET [[artifacts]] LastModified = \u0026#34;2020-04-03T12:20:23.876000+00:00\u0026#34; ETag = \u0026#34;\\\u0026#34;e3f195c20a2399bf1b5a20df12416115\\\u0026#34; StorageClass = \u0026#34;STANDARD\u0026#34; Key = \u0026#34;recipes/47411cbe-e249-40f2-8c13-0df7856b91a3/recipe.tar.gz\u0026#34; Size = 11234 [artifacts.Owner] DisplayName = \u0026#34;Image Management Service User\u0026#34; ID = \u0026#34;IMS\u0026#34; ... Retrieve Artifact Details Details of an artifact object in a bucket are found with the cray artifacts describe command. The output of this command provides information about the size of the artifact and any metadata associated with the object.\nIMPORTANT: The Cray-specific metadata provided by this command is automatically generated. This metadata should be considered deprecated and should not be used for future development.\nncn# cray artifacts describe S3_BUCKET S3_OBJECT_KEY [artifact] AcceptRanges = \u0026#34;bytes\u0026#34; ContentType = \u0026#34;binary/octet-stream\u0026#34; LastModified = \u0026#34;2020-04-03T12:20:23+00:00\u0026#34; ContentLength = 11234 VersionId = \u0026#34;.2aoRPGDGRuRIFrjc9urQiHLADvwPCU\u0026#34; ETag = \u0026#34;\\\u0026#34;e3f195c20a2399bf1b5a20df12416115\\\u0026#34; [artifact.Metadata] md5sum = \u0026#34;e3f195c20a2399bf1b5a20df12416115\u0026#34; "
},
{
	"uri": "/docs-csm/en-11/operations/artifact_management/use_s3_libraries_and_clients/",
	"title": "Use S3 Libraries And Clients",
	"tags": [],
	"description": "",
	"content": "Use S3 Libraries and Clients Several command line clients and language-specific libraries are available in addition to the Simple Storage Service (S3) RESTful API. Developers and system administrators can interact with artifacts in the S3 object store with these tools.\nTo learn more, refer to the following links:\n https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html - S3 Python client https://docs.aws.amazon.com/sdk-for-go/api/service/s3/ - Go client https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-welcome.html - Amazon Web Services (AWS) S3 CLI  "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/user_access_service_uas/",
	"title": "User Access Service (UAS)",
	"tags": [],
	"description": "",
	"content": "User Access Service (UAS) The User Access Service (UAS) is a containerized service managed by Kubernetes that enables application developers to create and run user applications. UAS runs on a non-compute node (NCN) that is acting as a Kubernetes worker node.\nUsers launch a User Access Instance (UAI) using the cray command. Users can also transfer data between the Cray system and external systems using the UAI.\nWhen a user requests a new UAI, the UAS service returns status and connection information to the newly created UAI. External access to UAS is routed through a node that hosts gateway services.\nThe timezone inside the UAI container matches the timezone on the host on which it is running, For example, if the timezone on the host is set to CDT, the UAIs on that host will also be set to CDT.\n   Component Function/Description     User Access Instance (UAI) An instance of UAS container.   uas-mgr Manages UAI life cycles.       Container Element Components     Operating system SLES15 SP1   kubectl command Utility to interact with Kubernetes.   cray command Command that allows users to create, describe, and delete UAIs.    Use cray uas list to list the following parameters for a UAI.\nNote: The example values below are used throughout the UAS procedures. They are used as examples only. Users should substitute with site-specific values.\n   Parameter Description Example value     uai_connect_string The UAI connection string ssh user@203.0.113.0 -i ~/.ssh/id\\_rsa   uai_img The UAI image ID registry.local/cray/cray-uas-sles15sp1-slurm:latest   uai_name The UAI name uai-user-be3a6770   uai_status The state of the UAI. Running: Ready   username The user who created the UAI. user   uai_age The age of the UAI. 11m   uai_host The node hosting the UAI. ncn-w001    Getting started UAS is highly configurable and it is recommended that administrators familiarize themselves with the service by reading this topic before allowing users to use UAIs.\nOnce administrators are familiar with the configurable options of UAS, they may want to create a UAI image that matches the booted compute nodes by following the procedure Customize End-User UAI Images.\n"
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/view_a_uai_class/",
	"title": "View A UAI Class",
	"tags": [],
	"description": "",
	"content": "View a UAI Class Display all the information for a specific UAI class by referencing its class ID.\nPrerequisites  Install and initialize the cray administrative CLI. Obtain the ID of a UAI class.  Procedure   View all the information about a specific UAI class.\nTo examine an existing UAI class, use a command of the following form:\nncn-m001-pit# cray uas admin config classes describe \u0026lt;class-id\u0026gt; The following example uses the --format yaml option to display the UAI class configuration in YAML format. Replace yaml with json to return JSON-formatted output. Omitting the --format option displays the UAI class in the default TOML format.\nReplace bb28a35a-6cbc-4c30-84b0-6050314af76b in the example command with the ID of the UAI class to be examined.\nncn-m001-pit# cray uas admin config classes describe \\ --format yaml bb28a35a-6cbc-4c30-84b0-6050314af76b class_id: bb28a35a-6cbc-4c30-84b0-6050314af76b comment: Non-Brokered UAI User Class default: false namespace: user opt_ports: [] priority_class_name: uai-priority public_ip: true resource_config: uai_compute_network: true uai_creation_class: uai_image: default: true image_id: ff86596e-9699-46e8-9d49-9cb20203df8c imagename: dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest volume_mounts: - mount_path: /etc/localtime volume_description: host_path: path: /etc/localtime type: FileOrCreate volume_id: 55a02475-5770-4a77-b621-f92c5082475c volumename: timezone - mount_path: /root/slurm_config/munge volume_description: secret: secret_name: munge-secret volume_id: 7aeaf158-ad8d-4f0d-bae6-47f8fffbd1ad volumename: munge-key - mount_path: /lus volume_description: host_path: path: /lus type: DirectoryOrCreate volume_id: 9fff2d24-77d9-467f-869a-235ddcd37ad7 volumename: lustre - mount_path: /etc/slurm volume_description: config_map: name: slurm-map volume_id: ea97325c-2b1d-418a-b3b5-3f6488f4a9e2 volumename: slurm-config Refer to UAI Classes and Elements of a UAI for an explanation of the output of this command.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/volumes/",
	"title": "Volumes",
	"tags": [],
	"description": "",
	"content": "Volumes Volumes provide a way to connect UAIs to external data, whether they be Kubernetes managed objects, external file systems or files, host node files and directories, or remote networked data to be used within the UAI.\nThe following are examples of how volumes are commonly used by UAIs:\n To connect UAIs to configuration files like /etc/localtime maintained by the host node To connect end-user UAIs to Slurm or PBS Professional Workload Manager configuration shared through Kubernetes To connect end-user UAIs to Programming Environment libraries and tools hosted on the UAI host nodes To connect end-user UAIs to Lustre or other external storage for user data To connect broker UAIs to a directory service (see Configure a Broker UAI Class) or SSH configuration (see Customize the Broker UAI Image) needed to authenticate and redirect user sessions  Any kind of volume recognized by the Kubernetes installation can be installed as a volume within UAS and will be used when creating UAIs. There is more information on Kubernetes volumes here.\nNOTE: As with UAI images, registering a volume with UAS creates the configuration that will be used to create a UAI. If the underlying object referred to by the volume does not exist at the time the UAI is created, the UAI will, in most cases, wait until the object becomes available before starting up. This will be visible in the UAI state which will eventually move to waiting.\n"
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/uas_and_uai_health_checks/",
	"title": "UAS And UAI Health Checks",
	"tags": [],
	"description": "",
	"content": "UAS and UAI Health Checks Initialize and authorize the CLI so a user may run procedures on any given node.\nInitialize and Authorize the CLI The procedures below use the CLI as an authorized user and run on two separate node types. The first part runs on the LiveCD node while the second part runs on a non-LiveCD Kubernetes master or worker node. When using the CLI on either node, the CLI configuration must be initialized and the user running the procedure must be authorized. This section describes how to initialize the CLI for use by a user and authorize the CLI as a user to run the procedures on any given node. The procedures will need to be repeated in both stages of the validation procedure.\nDiscontinue Use of the CRAY_CREDENTIALS Service Account Token Installation procedures leading up to production mode on Shasta use the CLI with a Kubernetes managed service account normally used for internal operations. There is a procedure for extracting the OAUTH token for this service account and assigning it to the CRAY_CREDENTIALS environment variable to permit simple CLI operations. The UAS / UAI validation procedure runs as a post-installation procedure and requires an actual user with Linux credentials, not this service account. Prior to running any of the steps below you must unset the CRAY_CREDENTIALS environment variable.\nncn-m002# unset CRAY_CREDENTIALS Initialize the CLI Configuration The CLI needs to know what host to use to obtain authorization and what user is requesting authorization so it can obtain an OAUTH token to talk to the API Gateway. This is accomplished by initializing the CLI configuration. This example uses the vers username. In practice, vers and the response to the password: prompt should be replaced with the username and password of the administrator running the validation procedure.\nTo check whether the CLI needs initialization, run the following command.\nncn-m002# cray config describe If the output appears as follows, the CLI requires initialization.\nUsage: cray config describe [OPTIONS] Error: No configuration exists. Run `cray init` If the output appears more like the following, then the CLI is initialized and logged in as vers. If that is the incorrect username, authorize the correct username and password in the next section. If vers is the correct user, proceed to the validation procedure on that node.\nIf the CLI must be initialized again, use the following command and include the correct username, password, and the password response.\nncn-m002# cray init Cray Hostname: api-gw-service-nmn.local Username: vers Password: Success! Initialization complete. Authorize the Correct CLI User If the CLI is initialized but authorized for a user different, run the following command and substitute the correct username and password.\nncn-m002# cray auth login Username: vers Password: Success! Authorization Is Local to a Host: whenever you are using the CLI (cray command) on a host (e.g. a workstation or NCN) where it has not been used before, it is necessary to authenticate on that host using cray auth login. There is no mechanism to distribute CLI authorization amongst hosts.\nTroubleshoot CLI Initialization or Authorization Issues If initialization or authorization fails in any of the preceding steps, there are several common causes.\n DNS failure looking up api-gw-service-nmn.local may be preventing the CLI from reaching the API Gateway and Keycloak for authorization Network connectivity issues with the NMN may be preventing the CLI from reaching the API Gateway and Keycloak for authorization Certificate mismatch or trust issues may be preventing a secure connection to the API Gateway Istio failures may be preventing traffic from reaching Keycloak Keycloak may not yet be set up to authorize you as a user  While resolving these issues is beyond the scope of this section, adding -vvvvv to the cray auth or cray init commands may offer clues as to why the initialization or authorization is failing.\nValidate the Basic UAS Installation This procedure and the following procedures run on separate nodes on the system and validate the basic UAS installation. Ensure this runs on the LiveCD node and that the CLI is authorized for the user.\nncn-m002# cray uas mgr-info list service_name = \u0026#34;cray-uas-mgr\u0026#34; version = \u0026#34;1.11.5\u0026#34; ncn-m001-pit# cray uas list results = [] This shows that UAS is installed and running version 1.11.5 and that no UAIs are running. If another user has been using the UAS, it is possible to see UAIs in the list. That is acceptable from a validation standpoint.\nTo verify that the pre-made UAI images are registered with UAS, run the following command.\nncn-m002# cray uas images list default_image = \u0026#34;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026#34; image_list = [ \u0026#34;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026#34;,] The output shows that the pre-made end-user UAI image, cray/cray-uai-sles15sp1:latest, is registered with UAS. This does not necessarily mean this image is installed in the container image registry, but it is configured for use. If other UAI images have been created and registered, they may also appear in the output.\nValidate UAI Creation This procedure:\n Must run on a master or worker node (and not ncn-w001). Must run on the HPE Cray EX system (or from an external host, but the procedure for that is not covered here). Requires that the CLI be initialized and authorized as for the current user.  To verify that the user account can create a UAI, use the following command.\nncn-w003# cray uas create --publickey ~/.ssh/id_rsa.pub uai_connect_string = \u0026#34;ssh vers@10.16.234.10\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uai-sles15sp1:latest\u0026#34; uai_ip = \u0026#34;10.16.234.10\u0026#34; uai_msg = \u0026#34; uai_name = \u0026#34;uai-vers-a00fb46b\u0026#34; uai_status = \u0026#34;Pending\u0026#34; username = \u0026#34;vers\u0026#34; [uai_portmap] The UAI is now created and in the process of initializing and running.\nThe following can be repeated as needed to view the UAIs state. If the results appear like the following, the UAI is ready for use.\nncn-w003# cray uas list [[results]] uai_age = \u0026#34;0m\u0026#34; uai_connect_string = \u0026#34;ssh vers@10.16.234.10\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uai-sles15sp1:latest\u0026#34; uai_ip = \u0026#34;10.16.234.10\u0026#34; uai_msg = \u0026#34; uai_name = \u0026#34;uai-vers-a00fb46b\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; username = \u0026#34;vers\u0026#34; Log into the UAI (without a password) as follows:\nncn-w003# ssh vers@10.16.234.10 The authenticity of host \u0026#39;10.16.234.10 (10.16.234.10)\u0026#39; can\u0026#39;t be established. ECDSA key fingerprint is SHA256:BifA2Axg5O0Q9wqESkLqK4z/b9e1usiDUZ/puGIFiyk. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added \u0026#39;10.16.234.10\u0026#39; (ECDSA) to the list of known hosts. vers@uai-vers-a00fb46b-6889b666db-4dfvn:~\u0026gt; ps -afe UID PID PPID C STIME TTY TIME CMD root 1 0 0 18:51 ? 00:00:00 /bin/bash /usr/bin/uai-ssh.sh munge 36 1 0 18:51 ? 00:00:00 /usr/sbin/munged root 54 1 0 18:51 ? 00:00:00 su vers -c /usr/sbin/sshd -e -f /etc/uas/ssh/sshd_config -D vers 55 54 0 18:51 ? 00:00:00 /usr/sbin/sshd -e -f /etc/uas/ssh/sshd_config -D vers 62 55 0 18:51 ? 00:00:00 sshd: vers [priv] vers 67 62 0 18:51 ? 00:00:00 sshd: vers@pts/0 vers 68 67 0 18:51 pts/0 00:00:00 -bash vers 120 68 0 18:52 pts/0 00:00:00 ps -afe vers@uai-vers-a00fb46b-6889b666db-4dfvn:~\u0026gt; exit logout Connection to 10.16.234.10 closed. Clean up the UAI and note that the UAI name used is the same as the name in the output from cray uas create above.\nncn-w003# cray uas delete --uai-list uai-vers-a00fb46b results = [ \u0026#34;Successfully deleted uai-vers-a00fb46b\u0026#34;,] Troubleshoot UAS and UAI Operations Issues Authorization Issues\nIf the user is not logged in as a valid Keycloak user or is inadvertently using the CRAY_CREDENTIALS environment variable (i.e. the variable is set if the user is logged in with the their username or another username), the output of running the cray uas list command will produce output like the following.\nncn-w003# cray uas list Usage: cray uas list [OPTIONS] Try \u0026#39;cray uas list --help\u0026#39; for help. Error: Bad Request: Token not valid for UAS. Attributes missing: [\u0026#39;gidNumber\u0026#39;, \u0026#39;loginShell\u0026#39;, \u0026#39;homeDirectory\u0026#39;, \u0026#39;uidNumber\u0026#39;, \u0026#39;name\u0026#39;] Fix this by logging in as a \u0026ldquo;real user\u0026rdquo; (a user with Linux credentials) and ensure that CRAY_CREDENTIALS is unset.\nUAS Cannot Access Keycloak If the output of the cray uas list command appears similar to the following, the wrong hostname to reach the API gateway may be in use. In that case, run the CLI initialization steps again.\nncn-w003# cray uas list Usage: cray uas list [OPTIONS] Try \u0026#39;cray uas list --help\u0026#39; for help. Error: Internal Server Error: An error was encountered while accessing Keycloak There also may be a problem with the Istio service mesh inside of the Shasta system. Troubleshooting this is beyond the scope of this section, but viewing the UAS pod logs in Kubernetes may provide useful information.\nThere are typically two UAS pods. View logs from both pods to identify the specific failure. The logs have a very large number of GET events listed as part of the aliveness checking. The following shows an example of viewing UAS logs (the example shows only one UAS manage, normally there would be two).\nncn-w003# kubectl get po -n services | grep uas-mgr | grep -v etcd cray-uas-mgr-6bbd584ccb-zg8vx 2/2 Running 0 12d ncn-w003# kubectl logs -n services cray-uas-mgr-6bbd584ccb-zg8vx cray-uas-mgr | grep -v \u0026#39;GET \u0026#39; | tail -25 2021-02-08 15:32:41,211 - uas_mgr - INFO - getting deployment uai-vers-87a0ff6e in namespace user 2021-02-08 15:32:41,225 - uas_mgr - INFO - creating deployment uai-vers-87a0ff6e in namespace user 2021-02-08 15:32:41,241 - uas_mgr - INFO - creating the UAI service uai-vers-87a0ff6e-ssh 2021-02-08 15:32:41,241 - uas_mgr - INFO - getting service uai-vers-87a0ff6e-ssh in namespace user 2021-02-08 15:32:41,252 - uas_mgr - INFO - creating service uai-vers-87a0ff6e-ssh in namespace user 2021-02-08 15:32:41,267 - uas_mgr - INFO - getting pod info uai-vers-87a0ff6e 2021-02-08 15:32:41,360 - uas_mgr - INFO - No start time provided from pod 2021-02-08 15:32:41,361 - uas_mgr - INFO - getting service info for uai-vers-87a0ff6e-ssh in namespace user 127.0.0.1 - - [08/Feb/2021 15:32:41] \u0026#34;POST /v1/uas?imagename=registry.local%2Fcray%2Fno-image-registered%3Alatest HTTP/1.1\u0026#34; 200 - 2021-02-08 15:32:54,455 - uas_auth - INFO - UasAuth lookup complete for user vers 2021-02-08 15:32:54,455 - uas_mgr - INFO - UAS request for: vers 2021-02-08 15:32:54,455 - uas_mgr - INFO - listing deployments matching: host None, labels uas=managed,user=vers 2021-02-08 15:32:54,484 - uas_mgr - INFO - getting pod info uai-vers-87a0ff6e 2021-02-08 15:32:54,596 - uas_mgr - INFO - getting service info for uai-vers-87a0ff6e-ssh in namespace user 2021-02-08 15:40:25,053 - uas_auth - INFO - UasAuth lookup complete for user vers 2021-02-08 15:40:25,054 - uas_mgr - INFO - UAS request for: vers 2021-02-08 15:40:25,054 - uas_mgr - INFO - listing deployments matching: host None, labels uas=managed,user=vers 2021-02-08 15:40:25,085 - uas_mgr - INFO - getting pod info uai-vers-87a0ff6e 2021-02-08 15:40:25,212 - uas_mgr - INFO - getting service info for uai-vers-87a0ff6e-ssh in namespace user 2021-02-08 15:40:51,210 - uas_auth - INFO - UasAuth lookup complete for user vers 2021-02-08 15:40:51,210 - uas_mgr - INFO - UAS request for: vers 2021-02-08 15:40:51,210 - uas_mgr - INFO - listing deployments matching: host None, labels uas=managed,user=vers 2021-02-08 15:40:51,261 - uas_mgr - INFO - deleting service uai-vers-87a0ff6e-ssh in namespace user 2021-02-08 15:40:51,291 - uas_mgr - INFO - delete deployment uai-vers-87a0ff6e in namespace user 127.0.0.1 - - [08/Feb/2021 15:40:51] \u0026#34;DELETE /v1/uas?uai_list=uai-vers-87a0ff6e HTTP/1.1\u0026#34; 200 - UAI Images not in Registry If output is similar to the following, the pre-made end-user UAI image is not in the user\u0026rsquo;s local registry (or whatever registry it is being pulled from, see the uai_img value for details). Locate and the image and push / import it to the registry.\nncn-w003# cray uas list [[results]] uai_age = \u0026#34;0m\u0026#34; uai_connect_string = \u0026#34;ssh vers@10.103.13.172\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uai-sles15sp1:latest\u0026#34; uai_ip = \u0026#34;10.103.13.172\u0026#34; uai_msg = \u0026#34;ErrImagePull\u0026#34; uai_name = \u0026#34;uai-vers-87a0ff6e\u0026#34; uai_status = \u0026#34;Waiting\u0026#34; username = \u0026#34;vers\u0026#34; Missing Volumes and Other Container Startup Issues Various packages install volumes in the UAS configuration. All of those volumes must also have the underlying resources available, sometimes on the host node where the UAI is running and sometimes from within Kubernetes. If the UAI gets stuck with a ContainerCreating uai_msg field for an extended time, this is a likely cause. UAIs run in the user Kubernetes namespace and are pods that can be examined using kubectl describe.\nRun the following command to locate the pod.\nncn-w003# kubectl get po -n user | grep \u0026lt;uai-name\u0026gt; Run the following command to investigate the problem.\nncn-w003# kubectl describe -n user \u0026lt;pod-name\u0026gt; If volumes are missing, they will be in the Events:section of the output. Other problems may show up there as well. The names of the missing volumes or other issues should indicate what needs to be fixed to enable the UAI.\n"
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/update_a_resource_specification/",
	"title": "Update A Resource Specification",
	"tags": [],
	"description": "",
	"content": "Update a Resource Specification Modify a specific UAI resource specification using the resource_id of that specification.\nPrerequisites  Install and initialize the cray administrative CLI. Verify that the resource specification to be updated exists within UAS. Perform eitherList UAI Resource Specifications or Retrieve Resource Specification Details.  Procedure To modify a particular resource specification, use a command of the following form:\nncn-m001-pit# cray uas admin config resources update [OPTIONS] RESOURCE_ID The [OPTIONS] used by this command are the same options used to create resource specifications. See Create a UAI Resource Specification and Elements of a UAI for a full description of those options.\n  Update a UAI resource specification.\nThe following example changes the CPU and memory limits on a UAI resource specification to 0.1 CPU and 10MiB, respectively.\nncn-m001-pit# cray uas admin config resources update \\ --limit \u0026#39;{\u0026#34;cpu\u0026#34;: \u0026#34;100m\u0026#34;, \u0026#34;memory\u0026#34;: \u0026#34;10Mi\u0026#34;}\u0026#39; 85645ff3-1ce0-4f49-9c23-05b8a2d31849   "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/update_a_uai_image_registration/",
	"title": "Update A UAI Image Registration",
	"tags": [],
	"description": "",
	"content": "Update a UAI Image Registration Modify the UAS registration information of a UAI image.\nPrerequisites Verify that the image to be updated is registered with UAS. Refer to Retrieve UAI Image Registration Information.\nProcedure Once an allowable UAI image has been created, it may be necessary to change its attributes. For example, the default image may need to change.\n  Modify the registration information of a UAI image by using a command of the form:\nncn-m001-pit# cray uas admin config images update OPTIONS IMAGE_ID Use the --default or --imagename options as specified when registering an image to update those specific elements of an existing image registration. For example, to make the registry.local/cray/custom-end-user-uai:latest image shown above the default image, use the following command:\nncn-m001-pit# cray uas admin config images update --default yes 8fdf5d4a-c190-24c1-2b96-74ab98c7ec07   "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/update_a_uas_volume/",
	"title": "Update A UAS Volume",
	"tags": [],
	"description": "",
	"content": "Update a UAS Volume Modify the configuration of an already-registered UAS volume. Almost any part of the configuration of a UAS volume can be modified.\nPrerequisites  Install and initialize the cray administrative CLI. Obtain the UAS volume ID of a volume. Perform List Volumes Registered in UAS if needed. Read Add a Volume to UAS. The options and caveats for updating volumes are the same as for creating volumes.  Procedure   Modify the configuration of a UAS volume.\nOnce a UAS volume has been configured, any part of it except for the volume_id can be updated with a command of the following form:\ncray uas admin config volumes update [options] \u0026lt;volume-id\u0026gt; For example:\nncn-m001-pit# cray uas admin config volumes update --volumename 'my-example-volume' a0066f48-9867-4155-9268-d001a4430f5c The --volumename, --volume-description, and --mount-path options may be used in any combination to update the configuration of a given volume.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/uai_macvlans_network_attachments/",
	"title": "UAI Macvlans Network Attachments",
	"tags": [],
	"description": "",
	"content": "UAI macvlans Network Attachments UAIs need to be able to reach compute nodes across the node management network (NMN). When the compute node NMN is structured as multiple subnets, this requires routing form the UAIs to those subnets. The default route in a UAI goes to the public network through the Customer Access Network (CAN) so that will not work for reaching compute nodes. To solve this problem, UAS installs Kubernetes network attachments within the Kubernetes user namespace, one of which is used by UAIs.\nThe type of network attachment used on HPE Cray EX hardware for this purpose is a macvlan network attachment, so this is often referred to on HPE Cray EX systems as \u0026ldquo;macvlans\u0026rdquo;. This network attachment integrates the UAI into the NMN on the UAI host node where the UAI is running and assigns the UAI an IP address on that network. It also installs a set of routes in the UAI that are used to reach the compute node subnets on the NMN.\n"
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/uai_management/",
	"title": "UAI Management",
	"tags": [],
	"description": "",
	"content": "UAI Management UAS supports two manual methods and one automated method of UAI management:\n Direct administrative UAI management Legacy mode user driven UAI management UAI broker mode UAI management  Direct administrative UAI management is available mostly to allow administrators to set up UAI brokers for the UAI broker mode of UAI management and to control UAIs that are created under one of the other two methods. It is unlikely that a site will choose to create end-user UAIs this way, but it is possible to do. The administrative UAI management API provides an administrative way to list, create, examine and delete UAIs.\nThe legacy mode of UAI management gives users the authority to create, list and delete UAIs that belong to them. While this is a conceptually simple mode, it can lead to an unnecessary proliferation of UAIs belonging to a single user if the user is not careful to create UAIs only when needed. The legacy mode also cannot take advantage of UAI classes to create more than one kind of UAI for different users' needs.\nThe UAI broker mode creates / re-uses UAIs on demand when a user logs into a broker UAI using SSH. A site may run multiple broker UAIs, each configured to create UAIs of a different UAI class and each running with its own externally visible IP address. By choosing the correct IP address and logging into the broker, a user ultimately arrives in a UAI tailored for a given use case. Because the broker is responsible for managing the underlying end-user UAIs, users need not be given authority to create UAIs directly and, therefore, cannot cause a proliferation of unneeded UAIs. Because the broker UAIs each run separately on different IP addresses with, potentially, different user authorizations configured, a site can control which users are given access to which classes of end-user UAIs.\n"
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/uai_network_attachments/",
	"title": "UAI Network Attachments",
	"tags": [],
	"description": "",
	"content": "UAI Network Attachments The UAI network attachment configuration flows from the CRAY Site Initializer (CSI) localization data through customizations.yaml into the UAS Helm chart and, ultimately, into Kubernetes in the form of a \u0026ldquo;network-attachment-definition\u0026rdquo;.\nThis section describes the data at each of those stages to show how the final network attachment gets created.\nCSI Localization Data The details of CSI localization are beyond the scope of this guide, but here are the important settings, and the values used in the following examples:\n The interface name on which the Kubernetes worker nodes reach their NMN subnet: vlan002 The network and CIDR configured on that interface: 10.252.0.0/17 The IP address of the gateway to other NMN subnets found on that network: 10.252.0.1 The subnets where compute nodes reside on this system:  10.92.100.0/24 10.106.0.0/17 10.104.0.0/17    Contents of customizations.yaml When CSI runs, it produces the following data structure in the spec section of customizations.yaml:\nspec: ... wlm: ... macvlansetup: nmn_subnet: 10.252.2.0/23 nmn_supernet: 10.252.0.0/17 nmn_supernet_gateway: 10.252.0.1 nmn_vlan: vlan002 # NOTE: the term DHCP here is misleading, this is merely # a range of reserved IPs for UAIs that should not # be handed out to others because the network # attachment will hand them out to UAIs. nmn_dhcp_start: 10.252.2.10 nmn_dhcp_end: 10.252.3.254 routes: - dst: 10.92.100.0/24 gw: 10.252.0.1 - dst: 10.106.0.0/17 gw: 10.252.0.1 - dst: 10.104.0.0/17 gw: 10.252.0.1 The nmn_subnet value shown here is not relevant for this section.\nThese values, in turn, feed into the following translation to UAS Helm chart settings:\n cray-uas-mgr: uasConfig: uai_macvlan_interface: '{{ wlm.macvlansetup.nmn_vlan }}' uai_macvlan_network: '{{ wlm.macvlansetup.nmn_supernet }}' uai_macvlan_range_start: '{{ wlm.macvlansetup.nmn_dhcp_start }}' uai_macvlan_range_end: '{{ wlm.macvlansetup.nmn_dhcp_end }}' uai_macvlan_routes: '{{ wlm.macvlansetup.routes }}' UAS Helm Chart The inputs in the previous section tell the UAS Helm chart how to install the network attachment for UAIs. While the actual template used for this is more complex, the following is a simplified view of the template used to generate the network attachment.\nIf reading this document from the UAS source code, the real template in the Helm chart is located there.\napiVersion: \u0026quot;k8s.cni.cncf.io/v1\u0026quot; kind: NetworkAttachmentDefinition ... spec: config: '{ \u0026quot;cniVersion\u0026quot;: \u0026quot;0.3.0\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;macvlan\u0026quot;, \u0026quot;master\u0026quot;: \u0026quot;{{ .Values.uasConfig.uai_macvlan_interface }}\u0026quot;, \u0026quot;mode\u0026quot;: \u0026quot;bridge\u0026quot;, \u0026quot;ipam\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;host-local\u0026quot;, \u0026quot;subnet\u0026quot;: \u0026quot;{{ .Values.uasConfig.uai_macvlan_network }}\u0026quot;, \u0026quot;rangeStart\u0026quot;: \u0026quot;{{ .Values.uasConfig.uai_macvlan_range_start }}\u0026quot;, \u0026quot;rangeEnd\u0026quot;: \u0026quot;{{ .Values.uasConfig.uai_macvlan_range_end }}\u0026quot;, \u0026quot;routes\u0026quot;: [ {{- range $index, $route := .Values.uasConfig.uai_macvlan_routes }} {{- range $key, $value := $route }} { \u0026quot;{{ $key }}\u0026quot;: \u0026quot;{{ $value }}\u0026quot;, }, {{- end }} {{- end }} ] } }' The range templating in the routes section expands the routes from customizations.yaml into the network attachment routes.\nUAI Network Attachment in Kubernetes All of this produces a network attachment definition in Kubernetes called macvlan-uas-nmn-conf which is used by UAS.\nThe following contents would result from the above data:\napiVersion: v1 items: - apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition ... spec: config: '{ \u0026quot;cniVersion\u0026quot;: \u0026quot;0.3.0\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;macvlan\u0026quot;, \u0026quot;master\u0026quot;: \u0026quot;vlan002\u0026quot;, \u0026quot;mode\u0026quot;: \u0026quot;bridge\u0026quot;, \u0026quot;ipam\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;host-local\u0026quot;, \u0026quot;subnet\u0026quot;: \u0026quot;10.252.0.0/17\u0026quot;, \u0026quot;rangeStart\u0026quot;: \u0026quot;10.252.124.10\u0026quot;, \u0026quot;rangeEnd\u0026quot;: \u0026quot;10.252.125.244\u0026quot;, \u0026quot;routes\u0026quot;: [ { \u0026quot;dst\u0026quot;: \u0026quot;10.92.100.0/24\u0026quot;, \u0026quot;gw\u0026quot;: \u0026quot;10.252.0.1\u0026quot; }, { \u0026quot;dst\u0026quot;: \u0026quot;10.106.0.0/17\u0026quot;, \u0026quot;gw\u0026quot;: \u0026quot;10.252.0.1\u0026quot; }, { \u0026quot;dst\u0026quot;: \u0026quot;10.104.0.0/17\u0026quot;, \u0026quot;gw\u0026quot;: \u0026quot;10.252.0.1\u0026quot; } ] } }' ... In this example, Kubernetes will assign UAI IP addresses in the range 10.252.2.10 through 10.252.3.244 on the network attachment, and will permit those UAIs to reach compute nodes on any of four possible NMN subnets:\n Directly through the NMN subnet hosting the UAI host node itself (10.252.0.0/17 here) Through the gateway in the local NMN subnet (10.252.0.1 here) to  10.92.100.0/24 10.106.0.0/17 10.104.0.0/17    "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/uas_limitations/",
	"title": "UAS Limitations",
	"tags": [],
	"description": "",
	"content": "UAS Limitations Functionality that is currently not supported while using UAS.\nFunctionality Not Currently Supported by the User Access Service  Lustre (lfs) commands within the UAS service pod Executing Singularity containers within the UAS service Building Docker containers within the UAS environment Building containerd containers within the UAS environment dmesg cannot run inside a UAI because of container security limitations Users cannot ssh from ncn-w001 to a UAI. This is because UAIs use LoadBalancer IP addresses on the Customer Access Network (CAN) instead of NodePorts and the LoadBalancer IP addresses are not accessible from ncn-w001.  Other Limitations  There is a known issue where X11 traffic may not forward DISPLAY correctly if the user logs into an NCN node before logging into a UAI. The cray uas uais commands are not restricted to the user authenticated with cray auth login.  Limitations Related To Restarts Changes made to a running UAI will be lost if the UAI is restarted or deleted. The only changes in a UAI that will persist are those written to an externally mounted file system (such as Lustre or NFS). To make changes to the base image for a UAI, see Create and Register a Custom UAI Image.\nA UAI may restart because of an issue on the physical node, scheduled node maintenance, or intentional restarts by a site administrator. In this case, any running processes (such as compiles), Slurm interactive jobs, or changes made to the UAI (such as package installations) are lost.\nIf a UAI restarts on a node that was recently rebooted, some of the configured volumes may not be ready and it could appear that content in the UAI is missing. In this case, restart the UAI.\n"
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/uai_host_node_selection/",
	"title": "UAI Host Node Selection",
	"tags": [],
	"description": "",
	"content": "UAI Host Node Selection When selecting UAI host nodes, it is a good idea to take into account the amount of combined load users and system services will bring to those nodes. UAIs run by default at a lower priority than system services on worker nodes which means that, if the combined load exceeds the capacity of the nodes, Kubernetes will eject UAIs and/or refuse to schedule them to protect system services. This can be disruptive or frustrating for users. This section explains how to identify the currently configured UAI host nodes and how to adjust that selection to meet the needs of users.\nIdentify UAI Host Nodes UAI host node identification is an exclusive activity, not an inclusive one, so it starts by identifying the nodes that could potentially be UAI host nodes by their Kubernetes role:\n Identify nodes that could potentially be UAI host nodes by their Kubernetes role.  ncn-m001-pit# kubectl get nodes | grep -v master NAME STATUS ROLES AGE VERSION ncn-w001 Ready \u0026lt;none\u0026gt; 10d v1.18.6 ncn-w002 Ready \u0026lt;none\u0026gt; 25d v1.18.6 ncn-w003 Ready \u0026lt;none\u0026gt; 23d v1.18.6 In this example, there are three nodes known by Kubernetes that are not running as Kubernetes master nodes. These are all potential UAI host nodes.\n Identify the nodes that are excluded from eligibility as UAI host nodes.  ncn-m001-pit# kubectl get no -l uas=False NAME STATUS ROLES AGE VERSION ncn-w001 Ready \u0026lt;none\u0026gt; 10d v1.18.6 NOTE: Given the fact that labels are textual not boolean, it is a good idea to try various common spellings of false. The ones that will prevent UAIs from running are \u0026lsquo;False\u0026rsquo;, \u0026lsquo;false\u0026rsquo; and \u0026lsquo;FALSE\u0026rsquo;. Repeat the above with all three options to be sure.\nOf the non-master nodes, there is one node that is configured to reject UAIs, ncn-w001. So, ncn-w002 and ncn-w003 are UAI host nodes.\nSpecify UAI Host Nodes UAI host nodes are determined by tainting the nodes against UAIs. For example:\nncn-m001-pit# kubectl label node ncn-w001 uas=False --overwrite Please note here that setting uas=True or any variant of that, while potentially useful for local book keeping purposes, does NOT transform the node into a UAS host node. With that setting the node will be a UAS node because the value of the uas flag is not in the list False, false or FALSE, but unless the node previously had one of the false values, it was a UAI node all along. Perhaps more to the point, removing the uas label from a node labeled uas=True does not take the node out of the list of UAI host nodes. The only way to make a non-master Kubernetes node not be a UAS host node is to explicitly set the label to False, false or FALSE.\nMaintain an HSM Group for UAI Host Nodes When it comes to customizing non-compute node (NCN) contents for UAIs, it is useful to have a Hardware State Manager (HSM) node group containing the NCNs that are UAI hosts nodes. The hpe-csm-scripts package provides a script called make_node_groups that is useful for this purpose. This script is normally installed as /opt/cray/csm/scripts/node_management/make_node_groups. It can create and update node groups for management master nodes, storage nodes, management worker nodes, and UAI host nodes.\nThe following summarizes its use:\nncn-m001# /opt/cray/csm/scripts/node_management/make_node_groups --help getopt: unrecognized option '--help' usage: make_node_groups [-m][-s][-u][w][-A][-R][-N] Where: -m - creates a node group for management master nodes -s - creates a node group for management storage nodes -u - creates a node group for UAI worker nodes -w - creates a node group for management worker nodes -A - creates all of the above node groups -N - executes a dry run, showing commands not running them -R - deletes existing node group(s) before creating them Here is an example of a dry-run that will create or update a node group for UAI host nodes:\nncn-m001# /opt/cray/csm/scripts/node_management/make_node_groups -N -R -u (dry run)cray hsm groups delete uai (dry run)cray hsm groups create --label uai (dry run)cray hsm groups members create uai --id x3000c0s4b0n0 (dry run)cray hsm groups members create uai --id x3000c0s5b0n0 (dry run)cray hsm groups members create uai --id x3000c0s6b0n0 (dry run)cray hsm groups members create uai --id x3000c0s7b0n0 (dry run)cray hsm groups members create uai --id x3000c0s8b0n0 (dry run)cray hsm groups members create uai --id x3000c0s9b0n0 Notice that when run in dry-run (-N option) mode, the script only prints out the CLI commands it will execute without actually executing them. When run with the -R option, the script removes any existing node groups before recreating them, effectively updating the contents of the node group. The -u option tells the script to create or update only the node group for UAI host nodes. That node group is named uai in the HSM.\nSo, to create a new node group or replace an existing one, called uai, containing the list of UAI host nodes, use the following command:\n# /opt/cray/csm/scripts/node_management/make_node_groups -R -u "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/uai_host_nodes/",
	"title": "UAI Host Nodes",
	"tags": [],
	"description": "",
	"content": "UAI Host Nodes UAIs run on Kubernetes worker nodes. There is a mechanism using Kubernetes labels to prevent UAIs from running on a specific worker node, however. Any Kubernetes node that is not labeled to prevent UAIs from running on it is considered to be a UAI host node. The administrator of a given site may control the set of UAI host nodes by labeling Kubernetes worker nodes appropriately.\n"
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/uai_images/",
	"title": "UAI Images",
	"tags": [],
	"description": "",
	"content": "UAI Images There are three kinds of UAI images used by UAS:\n A pre-packaged broker UAI image provided with the UAS A pre-packaged basic end-user UAI Image provided with the UAS Custom end-user UAI images created on site, usually based on compute node contents  UAS provides two stock UAI images when installed. The first is a standard end-user UAI Image that has the necessary software installed in it to support a basic Linux distribution login experience. This image also comes with with the Slurm and PBS Professional workload management client software installed, allowing users to take advantage of one or both of these if the underlying support is installed on the host system.\nThe second image is a broker UAI image. Broker UAIs are a special type of UAIs used in the \u0026ldquo;broker based\u0026rdquo; operation model. Broker UAIs present a single SSH endpoint that responds to each SSH connection by locating or creating a suitable end-user UAI and redirecting the SSH session to that end-user UAI.\n"
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/troubleshoot_uais_with_administrative_access/",
	"title": "Troubleshoot UAIs With Administrative Access",
	"tags": [],
	"description": "",
	"content": "Troubleshoot UAIs with Administrative Access Sometimes there is no better way to figure out a problem with a UAI than to get inside it and look around as an administrator. This is done using kubectl exec to start a shell inside the running container as \u0026ldquo;root\u0026rdquo; (in the container). With this an administrator can diagnose problems, make changes to the running UAI and find solutions. It is important to remember that any change made inside a UAI is transitory. These changes only last as long as the UAI is running. To make a permanent change, either the UAI image has to be changed or external customizations must be applied.\nHere is an example session showing a ps command inside the container of a UAI by an administrator:\nncn-m001-pit# cray uas admin uais list [[results]] uai_age = \u0026quot;1d4h\u0026quot; uai_connect_string = \u0026quot;ssh broker@10.103.13.162\u0026quot; uai_host = \u0026quot;ncn-w001\u0026quot; uai_img = \u0026quot;dtr.dev.cray.com/cray/cray-uai-broker:latest\u0026quot; uai_ip = \u0026quot;10.103.13.162\u0026quot; uai_msg = \u0026quot; uai_name = \u0026quot;uai-broker-2e6ce6b7\u0026quot; uai_status = \u0026quot;Running: Ready\u0026quot; username = \u0026quot;broker\u0026quot; [[results]] uai_age = \u0026quot;0m\u0026quot; uai_connect_string = \u0026quot;ssh vers@10.29.162.104\u0026quot; uai_host = \u0026quot;ncn-w001\u0026quot; uai_img = \u0026quot;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026quot; uai_ip = \u0026quot;10.29.162.104\u0026quot; uai_msg = \u0026quot; uai_name = \u0026quot;uai-vers-4ebe1966\u0026quot; uai_status = \u0026quot;Running: Ready\u0026quot; username = \u0026quot;vers\u0026quot; ncn-m001-pit# kubectl get po -n user | grep uai-vers-4ebe1966 uai-vers-4ebe1966-77b7c9c84f-xgqm4 1/1 Running 0 77s ncn-m001-pit# kubectl exec -it -n user uai-vers-4ebe1966-77b7c9c84f-xgqm4 -c uai-vers-4ebe1966 -- /bin/sh sh-4.4# ps -afe UID PID PPID C STIME TTY TIME CMD root 1 0 0 22:56 ? 00:00:00 /bin/bash /usr/bin/uai-ssh.sh munge 36 1 0 22:56 ? 00:00:00 /usr/sbin/munged root 54 1 0 22:56 ? 00:00:00 su vers -c /usr/sbin/sshd -e -f /etc/uas/ssh/sshd_config -D vers 55 54 0 22:56 ? 00:00:00 /usr/sbin/sshd -e -f /etc/uas/ssh/sshd_config -D root 90 0 0 22:58 pts/0 00:00:00 /bin/sh root 97 90 0 22:58 pts/0 00:00:00 ps -afe sh-4.4# The procedure is to find the name of the UAI in question, use that with kubectl to find the pod containing that UAI, use the pod name and the user namespace to set up the kubectl exec specifying the UAI name as the container to exec into and specifying /bin/sh as the command to run. From there, the administrator can look around inside the UAI as needed.\n"
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/troubleshoot_uas_by_viewing_log_output/",
	"title": "Troubleshoot UAS By Viewing Log Output",
	"tags": [],
	"description": "",
	"content": "Troubleshoot UAS by Viewing Log Output At times there will be problems with UAS. Usually this takes the form of errors showing up on CLI commands that are not immediately interpretable as some sort of input error. It is sometimes useful to examine the UAS service logs to find out what is wrong.\nThe first thing to do is to find out the names of the Kubernetes pods running UAS:\nncn-m001-pit# kubectl get po -n services | grep uas | grep -v etcd cray-uas-mgr-6bbd584ccb-zg8vx 2/2 Running 0 7d7h cray-uas-mgr-6bbd584ccb-acg7y 2/2 Running 0 7d7h The logs are collected in the pods, and can be seen using the kubectl logs command on each of the pods. Since the pods produce a lot of debug logging in the form:\n127.0.0.1 - - [02/Feb/2021 22:57:18] \u0026quot;GET /v1/mgr-info HTTP/1.1\u0026quot; 200 - It is a good idea to filter this out unless the problem lies in specifically in the area of GET operations or aliveness checks. The following is an example where the last 25 lines of useful log output are retrieved from the pod cray-uas-mgr-6bbd584ccb-zg8vx:\nkubectl logs -n services cray-uas-mgr-6bbd584ccb-zg8vx cray-uas-mgr | grep -v '\u0026quot;GET ' | tail -25 2021-02-03 22:02:01,576 - uas_mgr - INFO - UAS request for: vers 2021-02-03 22:02:01,628 - uas_mgr - INFO - opt_ports: [] 2021-02-03 22:02:01,702 - uas_mgr - INFO - cfg_ports: [30123] 2021-02-03 22:02:01,702 - uas_mgr - INFO - UAI Name: uai-vers-32079250; Container ports: [{'container_port': 30123, 'host_ip': None, 'host_port': None, 'name': 'port30123', 'protocol': 'TCP'}]; Optional ports: [] 2021-02-03 22:02:02,211 - uas_mgr - INFO - opt_ports: [] 2021-02-03 22:02:02,566 - uas_mgr - INFO - cfg_ports: [30123] 2021-02-03 22:02:02,703 - uas_mgr - INFO - getting deployment uai-vers-32079250 in namespace user 2021-02-03 22:02:02,718 - uas_mgr - INFO - creating deployment uai-vers-32079250 in namespace user 2021-02-03 22:02:02,734 - uas_mgr - INFO - creating the UAI service uai-vers-32079250-ssh 2021-02-03 22:02:02,734 - uas_mgr - INFO - getting service uai-vers-32079250-ssh in namespace user 2021-02-03 22:02:02,746 - uas_mgr - INFO - creating service uai-vers-32079250-ssh in namespace user 2021-02-03 22:02:02,757 - uas_mgr - INFO - getting pod info uai-vers-32079250 2021-02-03 22:02:02,841 - uas_mgr - INFO - No start time provided from pod 2021-02-03 22:02:02,841 - uas_mgr - INFO - getting service info for uai-vers-32079250-ssh in namespace user 127.0.0.1 - - [03/Feb/2021 22:02:02] \u0026quot;POST /v1/uas HTTP/1.1\u0026quot; 200 - 2021-02-03 22:15:32,697 - uas_auth - INFO - UasAuth lookup complete for user vers 2021-02-03 22:15:32,698 - uas_mgr - INFO - UAS request for: vers 2021-02-03 22:15:32,698 - uas_mgr - INFO - listing deployments matching: host None, labels uas=managed,user=vers 2021-02-03 22:15:32,770 - uas_mgr - INFO - deleting service uai-vers-32079250-ssh in namespace user 2021-02-03 22:15:32,802 - uas_mgr - INFO - delete deployment uai-vers-32079250 in namespace user 127.0.0.1 - - [03/Feb/2021 22:15:32] \u0026quot;DELETE /v1/uas?uai_list=uai-vers-32079250 HTTP/1.1\u0026quot; 200 - If an error had occurred in UAS that error would likely show up here. Because there are two replicas of cray-uas-mgr running, the logging of interest may be in the other pod, so apply the same command to the other pod if the information is not here.\n"
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/troubleshoot_uas_issues/",
	"title": "Troubleshoot UAS Issues",
	"tags": [],
	"description": "",
	"content": "Troubleshoot UAS Issues This section provides examples of some commands that can be used to troubleshoot UAS-related issues.\nTroubleshoot Connection Issues packet_write_wait: Connection to 203.0.113.0 port 30841: Broken pipe If an error message related to broken pipes returns, enable keep-alives on the client side. The admin should update the /etc/ssh/sshd_config and /etc/ssh/ssh_config files to add the following:\nTCPKeepAlive yes ServerAliveInterval 120 ServerAliveCountMax 720 Invalid Credentials ncn-w001 # cray auth login --username USER --password WRONGPASSWORD Usage: cray auth login [OPTIONS] Try \u0026#34;cray auth login --help\u0026#34; for help. Error: Invalid Credentials To resolve this issue:\n Log in to Keycloak and verify the user exists. Make sure the username and password are correct.  Retrieve UAS Logs The system administrator can execute the following commands to retrieve UAS and the remote execution service logs:\nncn-w001# kubectl logs -n services -c cray-uas-mgr -l \u0026#34;app=cray-uas-mgr\u0026#34; Ensure that Slurm is Running and Configured Correctly Check if Slurm is running:\n[user@uai-user-be3a6770-6876c88676-2p2lk ~] $ sinfo The system returns a message similar to the following if Slurm is not running:\nslurm_load_partitions: Unable to contact slurm controller (connect failure) If this error is returned, it is likely that Slurm is not running. The system administrator can use the following commands to debug the issue:\nncn-w001# kubectl logs -n user -l app=slurmdb -c slurmdb --tail=-1 ncn-w001# kubectl logs -n user -l app=slurmdbd -c slurmdbd --tail=-1 ncn-w001# kubectl logs -n user -l app=slurmctld -c slurmctld --tail=-1 Troubleshoot Default Images Issues when Using the CLI If the image name provided while creating a new UAI is not registered for use by the system, the system returns an error message similar to the following:\nncn-w001# cray uas create --publickey ~/.ssh/id_rsa.pub --imagename fred Usage: cray uas create [OPTIONS] Try \u0026#34;cray uas create --help\u0026#34; for help. Error: Bad Request: Invalid image (fred). Valid images: [\u0026#39;dtr.dev.cray.com:443/cray/cray-uas-sles15sp1:latest\u0026#39;]. Default: dtr.dev.cray.com:443/cray/cray-uas-sles15sp1:latest Retry creating the UAI using the list of images and the name of the default image provided in the error message.\nVerify that the User Access Instances (UAIs) are Running The system administrator can use the kubectl command to check the status of the UAI.\nncn-w001# kubectl get pod -n user -l uas=managed -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES uai-user-603d55f1-85d5ddb4b7-zk6nl 0/1 ContainerCreating 0 109s \u0026lt;none\u0026gt; sms-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; uai-user-d7f8d2e7-6dbdc64d98-7h5t5 0/1 ContainerCreating 0 116s \u0026lt;none\u0026gt; sms-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; uai-user-f6b72c9f-5dccd879bd-grbjw 0/1 ContainerCreating 0 113s \u0026lt;none\u0026gt; sms-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; If UAS pods are stuck in the Pending state, the admin needs to ensure the Kubernetes cluster has nodes available for running UAIs. Check that nodes are labeled with uas=True and are in the Ready state.\nncn-w001# kubectl get nodes -l uas NAME STATUS ROLES AGE VERSION ncn-w001 Ready master 11d v1.13.3 If none of the nodes are found or if the nodes listed are marked as NotReady, the UAI pods will not be scheduled and will not start.\nTroubleshoot kubectl Certificate Issues While kubectl is supported in a UAI, kubeconfig file to access a Kubernetes cluster is not provided. To use kubectl to interface with a Kubernetes cluster, the user must supply their own kubeconfig.\n[user@uai-user-be3a6770-6876c88676-2p2lk ~]# kubectl get nodes The connection to the server localhost:8080 was refused - did you specify the right host or port? For instructions to copy certificates into a UAI, see Move Application Data from an External Workstation to a UAI.\nSpecify the location of the Kubernetes certificate with KUBECONFIG.\n[user@uai-user-be3a6770-6876c88676-2p2lk ~]# KUBECONFIG=/tmp/CONFIG kubectl get nodes NAME STATUS ROLES AGE VERSION ncn-m001 Ready master 16d v1.13.3 ncn-m002 Ready master 16d v1.13.3 Users must specify KUBECONFIG with every kubectl command or specify the kubeconfig file location for the life of the UAI. To do this, either set the KUBECONFIG environment variable or set the --kubeconfig flag .\nTroubleshoot X11 Issues The system may return the following error if the user attempts to use an application that requires an X window (such as xeyes):\n$ ssh user@203.0.113.0 -i ~/.ssh/id_rsa ______ ____ ___ __ __ __ __ ___ ____ / ____// __ \\ / |\\ \\/ / / / / // | / _/ / / / /_/ // /| | \\  / / / / // /| | / / / /___ / _, _// ___ | / / / /_/ // ___ | _/ / \\____//_/ |_|/_/ |_|/_/ \\____//_/ |_|/___/ [user@uai-user-be3a6770-6876c88676-2p2lk ~]$ xeyes Error: Can\u0026#39;t open display: To resolve this issue, pass the -X option with the ssh command as show below:\n$ ssh UAI_USERNAME@UAI_IP_ADDRESS -i ~/.ssh/id_rsa -X ______ ____ ___ __ __ __ __ ___ ____ / ____// __ \\ / |\\ \\/ / / / / // | / _/ / / / /_/ // /| | \\  / / / / // /| | / / / /___ / _, _// ___ | / / / /_/ // ___ | _/ / \\____//_/ |_|/_/ |_|/_/ \\____//_/ |_|/___/ /usr/bin/xauth: file /home/users/user/.Xauthority does not exist [user@uai-user-be3a6770-6876c88676-2p2lk ~]$ echo $DISPLAY 203.0.113.0 The warning stating \u0026ldquo;Xauthority does not exist\u0026rdquo; will disappear with subsequent logins.\nTroubleshoot SSH Host Key Issues If strict host key checking enabled is enabled on the user\u0026rsquo;s client, the below error may appear when connecting to a UAI over ssh.\nWARNING: REMOTE HOST IDENTIFICATION HAS CHANGED This can occur in a few circumstances, but is most likely to occur after the UAI container is restarted. If this occurs, remove the offending ssh hostkey from the local known_hosts file and try to connect again. The error message from ssh will contain the correct path to the known_hosts file and the line number of the problematic key.\nDelete UAS Objects with kubectl If Kubernetes resources used to create a UAI are not cleaned up during the normal deletion process, resources can be deleted with the following commands.\nDelete anything created by the User Access Service (uas-mgr):\nWARNING: This command will delete all UAS resources for the entire system, it is not for targeted cleanup of a single UAI.\nncn-w001# kubectl delete all -n user -l uas=managed Delete all objects associated with a particular UAI:\nncn-w001# kubectl delete all -n user -l app=UAI-NAME Delete all objects for a single user:\nncn-w001# kubectl delete all -n user -l user=USERNAME Hard limits on UAI Creation Each Kubernetes worker node has limits on how many pods it can run. Nodes are installed by default with a hard limit of 110 pods per node, but the number of pods may be further limited by memory and CPU utilization constraints. For a standard node the maximum number of UAIs per node is 110; if other pods are co-scheduled on the node, the number will be reduced.\nDetermine the hard limit on Kubernetes pods with kubectl describe node and look for the Capacity section.\n# kubectl describe node NODE_NAME -o yaml ... capacity: cpu: \u0026quot;16\u0026quot; ephemeral-storage: 1921298528Ki hugepages-1Gi: \u0026quot;0\u0026quot; hugepages-2Mi: \u0026quot;0\u0026quot; memory: 181009640Ki pods: \u0026quot;110\u0026quot; ... When UAIs are created, some UAIs might left in the \u0026ldquo;Pending\u0026rdquo; state. The Kubernetes scheduler is unable to schedule them to a node, because of CPU, memory, or pod limit constraints. Use kubectl describe pod to check why the pod is pending. For example, this pod is \u0026ldquo;Pending\u0026rdquo; because the node has reached the hard limit of 110 pods.\n# kubectl describe pod UAI-POD Warning Failed Scheduling 21s (x20 over 4m31s) default-scheduler 0/4 nodes are available: 1 Insufficient pods, 3 node(s) didn't match node selector. "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/uai_classes/",
	"title": "UAI Classes",
	"tags": [],
	"description": "",
	"content": "UAI Classes This topic explains all the fields in a User Access Instance (UAI) class and gives guidance on setting them when creating UAI classes.\nExample Listing and Overview The following is JSON-formatted example output from the cray uas admin config classes list command (see List Available UAI Classes). This output contains examples of three UAI classes:\n A UAI broker class A brokered end-user UAI class A non-brokered end-user UAI class  This topic uses the end-user UAI class section to explain each of fields within a UAI class.\nncn-m001-pit# cray uas admin config classes list --format json [ { \u0026#34;class_id\u0026#34;: \u0026#34;05496a5f-7e35-435d-a802-882c6425e5b2\u0026#34;, \u0026#34;comment\u0026#34;: \u0026#34;UAI Broker Class\u0026#34;, \u0026#34;default\u0026#34;: false, \u0026#34;namespace\u0026#34;: \u0026#34;uas\u0026#34;, \u0026#34;opt_ports\u0026#34;: [], \u0026#34;priority_class_name\u0026#34;: \u0026#34;uai-priority\u0026#34;, \u0026#34;public_ip\u0026#34;: true, \u0026#34;resource_config\u0026#34;: null, \u0026#34;uai_compute_network\u0026#34;: false, \u0026#34;uai_creation_class\u0026#34;: \u0026#34;a623a04a-8ff0-425e-94cc-4409bdd49d9c\u0026#34;, \u0026#34;uai_image\u0026#34;: { \u0026#34;default\u0026#34;: false, \u0026#34;image_id\u0026#34;: \u0026#34;c5dcb261-5271-49b3-9347-afe7f3e31941\u0026#34;, \u0026#34;imagename\u0026#34;: \u0026#34;registry.local/cray/cray-uai-broker:latest\u0026#34; }, \u0026#34;volume_mounts\u0026#34;: [ { \u0026#34;mount_path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;FileOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;55a02475-5770-4a77-b621-f92c5082475c\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;timezone\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;DirectoryOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;9fff2d24-77d9-467f-869a-235ddcd37ad7\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;lustre\u0026#34; } ] }, { \u0026#34;class_id\u0026#34;: \u0026#34;a623a04a-8ff0-425e-94cc-4409bdd49d9c\u0026#34;, \u0026#34;comment\u0026#34;: \u0026#34;UAI User Class\u0026#34;, \u0026#34;default\u0026#34;: false, \u0026#34;namespace\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;opt_ports\u0026#34;: [], \u0026#34;priority_class_name\u0026#34;: \u0026#34;uai-priority\u0026#34;, \u0026#34;public_ip\u0026#34;: false, \u0026#34;resource_config\u0026#34;: null, \u0026#34;uai_compute_network\u0026#34;: true, \u0026#34;uai_creation_class\u0026#34;: null, \u0026#34;uai_image\u0026#34;: { \u0026#34;default\u0026#34;: true, \u0026#34;image_id\u0026#34;: \u0026#34;ff86596e-9699-46e8-9d49-9cb20203df8c\u0026#34;, \u0026#34;imagename\u0026#34;: \u0026#34;registry.local/cray/cray-uai-sles15sp1:latest\u0026#34; }, \u0026#34;volume_mounts\u0026#34;: [ { \u0026#34;mount_path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;FileOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;55a02475-5770-4a77-b621-f92c5082475c\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;timezone\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;DirectoryOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;9fff2d24-77d9-467f-869a-235ddcd37ad7\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;lustre\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/root/slurm_config/munge\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;secret\u0026#34;: { \u0026#34;secret_name\u0026#34;: \u0026#34;munge-secret\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;7aeaf158-ad8d-4f0d-bae6-47f8fffbd1ad\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;munge-key\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/etc/slurm\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;config_map\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;slurm-map\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;ea97325c-2b1d-418a-b3b5-3f6488f4a9e2\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;slurm-config\u0026#34; } ] }, { \u0026#34;class_id\u0026#34;: \u0026#34;bb28a35a-6cbc-4c30-84b0-6050314af76b\u0026#34;, \u0026#34;comment\u0026#34;: \u0026#34;Non-Brokered UAI User Class\u0026#34;, \u0026#34;default\u0026#34;: false, \u0026#34;namespace\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;opt_ports\u0026#34;: [], \u0026#34;priority_class_name\u0026#34;: \u0026#34;uai-priority\u0026#34;, \u0026#34;public_ip\u0026#34;: true, \u0026#34;resource_config\u0026#34;: null, \u0026#34;uai_compute_network\u0026#34;: true, \u0026#34;uai_creation_class\u0026#34;: null, \u0026#34;uai_image\u0026#34;: { \u0026#34;default\u0026#34;: true, \u0026#34;image_id\u0026#34;: \u0026#34;ff86596e-9699-46e8-9d49-9cb20203df8c\u0026#34;, \u0026#34;imagename\u0026#34;: \u0026#34;registry.local/cray/cray-uai-sles15sp1:latest\u0026#34; }, \u0026#34;volume_mounts\u0026#34;: [ { \u0026#34;mount_path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;FileOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;55a02475-5770-4a77-b621-f92c5082475c\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;timezone\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;DirectoryOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;9fff2d24-77d9-467f-869a-235ddcd37ad7\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;lustre\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/root/slurm_config/munge\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;secret\u0026#34;: { \u0026#34;secret_name\u0026#34;: \u0026#34;munge-secret\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;7aeaf158-ad8d-4f0d-bae6-47f8fffbd1ad\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;munge-key\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/etc/slurm\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;config_map\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;slurm-map\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;ea97325c-2b1d-418a-b3b5-3f6488f4a9e2\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;slurm-config\u0026#34; } ] } ] UAI Class Parameters The following selection is the first part of the end-user UAI class section:\n\u0026#34;class_id\u0026#34;: \u0026#34;bb28a35a-6cbc-4c30-84b0-6050314af76b\u0026#34;, \u0026#34;comment\u0026#34;: \u0026#34;Non-Brokered UAI User Class\u0026#34;, \u0026#34;default\u0026#34;: false, \u0026#34;namespace\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;opt_ports\u0026#34;: [], \u0026#34;priority_class_name\u0026#34;: \u0026#34;uai-priority\u0026#34;, \u0026#34;public_ip\u0026#34;: true, \u0026#34;resource_config\u0026#34;: null, \u0026#34;uai_compute_network\u0026#34;: true, \u0026#34;uai_creation_class\u0026#34;: null, The following table explains each of these fields.\n   Field Description Notes     class_id The identifier used for this class when examining, updating, and deleting the class. This identifier is also used to create UAIs using this class with the cray uas admin uais create   comment A free-form string describing the UAI class     default A boolean value (flag) indicating whether this class is the default class. When this field is set to true, this class overrides both the default UAI image and any specified image name when the cray uas create command is used to create an end-user UAI for a user. Setting a class to \u0026quot;default\u0026quot;: true, gives the administrator fine-grained control over the behavior of end-user UAIs that are created by authorized users in legacy mode.   namespace The Kubernetes namespace in which this UAI will run. The default setting is user.   opt_ports An optional list of TCP port numbers that will be opened on the external IP address of the UAI when it runs. This field controls whether services other than SSH can be run and reached publicly on the UAI. If this list is empty (like the preceding example), only SSH will be externally accessible.   priority_class_name The Kubernetes priority class of the UAI. \u0026ldquo;uai_priority\u0026rdquo; is the default. Using other values affects both Kubernetes default resource limit and request assignments and the Kubernetes scheduling priority for the UAI.   public_ip A boolean value that indicates whether the UAI will be given an external IP address from the LoadBalancer service. Such an address enables clients outside the Kubernetes cluster to reach the UAI. This field controls whether the UAI is reachable by SSH from external clients, but it also controls whether the ports in opt_ports are reachable. If this field is set to false, the UAI will have only an internal IP address, reachable from within the Kubernetes cluster.   resource_config Can be set to a resource specification to override namespace defaults on Kubernetes resource requests and limits. This field is not set in the preceding example.   uai_compute_network A flag that indicates whether this UAI uses the macvlan mechanism to gain access to the HPE Cray EX compute node network. This field must be true to support workload management.   uai_creation_class A field used by broker UAIs to tell the broker what kind of UAI to create when automatically generating a UAI. This field is not set in the preceding example.    UAI Images and Volumes in UAI Classes The following section is used to create UAIs of this class:\n\u0026#34;uai_image\u0026#34;: { \u0026#34;default\u0026#34;: true, \u0026#34;image_id\u0026#34;: \u0026#34;ff86596e-9699-46e8-9d49-9cb20203df8c\u0026#34;, \u0026#34;imagename\u0026#34;: \u0026#34;registry.local/cray/cray-uai-sles15sp1:latest\u0026#34; }, At the end of this end-user UAI class, there is a list of volumes that will be mounted by UAIs created using the class:\n\u0026#34;volume_mounts\u0026#34;: [ { \u0026#34;mount_path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;FileOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;55a02475-5770-4a77-b621-f92c5082475c\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;timezone\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;DirectoryOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;9fff2d24-77d9-467f-869a-235ddcd37ad7\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;lustre\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/root/slurm_config/munge\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;secret\u0026#34;: { \u0026#34;secret_name\u0026#34;: \u0026#34;munge-secret\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;7aeaf158-ad8d-4f0d-bae6-47f8fffbd1ad\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;munge-key\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/etc/slurm\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;config_map\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;slurm-map\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;ea97325c-2b1d-418a-b3b5-3f6488f4a9e2\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;slurm-config\u0026#34; } Refer to Elements of a UAI for a full explanation of UAI images and volumes.\nIn the preceding section of output, the end-user UAI inherits the timezone from the host node by importing /etc/localtime. This UAI also gains access to the Lustre file system mounted on the host node. On the host node, the file system is mounted at /lus and the UAI mounts the file system at the same mount point as the host node. Lastly, the UAI class includes two Slurm configuration items, the munge key and the Slurm configuration file. These are obtained from Kubernetes and the UAI mounts them as files at /root/slurm_config/munge and /etc/slurm respectively.\n"
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/troubleshoot_stale_brokered_uais/",
	"title": "Troubleshoot Stale Brokered UAIs",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Stale Brokered UAIs When a broker UAI terminates and restarts, the SSH key used to forward SSH sessions to end-user UAIs changes (this is a known problem) and subsequent broker UAIs are unable to forward sessions to end-user UAIs. The symptom of this is that a user logging into a broker UAI will receive a password prompt from the end-user UAI and be unable to log in even if providing the correct password. To fix this, remove the stale end-user UAIs and allow the broker UAI to recreate them. The easy way to do this is to use the command specifying the uai-creation-class identifier from the broker\u0026rsquo;s UAI class.\ncray uas admin uais delete --class-id \u0026lt;creation-class-id\u0026gt; For example:\nncn-m001-pit# cray uas admin config classes list | grep -e class_id -e comment class_id = \u0026quot;74970cdc-9f94-4d51-8f20-96326212b468\u0026quot; comment = \u0026quot;UAI broker class\u0026quot; class_id = \u0026quot;a623a04a-8ff0-425e-94cc-4409bdd49d9c\u0026quot; comment = \u0026quot;UAI User Class\u0026quot; class_id = \u0026quot;bb28a35a-6cbc-4c30-84b0-6050314af76b\u0026quot; comment = \u0026quot;Non-Brokered UAI User Class\u0026quot; ncn-m001-pit# cray uas admin config classes describe 74970cdc-9f94-4d51-8f20-96326212b468 | grep uai_creation_class uai_creation_class = \u0026quot;a623a04a-8ff0-425e-94cc-4409bdd49d9c\u0026quot; ncn-m001-pit# cray uas admin uais delete --class-id a623a04a-8ff0-425e-94cc-4409bdd49d9c results = [ \u0026quot;Successfully deleted uai-vers-6da50e7a\u0026quot;,] After that, users should be able to log into the broker UAI and be directed to an end-user UAI as before.\n"
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/troubleshoot_uai_authentication_issues/",
	"title": "Troubleshoot UAI Authentication Issues",
	"tags": [],
	"description": "",
	"content": "Troubleshoot UAI Authentication Issues Several troubleshooting steps related to authentication in a UAI.\nInternal Server Error An error was encountered while accessing Keycloak because of an invalid token.\n# cray uas create --publickey ~/.ssh/id_rsa.pub Usage: cray uas create [OPTIONS] Try \u0026#34;cray uas create --help\u0026#34; for help. Error: Internal Server Error: An error was encountered while accessing Keycloak The uas-mgr logs show:\n2020-03-06 18:52:07,642 - uas_auth - ERROR - \u0026lt;class \u0026#39;requests.exceptions.HTTPError\u0026#39;\u0026gt; HTTPError(\u0026#39;401 Client Error: Unauthorized for url: https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/userinfo\u0026#39;) 2020-03-06 18:52:07,643 - uas_auth - ERROR - UasAuth HTTPError:401 Client Error: Unauthorized for url: https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/userinfo The Keycloak pod logs shows:\n18:53:19,617 WARN [org.keycloak.events] (default task-1) type=USER_INFO_REQUEST_ERROR, realmId=028be52c-ceca-4dbd-b765-0386b42b1866, clientId=cray, userId=null, ipAddress=10.40.0.0, error=user_session_not_found, auth_method=validate_access_token This is caused by the authentication token being invalid. This can happen for many reasons, such as the token expiring after its lifetime has ended or the Keycloak server restarting because of a failure or being moved to a different node.\nTo resolve this issue, run cray auth login to refresh the access token.\nAuthorization Is Local to a Host: whenever you are using the CLI (cray command) on a host (e.g. a workstation or NCN) where it has not been used before, it is necessary to authenticate on that host using cray auth login. There is no mechanism to distribute CLI authorization amongst hosts.\nInvalid Token # cray uas create --publickey ~/.ssh/id_rsa.pub Usage: cray uas create [OPTIONS] Try \u0026#34;cray uas create --help\u0026#34; for help. Error: Bad Request: Token not valid for UAS. Attributes missing: [\u0026#39;name\u0026#39;, \u0026#39;uidNumber\u0026#39;, \u0026#39;preferred_username\u0026#39;, \u0026#39;gidNumber\u0026#39;, \u0026#39;loginShell\u0026#39;, \u0026#39;homeDirectory\u0026#39;] To resolve this issue, make sure the cray command is configured to use one of the following URLs for an API gateway (excluding the /keycloak/realms/shastaendpoint).\n# kubectl exec -c api-gateway api-gateway-544d5c676f-682m2 -- curl -s http://localhost:8001/consumers/remote-admin/jwt | python -mjson.tool | grep \u0026#34;key\u0026#34; \u0026#34;key\u0026#34;: \u0026#34;https://api-gateway.default.svc.cluster.local/keycloak/realms/shasta\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/keycloak/realms/shasta\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;https://mgmt-plane-cmn.local/keycloak/realms/shasta\u0026#34;, # cray config describe | grep hostname \u0026#34;hostname\u0026#34;: \u0026#34;https://172.30.51.127:30443\u0026#34; \u0026lt;---- 172.30.51.127:30443 will not work # Change to \u0026#34;https://api-gw-service-nmn.local\u0026#34; cray init --hostname \u0026#34;https://api-gw-service-nmn.local\u0026#34; Overwrite configuration file at: /root/.config/cray/configurations/default ? [y/N]: y Username: user Password: Success! Initialization complete. Invalid Credentials # cray auth login --username \u0026lt;user\u0026gt; --password \u0026lt;wrongpassword\u0026gt; Usage: cray auth login [OPTIONS] Try \u0026#34;cray auth login --help\u0026#34; for help. Error: Invalid Credentials To resolve this issue:\n Log in to Keycloak and verify the user exists. Make sure the username and password are correct.  cray uas describe \u0026lt;user\u0026gt; Does Not Work The cray uas describe \u0026lt;user\u0026gt; is no longer a valid command.\n# cray uas describe \u0026lt;user\u0026gt; Usage: cray uas [OPTIONS] COMMAND [ARGS]... Try \u0026#34;cray uas --help\u0026#34; for help. Error: No such command \u0026#34;describe\u0026#34;. Use cray uas list instead.\n# cray uas list [[results]] username = \u0026#34; uai_host = \u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; uai_connect_string = \u0026#34; uai_img = \u0026#34; uai_age = \u0026#34;11m\u0026#34; uai_name = \u0026#34; "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/troubleshoot_uai_stuck_in_containercreating/",
	"title": "Troubleshoot UAI Stuck In &#34;containercreating&#34;",
	"tags": [],
	"description": "",
	"content": "Troubleshoot UAI Stuck in \u0026ldquo;ContainerCreating\u0026rdquo; Resolve an issue causing UAIs to show a uai_status field of Waiting, and a uai_msg field of ContainerCreating. It is possible that this is just a matter of starting the UAI taking longer than normal, perhaps as it pulls in a new UAI image from a registry. If the issue persists for a long time, it is worth investigating.\nPrerequisites The UAI has been in the ContainerCreating status for several minutes.\nProcedure   Find the UAI.\nncn-m001-pit# cray uas admin uais list --owner ctuser [[results]] uai_age = \u0026quot;1m\u0026quot; uai_connect_string = \u0026quot;ssh ctuser@10.103.13.159\u0026quot; uai_host = \u0026quot;ncn-w001\u0026quot; uai_img = \u0026quot;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026quot; uai_ip = \u0026quot;10.103.13.159\u0026quot; uai_msg = \u0026quot;ContainerCreating\u0026quot; uai_name = \u0026quot;uai-ctuser-bcd1ff74\u0026quot; uai_status = \u0026quot;Waiting\u0026quot; username = \u0026quot;ctuser\u0026quot;   Look up the UAI\u0026rsquo;s pod in Kubernetes.\nncn-m001-pit# kubectl get po -n user | grep uai-ctuser-bcd1ff74 uai-ctuser-bcd1ff74-7d94967bdc-4vm66 0/1 ContainerCreating 0 2m58s   Describe the pod in Kubernetes.\nncn-m001-pit# kubectl describe pod -n user uai-ctuser-bcd1ff74-7d94967bdc-4vm66 Name: uai-ctuser-bcd1ff74-7d94967bdc-4vm66 Namespace: user Priority: -100 Priority Class Name: uai-priority Node: ncn-w001/10.252.1.12 Start Time: Wed, 03 Feb 2021 18:33:00 -0600 ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled \u0026lt;unknown\u0026gt; default-scheduler Successfully assigned user/uai-ctuser-bcd1ff74-7d94967bdc-4vm66 to ncn-w001 Warning FailedMount 2m53s (x8 over 3m57s) kubelet, ncn-w001 MountVolume.SetUp failed for volume \u0026quot;broker-sssd-config\u0026quot; : secret \u0026quot;broker-sssd-conf\u0026quot; not found Warning FailedMount 2m53s (x8 over 3m57s) kubelet, ncn-w001 MountVolume.SetUp failed for volume \u0026quot;broker-sshd-config\u0026quot; : configmap \u0026quot;broker-sshd-conf\u0026quot; not found Warning FailedMount 2m53s (x8 over 3m57s) kubelet, ncn-w001 MountVolume.SetUp failed for volume \u0026quot;broker-entrypoint\u0026quot; : configmap \u0026quot;broker-entrypoint\u0026quot; not found Warning FailedMount 114s kubelet, ncn-w001 Unable to attach or mount volumes: unmounted volumes=[broker-sssd-config broker-entrypoint broker-sshd-config], unattached volumes=[optcraype optlmod etcprofiled optr optforgelicense broker-sssd-config lustre timezone optintel optmodulefiles usrsharelmod default-token-58t5p optarmlicenceserver optcraycrayucx slurm-config opttoolworks optnvidiahpcsdk munge-key optamd opttotalview optgcc opttotalviewlicense broker-entrypoint broker-sshd-config etccrayped opttotalviewsupport optcraymodulefilescrayucx optforge usrlocalmodules varoptcraypepeimages]: timed out waiting for the condition This produces a lot of output, all of which can be useful for diagnosis. A good place to start is in the Events section at the bottom. Notice the warnings here about volumes whose secrets and configmaps are not found. In this case, that means the UAI cannot start because it was started in legacy mode without a default UAI class, and some of the volumes configured in the UAS are in the uas namespace to support localization of broker UAIs and cannot be found in the user namespace. To solve this particular problem, the best move would be to configure a default UAI class with the correct volume list in it, delete the UAI, and allow the user to try creating it again using the default class.\nOther problems can usually be quickly identified using this and other information found in the output from the kubectl describe pod command.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/troubleshoot_uais_by_viewing_log_output/",
	"title": "Troubleshoot UAIs By Viewing Log Output",
	"tags": [],
	"description": "",
	"content": "Troubleshoot UAIs by Viewing Log Output Sometimes a UAI will come up and run but will not work correctly. It is possible to see errors reported by elements of the UAI entrypoint script using the kubectl logs command. First find the UAI of interest. This starts by identifying the UAI name using the CLI:\nncn-m001-pit# cray uas admin uais list [[results]] uai_age = \u0026quot;4h30m\u0026quot; uai_connect_string = \u0026quot;ssh broker@10.103.13.162\u0026quot; uai_host = \u0026quot;ncn-w001\u0026quot; uai_img = \u0026quot;dtr.dev.cray.com/cray/cray-uai-broker:latest\u0026quot; uai_ip = \u0026quot;10.103.13.162\u0026quot; uai_msg = \u0026quot; uai_name = \u0026quot;uai-broker-2e6ce6b7\u0026quot; uai_status = \u0026quot;Running: Ready\u0026quot; username = \u0026quot;broker\u0026quot; [[results]] uai_age = \u0026quot;1h12m\u0026quot; uai_connect_string = \u0026quot;ssh vers@10.20.49.135\u0026quot; uai_host = \u0026quot;ncn-w001\u0026quot; uai_img = \u0026quot;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026quot; uai_ip = \u0026quot;10.20.49.135\u0026quot; uai_msg = \u0026quot; uai_name = \u0026quot;uai-vers-6da50e7a\u0026quot; uai_status = \u0026quot;Running: Ready\u0026quot; username = \u0026quot;vers\u0026quot; Using this, find the UAI in question, remembering that end-user UAIs run in the user Kubernetes namespace and broker UAIs run in the uas Kubernetes namespace.\nncn-m001-pit# kubectl get po -n user | grep uai-vers-6da50e7a uai-vers-6da50e7a-54dbc99fdd-csxmk 1/1 Running 0 76m or\nncn-m001-pit# kubectl get po -n uas | grep uai-broker-2e6ce6b7 uai-broker-2e6ce6b7-68d78c6c95-s28dh 2/2 Running 0 4h34m Using the UAI\u0026rsquo;s pod name and the user namespace, get the logs:\nncn-m001-pit# kubectl logs -n user uai-vers-6da50e7a-54dbc99fdd-csxmk uai-vers-6da50e7a Setting up passwd and group entries for vers Setting profile for vers Adding vers to groups Disabling password based login passwd: password expiry information changed. Checking to see if /home/users/vers exists If this hangs, please ensure that /home/users/vers is properly mounted/working on the host of this pod No home directory exists, creating one Checking for munge.key Setting up munge.key Check for pbs.conf Generating ssh keys and sshd_config ssh-keygen: generating new host keys: RSA DSA ECDSA ED25519 ... This can also be done for the broker using the broker UAI pod\u0026rsquo;s name and the uas namespace:\nncn-m001-pit# kubectl logs -n uas uai-broker-2e6ce6b7-68d78c6c95-s28dh uai-broker-2e6ce6b7 /bin/bash: warning: setlocale: LC_ALL: cannot change locale (C.UTF-8) Configure PAM to use sssd... Generating broker host keys... ssh-keygen: generating new host keys: RSA DSA ECDSA ED25519 Checking for UAI_CREATION_CLASS... Starting sshd... Starting sssd... (Wed Feb 3 18:34:41:792821 2021) [sssd] [sss_ini_get_config] (0x0020): Config merge error: Directory /etc/sssd/conf.d does not exist. The above is from a successful broker starting and running.\n"
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/troubleshoot_common_mistakes_when_creating_a_custom_end-user_uai_image/",
	"title": "Troubleshoot Common Mistakes When Creating A Custom End-user UAI Image",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Common Mistakes when Creating a Custom End-User UAI Image There a several problems that may occur while making or working with a custom end-user UAI images. The following are some basic troubleshooting questions to ask:\n Does SESSION_NAME match an actual entry in cray bos sessiontemplate list? Is the SESSION_ID set to an appropriate uuid format? Did the awk command not parse the uuid correctly? Did the file /etc/security/limits.d/99-slingshot-network.conf get removed from the tarball correctly? Does the ENTRYPOINT /usr/bin/uai-ssh.sh exist? Did the container image get pushed and registered with UAS? Did the creation process run from a real worker or master node as opposed to a LiveCD node?  "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/troubleshoot_duplicate_mount_paths_in_a_uai/",
	"title": "Troubleshoot Duplicate Mount Paths In A UAI",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Duplicate Mount Paths in a UAI If a user attempts to create a UAI in the legacy mode and cannot create the UAI at all, a good place to look is at volumes. Duplicate mount_path specifications in the list of volumes in a UAI will cause a failure that looks like this:\nncn-m001-pit# cray uas create --publickey ~/.ssh/id_rsa.pub Usage: cray uas create [OPTIONS] Try 'cray uas create --help' for help. Error: Unprocessable Entity: Failed to create deployment uai-erl-543cdbbc: Unprocessable Entity Currently, there is not a lot of UAS log information available from this error (this is a known problem), but a likely cause is duplicate mount_path specifications in volumes. Looking through the configured volumes for duplicates can be helpful.\nncn-m001-pit# cray uas admin config volumes list | grep -e mount_path -e volumename -e volume_id mount_path = \u0026quot;/app/broker\u0026quot; volume_id = \u0026quot;1f3bde56-b2e7-4596-ab3a-6aa4327d29c7\u0026quot; volumename = \u0026quot;broker-entrypoint\u0026quot; mount_path = \u0026quot;/etc/sssd\u0026quot; volume_id = \u0026quot;4dc6691e-e7d9-4af3-acde-fc6d308dd7b4\u0026quot; volumename = \u0026quot;broker-sssd-config\u0026quot; mount_path = \u0026quot;/etc/localtime\u0026quot; volume_id = \u0026quot;55a02475-5770-4a77-b621-f92c5082475c\u0026quot; volumename = \u0026quot;timezone\u0026quot; mount_path = \u0026quot;/root/slurm_config/munge\u0026quot; volume_id = \u0026quot;7aeaf158-ad8d-4f0d-bae6-47f8fffbd1ad\u0026quot; volumename = \u0026quot;munge-key\u0026quot; mount_path = \u0026quot;/opt/forge\u0026quot; volume_id = \u0026quot;7b924270-c9e9-4b0e-85f5-5bc62c02457e\u0026quot; volumename = \u0026quot;delete-me\u0026quot; mount_path = \u0026quot;/lus\u0026quot; volume_id = \u0026quot;9fff2d24-77d9-467f-869a-235ddcd37ad7\u0026quot; volumename = \u0026quot;lustre\u0026quot; mount_path = \u0026quot;/etc/switchboard\u0026quot; volume_id = \u0026quot;d5058121-c1b6-4360-824d-3c712371f042\u0026quot; volumename = \u0026quot;broker-sshd-config\u0026quot; mount_path = \u0026quot;/etc/slurm\u0026quot; volume_id = \u0026quot;ea97325c-2b1d-418a-b3b5-3f6488f4a9e2\u0026quot; volumename = \u0026quot;slurm-config\u0026quot; mount_path = \u0026quot;/opt/forge_license\u0026quot; volume_id = \u0026quot;ecfae4b2-d530-4c06-b757-49b30061c90a\u0026quot; volumename = \u0026quot;optforgelicense\u0026quot; mount_path = \u0026quot;/opt/forge\u0026quot; volume_id = \u0026quot;fc95d0da-6296-4d0b-8f26-2d4338604991\u0026quot; volumename = \u0026quot;optforge\u0026quot; Looking through this list, the mount path for the volume named delete-me and the mount path for the volume named optforge are the same. The obvious candidate for deletion in this case is delete-me, so it can be deleted.\nncn-m001-pit# cray uas admin config volumes delete 7b924270-c9e9-4b0e-85f5-5bc62c02457e mount_path = \u0026quot;/opt/forge\u0026quot; volume_id = \u0026quot;7b924270-c9e9-4b0e-85f5-5bc62c02457e\u0026quot; volumename = \u0026quot;delete-me\u0026quot; [volume_description.host_path] path = \u0026quot;/tmp/foo\u0026quot; type = \u0026quot;DirectoryOrCreate\u0026quot; "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/troubleshoot_missing_or_incorrect_uai_images/",
	"title": "Troubleshoot Missing Or Incorrect UAI Images",
	"tags": [],
	"description": "",
	"content": "Troubleshoot Missing or Incorrect UAI Images If a UAI shows a uai_status of Waiting and a uai_msg of ImagePullBackOff, that indicates that the UAI or the UAI class is configured to use an image that is not in the image registry.\nEither obtaining and pushing the image to the image registry, or correcting the name or version of the image in the UAS configuration will usually resolve this.\n"
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/retrieve_uai_image_registration_information/",
	"title": "Retrieve UAI Image Registration Information",
	"tags": [],
	"description": "",
	"content": "Retrieve UAI Image Registration Information Use this procedure to obtain the default and imagename values for a UAI image that has been registered with UAS. This procedure can also be used to confirm that a specific image ID is still registered with UAS.\nThis procedure returns the same information as List Registered UAI Images, but only for one image.\nPrerequisites Obtain a valid UAS image ID.\nProcedure   Obtain the image ID for a UAI that has been registered with UAS.\n  Query UAS for the registration details for a specific registered UAI.\nncn-m001-pit# cray uas admin config images describe 8fdf5d4a-c190-24c1-2b96-74ab98c7ec07 [[results]] default = false image_id = \u0026#34;8fdf5d4a-c190-24c1-2b96-74ab98c7ec07\u0026#34; imagename = \u0026#34;registry.local/cray/custom-end-user-uai:latest\u0026#34;   "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/select_and_configure_host_nodes_for_uais/",
	"title": "Select And Configure Host Nodes For UAIs",
	"tags": [],
	"description": "",
	"content": "Select and Configure Host Nodes for UAIs Site administrators can control the set of UAI host nodes by labeling Kubernetes worker nodes appropriately.\nUAIs run on NCNs that function as Kubernetes worker nodes. Use Kubernetes labels to prevent UAIs from running on one or more specific worker nodes. Any Kubernetes node that is not labeled to prevent UAIs from running on it is considered to be a UAI host node. In other words, UAI host node selection is an exclusive activity, not an inclusive one.\nThis procedure explains both how to identify and modify the list of current UAI host nodes to meet the needs of users.\nProcedure Identify Current UAI Host Nodes\n  Identify the nodes that could potentially be UAI host nodes.\nKubernetes master NCNs cannot host UAIs.\nncn-m001-pit# kubectl get no | grep -v master NAME STATUS ROLES AGE VERSION ncn-w001 Ready \u0026lt;none\u0026gt; 10d v1.18.6 ncn-w002 Ready \u0026lt;none\u0026gt; 25d v1.18.6 ncn-w003 Ready \u0026lt;none\u0026gt; 23d v1.18.6   Identify the nodes that are currently excluded from hosting UAIs.\nIn the following example one NCN, ncn-w001, is configured to not host UAIs. Therefore, ncn-w002 and ncn-w003 are UAI host nodes.\nncn-m001-pit# kubectl get no -l uas=False NAME STATUS ROLES AGE VERSION ncn-w001 Ready \u0026lt;none\u0026gt; 10d v1.18.6   Repeat the previous command with alternative capitalizations of False (for example, false and FALSE).\nHewlett Packard Enterprise recommends this step to identify all labeled NCNs, because labels are case-sensitive text strings and not boolean values.\n  Configure the NCNs that will Host UAIs\n Determine which and how many worker NCNs will host UAIs.\nConsider the amount of combined load that users and system services will bring to those nodes. UAIs run by default at a lower priority than system services on worker NCNs. Therefore, if the combined load exceeds the capacity of the nodes, Kubernetes will eject UAIs, refuse to schedule them, or both to protect system services. These actions can be disruptive or frustrating for users.\n  Exclude the appropriate NCNs from hosting UAIs by labeling those NCNs.\nSite administrators may set uas=True, or any capitalization variant of that, for local bookkeeping purposes. Such a setting does not transform the node into a UAS host node. Any node that does not have a uas label value of either False, false, or FALSE is a UAI host node.\nTherefore, removing the uas label from a node labeled uas=True does not take the node out of the list of UAI host nodes. The only way to prevent a non-master NCN from hosting UAIs is to explicitly set the uas label to False, false, or FALSE.\nncn-m001-pit# kubectl label node ncn-w001 uas=False --overwrite   "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/special_purpose_uais/",
	"title": "Special Purpose UAIs",
	"tags": [],
	"description": "",
	"content": "Special Purpose UAIs Even though most UAIs are end-user UAIs, UAI classes make it possible to construct UAIs to serve special purposes that are not strictly end-user oriented.\nOne kind of special purpose UAI is the broker UAI, which provides on demand end-user UAI launch and management (see Broker Mode UAI Management). While no other specialty UAI types currently exist, other applications are expected to arise and users are encouraged to innovate as needed.\n"
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/start_a_broker_uai/",
	"title": "Start A Broker UAI",
	"tags": [],
	"description": "",
	"content": "Start a Broker UAI Create a broker UAI after a broker UAI class has been created.\nPrerequisites A broker UAI class has been set up. See Configure a Broker UAI Class.\nProcedure Use the following command to create a broker UAI:\nncn-m001-pit# cray uas admin uais create --class-id \u0026lt;class-id\u0026gt; [--owner \u0026lt;name\u0026gt;] To make the broker obvious in the list of UAIs, giving it an owner name of broker is handy. The owner name on a broker is used for naming and listing, but nothing else, so this is a convenient convention.\nThe following is an example using the class created above:\nncn-m001-pit# cray uas admin uais create --class-id 74970cdc-9f94-4d51-8f20-96326212b468 --owner broker uai_connect_string = \u0026quot;ssh broker@10.103.13.162\u0026quot; uai_img = \u0026quot;dtr.dev.cray.com/cray/cray-uai-broker:latest\u0026quot; uai_ip = \u0026quot;10.103.13.162\u0026quot; uai_msg = \u0026quot; uai_name = \u0026quot;uai-broker-11f36815\u0026quot; uai_status = \u0026quot;Pending\u0026quot; username = \u0026quot;broker\u0026quot; [uai_portmap] "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/register_a_uai_image/",
	"title": "Register A UAI Image",
	"tags": [],
	"description": "",
	"content": "Register a UAI Image Register a UAI image with UAS. Registration tells UAS where to locate the image and whether to use the image as the default for UAIs.\nPrerequisites  Initialize cray administrative CLI. Create a UAI image and upload it to the container registry. See Customize End-User UAI Images.  Procedure   Register a UAI image with UAS.\nThe following is the minimum required CLI command form:\nncn-m001-pit# cray uas admin config images create --imagename \u0026lt;image_name\u0026gt; To register the image registry.local/cray/custom-end-user-uai:latest, the stock end-user UAI image, use:\nncn-m001-pit# cray uas admin config images create --imagename registry.local/cray/custom-end-user-uai:latest The following example registers the stock UAI image registry.local/cray/custom-end-user-uai:latest. This example also implicitly sets the default attribute to false because the --default option is omitted in the command.\nncn-m001-pit# cray uas admin config images create --imagename registry.local/cray/custom-end-user-uai:latest --default yes To register the image explicitly as non-default:\nncn-m001-pit# cray uas admin config images create --imagename registry.local/cray/custom-end-user-uai:latest --default no Registering an image with the --default no option is usually unnecessary. Omitting the --default option causes UAS to set the default attribute as false internally.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/reset_the_uas_configuration_to_original_installed_settings/",
	"title": "Reset The UAS Configuration To Original Installed Settings",
	"tags": [],
	"description": "",
	"content": "Reset the UAS Configuration to Original Installed Settings How to remove a customized UAS configuration and restore the base installed configuration.\nThe configuration set up using the Cray CLI to interact with UAS persists as long as UAS remains installed and survives upgrades. This is called the running configuration and it is both persistent and malleable. During installation and localization, however, the installer creates a base installed configuration. It may be necessary to return to this base configuration. To do this, delete the running configuration, which will cause the UAS to reset to the base installed configuration.\nWARNING: Deleting the running configuration discards all changes that have been made since initial installation and is not recoverable. Be certain this is acceptable before proceeding.\nPrerequisites This procedure requires administrator privileges.\nProcedure   Delete the running configuration.\nncn-w001 # cray uas admin config delete This will delete all locally applied configuration, Are you sure? [y/N]:   Confirm the command. This will delete the running configuration and cannot be undone.\nncn-w001 # cray uas admin config delete This will delete all locally applied configuration, Are you sure? [y/N]: y Alternatively, note that the interactive prompt can be bypassed by supplying the -y option.\nncn-w001 # cray uas admin config delete -y   "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/resource_specifications/",
	"title": "Resource Specifications",
	"tags": [],
	"description": "",
	"content": "Resource Specifications Kubernetes uses resource limits and resource requests, to manage the system resources available to pods. Because UAIs run as pods under Kubernetes, UAS takes advantage of Kubernetes to manage the system resources available to UAIs.\nIn the UAS configuration, resource specifications contain that configuration. A UAI that is assigned a resource specification will use that instead of the default resource limits or requests on the Kubernetes namespace containing the UAI. This can be used to fine-tune resources assigned to UAIs.\n"
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/retrieve_resource_specification_details/",
	"title": "Retrieve Resource Specification Details",
	"tags": [],
	"description": "",
	"content": "Retrieve Resource Specification Details Display a specific resource specification using the resource_id of that specification.\nPrerequisites Install and initialize the cray administrative CLI.\nProcedure   Print out a resource specification.\nTo examine a particular resource specification, use a command of the following form:\nncn-m001-pit# cray uas admin config resources describe RESOURCE_ID For example:\nncn-m001-pit# cray uas admin config resources describe 85645ff3-1ce0-4f49-9c23-05b8a2d31849 comment = \u0026#34;my first example resource specification\u0026#34; limit = \u0026#34;{\u0026#34;cpu\u0026#34;: \u0026#34;300m\u0026#34;, \u0026#34;memory\u0026#34;: \u0026#34;250Mi\u0026#34;}\u0026#34; request = \u0026#34;{\u0026#34;cpu\u0026#34;: \u0026#34;300m\u0026#34;, \u0026#34;memory\u0026#34;: \u0026#34;250Mi\u0026#34;}\u0026#34; resource_id = \u0026#34;85645ff3-1ce0-4f49-9c23-05b8a2d31849\u0026#34;   "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/login_to_a_users_uai_to_troubleshoot_issues/",
	"title": "Log In To A User&#39;s UAI To Troubleshoot Issues",
	"tags": [],
	"description": "",
	"content": "Log in to a User\u0026rsquo;s UAI to Troubleshoot Issues Log in to a user\u0026rsquo;s User Access Instance (UAI) to help the user troubleshoot issues.\nPrerequisites This procedure requires root access.\nLimitations This procedure does not work if the pod is in either \u0026ldquo;Error\u0026rdquo; or \u0026ldquo;Terminating\u0026rdquo; states.\nProcedure   Log in to the first NCN acting as a Kubernetes master node (ncn-m001) as root.\n  Find and record the name of the UAI.\nncn-m001# cray uas uais list [[results]] username = \u0026#34;user\u0026#34; uai_host = \u0026#34;ncn-m001\u0026#39; uai_status = \u0026#34;Running: Ready\u0026#34; uai_connect_string = \u0026#34;ssh user@203.0.113.0 -i ~/.ssh/id\\_rsa\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uas-sles15sp1-slurm:latest\u0026#34; uai_age = \u0026#34;14m\u0026#34; uai_name = \u0026#34;**uai-uastest-0abd2928**\u0026#34;   Find the full name of the pod that represents this user\u0026rsquo;s UAI.\nncn-m001# kubectl get pods -n user -l app=USERS_UAI_NAME NAME READY STATUS RESTARTS AGE ** uai-uastest-0abd2928-575fcf8cf7-ftgxw** 1/1 Running 0 89m Note the full name of the pod (in this example: uai-root-0abd2928-575fcf8cf7-ftgxw).\n  Connect to the pod.\nncn-m001# kubectl exec -n user -it FULL_POD_NAME /bin/bash As root in the user\u0026rsquo;s UAI, an administrator will have the user\u0026rsquo;s UID, GID, and full access to their file system mounts.\n  Assist the user with issues, and use exit to exit the UAI.\n"
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/modify_a_uai_class/",
	"title": "Modify A UAI Class",
	"tags": [],
	"description": "",
	"content": "Modify a UAI Class Update a UAI class with a modified configuration.\nPrerequisites  Install and initialize the cray administrative CLI. Obtain the ID of the UAI class that will be modified.  Limitations The ID of the UAI class cannot be modified.\nProcedure To update an existing UAI class, use a command of the following form:\ncray uas admin config classes update OPTIONS UAI_CLASS_ID OPTIONS are the same options supported for UAI class creation (see Create a UAI Class) and UAI_CLASS_ID is the ID of the UAI class.\n  Modify a UAI class.\nThe following example changes the comment on the UAI class with an ID of bb28a35a-6cbc-4c30-84b0-6050314af76b.\nncn-m001-pit# cray uas admin config classes update \\ --comment \u0026#34;a new comment for my UAI class\u0026#34; \\ bb28a35a-6cbc-4c30-84b0-6050314af76b Any change made using this command affects only UAIs that are both created using the modified class and are created after the modification. Existing UAIs using the class will not change.\n  Optional: Update currently running UAIs by deleting and recreating them.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/obtain_configuration_of_a_uas_volume/",
	"title": "Obtain The Configuration Of A UAS Volume",
	"tags": [],
	"description": "",
	"content": "Obtain the Configuration of a UAS Volume View the configuration information of a specific UAS volume. This procedure requires the volume_ID of that volume.\nPrerequisites  Install and initialize the cray administrative CLI. Obtain the UAS volume ID of a volume. Perform List Volumes Registered in UAS if needed.  Procedure   View the configuration of a specific UAS volume.\nThis command returns output in TOML format by default. JSON or YAML formatted output can be obtained by using the --format json or --format yaml options respectively.\nncn-m001-pit# cray uas admin config volumes describe \\ a0066f48-9867-4155-9268-d001a4430f5c --format json { \u0026#34;mount_path\u0026#34;: \u0026#34;/host_files/host_passwd\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/etc/passwd\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;FileOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;a0066f48-9867-4155-9268-d001a4430f5c\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;my-volume-with-passwd-from-the-host-node\u0026#34; }   "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/list_and_delete_all_uais/",
	"title": "List And Delete All UAIs",
	"tags": [],
	"description": "",
	"content": "List and Delete All UAIs Delete all UAIs currently on the system.\nPrerequisites At least one UAI is running.\nProcedure   Log in to an NCN as root.\n  List all the UAIs on the system.\nncn-m001# cray uas uais list [[results]] username = \u0026#34;uastest\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; uai_connect_string = \u0026#34;ssh uastest@203.0.113.0 -p 32486 -i ~/.ssh/id_rsa\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uas-sles15sp1-slurm:latest\u0026#34; uai_age = \u0026#34;2m\u0026#34; uai_name = \u0026#34;uai-uastest-f488eef6\u0026#34; [[results]] username = \u0026#34;uastest\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; uai_connect_string = \u0026#34;ssh uastest@203.0.113.0 -p 31833 -i ~/.ssh/id_rsa\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uas-sles15sp1-slurm:latest\u0026#34; uai_age = \u0026#34;11m\u0026#34; uai_name = \u0026#34;uai-uastest-391da133\u0026#34; [[results]] username = \u0026#34;uasuser\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; uai_connect_string = \u0026#34;ssh uasuser@203.0.113.0 -p 32736 -i ~/.ssh/id_rsa\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uas-sles15sp1-slurm:latest\u0026#34; uai_name = \u0026#34;uai-uasuser-66f8a478\u0026#34;   Delete all the UAIs on the system.\nncn-m001 # cray uas uais delete This will delete all running UAIs, Are you sure? [y/N]: y [ \u0026#34;Successfully deleted uai-uastest-f488eef6\u0026#34;, \u0026#34;Successfully deleted uai-uastest-391da133\u0026#34;, \u0026#34;Successfully deleted uai-uasuser-66f8a478\u0026#34;, ]   Verify all UAIs are in the \u0026ldquo;Terminating\u0026rdquo; status.\nncn-m001# cray uas uais list [[results]] username = \u0026#34;uastest\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_status = \u0026#34;Terminating\u0026#34; uai_connect_string = \u0026#34;ssh uastest@203.0.113.0 -p 32486 -i ~/.ssh/id_rsa\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uas-sles15sp1-slurm:latest\u0026#34; uai_age = \u0026#34;2m\u0026#34; uai_name = \u0026#34;uai-uastest-f488eef6\u0026#34; [[results]] username = \u0026#34;uastest\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_status = \u0026#34;Terminating\u0026#34; uai_connect_string = \u0026#34;ssh uastest@203.0.113.0 -p 31833 -i ~/.ssh/id_rsa\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uas-sles15sp1-slurm:latest\u0026#34; uai_age = \u0026#34;11m\u0026#34; uai_name = \u0026#34;uai-uastest-391da133\u0026#34; [[results]] username = \u0026#34;uasuser\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_status = \u0026#34;Terminating\u0026#34; uai_connect_string = \u0026#34;ssh uasuser@203.0.113.0 -p 32736 -i ~/.ssh/id_rsa\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uas-sles15sp1-slurm:latest\u0026#34; uai_name = \u0026#34;uai-uasuser-66f8a478\u0026#34;   Verify there are no running UAIs on the system.\nncn-m001# cray uas uais list [[results]]   "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/list_uas_information/",
	"title": "List UAS Information",
	"tags": [],
	"description": "",
	"content": "List UAS Information Use the cray uas command to gather information about the User Access Service\u0026rsquo;s version, images, and running User Access Instances (UAIs).\nList UAS Version with cray uas mgr-info list ncn-w001# cray uas mgr-info list service_name = \u0026quot;cray-uas-mgr\u0026quot;, version = \u0026quot;0.11.3\u0026quot; List Available UAS Images with cray uas images list ncn-w001# cray uas images list default_image = \u0026#34;registry.local/cray/cray-uas-sles15sp1-slurm:latest\u0026#34; image_list = [ \u0026#34;registry.local/cray/cray-uas-sles15sp1-slurm:latest\u0026#34;, \u0026#34;registry.local/cray/cray-uas-sles15sp1:latest\u0026#34;,] List All Running UAIs with cray uas uais list ncn-w001# cray uas uais list [[results]] username = \u0026#34;user\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; uai_connect_string = \u0026#34;ssh user@203.0.113.0 -i ~/.ssh/id_rsa\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uas-sles15sp1-slurm:latest\u0026#34; uai_age = \u0026#34;11m\u0026#34; uai_name = \u0026#34;uai-user-be3a6770\u0026#34; [[results]] username = \u0026#34;user\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; uai_connect_string = \u0026#34;ssh user@203.0.113.0 -i ~/.ssh/id_rsa\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uas-sles15sp1-slurm:latest\u0026#34; uai_age = \u0026#34;14m\u0026#34; uai_name = \u0026#34;uai-user-f488eef6\u0026#34; List UAI Information for Current User with cray uas list ncn-w001# cray uas list [[results]] username = \u0026#34;user\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; uai_connect_string = \u0026#34;ssh user@203.0.113.0 -i ~/.ssh/id\\_rsa\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uas-sles15sp1-slurm:latest\u0026#34; uai_age = \u0026#34;11m\u0026#34; uai_name = \u0026#34;uai-user-be3a6770\u0026#34; List UAIs on a Specific Host Node ncn-w001# cray uas uais list --host ncn-w001 [[results]] username = \u0026#34;user\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; uai_connect_string = \u0026#34;ssh user@203.0.113.0 -i ~/.ssh/id_rsa\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uas-sles15sp1-slurm:latest\u0026#34; uai_age = \u0026#34;2h56m\u0026#34; uai_name = \u0026#34;uai-user-f3b8eee0\u0026#34; [[results]] username = \u0026#34;user\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; uai_connect_string = \u0026#34;ssh user@203.0.113.0 -i ~/.ssh/id_rsa\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uas-sles15sp1-slurm:latest\u0026#34; uai_age = \u0026#34;1d5h\u0026#34; uai_name = \u0026#34;uai-user-f8671d33\u0026#34; "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/list_volumes_registered_in_uas/",
	"title": "List Volumes Registered In UAS",
	"tags": [],
	"description": "",
	"content": "List Volumes Registered in UAS List the details of all volumes registered in UAS with the cray uas admin config volumes list command. Use this command to obtain the volume_id value of volume, which is required for other UAS administrative commands.\nPrerequisites Install and initialize the cray administrative CLI.\nProcedure   List the details of all the volumes registered in UAS.\n  Print out the list in TOML.\nncn-m001-pit# cray uas admin config volumes list [[results]] mount_path = \u0026#34;/lus\u0026#34; volume_id = \u0026#34;2b23a260-e064-4f3e-bee5-3da8e3664f29\u0026#34; volumename = \u0026#34;lustre\u0026#34; [results.volume_description.host_path] path = \u0026#34;/lus\u0026#34; type = \u0026#34;DirectoryOrCreate\u0026#34; [[results]] mount_path = \u0026#34;/etc/slurm\u0026#34; volume_id = \u0026#34;53ea3f18-b202-455f-a8ec-79f9463aeb7b\u0026#34; volumename = \u0026#34;slurm-config\u0026#34; [results.volume_description.config_map] name = \u0026#34;slurm-map\u0026#34; [[results]] mount_path = \u0026#34;/root/slurm_config/munge\u0026#34; volume_id = \u0026#34;656aed94-fb5a-4b94-bcb7-19607bd8670f\u0026#34; volumename = \u0026#34;munge-key\u0026#34; [results.volume_description.secret] secret_name = \u0026#34;munge-secret\u0026#34; [[results]] mount_path = \u0026#34;/etc/pbs\u0026#34; volume_id = \u0026#34;7ee2bbe9-6428-43c8-b626-0d2316f3aff8\u0026#34; volumename = \u0026#34;pbs-config\u0026#34; [results.volume_description.config_map] name = \u0026#34;pbs-config\u0026#34; [[results]] mount_path = \u0026#34;/opt/forge_license\u0026#34; volume_id = \u0026#34;99e705a2-9bde-48cf-934d-ae721403d8fa\u0026#34; volumename = \u0026#34;optforgelicense\u0026#34; [results.volume_description.host_path] path = \u0026#34;/opt/forge_license\u0026#34; type = \u0026#34;DirectoryOrCreate\u0026#34; [[results]] mount_path = \u0026#34;/opt/forge\u0026#34; volume_id = \u0026#34;de224953-f5de-42f4-9d18-638855799dba\u0026#34; volumename = \u0026#34;opt-forge\u0026#34; [results.volume_description.host_path] path = \u0026#34;/opt/forge\u0026#34; type = \u0026#34;DirectoryOrCreate\u0026#34; [[results]] mount_path = \u0026#34;/etc/localtime\u0026#34; volume_id = \u0026#34;ef4be476-79c4-4b76-a9ba-e6dccf2a16db\u0026#34; volumename = \u0026#34;timezone\u0026#34; [results.volume_description.host_path] path = \u0026#34;/etc/localtime\u0026#34; type = \u0026#34;FileOrCreate\u0026#34;   Print out the list in YAML format.\nncn-m001-pit# cray uas admin config volumes list --format yaml - mount_path: /lus volume_description: host_path: path: /lus type: DirectoryOrCreate volume_id: 2b23a260-e064-4f3e-bee5-3da8e3664f29 volumename: lustre - mount_path: /etc/slurm volume_description: config_map: name: slurm-map volume_id: 53ea3f18-b202-455f-a8ec-79f9463aeb7b volumename: slurm-config - mount_path: /root/slurm_config/munge volume_description: secret: secret_name: munge-secret volume_id: 656aed94-fb5a-4b94-bcb7-19607bd8670f volumename: munge-key - mount_path: /etc/pbs volume_description: config_map: name: pbs-config volume_id: 7ee2bbe9-6428-43c8-b626-0d2316f3aff8 volumename: pbs-config - mount_path: /opt/forge_license volume_description: host_path: path: /opt/forge_license type: DirectoryOrCreate volume_id: 99e705a2-9bde-48cf-934d-ae721403d8fa volumename: optforgelicense - mount_path: /opt/forge volume_description: host_path: path: /opt/forge type: DirectoryOrCreate volume_id: de224953-f5de-42f4-9d18-638855799dba volumename: opt-forge - mount_path: /etc/localtime volume_description: host_path: path: /etc/localtime type: FileOrCreate volume_id: ef4be476-79c4-4b76-a9ba-e6dccf2a16db volumename: timezone   Print out the list in JSON format.\nncn-m001-pit# cray uas admin config volumes list --format json [ { \u0026#34;mount_path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;DirectoryOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;2b23a260-e064-4f3e-bee5-3da8e3664f29\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;lustre\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/etc/slurm\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;config_map\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;slurm-map\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;53ea3f18-b202-455f-a8ec-79f9463aeb7b\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;slurm-config\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/root/slurm_config/munge\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;secret\u0026#34;: { \u0026#34;secret_name\u0026#34;: \u0026#34;munge-secret\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;656aed94-fb5a-4b94-bcb7-19607bd8670f\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;munge-key\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/etc/pbs\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;config_map\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;pbs-config\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;7ee2bbe9-6428-43c8-b626-0d2316f3aff8\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;pbs-config\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/opt/forge_license\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/opt/forge_license\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;DirectoryOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;99e705a2-9bde-48cf-934d-ae721403d8fa\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;optforgelicense\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/opt/forge\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/opt/forge\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;DirectoryOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;de224953-f5de-42f4-9d18-638855799dba\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;opt-forge\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;FileOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;ef4be476-79c4-4b76-a9ba-e6dccf2a16db\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;timezone\u0026#34; } ]   The JSON formatted output can help guide administrators to construct the volume descriptions required to add or update a volume description in UAS. JSON is the required input format for volume descriptions in UAS. Refer to Elements of a UAI for descriptions of mount_path, volume_description, volume_id, and volumename.\n  Looking at the above output, each volume has a mount_path, volume_description, volume_name and volume_id entry. The mount_path specifies where in the UAI the volume will be mounted.\nNOTE: While it is acceptable to have multiple volumes configured in UAS with the same mount_path, any given UAI will fail creation if it has more than one volume specified for a given mount path. If multiple volumes with the same mount path exist in the UAS configuration, all UAIs must be created using UAI classes that specify a workable subset of volumes. A UAI created without a UAI Class under such a UAS configuration will try to use all configured volumes and creation will fail.\nThe volume_description is the JSON description of the volume, specified as a dictionary with one entry, whose key identifies the kind of Kubernetes volume is described (i.e. host_path, configmap, secret, etc.) whose value is another dictionary containing the Kubernetes volume description itself. See Kubernetes documentation for details on what goes in various kinds of volume descriptions.\nThe volumename is a string the creator of the volume may chose to describe or name the volume. It must be comprised of only lower case alphanumeric characters and dashes ('-') and must begin and end with an alphanumeric character. It is used inside the UAI pod specification to identify the volume that is mounted in a given location in a container. It is required and administrators are free to use any name that meets the requirements. Volume names do need to be unique within any given UAI and are far more useful when searching for a volume if they are unique across the UAS configuration.\nThe volume_id is a unique identifier used to identify the UAS volume when examining, updating or deleting a volume and when linking a volume to a UAI class.\n"
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/log_in_to_a_broker_uai/",
	"title": "Log In To A Broker UAI",
	"tags": [],
	"description": "",
	"content": "Log in to a Broker UAI SSH to log into a broker UAI and reach the end-user UAIs on demand.\nPrerequisites The broker UAI is running. See Start a Broker UAI.\nProcedure   Log in to the broker UAI.\nThe following example is the first login for the vers user:\nvers\u0026gt; ssh vers@10.103.13.162 The authenticity of host '10.103.13.162 (10.103.13.162)' can't be established. ECDSA key fingerprint is SHA256:k4ef6vTtJ1Dtb6H17cAFh5ljZYTl4IXtezR3fPVUKZI. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added '10.103.13.162' (ECDSA) to the list of known hosts. Password: Creating a new UAI... The authenticity of host '10.21.138.52 (10.21.138.52)' can't be established. ECDSA key fingerprint is SHA256:TX5DMAMQ8yQuL4YHo9qFEJWpKaaiqfeSs4ndYXOTjkU. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added '10.21.138.52' (ECDSA) to the list of known hosts. There are several things to notice here:\n The first time the user logs in the broker UAI\u0026rsquo;s SSH host key is unknown, as is normal for SSH. The user is asked for a password in this example. If the user\u0026rsquo;s home directory, as defined in LDAP had been mounted in the broker UAI and a .ssh/authorized_keys entry had been present, there would not have been a password prompt. Home directory trees can be mounted as volumes just as any other directory can. The broker mechanism in the broker UAI creates a new UAI because vers has never logged into this broker UAI before. There is a second prompt to acknowledge an unknown host which is, in this case, the end-user UAI itself. The broker UAI constructs a public/private key pair for the hidden SSH connection between the broker and the end-user UAI shown in the image in Broker Mode UAI Management.    Log out of the broker UAI.\n  Log in to the broker UAI again.\nThe next time vers logs in, it will look similar to the following:\nvers\u0026gt; ssh vers@10.103.13.162 Password: vers@uai-vers-ee6f427e-6c7468cdb8-2rqtv\u0026gt; Only the password prompt appears now, because the hosts are all known and the end-user UAI exists but there is no .ssh/authorized_keys known yet by the broker UAI for vers.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/list_available_uai_images_in_legacy_mode/",
	"title": "List Available UAI Images In Legacy Mode",
	"tags": [],
	"description": "",
	"content": "List Available UAI Images in Legacy Mode A user can list the UAI images available for creating a UAI with a command of the form:\nuser\u0026gt; cray uas images list For example:\nvers\u0026gt; cray uas images list default_image = \u0026quot;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026quot; image_list = [ \u0026quot;dtr.dev.cray.com/cray/cray-uai-broker:latest\u0026quot;, \u0026quot;dtr.dev.cray.com/cray/cray-uas-sles15:latest\u0026quot;, \u0026quot;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026quot;,] "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/list_registered_uai_images/",
	"title": "List Registered UAI Images",
	"tags": [],
	"description": "",
	"content": "List Registered UAI Images Administrators can use the cray uas admin config images list command to see the list of registered images. This command also displays the UAS registration information about each image.\nRegistering a UAI image name is insufficient to make that image available for UAIs. UAI images must also be registered, but also created and stored in the container registry need to link to procedure on how to do that.\nPrerequisites This procedure requires administrator privileges and the cray administrative CLI.\nProcedure   Obtain the list of UAI images that are currently registered with UAS.\nncn-m001-pit# cray uas admin config images list [[results]] default = true image_id = \u0026#34;08a04462-195a-4e66-aa31-08076072c9b3\u0026#34; imagename = \u0026#34;registry.local/cray/cray-uas-sles15:latest\u0026#34; [[results]] default = false image_id = \u0026#34;f8d5f4da-c910-421c-92b6-794ab8cc7e70\u0026#34; imagename = \u0026#34;registry.local/cray/cray-uai-broker:latest\u0026#34; [[results]] default = false image_id = \u0026#34;8fdf5d4a-c190-24c1-2b96-74ab98c7ec07\u0026#34; imagename = \u0026#34;registry.local/cray/custom-end-user-uai:latest\u0026#34; The output shown above shows three image registrations. Each has an imagename indicating the image to be used to construct a UAI.\nNOTE: Simply registering a UAI image name does not make the image available. The image must also be created and stored in the container registry. See Customize End-User UAI Images.\nThere is also a default flag. If this flag is true, the image will be used whenever a UAI is created without specifying an image or UAI class as part of the creation. Finally, there is an image_id, which identifies this image registration for later inspection, update, or deletion and for linking the image to a UAI class.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/list_uai_resource_specifications/",
	"title": "List UAI Resource Specifications",
	"tags": [],
	"description": "",
	"content": "List UAI Resource Specifications Obtain a list of all the UAI resource specifications registered with UAS.\nPrerequisites The cray administrative CLI must be installed and initialized.\nProcedure   List all the resource specifications registered in UAS.\nThe resource specifications returned by the following command are available for UAIs to use:\nncn-m001-pit# cray uas admin config resources list [[results]] comment = \u0026quot;my first example resource specification\u0026quot; limit = \u0026quot;{\\\u0026quot;cpu\\\u0026quot;: \\\u0026quot;300m\\\u0026quot;, \\\u0026quot;memory\\\u0026quot;: \\\u0026quot;250Mi\\\u0026quot;}\u0026quot; request = \u0026quot;{\\\u0026quot;cpu\\\u0026quot;: \\\u0026quot;300m\\\u0026quot;, \\\u0026quot;memory\\\u0026quot;: \\\u0026quot;250Mi\\\u0026quot;}\u0026quot; resource_id = \u0026quot;85645ff3-1ce0-4f49-9c23-05b8a2d31849\u0026quot; [[results]] comment = \u0026quot;my second example resource specification\u0026quot; limit = \u0026quot;{\\\u0026quot;cpu\\\u0026quot;: \\\u0026quot;4\\\u0026quot;, \\\u0026quot;memory\\\u0026quot;: \\\u0026quot;1Gi\\\u0026quot;}\u0026quot; request = \u0026quot;{\\\u0026quot;cpu\\\u0026quot;: \\\u0026quot;4\\\u0026quot;, \\\u0026quot;memory\\\u0026quot;: \\\u0026quot;1Gi\\\u0026quot;}\u0026quot; resource_id = \u0026quot;eff9e1f2-3560-4ece-a9ce-8093e05e032d\u0026quot; The following are the configurable parts of a resource specification:\n limit - A JSON string describing a Kubernetes resource limit request - A JSON string describing a Kubernetes resource request comment - An optional free form string containing any information an administrator might find useful about the resource specification resource-id - Used for examining, updating or deleting the resource specification as well as linking the resource specification into a UAI class  Refer to Elements of a UAI for more information.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/list_uais/",
	"title": "List UAIs",
	"tags": [],
	"description": "",
	"content": "List UAIs View the details of every UAI that is running by using a direct UAS administrative command.\nPrerequisites  This procedure requires administrative privileges. Install and initialize the cray administrative CLI.  Procedure   List the existing UAIs.\nUse a command of the following form:\nncn-m001-pit# cray uas admin uais list [options] The [options] parameter includes the following selection options:\n --owner '\u0026lt;user-name\u0026gt;' show only UAIs owned by the named user --class-id '\u0026lt;class-id' show only UAIs of the specified UAI class  For example:\nncn-m001-pit# cray uas admin uais list --owner vers [[results]] uai_age = \u0026quot;6h22m\u0026quot; uai_connect_string = \u0026quot;ssh vers@10.28.212.166\u0026quot; uai_host = \u0026quot;ncn-w001\u0026quot; uai_img = \u0026quot;registry.local/cray/cray-uai-sles15sp1:latest\u0026quot; uai_ip = \u0026quot;10.28.212.166\u0026quot; uai_msg = \u0026quot; uai_name = \u0026quot;uai-vers-715fa89d\u0026quot; uai_status = \u0026quot;Running: Ready\u0026quot; username = \u0026quot;vers\u0026quot;   "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/examine_a_uai_using_a_direct_administrative_command/",
	"title": "Examine A UAI Using A Direct Administrative Command",
	"tags": [],
	"description": "",
	"content": "Examine a UAI Using a Direct Administrative Command Print out information about a UAI.\nPrerequisites Install and initialize the cray administrative CLI.\nProcedure   Print out information about a UAI.\nTo examine an existing UAI use a command of the following form:\ncray uas admin uais describe \u0026lt;uai-name\u0026gt; For example:\nncn-m001-pit# cray uas admin uais describe uai-vers-715fa89d uai_age = \u0026#34;2d23h\u0026#34; uai_connect_string = \u0026#34;ssh vers@10.28.212.166\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uai-sles15sp1:latest\u0026#34; uai_ip = \u0026#34;10.28.212.166\u0026#34; uai_msg = \u0026#34; uai_name = \u0026#34;uai-vers-715fa89d\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; username = \u0026#34;vers\u0026#34; [uai_portmap]   "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/legacy_mode_user-driven_uai_management/",
	"title": "Legacy Mode User-driven UAI Management",
	"tags": [],
	"description": "",
	"content": "Legacy Mode User-Driven UAI Management In the legacy mode, users create and manage their own UAIs through the Cray CLI. A user may create, list and delete only UAIs owned by the user. The user may not create a UAI for another user, nor may the user see or delete UAIs owned by another user. Once created, the information describing the UAI gives the user the information needed to reach the UAI using SSH and log into it.\nThe following diagram illustrates a system running with UAIs created in the legacy mode by four users, each of whom has created at least one end-user UAI. Notice that the example user Pat has created two end-user UAIs:\nIn the simplest UAS configuration, there is some number of UAI images available for use in legacy mode and there is a set of volumes defined. In this configuration, when a UAI is created, the user may specify the UAI image to use as an option when creating the UAI, or may allow a default UAI image, if one is assigned, to be used. Every volume defined at the time the UAI is created will be mounted unconditionally in every newly created UAI if this approach is used. This can lead to problems with conflicting volume mount points (see Troubleshoot Duplicate Mount Paths in a UAI) and unresolvable volumes (see Troubleshoot UAI Stuck in \u0026ldquo;ContainerCreating\u0026rdquo;) in some configurations of UAS. Unless UAI classes are used to make UAIs, care must be taken to ensure all volumes have unique mount-path settings and are accessible in the user Kubernetes namespace.\nThe Benefits of Using UAI Classes with Legacy Mode A slightly more sophisticated configuration approach defines a default UAI Class that is always used by legacy mode UAI creation. When this approach is taken, the user can no longer specify the image to use, as it will be supplied by the UAI class, and the volumes mounted in any UAI created in legacy mode will be based on the specified UAI class. As long as volumes do not conflict within the list of volumes in a given UAI class, there is no need to avoid duplicate mount-path settings in the global list of volumes when this approach is used.\n"
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/list_available_uai_classes/",
	"title": "List Available UAI Classes",
	"tags": [],
	"description": "",
	"content": "List Available UAI Classes View all the details of every available UAI class. Use this information to select a class to apply to one or more UAIs.\nPrerequisites Install and initialize the cray administrative CLI.\nProcedure   List all available UAI classes.\nTo list available UAI classes, use the following command:\nncn-m001-pit# cray uas admin config classes list The cray uas admin config classes list command supports the same --format options as the cray uas admin config volumes list command. See List Volumes Registered in UAS for details.\nFor example:\nncn-m001-pit# cray uas admin config classes list --format json [ { \u0026#34;class_id\u0026#34;: \u0026#34;05496a5f-7e35-435d-a802-882c6425e5b2\u0026#34;, \u0026#34;comment\u0026#34;: \u0026#34;UAI Broker Class\u0026#34;, \u0026#34;default\u0026#34;: false, \u0026#34;namespace\u0026#34;: \u0026#34;uas\u0026#34;, \u0026#34;opt_ports\u0026#34;: [], \u0026#34;priority_class_name\u0026#34;: \u0026#34;uai-priority\u0026#34;, \u0026#34;public_ip\u0026#34;: true, \u0026#34;resource_config\u0026#34;: null, \u0026#34;uai_compute_network\u0026#34;: false, \u0026#34;uai_creation_class\u0026#34;: \u0026#34;a623a04a-8ff0-425e-94cc-4409bdd49d9c\u0026#34;, \u0026#34;uai_image\u0026#34;: { \u0026#34;default\u0026#34;: false, \u0026#34;image_id\u0026#34;: \u0026#34;c5dcb261-5271-49b3-9347-afe7f3e31941\u0026#34;, \u0026#34;imagename\u0026#34;: \u0026#34;registry.local/cray/cray-uai-broker:latest\u0026#34; }, \u0026#34;volume_mounts\u0026#34;: [ { \u0026#34;mount_path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;FileOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;55a02475-5770-4a77-b621-f92c5082475c\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;timezone\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;DirectoryOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;9fff2d24-77d9-467f-869a-235ddcd37ad7\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;lustre\u0026#34; } ] }, { \u0026#34;class_id\u0026#34;: \u0026#34;a623a04a-8ff0-425e-94cc-4409bdd49d9c\u0026#34;, \u0026#34;comment\u0026#34;: \u0026#34;UAI User Class\u0026#34;, \u0026#34;default\u0026#34;: false, \u0026#34;namespace\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;opt_ports\u0026#34;: [], \u0026#34;priority_class_name\u0026#34;: \u0026#34;uai-priority\u0026#34;, \u0026#34;public_ip\u0026#34;: false, \u0026#34;resource_config\u0026#34;: null, \u0026#34;uai_compute_network\u0026#34;: true, \u0026#34;uai_creation_class\u0026#34;: null, \u0026#34;uai_image\u0026#34;: { \u0026#34;default\u0026#34;: true, \u0026#34;image_id\u0026#34;: \u0026#34;ff86596e-9699-46e8-9d49-9cb20203df8c\u0026#34;, \u0026#34;imagename\u0026#34;: \u0026#34;registry.local/cray/cray-uai-sles15sp1:latest\u0026#34; }, \u0026#34;volume_mounts\u0026#34;: [ { \u0026#34;mount_path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;FileOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;55a02475-5770-4a77-b621-f92c5082475c\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;timezone\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;DirectoryOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;9fff2d24-77d9-467f-869a-235ddcd37ad7\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;lustre\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/root/slurm_config/munge\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;secret\u0026#34;: { \u0026#34;secret_name\u0026#34;: \u0026#34;munge-secret\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;7aeaf158-ad8d-4f0d-bae6-47f8fffbd1ad\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;munge-key\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/etc/slurm\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;config_map\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;slurm-map\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;ea97325c-2b1d-418a-b3b5-3f6488f4a9e2\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;slurm-config\u0026#34; } ] }, { \u0026#34;class_id\u0026#34;: \u0026#34;bb28a35a-6cbc-4c30-84b0-6050314af76b\u0026#34;, \u0026#34;comment\u0026#34;: \u0026#34;Non-Brokered UAI User Class\u0026#34;, \u0026#34;default\u0026#34;: false, \u0026#34;namespace\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;opt_ports\u0026#34;: [], \u0026#34;priority_class_name\u0026#34;: \u0026#34;uai-priority\u0026#34;, \u0026#34;public_ip\u0026#34;: true, \u0026#34;resource_config\u0026#34;: null, \u0026#34;uai_compute_network\u0026#34;: true, \u0026#34;uai_creation_class\u0026#34;: null, \u0026#34;uai_image\u0026#34;: { \u0026#34;default\u0026#34;: true, \u0026#34;image_id\u0026#34;: \u0026#34;ff86596e-9699-46e8-9d49-9cb20203df8c\u0026#34;, \u0026#34;imagename\u0026#34;: \u0026#34;registry.local/cray/cray-uai-sles15sp1:latest\u0026#34; }, \u0026#34;volume_mounts\u0026#34;: [ { \u0026#34;mount_path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/etc/localtime\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;FileOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;55a02475-5770-4a77-b621-f92c5082475c\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;timezone\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;host_path\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/lus\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;DirectoryOrCreate\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;9fff2d24-77d9-467f-869a-235ddcd37ad7\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;lustre\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/root/slurm_config/munge\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;secret\u0026#34;: { \u0026#34;secret_name\u0026#34;: \u0026#34;munge-secret\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;7aeaf158-ad8d-4f0d-bae6-47f8fffbd1ad\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;munge-key\u0026#34; }, { \u0026#34;mount_path\u0026#34;: \u0026#34;/etc/slurm\u0026#34;, \u0026#34;volume_description\u0026#34;: { \u0026#34;config_map\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;slurm-map\u0026#34; } }, \u0026#34;volume_id\u0026#34;: \u0026#34;ea97325c-2b1d-418a-b3b5-3f6488f4a9e2\u0026#34;, \u0026#34;volumename\u0026#34;: \u0026#34;slurm-config\u0026#34; } ] } ]   Examine the output.\n  In the returned output, there are three UAI classes:\n A UAI broker class A brokered end-user UAI class A non-brokered end-user UAI class  Taking apart the non-brokered end-user UAI class, the first part is:\n \u0026quot;class_id\u0026quot;: \u0026quot;bb28a35a-6cbc-4c30-84b0-6050314af76b\u0026quot;, \u0026quot;comment\u0026quot;: \u0026quot;Non-Brokered UAI User Class\u0026quot;, \u0026quot;default\u0026quot;: false, \u0026quot;namespace\u0026quot;: \u0026quot;user\u0026quot;, \u0026quot;opt_ports\u0026quot;: [], \u0026quot;priority_class_name\u0026quot;: \u0026quot;uai-priority\u0026quot;, \u0026quot;public_ip\u0026quot;: true, \u0026quot;resource_config\u0026quot;: null, \u0026quot;uai_compute_network\u0026quot;: true, \u0026quot;uai_creation_class\u0026quot;: null, The class_id field is the identifier used to refer to this class when examining, updating, and deleting this class as well as when using the class with the command:\nncn-m001-pit# cray uas admin uais create The comment field is a free form string describing the UAI class. The default field is a flag indicating whether this class is the default class. The default class will be applied, overriding both the default UAI image and any specified image name, when the following command is used to create an end-user UAI for a user:\nncn-m001-pit# cray uas create Setting a class to default gives the administrator fine grained control over the behavior of end-user UAIs that are created by authorized users in legacy mode (see Legacy Mode User-Driven UAI Management).\nThe remaining fields are as follows:\n The namespace field specifies the Kubernetes namespace in which this UAI will run. It has the default setting of user here. The opt_ports field is an empty list of TCP port numbers that will be opened on the external IP address of the UAI when it runs. This controls whether services other than SSH can be run and reached publicly on the UAI. The priority_class_name \u0026quot;uai_priority\u0026quot; is the default Kubernetes priority class of UAIs. If it were a different class, it would affect both Kubernetes default resource limit / request assignments and Kubernetes scheduling priority for the UAI. The public_ip field is a flag that indicates whether the UAI should be given an external IP address LoadBalancer service so that clients outside the Kubernetes cluster can reach it, or only be given a Kubernetes Cluster-IP address. For the most part, this controls whether the UAI is reachable by SSH from external clients, but it also controls whether the ports in opt_ports are reachable as well. The resource_config field is not set, but could be set to a resource specification to override namespace defaults on Kubernetes resource requests / limits. The uai_compute_network flag indicates whether this UAI uses the macvlan mechanism to gain access to the Shasta compute node network. This needs to be true to support workload management. The uai_creation_class field is used by broker UAIs to tell the broker what kind of UAI to create when automatically generating a UAI.  After all these individual items, we see the UAI Image to be used to create UAIs of this class:\n \u0026quot;uai_image\u0026quot;: { \u0026quot;default\u0026quot;: true, \u0026quot;image_id\u0026quot;: \u0026quot;ff86596e-9699-46e8-9d49-9cb20203df8c\u0026quot;, \u0026quot;imagename\u0026quot;: \u0026quot;registry.local/cray/cray-uai-sles15sp1:latest\u0026quot; }, Finally, there is a list of volumes that will show up in UAIs created using this class:\n \u0026quot;volume_mounts\u0026quot;: [ { \u0026quot;mount_path\u0026quot;: \u0026quot;/etc/localtime\u0026quot;, \u0026quot;volume_description\u0026quot;: { \u0026quot;host_path\u0026quot;: { \u0026quot;path\u0026quot;: \u0026quot;/etc/localtime\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;FileOrCreate\u0026quot; } }, \u0026quot;volume_id\u0026quot;: \u0026quot;55a02475-5770-4a77-b621-f92c5082475c\u0026quot;, \u0026quot;volumename\u0026quot;: \u0026quot;timezone\u0026quot; }, { \u0026quot;mount_path\u0026quot;: \u0026quot;/lus\u0026quot;, \u0026quot;volume_description\u0026quot;: { \u0026quot;host_path\u0026quot;: { \u0026quot;path\u0026quot;: \u0026quot;/lus\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;DirectoryOrCreate\u0026quot; } }, \u0026quot;volume_id\u0026quot;: \u0026quot;9fff2d24-77d9-467f-869a-235ddcd37ad7\u0026quot;, \u0026quot;volumename\u0026quot;: \u0026quot;lustre\u0026quot; }, { \u0026quot;mount_path\u0026quot;: \u0026quot;/root/slurm_config/munge\u0026quot;, \u0026quot;volume_description\u0026quot;: { \u0026quot;secret\u0026quot;: { \u0026quot;secret_name\u0026quot;: \u0026quot;munge-secret\u0026quot; } }, \u0026quot;volume_id\u0026quot;: \u0026quot;7aeaf158-ad8d-4f0d-bae6-47f8fffbd1ad\u0026quot;, \u0026quot;volumename\u0026quot;: \u0026quot;munge-key\u0026quot; }, { \u0026quot;mount_path\u0026quot;: \u0026quot;/etc/slurm\u0026quot;, \u0026quot;volume_description\u0026quot;: { \u0026quot;config_map\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;slurm-map\u0026quot; } }, \u0026quot;volume_id\u0026quot;: \u0026quot;ea97325c-2b1d-418a-b3b5-3f6488f4a9e2\u0026quot;, \u0026quot;volumename\u0026quot;: \u0026quot;slurm-config\u0026quot; } The timezone is taken from the host node by importing /etc/localtime to the UAI. Access is given to the Lustre file system mounted on the host node as /lus and mounting that within the UAI at the same path.Then, two pieces of Slurm configuration, the munge key and the slurm configuration file, are taken from Kubernetes and mounted as files at /root/slurm_config/munge and /etc/slurm respectively.\nSee UAI Classes and Elements of a UAI for more details on the output.\n"
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/delete_a_uai_using_an_administrative_command/",
	"title": "Delete A UAI Using An Administrative Command",
	"tags": [],
	"description": "",
	"content": "Delete a UAI Using an Administrative Command Manually delete one or more UAIs.\nPrerequisites Install and initialize the cray administrative CLI.\nProcedure   Delete one or more UAIs using a command of the following form:\nncn-m001-pit# cray uas admin uais delete OPTIONS OPTIONS is one or more of the following:\n --owner USERNAME: delete all UAIs owned by the named user --class-id CLASS_ID: delete all UAIs of the specified UAI class --uai-list LIST_OF_UAI_NAMES: delete all the listed UAIs  The following example deletes two UAIs by name:\nncn-m001-pit# cray uas admin uais delete --uai-list \\ \u0026#39;uai-vers-715fa89d,uai-ctuser-0aed4970\u0026#39; results = [ \u0026#34;Successfully deleted uai-vers-715fa89d\u0026#34;, \u0026#34;Successfully deleted uai-ctuser-0aed4970\u0026#34;,]   "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/delete_a_volume_configuration/",
	"title": "Delete A Volume Configuration",
	"tags": [],
	"description": "",
	"content": "Delete a Volume Configuration Delete an existing volume configuration. This procedure does not delete the underlying object referred to by the UAS volume configuration.\nPrerequisites  Install and initialize the cray administrative CLI. Obtain the volume_id of the UAS volume to delete. Perform List Volumes Registered in UAS if necessary.  Procedure   Delete the target volume configuration.\nTo delete a UAS Volume, use a command of the following form:\nncn-m001-pit# cray uas admin config volumes delete \u0026lt;volume-id\u0026gt; For example:\nncn-m001-pit# cray uas admin config volumes delete a0066f48-9867-4155-9268-d001a4430f5c   If wanted, perform List Volumes Registered in UAS to confirm that the UAS volume has been deleted.\n"
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/elements_of_a_uai/",
	"title": "Elements Of A UAI",
	"tags": [],
	"description": "",
	"content": "Elements of a UAI All UAIs can have the following attributes associated with them:\n A required container image An optional set of volumes An optional resource specification An optional collection of other configuration items  This topic explains each of these attributes.\nUAI container image The container image for a UAI (UAI image) defines and provides the basic environment available to the user. This environment includes, among other things:\n The operating system (including version) Preinstalled packages  A site can customize UAI images and add those images to UAS, allowing them to be used for UAI creation. Any number of UAI images can be configured in UAS, though only one will be used by any given UAI.\nUAS provides two UAI images by default. These images enable HPE Cray EX administrators to set up UAIs and run many common tasks. The first image is a standard end-user UAI image that has the software necessary to support a basic Linux login experience. This image also comes with the Slurm and PBS Professional workload management client software installed. These clients allow users to take advantage of one or both of these workload managers if they are installed on the host system. The second image is a broker UAI image. Broker UAIs are a special type of UAIs used in the \u0026ldquo;broker based\u0026rdquo; operation model. Broker UAIs present a single SSH endpoint that every user logs into. The broker UAI then locates or creates a suitable end-user UAI and redirects the SSH session to that end-user UAI.\nUAI Volumes The volumes defined for a UAI provide external access to data provided by the host node. Anything that can be defined as a volume in a Kubernetes pod specification can be configured in UAS as a volume and used within a UAI. Examples include:\n Kubernetes ConfigMaps and Secrets External file systems used for persistent storage or external data access Host node files and directories  When UAIs are created they mount a list of volumes inside their containers to give them access to various data provided either by Kubernetes resources or through Kubernetes by the host node where the UAI runs. Which volumes are in that list depends on how the UAI is created:\n UAIs not created using a UAI class mount all volumes configured in UAS. UAIs created using a class only mount the volumes listed in the class and configured in UAS.  The following are some example use cases for UAI volumes:\n Connecting UAIs to configuration files like /etc/localtime maintained by the host node. Connect end-user UAIs to Slurm or PBS Professional Workload Manager configuration shared through Kubernetes. Connecting end-user UAIs to Programming Environment libraries and tools hosted on the UAI host nodes. Connecting end-user UAIs to Lustre or other external storage for user data. Connecting broker UAIs to a directory service or SSH configuration to authenticate and redirect user sessions.  Every UAS volume includes the following values in its registration information:\n mount_path: Specifies where in the UAI the volume will be mounted. volume_description: A dictionary with one entry, whose key identifies the kind of Kubernetes volume is described (for example, host_path, configmap, secret). The value is another dictionary containing the Kubernetes volume description itself. volumename: A required string chosen by the creator of the volume. This may describe or name the volume. It is used inside the UAI pod specification to identify the volume that is mounted in a given location in a container. A volumename is unique within any given UAI, but not necessarily within UAS. These are useful when searching for a volume if they are unique across the UAS configuration. volume_id: Used to identify the UAS volume when examining, updating, or deleting a volume and when linking a volume to a UAI class. A volume_id is unique within UAS.  Refer to https://kubernetes.io/docs/concepts/storage/volumes for more information about Kubernetes volumes.\nResource Specifications A resource request tells Kubernetes the minimum amount of memory and CPU to give to each UAI. A resource limit sets the maximum amount that Kubernetes can give to any UAI. Kubernetes uses resource limits and requests to manage the system resources available to pods. Because UAIs run as pods under Kubernetes, UAS takes advantage of Kubernetes to manage the system resources available to UAIs. In UAS, resource specifications contain that configuration. A UAI that is assigned a resource specification will use that instead of the default resource limits or requests on the Kubernetes namespace containing the UAI. This way, resource specifications can be used to fine-tune resources assigned to UAIs.\nUAI resource specifications have three configurable parameters:\n A limit which is a JSON string describing a Kubernetes resource limit. A request which is a JSON string describing a Kubernetes resource request. An optional comment which is a free-form string containing any information an administrator might find useful about the resource specification.  Resource specifications also contain a resource-id that is used for examining, updating, or deleting the resource specification as well as linking the resource specification into a UAI class.\nResource specifications configured in UAS contain resource requests, limits, or both, that can be associated with a UAI. Any resource request or limit that can be set up on a Kubernetes pod can be set up as a resource specification under UAS.\nOther Configuration Items There are also smaller configuration items that control things such as:\n Whether the UAI can talk to compute nodes over the high-speed network (needed for workload management). Whether the UAI presents a public facing or private facing IP address for SSH. Kubernetes scheduling priority.  A UAI class template is required to configure such items.\nUAI Configuration and UAI Classes The container image for a UAI (UAI image), defines the basic environment including the flavor of operating system, the installed packages, and so forth available to the user. UAI images can be customized by a site and added to the UAS configuration to be used in UAI creation. Any number of UAI images can be configured in the UAS, though only one will be used by any given UAI. The UAS comes with some pre-defined UAI images that make it possible to set up UAIs and run many common tasks without further customization. Refer to Customize End-User UAI Images.\nThe volumes defined for a UAI provide for external access to data provided by the host system. Examples of this range from Kubernetes \u0026ldquo;configmaps\u0026rdquo; and \u0026ldquo;secrets\u0026rdquo; to external file systems used for persistent storage or external data access. Anything that can be defined as a volume in a Kubernetes pod specification can be configured in UAS as a volume and used within a UAI.\nResource requests and limits tell Kubernetes how much memory and CPU a given UAI wants all the time (request) and how much memory and CPU a UAI can ever be given (limit). Resource specifications configured into UAS contain resource requests and / or limits that can be associated with a UAI. Any resource request or limit that can be set up on a Kubernetes pod can be set up as a resource specification under UAS.\nThe smaller configuration items control things like whether the UAI can talk to compute nodes over the high-speed network (needed for workload management), whether the UAI presents a public facing or private facing IP address for SSH, Kubernetes scheduling priority and others.\nAll of the above can be customized on a given set of UAIs by defining a UAI class. UAI classes are templates used to create UAIs, and provide access to fine grained configuration and selection of image, volumes and resource specification. While an end-user UAI can be created by simply specifying its UAI image and the user\u0026rsquo;s public key, to make more precisely constructed UAIs a UAI class must be used.\n"
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/end_user_uais/",
	"title": "End-user UAIs",
	"tags": [],
	"description": "",
	"content": "End-User UAIs UAIs used for interactive logins are called end-user UAIs. End-user UAIs can be seen as lightweight User Access Nodes (UANs), but there are important differences between UAIs and UANs. First, end-user UAIs are not dedicated hardware like UANs. They are implemented as containers orchestrated by Kubernetes, which makes them subject to Kubernetes scheduling and resource management rules. One key element of Kubernetes orchestration is impermanence. While end-user UAIs are often long running, Kubernetes can reschedule or recreate them as needed to meet resource and node availability constraints. UAIs can also be removed administratively. When either of these things happen, a new UAI may be created, but that new UAI reverts to its initial state, discarding any internal changes that might have been made in its previous incarnation. An administratively removed end-user UAI may or may not ever be re-created, and an end-user UAI that is preempted because of resource pressure may become unavailable for an extended time until the pressure is relieved.\nThe impermanence of end-user UAIs makes them suitable for tasks that are immediate and interactive over relatively short time frames, such as building and testing software or launching workloads. It makes them unsuitable for unattended activities like executing cron jobs or monitoring progress of a job in a logged-in shell unless those activities are built into the UAI image itself (more on custom UAI images later). These kinds of activities are more suited to UANs, which are more permanent and, unless they are re-installed, retain modified state through reboots and so forth.\nAnother way end-user UAIs differ from UANs is that any given end-user UAI is restricted to serving a single user. This protects users from interfering with each other within UAIs and means that any user who wants to use a UAI has to arrange for the UAI to be created and assigned. Once a user has an end-user UAI assigned, the user may initiate any number of SSH sessions to that UAI, but no other user will be recognized by the UAI when attempting to connect.\n"
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/delete_a_uai/",
	"title": "Delete A UAI",
	"tags": [],
	"description": "",
	"content": "Delete a UAI The cray uas command allows users to manage UAIs. This procedure deletes one of the user\u0026rsquo;s UAIs. To delete all UAIs on the system, see List and Delete All UAIs for more information.\nPrerequisites A UAI is up and running.\nLimitations Currently, the user must SSH to the system as root.\nProcedure   Log in to an NCN as root.\n  List existing UAIs.\nncn-w001# cray uas list username = \u0026#34;user\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; uai_connect_string = \u0026#34;ssh user@203.0.113.0 -i ~/.ssh/id_rsa\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uas-sles15sp1-slurm:latest\u0026#34; uai_age = \u0026#34;0m\u0026#34; uai_name = \u0026#34;uai-user-be3a6770\u0026#34; username = \u0026#34;user\u0026#34; uai_host = \u0026#34;ncn-s001\u0026#34; uai_status = \u0026#34;Running: Ready\u0026#34; uai_connect_string = \u0026#34;ssh user@203.0.113.0 -i ~/.ssh/id_rsa\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uas-sles15sp1-slurm:latest\u0026#34; uai_age = \u0026#34;11m\u0026#34; uai_name = \u0026#34;uai-user-f488eef6\u0026#34;   Delete a UAI.\nTo delete one or more UAIs, use a command of the following form:\ncray uas admin uais delete [options] Where options may be any of the following:\n --uai-list '\u0026lt;list-of-uai-names\u0026gt;' - Delete all the listed UAIs --owner \u0026lt;owner-name\u0026gt; - Delete all UAIs owned by the named owner --class-id \u0026lt;uai-class-id\u0026gt; - Delete all UAIs of the specified UAI class  For example:\nncn-w001# cray uas delete -–uai-list UAI_NAME results = [ \u0026#34;Successfully deleted uai-user-be3a6770\u0026#34;,]   When a UAI is deleted, WLM jobs are not cancelled or cleaned up.\n"
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/delete_a_uai_class/",
	"title": "Delete A UAI Class",
	"tags": [],
	"description": "",
	"content": "Delete a UAI Class Delete a UAI class. After deletion, the class will no longer be available for new UAIs.\nPrerequisites  Install and initialize the cray administrative CLI. Obtain the ID of the UAI class that will be deleted.  Procedure Delete a UAI by using a command of the following form:\ncray uas admin config classes delete UAI_CLASS_ID UAI_CLASS_ID is the UAS ID of the UAI class.\n  Delete a UAI class.\nncn-m001-pit# cray uas admin config classes delete bb28a35a-6cbc-4c30-84b0-6050314af76b   "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/delete_a_uai_image_registration/",
	"title": "Delete A UAI Image Registration",
	"tags": [],
	"description": "",
	"content": "Delete a UAI Image Registration Unregister a UAI image from UAS.\nPrerequisites Verify that the UAI image to be deleted is registered with UAS. See Retrieve UAI Image Registration Information for instructions.\nProcedure Deleting a UAI image from UAS effectively unregisters the UAI image from UAS. This procedure does delete the actual UAI image artifact.\n  Delete a UAS image registration by using a command of the following form:\nncn-m001-pit# cray uas admin config images delete IMAGE_ID Replace IMAGE_ID with image ID of the UAI image to unregister from UAS.\nFor example:\nncn-m001-pit# cray uas admin config images delete 8fdf5d4a-c190-24c1-2b96-74ab98c7ec07   "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/delete_a_uai_resource_specification/",
	"title": "Delete A UAI Resource Specification",
	"tags": [],
	"description": "",
	"content": "Delete a UAI Resource Specification Delete a specific UAI resource specification using the resource_id of that specification. Once deleted, UAIs will no longer be able to use that specification.\nPrerequisites Install and initialize the cray administrative CLI.\nPrerequisites To delete a particular resource specification, use a command of the following form:\nncn-m001-pit# cray uas admin config resources delete RESOURCE_ID   Remove a UAI resource specification from UAS.\nncn-m001-pit# cray uas admin config resources delete 7c78f5cf-ccf3-4d69-ae0b-a75648e5cddb   "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/create_and_use_default_uais_in_legacy_mode/",
	"title": "Create And Use Default UAIs In Legacy Mode",
	"tags": [],
	"description": "",
	"content": "Create and Use Default UAIs in Legacy Mode Create a UAI using the default UAI image or the default UAI class in legacy mode.\nProcedure   Create a UAI with a command of the following form:\nuser\u0026gt; cray uas create --public-key '\u0026lt;path\u0026gt;' \u0026lt;path\u0026gt; is the path to a file containing an SSH public-key matched to the SSH private key belonging to the user.\n  Watch the UAI and see when it is ready for logins.\nuser\u0026gt; cray uas list   Log into the UAI using the ssh command.\n  Delete the UAI when finished working with it.\nuser\u0026gt; cray uas delete --uai-list '\u0026lt;uai-list\u0026gt;'   Example UAI Lifecycle In the following example, the user logs into the CLI using cray auth login with a user name and password matching that user\u0026rsquo;s credentials in Keycloak.\nvers\u0026gt; cray auth login Username: vers Password: Success! vers\u0026gt; cray uas list results = [] From there the user creates a UAI. The UAI starts out in a Pending or Waiting state as Kubernetes constructs its pod and starts its container running.\nvers\u0026gt; cray uas create --publickey ~/.ssh/id_rsa.pub uai_age = \u0026quot;0m\u0026quot; uai_connect_string = \u0026quot;ssh vers@10.103.13.157\u0026quot; uai_host = \u0026quot;ncn-w001\u0026quot; uai_img = \u0026quot;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026quot; uai_ip = \u0026quot;10.103.13.157\u0026quot; uai_msg = \u0026quot;ContainerCreating\u0026quot; uai_name = \u0026quot;uai-vers-8ee103bf\u0026quot; uai_status = \u0026quot;Waiting\u0026quot; username = \u0026quot;vers\u0026quot; [uai_portmap] vers\u0026gt; cray uas list [[results]] uai_age = \u0026quot;0m\u0026quot; uai_connect_string = \u0026quot;ssh vers@10.103.13.157\u0026quot; uai_host = \u0026quot;ncn-w001\u0026quot; uai_img = \u0026quot;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026quot; uai_ip = \u0026quot;10.103.13.157\u0026quot; uai_msg = \u0026quot; uai_name = \u0026quot;uai-vers-8ee103bf\u0026quot; uai_status = \u0026quot;Running: Ready\u0026quot; username = \u0026quot;vers\u0026quot; Using cray uas list, the user watches the UAI until it reaches a Running: Ready state. The UAI is now ready to accept SSH logins from the user, and the user then logs into the UAI to run a simple Slurm job, and logs out.\nvers\u0026gt; ssh vers@10.103.13.157 The authenticity of host '10.103.13.157 (10.103.13.157)' can't be established. ECDSA key fingerprint is SHA256:XQukF3V1q0Hh/aTiFmijhLMcaOzwAL+HjbM66YR4mAg. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added '10.103.13.157' (ECDSA) to the list of known hosts. vers@uai-vers-8ee103bf-95b5d774-88ssd:/tmp\u0026gt; sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST workq* up infinite 4 comp nid[000001-000004] vers@uai-vers-8ee103bf-95b5d774-88ssd\u0026gt; srun -n 3 -N 3 hostname nid000001 nid000002 nid000003 vers@uai-vers-8ee103bf-95b5d774-88ssd\u0026gt; exit logout Connection to 10.103.13.157 closed. Now finished with the UAI, the user deletes it with cray uas delete. If the user has more than one UAI to delete, the argument to the --uai-list option can be a comma separated list of UAI names.\nvers\u0026gt; cray uas delete --uai-list uai-vers-8ee103bf results = [ \u0026quot;Successfully deleted uai-vers-8ee103bf\u0026quot;,] "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/customize_end-user_uai_images/",
	"title": "Customize End-user UAI Images",
	"tags": [],
	"description": "",
	"content": "Customize End-User UAI Images The provided end-user UAI image is a basic UAI image that includes an up-to-date version of the Sles Linux Distribution and client support for both the Slurm and PBS Professional workload managers. It provides an entrypoint to using UAIs and doing workload management from UAIs. This UAI image is not suitable for use with the Cray PE because it cannot be assured of being up-to-date with what is running on Shasta compute nodes at a given site. To support building software to be run in compute nodes, it is necessary to create a custom end-user UAI image and use that.\nA custom end-user UAI image can be any container image set up with the end-user UAI entrypoint script. For this case, it will be a UAI image built from the squashfs image used on compute nodes on the host system. This section describes how to create this kind of custom end-user UAI image.\nPrerequisites  This procedure requires administrator privileges. All steps in this procedure must be run from a true NCN (master or worker node), not from the LiveCD node. In particular, pushing the final image to registry.local will fail with an error reporting a bad x509 certificate if it is attempted on the LiveCD node.  Procedure   Build a custom end-user UAI image.\nThe following steps are used to build a custom End-User UAI image called registry.local/cray/cray-uai-compute:latest. Alter this name as needed by changing the following in the procedure to use a different name:\nncn-w001# UAI_IMAGE_NAME=registry.local/cray/cray-uai-compute:latest   Query BOS for a sessiontemplate ID.\nIdentify the Sessiontemplate name to use. A full list may be found with the following command:\nncn-w001# cray bos sessiontemplate list --format yaml - boot_sets: compute: boot_ordinal: 2 etag: d54782b3853a2d8713a597d80286b93e kernel_parameters: ip=dhcp quiet spire_join_token=${SPIRE_JOIN_TOKEN} network: nmn node_roles_groups: - Compute path: s3://boot-images/0c0d4081-2e8b-433f-b6f7-e1ef0b907be3/manifest.json rootfs_provider: cpss3 rootfs_provider_passthrough: dvs:api-gw-service-nmn.local:300:nmn0 type: s3 cfs: configuration: wlm-config-0.1.0 enable_cfs: true name: wlm-sessiontemplate-0.1.0 Alternatively, collect the sessiontemplate name used during the Cray Operating System (COS) install. Refer to the \u0026ldquo;Boot COS\u0026rdquo; procedure in the COS product stream documentation. Near the end of that procedure, the step to create a BOS session to boot the compute nodes should contain the name.\nncn-w001# SESSION_NAME=wlm-sessiontemplate-0.1.0   Download a compute node SquashFS.\nUse the Sessiontemplate name to download a compute node squashfs from a BOS sessiontemplate name:\nncn-w001# SESSION_ID=$(cray bos sessiontemplate describe $SESSION_NAME --format json | jq -r '.boot_sets.compute.path' | awk -F/ '{print $4}') ncn-w001# cray artifacts get boot-images $SESSION_ID/rootfs rootfs.squashfs   Mount the SquashFS and create a tarball.\n  Create a directory to mount the SquashFS:\nncn-w001# mkdir -v mount ncn-w001# mount -v -o loop,ro rootfs.squashfs `pwd`/mount   Create the tarball.\nIMPORTANT: 99-slingshot-network.conf is omitted from the tarball as that prevents the UAI from running sshd as the UAI user with the su command:\nncn-w001# (cd `pwd`/mount; tar --xattrs --xattrs-include='*' --exclude=\u0026quot;99-slingshot-network.conf\u0026quot; -cf \u0026quot;../$SESSION_ID.tar\u0026quot; .) \u0026gt; /dev/null This may take several minutes. Notice that this does not create a compressed tarball. Using an uncompressed format makes it possible to add files if needed once the tarball is made. It also makes the procedure run just a bit more quickly. If warnings related to xattr are displayed, continue with the procedure as the resulting tarball should still result in a functioning UAI container image.\n  Check that the tarball contains \u0026lsquo;./usr/bin/uai-ssh.sh\u0026rsquo;.\nncn-w001# tar tf $SESSION_ID.tar | grep '[.]/usr/bin/uai-ssh[.]sh' ./usr/bin/uai-ssh.sh If the script is not present, the easiest place to get a copy of the script is from a UAI built from the end-user UAI image provided with UAS, and it can be appended to the tarball:\nncn-w001# mkdir -pv ./usr/bin ncn-w001# cray uas create --publickey ~/.ssh/id_rsa.pub uai_connect_string = \u0026quot;ssh vers@10.26.23.123\u0026quot; uai_host = \u0026quot;ncn-w001\u0026quot; uai_img = \u0026quot;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026quot; uai_ip = \u0026quot;10.26.23.123\u0026quot; uai_msg = \u0026quot; uai_name = \u0026quot;uai-vers-32079250\u0026quot; uai_status = \u0026quot;Pending\u0026quot; username = \u0026quot;vers\u0026quot; [uai_portmap] ncn-w001# scp vers@10.26.23.123:/usr/bin/uai-ssh.sh ./usr/bin/uai-ssh.sh The authenticity of host '10.26.23.123 (10.26.23.123)' can't be established. ECDSA key fingerprint is SHA256:voQUCKDG4C9FGkmUcHZVrYJBXVKVYqcJ4kmTpe4tvOA. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added '10.26.23.123' (ECDSA) to the list of known hosts. uai-ssh.sh 100% 5035 3.0MB/s 00:00 ncn-w001# cray uas delete --uai-list uai-vers-32079250 results = [ \u0026quot;Successfully deleted uai-vers-32079250\u0026quot;,] ncn-w001# tar rvf 0c0d4081-2e8b-433f-b6f7-e1ef0b907be3.tar ./usr/bin/uai-ssh.sh     Create and push the container image.\nCreate a container image using podman or docker and push it to the site container registry. Any container-specific modifications may also be done here with a Dockerfile. The ENTRYPOINT layer must be /usr/bin/uai-ssh.sh as that starts SSHD for the user in the UAI container started by UAS.\nncn-w001# UAI_IMAGE_NAME=registry.local/cray/cray-uai-compute:latest ncn-w001# podman import --change \u0026quot;ENTRYPOINT /usr/bin/uai-ssh.sh\u0026quot; $SESSION_ID.tar $UAI_IMAGE_NAME ncn-w001# podman push $UAI_IMAGE_NAME   Register the new container image with UAS.\nncn-w001# cray uas admin config images create --imagename $UAI_IMAGE_NAME   Cleanup the mount directory and tarball.\nncn-w001# umount -v mount; rmdir -v mount ncn-w001# rm $SESSION_ID.tar rootfs.squashfs # NOTE: the next step could be done as an `rm -rf` but, because the user # is `root` and the path is very similar to an important system # path a more cautious approach is taken. ncn-w001# rm -fv ./usr/bin/uai-ssh.sh \u0026amp;\u0026amp; rmdir ./usr/bin ./usr   "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/customize_the_broker_uai_image/",
	"title": "Customize The Broker UAI Image",
	"tags": [],
	"description": "",
	"content": "Customize the Broker UAI Image The broker UAI image that comes with UAS is the image used to construct broker UAIs.\nThe key pieces of the broker UAI image are:\n An entrypoint shell script that initializes the container and starts the SSH daemon running. An SSH configuration that forces logged in users into the switchboard command which creates / selects end-user UAIs and redirects connections.  The primary way to customize the broker UAI image is by defining volumes and connecting them to the broker UAI class for a given broker. An example of this is configuring the broker for LDAP is shown in Configure a Broker UAI Class. Some customizations may require action that cannot be covered simply by using a volume. Those cases can be covered either by volume mounting a customized entrypoint script, or volume mounting a customized SSH configuration. Both of these cases are shown in the following examples.\nCustomize the Broker UAI Entrypoint Script The broker UAI entrypoint script runs once every time the broker UAI starts. It resides at /app/broker/entrypoint.sh in the broker UAI image. The entrypoint script is the only file in that directory, so it can be overridden by creating a Kubernetes ConfigMap in the uas namespace containing the modified script and creating a volume using that ConfigMap with a mount point of /app/broker. There is critical content in the entrypoint script that should not be modified.\nThe following shows the contents of an unmodified script:\n#!/bin/bash # Copyright 2020 Hewlett Packard Enterprise Development LP echo \u0026quot;Configure PAM to use sssd...\u0026quot; pam-config -a --sss --mkhomedir echo \u0026quot;Generating broker host keys...\u0026quot; ssh-keygen -A echo \u0026quot;Checking for UAI_CREATION_CLASS...\u0026quot; if ! [ -z $UAI_CREATION_CLASS ]; then echo UAI_CREATION_CLASS=$UAI_CREATION_CLASS \u0026gt;\u0026gt; /etc/environment fi echo \u0026quot;Starting sshd...\u0026quot; /usr/sbin/sshd -f /etc/switchboard/sshd_config echo \u0026quot;Starting sssd...\u0026quot; sssd sleep infinity Starting at the top, pam_config ... can be customized to set up PAM as needed. The configuration here assumes the broker is using SSSD to reach a directory server for authentication and that, if a home directory is not present for a user at login, one should be made on the broker. The ssh-keygen... part is needed to set up the SSH host key for the broker and should be left alone. The UAI_CREATION_CLASS code should be left alone, as it sets up information used by switchboard to create end-user UAIs. The /usr/sbin/sshd... part starts the SSH server on the broker and should be left alone. Configuration of SSH is covered in the next section and is done by replacing /etc/switchboard/sshd_config not by modifying this line. The sssd part assumes the broker is using SSSD to reach a directory server, it can be changed as needed. The sleep infinity prevents the script from exiting which keeps the broker UAI running. It should not be removed or altered. As long as the basic flow and contents described here are honored, other changes to this script should work without compromising the broker UAI\u0026rsquo;s function.\nThe following is an example of replacing the entrypoint script with a new entrypoint script that changes the SSSD invocation to explicitly specify the sssd.conf file path (the standard path is used here, but a different path might make customizing SSSD for a given site simpler under some set of circumstances):\n# Notice special here document form to prevent variable substitution in the file ncn-m001-pit# cat \u0026lt;\u0026lt;-\u0026quot;EOF\u0026quot; \u0026gt; entrypoint.sh #!/bin/bash # Copyright 2020 Hewlett Packard Enterprise Development LP echo \u0026quot;Configure PAM to use sssd...\u0026quot; pam-config -a --sss --mkhomedir echo \u0026quot;Generating broker host keys...\u0026quot; ssh-keygen -A echo \u0026quot;Checking for UAI_CREATION_CLASS...\u0026quot; if ! [ -z $UAI_CREATION_CLASS ]; then echo UAI_CREATION_CLASS=$UAI_CREATION_CLASS \u0026gt;\u0026gt; /etc/environment fi echo \u0026quot;Starting sshd...\u0026quot; /usr/sbin/sshd -f /etc/switchboard/sshd_config echo \u0026quot;Starting sssd...\u0026quot; # LOCAL MODIFICATION # change the normal SSSD invocation # sssd # to specify the config file path sssd --config /etc/sssd/sssd.conf # END OF LOCAL MODIFICATION sleep infinity EOF ncn-m001-pit# kubectl create configmap -n uas broker-entrypoint --from-file=entrypoint.sh # Notice that the `default_mode` setting, which will set the mode on the file # /app/broker/entrypoint.sh is decimal 493 here instead of octal 0755. # The octal notation is not permitted in a JSON specification. Decimal # numbers have to be used. ncn-m001-pit# cray uas admin config volumes create --mount-path /app/broker --volume-description '{\u0026quot;config_map\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;broker-entrypoint\u0026quot;, \u0026quot;default_mode\u0026quot;: 493}}' --volumename broker-entrypoint mount_path = \u0026quot;/app/broker\u0026quot; volume_id = \u0026quot;1f3bde56-b2e7-4596-ab3a-6aa4327d29c7\u0026quot; volumename = \u0026quot;broker-entrypoint\u0026quot; [volume_description.config_map] default_mode = 493 name = \u0026quot;broker-entrypoint\u0026quot; ncn-m001-pit# cray uas admin config classes list | grep -e class_id -e comment class_id = \u0026quot;74970cdc-9f94-4d51-8f20-96326212b468\u0026quot; comment = \u0026quot;UAI broker class\u0026quot; class_id = \u0026quot;a623a04a-8ff0-425e-94cc-4409bdd49d9c\u0026quot; comment = \u0026quot;UAI User Class\u0026quot; class_id = \u0026quot;bb28a35a-6cbc-4c30-84b0-6050314af76b\u0026quot; comment = \u0026quot;Non-Brokered UAI User Class\u0026quot; ncn-m001-pit# cray uas admin config classes describe 74970cdc-9f94-4d51-8f20-96326212b468 --format yaml class_id: 74970cdc-9f94-4d51-8f20-96326212b468 comment: UAI broker class default: false namespace: uas opt_ports: [] priority_class_name: uai-priority public_ip: true resource_config: uai_compute_network: false uai_creation_class: a623a04a-8ff0-425e-94cc-4409bdd49d9c uai_image: default: false image_id: c5dcb261-5271-49b3-9347-afe7f3e31941 imagename: dtr.dev.cray.com/cray/cray-uai-broker:latest volume_mounts: - mount_path: /etc/sssd volume_description: secret: default_mode: 384 secret_name: broker-sssd-conf volume_id: 4dc6691e-e7d9-4af3-acde-fc6d308dd7b4 volumename: broker-sssd-config - mount_path: /etc/localtime volume_description: host_path: path: /etc/localtime type: FileOrCreate volume_id: 55a02475-5770-4a77-b621-f92c5082475c volumename: timezone - mount_path: /lus volume_description: host_path: path: /lus type: DirectoryOrCreate volume_id: 9fff2d24-77d9-467f-869a-235ddcd37ad7 volumename: lustre ncn-m001-pit# cray uas admin config classes update --volume-list '4dc6691e-e7d9-4af3-acde-fc6d308dd7b4,55a02475-5770-4a77-b621-f92c5082475c,9fff2d24-77d9-467f-869a-235ddcd37ad7,1f3bde56-b2e7-4596-ab3a-6aa4327d29c7' --format yaml 74970cdc-9f94-4d51-8f20-96326212b468 class_id: 74970cdc-9f94-4d51-8f20-96326212b468 comment: UAI broker class default: false namespace: uas opt_ports: [] priority_class_name: uai-priority public_ip: true resource_config: uai_compute_network: false uai_creation_class: a623a04a-8ff0-425e-94cc-4409bdd49d9c uai_image: default: false image_id: c5dcb261-5271-49b3-9347-afe7f3e31941 imagename: dtr.dev.cray.com/cray/cray-uai-broker:latest volume_mounts: - mount_path: /etc/sssd volume_description: secret: default_mode: 384 secret_name: broker-sssd-conf volume_id: 4dc6691e-e7d9-4af3-acde-fc6d308dd7b4 volumename: broker-sssd-config - mount_path: /etc/localtime volume_description: host_path: path: /etc/localtime type: FileOrCreate volume_id: 55a02475-5770-4a77-b621-f92c5082475c volumename: timezone - mount_path: /lus volume_description: host_path: path: /lus type: DirectoryOrCreate volume_id: 9fff2d24-77d9-467f-869a-235ddcd37ad7 volumename: lustre - mount_path: /app/broker volume_description: config_map: default_mode: 493 name: broker-entrypoint volume_id: 1f3bde56-b2e7-4596-ab3a-6aa4327d29c7 volumename: broker-entrypoint With the broker UAI class updated, all that remains is to clear out any existing end-user UAIs (existing UAIs will not work with the new broker because the new broker will have a new key-pair shared with its UAIs) and the existing broker UAI (if any) and create a new broker UAI.\nNOTE: Clearing out existing UAIs will terminate any user activity on those UAIs, make sure that users are warned of the disruption.\nncn-m001-pit# cray uas admin uais delete --class-id a623a04a-8ff0-425e-94cc-4409bdd49d9c results = [ \u0026quot;Successfully deleted uai-vers-ee6f427e\u0026quot;,] ncn-m001-pit# cray uas admin uais delete --class-id 74970cdc-9f94-4d51-8f20-96326212b468 results = [ \u0026quot;Successfully deleted uai-broker-11f36815\u0026quot;,] ncn-m001-pit# cray uas admin uais create --class-id 74970cdc-9f94-4d51-8f20-96326212b468 --owner broker uai_connect_string = \u0026quot;ssh broker@10.103.13.162\u0026quot; uai_img = \u0026quot;dtr.dev.cray.com/cray/cray-uai-broker:latest\u0026quot; uai_ip = \u0026quot;10.103.13.162\u0026quot; uai_msg = \u0026quot; uai_name = \u0026quot;uai-broker-a50407d5\u0026quot; uai_status = \u0026quot;Pending\u0026quot; username = \u0026quot;broker\u0026quot; [uai_portmap] Customize the Broker UAI SSH Configuration The SSH configuration used on broker UAIs resides in /etc/switchboard/sshd_config and contains the following:\nPort 30123 AuthorizedKeysFile\t.ssh/authorized_keys UsePAM yes X11Forwarding yes Subsystem\tsftp\t/usr/lib/ssh/sftp-server AcceptEnv LANG LC_CTYPE LC_NUMERIC LC_TIME LC_COLLATE LC_MONETARY LC_MESSAGES AcceptEnv LC_PAPER LC_NAME LC_ADDRESS LC_TELEPHONE LC_MEASUREMENT AcceptEnv LC_IDENTIFICATION LC_ALL AcceptEnv UAI_ONE_SHOT UseDNS no Match User !root,* PermitTTY yes ForceCommand /usr/bin/switchboard broker --class-id $UAI_CREATION_CLASS The important content here is as follows:\n Port 30123 tells sshd to listen on a port that can be reached through port forwarding by the publicly visible Kubernetes service. The UseDNS no avoids any DNS issues resulting from the broker UAI running in the Kubernetes network space. The permitTTY yes setting permits interactive UAI logins. The ForceCommand ... statement ensures that users are always sent on to end-user UAIs or drop out of the broker UAI on failure, preventing users from directly accessing the broker UAI. The AcceptEnv UAI_ONE_SHOT setting is not required, but it allows a user to set the UAI_ONE_SHOT variable which instructs the broker to delete any created end-user UAI after the user logs out.  These should be left unchanged. The rest of the configuration can be customized as needed.\nThe following is an example that follows on from the previous section and configures SSH to provide a pre-login banner. Both a new banner file and a new sshd_config are placed in a Kubernetes ConfigMap and mounted over /etc/switchboard:\n# Notice special here document form to prevent variable substitution in the file ncn-m001-pit# cat \u0026lt;\u0026lt;-\u0026quot;EOF\u0026quot; \u0026gt; banner Here is a banner that will be displayed before login on the broker UAI EOF # Notice special here document form to prevent variable substitution in the file ncn-m001-pit# cat \u0026lt;\u0026lt;-\u0026quot;EOF\u0026quot; \u0026gt; sshd_conf Port 30123 AuthorizedKeysFile\t.ssh/authorized_keys UsePAM yes X11Forwarding yes Subsystem\tsftp\t/usr/lib/ssh/sftp-server AcceptEnv LANG LC_CTYPE LC_NUMERIC LC_TIME LC_COLLATE LC_MONETARY LC_MESSAGES AcceptEnv LC_PAPER LC_NAME LC_ADDRESS LC_TELEPHONE LC_MEASUREMENT AcceptEnv LC_IDENTIFICATION LC_ALL AcceptEnv UAI_ONE_SHOT UseDNS no Banner /etc/switchboard/banner Match User !root,* PermitTTY yes ForceCommand /usr/bin/switchboard broker --class-id $UAI_CREATION_CLASS EOF ncn-m001-pit# kubectl create configmap -n uas broker-sshd-conf --from-file sshd_config --from-file banner ncn-m001-pit# cray uas admin config volumes create --mount-path /etc/switchboard --volume-description '{\u0026quot;config_map\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;broker-sshd-conf\u0026quot;, \u0026quot;default_mode\u0026quot;: 384}}' --volumename broker-sshd-config mount_path = \u0026quot;/etc/switchboard\u0026quot; volume_id = \u0026quot;d5058121-c1b6-4360-824d-3c712371f042\u0026quot; volumename = \u0026quot;broker-sshd-config\u0026quot; [volume_description.config_map] default_mode = 384 name = \u0026quot;broker-sshd-conf\u0026quot; ncn-m001-pit# cray uas admin config classes update --volume-list '4dc6691e-e7d9-4af3-acde-fc6d308dd7b4,55a02475-5770-4a77-b621-f92c5082475c,9fff2d24-77d9-467f-869a-235ddcd37ad7,1f3bde56-b2e7-4596-ab3a-6aa4327d29c7,d5058121-c1b6-4360-824d-3c712371f042' --format yaml 74970cdc-9f94-4d51-8f20-96326212b468 class_id: 74970cdc-9f94-4d51-8f20-96326212b468 comment: UAI broker class default: false namespace: uas opt_ports: [] priority_class_name: uai-priority public_ip: true resource_config: uai_compute_network: false uai_creation_class: a623a04a-8ff0-425e-94cc-4409bdd49d9c uai_image: default: false image_id: c5dcb261-5271-49b3-9347-afe7f3e31941 imagename: dtr.dev.cray.com/cray/cray-uai-broker:latest volume_mounts: - mount_path: /etc/sssd volume_description: secret: default_mode: 384 secret_name: broker-sssd-conf volume_id: 4dc6691e-e7d9-4af3-acde-fc6d308dd7b4 volumename: broker-sssd-config - mount_path: /etc/localtime volume_description: host_path: path: /etc/localtime type: FileOrCreate volume_id: 55a02475-5770-4a77-b621-f92c5082475c volumename: timezone - mount_path: /lus volume_description: host_path: path: /lus type: DirectoryOrCreate volume_id: 9fff2d24-77d9-467f-869a-235ddcd37ad7 volumename: lustre - mount_path: /app/broker volume_description: config_map: default_mode: 493 name: broker-entrypoint volume_id: 1f3bde56-b2e7-4596-ab3a-6aa4327d29c7 volumename: broker-entrypoint - mount_path: /etc/switchboard volume_description: config_map: default_mode: 384 name: broker-sshd-conf volume_id: d5058121-c1b6-4360-824d-3c712371f042 volumename: broker-sshd-config With the new configuration installed, clean out the old UAIs and restart the broker:\nNOTE: Clearing out existing UAIs will terminate any user activity on those UAIs, make sure that users are warned of the disruption.\nncn-m001-pit# cray uas admin uais delete --class-id a623a04a-8ff0-425e-94cc-4409bdd49d9c results = [ \u0026quot;Successfully deleted uai-vers-e937b810\u0026quot;,] ncn-m001-pit# cray uas admin uais delete --class-id 74970cdc-9f94-4d51-8f20-96326212b468 results = [ \u0026quot;Successfully deleted uai-broker-a50407d5\u0026quot;,] ncn-m001-pit# cray uas admin uais create --class-id 74970cdc-9f94-4d51-8f20-96326212b468 --owner broker uai_age = \u0026quot;0m\u0026quot; uai_connect_string = \u0026quot;ssh broker@10.103.13.162\u0026quot; uai_host = \u0026quot;ncn-w001\u0026quot; uai_img = \u0026quot;dtr.dev.cray.com/cray/cray-uai-broker:latest\u0026quot; uai_ip = \u0026quot;10.103.13.162\u0026quot; uai_msg = \u0026quot;PodInitializing\u0026quot; uai_name = \u0026quot;uai-broker-df7e6939\u0026quot; uai_status = \u0026quot;Waiting\u0026quot; username = \u0026quot;broker\u0026quot; [uai_portmap] To connect to the broker to log in:\nvers\u0026gt; ssh vers@10.103.13.162 Here is a banner that will be displayed before login to SSH on Broker UAIs Password: "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/create_a_uai_resource_specification/",
	"title": "Create A UAI Resource Specification",
	"tags": [],
	"description": "",
	"content": "Create a UAI Resource Specification Add a resource specification to UAS. Once added, a resource specification can be used to limit UAI resource consumption on host nodes and enable UAIs to access external data.\nPrerequisites Install and initialize the cray administrative CLI.\nProcedure   Add a resource specification.\nUse a command of the following form:\nncn-m001-pit # cray uas admin config resources create [--limit \u0026lt;k8s-resource-limit\u0026gt;] [--request \u0026lt;k8s-resource-request\u0026gt;] [--comment '\u0026lt;string\u0026gt;'] For example:\nncn-m001-pit# cray uas admin config resources create --request '{\u0026quot;cpu\u0026quot;: \u0026quot;300m\u0026quot;, \u0026quot;memory\u0026quot;: \u0026quot;250Mi\u0026quot;}' --limit '{\u0026quot;cpu\u0026quot;: \u0026quot;300m\u0026quot;, \u0026quot;memory\u0026quot;: \u0026quot;250Mi\u0026quot;}' --comment \u0026quot;my first example resource specification\u0026quot; See Elements of a UAI for an explanation of UAI resource specifications.\nThe example above specifies a request / limit pair that requests and is constrained to 300 milli-CPUs (0.3 CPUs) and 250 MiB of memory (250 * 1024 * 1024 bytes) for any UAI created with this limit specification. By keeping the request and the limit the same, this ensures that a host node will not be oversubscribed by UAIs. It is also legitimate to request less than the limit, though that risks over-subscription and is not recommended in most cases. If the request is greater than the limit, UAIs created with the request specification will never be scheduled because they will not be able to provide the requested resources.\nAll of the configurable parts are optional when adding a resource specification. If none are provided, an empty resource specification with only a resource_id will be created.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/create_a_uai_using_a_direct_administrative_command/",
	"title": "Create A UAI Using A Direct Administrative Command",
	"tags": [],
	"description": "",
	"content": "Create a UAI Using a Direct Administrative Command Administrators can use this method to manually create UAIs. This method is intended more for creating broker UAIs than for creating end-user UAIs.\nPrerequisites Install and initialize the cray administrative CLI.\nProcedure This method is intended more for creating broker UAIs than for creating end-user UAIs. Administrators can, however, create end-user UAIs using this method.\n  Create a UAI manually with a command of the form:\nncn-m001-pit# cray uas admin uais create OPTIONS OPTIONS is one or more of the following:\n --owner USERNAME: Create the UAI as owned by the specified user. --class-id CLASS_ID: The class of the UAI to be created. This option must be specified unless a default UAI class exists, in which case, it can be omitted and the default will be used. --passwd str PASSWORD_STRING: The /etc/password format string for the user who owns the UAI. This will be used to set up credentials within the UAI for the owner when the owner logs into the UAI. --publickey-str PUBLIC_SSH_KEY: The SSH public key that will be used to authenticate with the UAI. The key should be, for example, the contents of an id_rsa.pub file used by SSH.    "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/create_a_uai_with_additional_ports/",
	"title": "Create A UAI With Additional Ports",
	"tags": [],
	"description": "",
	"content": "Create a UAI with Additional Ports An option is available to expose UAI ports to the customer user network in addition to the the port used for SSH access. These ports are restricted to ports 80, 443, and 8888. This procedure allows a user or administrator to create a new UAI with these additional ports.\nPrerequisites A public SSH key\nLimitations Only ports 80, 443, and 8888 can be exposed. Attempting to open any other ports will result in an error.\nProcedure   Log in to a UAN.\n  Create a new UAI with the --ports option.\nncn-w001# cray uas create --publickey PUBLIC_SSH_KEY_FILE --ports PORT_LIST Troubleshooting: If the Cray CLI has not been initialized, the CLI commands will not work. See Configure the Cray Command Line Interface (cray CLI) for more information.\nWhen these ports are exposed in the UAI, they will be mapped to unique ports on the UAI IP address. The mapping of these ports is displayed in the uai_portmap element of the return output from cray uas create, cray uas describe, and cray uas uais list. The mapping is shown as a dictionary where the key is the port requested and the value is the port that key is mapped to.\nncn-w001# cray uas create --publickey /root/.ssh/id_rsa.pub --ports 80,443 username = \u0026#34;user\u0026#34; uai_msg = \u0026#34;ContainerCreating\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_status = \u0026#34;Waiting\u0026#34; uai_age = \u0026#34;0m\u0026#34; uai_connect_string = \u0026#34;ssh user@203.0.113.0 -i ~/.ssh/id\\_rsa\u0026#34; uai_img = \u0026#34;registry.local/cray/cray-uas-sles15sp1-slurm:latest\u0026#34; uai_name = \u0026#34;uai-user-be3a6770\u0026#34; [uai_portmap] 443 = 30173 80 = 32190 8888 = 32469   Log in to the UAI with the connection string.\n$ ssh USERNAME@UAI_IP_ADDRESS -i ~/.ssh/id_rsa   "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/create_and_register_a_custom_uai_image/",
	"title": "Create And Register A Custom UAI Image",
	"tags": [],
	"description": "",
	"content": "Create and Register a Custom UAI Image Create a custom UAI image based on the current compute node image. This UAI image can then be used to build compute node software with the Cray Programming Environment (PE).\nPrerequisites  This procedure requires administrator privileges. Log into either a master or worker NCN node (not a LiveCD node).  Procedure The default end-user UAI is not suitable for use with the Cray PE. The generic image cannot be guaranteed to be compatible with the software running on HPE Cray EX compute nodes at every customer site. Therefore, in order for users to build software for running on compute nodes, site administrators must create a custom end-user UAI for those users.\n  Query the Boot Orchestration Service (BOS) for a compute node session template name to use.\nThe following command returns a list of all registered BOS session templates in YAML format. Only a sample is shown in the example.\nIn the following example, the compute node BOS session template name is wlm-sessiontemplate-0.1.0.\nncn# cray bos sessiontemplate list --format yaml - boot_sets: compute: boot_ordinal: 2 etag: d54782b3853a2d8713a597d80286b93e kernel_parameters: console=ttyS0,115200 bad_page=panic crashkernel=340M hugepagelist=2m-2g intel_iommu=off intel_pstate=disable iommu=pt ip=dhcp numa_interleave_omit=headless numa_zonelist_order=node oops=panic pageblock_order=14 pcie_ports=native printk.synchronous=y rd.neednet=1 rd.retry=10 rd.shell turbo_boost_limit=999 spire_join_token=${SPIRE_JOIN_TOKEN} network: nmn node_roles_groups: - Compute path: s3://boot-images/0c0d4081-2e8b-433f-b6f7-e1ef0b907be3/manifest.json rootfs_provider: cpss3 rootfs_provider_passthrough: dvs:api-gw-service-nmn.local:300:nmn0 type: s3 cfs: configuration: wlm-config-0.1.0 enable_cfs: true name: wlm-sessiontemplate-0.1.0   Download the compute node SquashFS image specified by the BOS session template.\nncn# SESSION_ID=$(cray bos v1 sessiontemplate describe $SESSION_NAME \\ --format json | jq -r \u0026#39;.boot_sets.compute.path\u0026#39; | awk -F/ \u0026#39;{print $4}\u0026#39;) ncn# cray artifacts get boot-images $SESSION_ID/rootfs rootfs.squashfs   Create a directory to mount the downloaded SquashFS.\nncn# mkdir mount ncn# mount -o loop,ro rootfs.squashfs \\`pwd\\`/mount   Create a tarball of the SquashFS file system.\nThe file 99-slingshot-network.conf must be omitted from the tarball as that prevents the UAI from running sshd as the UAI user with the su command.\nncn# (cd `pwd`/mount; tar --xattrs --xattrs-include=\u0026#39;*\u0026#39; \\ --exclude=\u0026#34;99-slingshot-network.conf\u0026#34; -cf \u0026#34;../$SESSION_ID.tar\u0026#34; .) \\ \u0026gt; /dev/null This command may take several minutes to complete. This command creates an uncompressed tar archive so that files can be added after the tarball is made. Using an uncompressed tarball also shortens the time required to complete this procedure.\n  Wait for the previous command to complete. Then verify that the tarball contains the script /usr/bin/uai-ssh.sh from the SquashFS.\nncn# tar tf $SESSION_ID.tar | grep \u0026#39;[.]/usr/bin/uai-ssh[.]sh\u0026#39; ./usr/bin/uai-ssh.sh   Optional: Obtain the /usr/bin/uai-ssh.sh script for a UAI built from the end-user UAI image provided with UAS. Then append it to the tarball. Skip this step if the script was detected within the tarball.\nncn# mkdir -p ./usr/bin ncn# cray uas create --publickey ~/.ssh/id_rsa.pub uai_connect_string = \u0026#34;ssh vers@10.26.23.123\u0026#34; uai_host = \u0026#34;ncn-w001\u0026#34; uai_img = \u0026#34;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026#34; uai_ip = \u0026#34;10.26.23.123\u0026#34; uai_msg = \u0026#34; uai_name = \u0026#34;uai-vers-32079250\u0026#34; uai_status = \u0026#34;Pending\u0026#34; username = \u0026#34;vers\u0026#34; [uai_portmap] ncn# scp vers@10.26.23.123:/usr/bin/uai-ssh.sh ./usr/bin/uai-ssh.sh The authenticity of host \u0026#39;10.26.23.123 (10.26.23.123)\u0026#39; can\u0026#39;t be established. ECDSA key fingerprint is SHA256:voQUCKDG4C9FGkmUcHZVrYJBXVKVYqcJ4kmTpe4tvOA. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added \u0026#39;10.26.23.123\u0026#39; (ECDSA) to the list of known hosts. uai-ssh.sh 100% 5035 3.0MB/s 00:00 ncn# cray uas delete --uai-list uai-vers-32079250 results = [ \u0026#34;Successfully deleted uai-vers-32079250\u0026#34;,] ncn# tar rf 0c0d4081-2e8b-433f-b6f7-e1ef0b907be3.tar ./usr/bin/uai-ssh.sh   Create a container image using podman or docker and push it to the site container registry. Perform any container-specific modifications, if wanted, with a dockerfile before pushing the container image.\nThe ENTRYPOINT layer must be /usr/bin/uai-ssh.sh as that starts sshd for the user in the UAI container started by UAS.\nThe following example assumes that the custom end-user UAI image will be called registry.local/cray/cray-uai-compute:latest. Use a different name if wanted.\nncn# UAI_IMAGE_NAME=registry.local/cray/cray-uai-compute:latest ncn# podman import --change \u0026#34;ENTRYPOINT /usr/bin/uai-ssh.sh\u0026#34; \\ $SESSION_ID.tar $UAI_IMAGE_NAME ncn# podman push $UAI_IMAGE_NAME   Register the new container image with UAS.\nncn# cray uas admin config images create --imagename $UAI_IMAGE_NAME   Delete the SquashFS mount directory and tarball.\nBecause the commands in the following example are executed by the root user and these temporary directories are similar to an important system path, the second rm command does not use the common -r as a precaution.\nncn# umount mount; rmdir mount ncn# rm $SESSION_ID.tar rootfs.squashfs ncn# if [ -f ./usr/bin/uai-ssh.sh ]; then rm -f ./usr/bin/uai-ssh.sh \u0026amp;\u0026amp; rmdir ./usr/bin ./usr; fi   "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/create_a_uai/",
	"title": "Create A UAI",
	"tags": [],
	"description": "",
	"content": "Create a UAI It is rare that an an administrator would hand-craft a UAI in this way, but it is possible. This is the mechanism used to create broker UAIs for the broker mode of UAI management.\nRefer to Broker Mode UAI Management for more information.\nPrerequisites This procedure requires administrative privileges.\nProcedure   Create a UAI manually.\nUse a command of the following form:\ncray uas admin uais create [options] The following options are available for use:\n --class-id \u0026lt;class-id\u0026gt; - The class of the UAI to be created. This option must be specified unless a default UAI class exists, in which case, it can be omitted and the default will be used. --owner '\u0026lt;user-name\u0026gt;' - Create the UAI as owned by the specified user. --passwd str '\u0026lt;passwd-string\u0026gt;' - Specify the /etc/password format string for the user who owns the UAI. This will be used to set up credentials within the UAI for the owner when the owner logs into the UAI. --publickey-str '\u0026lt;public-ssh-key\u0026gt;' - Specify the SSH public key that will be used to authenticate with the UAI. The key should be, for example, the contents of an id_rsa.pub file used by SSH.    "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/create_a_uai_class/",
	"title": "Create A UAI Class",
	"tags": [],
	"description": "",
	"content": "Create a UAI Class Add a new User Access Instance (UAI) class to the User Access Service (UAS) so that the class can be used to configure UAIs.\nPrerequisites Install and initialize the cray administrative CLI.\nProcedure   Add a UAI class by using the command in the following example.\nncn-m001-pit# cray uas admin config classes create --image-id \u0026lt;image-id\u0026gt; [options] --image-id \u0026lt;image-id\u0026gt; specifies the UAI image identifier of the UAI image to be used in creating UAIs of the new class. Any number of classes using the same image id can be defined.\nThe following options are available:\n --image-id \u0026lt;image-id\u0026gt; set the UAI image to be used creating UAIs of this class (included here for completeness, this option is required for creation, not for updates) --volume-list '\u0026lt;volume-id\u0026gt;[,\u0026lt;volume-id[,...]]' set up the list of volumes mounted by UAIs of this class --resource-id \u0026lt;resource-id\u0026gt; set a resource specification to be used for UAIs of this class --uai-compute-network yes|no set the uai_compute_network flag described above in the UAI class --opt-ports '\u0026lt;port-number\u0026gt;[,\u0026lt;port-number[,...]]' sets up TCP ports in addition to SSH on which UAIs of this class will listen on their external IP address (i.e. the address SSH is listening on) -uai-creation-class \u0026lt;class-id\u0026gt; for broker UAIs only, the class of end-user UAIs the broker will create when handling a login --namespace '\u0026lt;namespace-name\u0026gt;' sets the Kubernetes namespace where UAIs of this class will run --priority-class-name '\u0026lt;priority-class-name\u0026gt;' set the Kubernetes priority class of UAIs created with this class --public-ip yes|no specify whether UAIs created with this class will listen on a public (LoadBalancer) IP address (yes) or a Kubernetes private (ClusterIP) IP address (no) --default yes|no specify whether this UAI class should be used as a default UAI class or not (see description in the previous section) --comment 'text' set a free-form text comment on the UAI class  Only the --image-id option is required to create a UAI class. In that case, a UAI class with the specified UAI Image and no volumes will be created.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/create_uais_from_specific_uai_images_in_legacy_mode/",
	"title": "Create UAIs From Specific Uai Images In Legacy Mode",
	"tags": [],
	"description": "",
	"content": "Create UAIs From Specific UAI Images in Legacy Mode A user can create a UAI from a specific UAI image (assuming no default UAI class exists) using a command of the form:\nuser\u0026gt; cray uas create --publickey \u0026lt;path\u0026gt; --imagename \u0026lt;image-name\u0026gt; \u0026lt;image-name\u0026gt; is the name shown above in the list of UAI images.\nFor example:\nvers\u0026gt; cray uas images list default_image = \u0026quot;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026quot; image_list = [ \u0026quot;dtr.dev.cray.com/cray/cray-uai-broker:latest\u0026quot;, \u0026quot;dtr.dev.cray.com/cray/cray-uas-sles15:latest\u0026quot;, \u0026quot;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026quot;,] vers\u0026gt; cray uas create --publickey ~/.ssh/id_rsa.pub --imagename dtr.dev.cray.com/cray/cray-uas-sles15:latest uai_connect_string = \u0026quot;ssh vers@10.103.13.160\u0026quot; uai_host = \u0026quot;ncn-w001\u0026quot; uai_img = \u0026quot;dtr.dev.cray.com/cray/cray-uas-sles15:latest\u0026quot; uai_ip = \u0026quot;10.103.13.160\u0026quot; uai_msg = \u0026quot; uai_name = \u0026quot;uai-vers-b386d655\u0026quot; uai_status = \u0026quot;Pending\u0026quot; username = \u0026quot;vers\u0026quot; [uai_portmap] "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/configure_a_broker_uai_class/",
	"title": "Configure A Broker UAI Class",
	"tags": [],
	"description": "",
	"content": "Configure a Broker UAI Class Configuring a broker UAI class consists of the following:\n Create volumes to hold any site-specific authentication, SSH, or other configuration required Choose the end-user UAI class for which the broker UAI will serve instances Create a UAI Class with (at a minimum):  namespace set to uas default set to false volume_mounts set to the list of customization volume-ids created above public_ip set to true uai_compute_network set to false uai_creation_class set to the class-id of the end-user UAI class    Example of Volumes to Connect Broker UAIs to LDAP Broker UAIs authenticate users in SSH, and pass the SSH connection on to the selected or created end-user UAI. An authentication source is required to authenticate users. For sites that use LDAP as a directory server for authentication, connecting broker UAIs to LDAP is simply a matter of replicating the LDAP configuration used by other nodes or systems at the site (UANs can be a good source of this configuration) inside the broker UAI. This section shows how to do that using volumes, which permits the standard broker UAI image to be used out of the box and reconfigured externally.\nWhile it would be possible to make the configuration available as files volume mounted from the host node of the broker UAI, this is difficult to set up and maintain because it means that the configuration files must be present and synchronized across all UAI host nodes. A more practical approach to this is to install the configuration files in Kubernetes as secrets, and then mount them from Kubernetes directly. This ensures that no matter where a broker UAI runs, it has access to the configuration.\nThis example, uses Kubernetes secrets and assumes that the broker UAIs run in the uas Kubernetes namespace. If a different namespace is used, the creation of the ConfigMaps is different but the contents are the same. Using a namespace other than uas for broker UAIs is not recommended and is beyond the scope of this document.\n  Configure LDAP and determine which files need to be changed in the broker UAI and what their contents should be.\nIn this example, the file is /etc/sssd/sssd.conf and its contents are (the contents have been sanitized, substitute appropriate contents in their place):\n[sssd] config_file_version = 2 services = nss, pam domains = My_DC [nss] filter_users = root filter_groups = root [pam] [domain/My_DC] ldap_search_base=dc=datacenter,dc=mydomain,dc=com ldap_uri=ldap://10.1.1.5,ldap://10.1.2.5 id_provider = ldap ldap_tls_reqcert = allow ldap_schema = rfc2307 cache_credentials = True entry_cache_timeout = 60 enumerate = False   Add the content from the previous step to a secret.\n  Create a file with the appropriate content.\nncn-m001-pit# cat \u0026lt;\u0026lt;EOF \u0026gt; sssd.conf [sssd] config_file_version = 2 services = nss, pam domains = My_DC [nss] filter_users = root filter_groups = root [pam] [domain/My_DC] ldap_search_base=dc=datacenter,dc=mydomain,dc=com ldap_uri=ldap://10.1.1.5,ldap://10.1.2.5 id_provider = ldap ldap_tls_reqcert = allow ldap_schema = rfc2307 cache_credentials = True entry_cache_timeout = 60 enumerate = False EOF   Make a secret from the file.\nncn-m001-pit# kubectl create secret generic -n uas broker-sssd-conf --from-file=sssd.conf     Make a volume for the secret in the UAS configuration.\nncn-m001-pit# cray uas admin config volumes create --mount-path /etc/sssd --volume-description '{\u0026quot;secret\u0026quot;: {\u0026quot;secret_name\u0026quot;: \u0026quot;broker-sssd-conf\u0026quot;, \u0026quot;default_mode\u0026quot;: 384}}' --volumename broker-sssd-config mount_path = \u0026quot;/etc/sssd\u0026quot; volume_id = \u0026quot;4dc6691e-e7d9-4af3-acde-fc6d308dd7b4\u0026quot; volumename = \u0026quot;broker-sssd-config\u0026quot; [volume_description.secret] default_mode = 384 secret_name = \u0026quot;broker-sssd-conf\u0026quot; Two important things to notice here are:\n The secret is mounted on the directory /etc/sssd not the file /etc/sssd/sssd.conf because Kubernetes does not permit the replacement of an existing regular file with a volume but does allow overriding a directory The value 384 is used here for the default mode of the file instead of 0600, which would be easier to read, because JSON does not accept octal numbers in the leading zero form    Obtain the information needed to create a UAI class for the broker UAI containing the updated configuration in the volume list.\nThe image-id of the broker UAI image, the volume-ids of the volumes to be added to the broker class, and the class-id of the end-user UAI class managed by the broker are required:\nncn-m001-pit# cray uas admin config images list [[results]] default = false image_id = \u0026quot;c5dcb261-5271-49b3-9347-afe7f3e31941\u0026quot; imagename = \u0026quot;dtr.dev.cray.com/cray/cray-uai-broker:latest\u0026quot; [[results]] default = false image_id = \u0026quot;c5f6377a-dfc0-41da-89c9-6c88c8a2cda8\u0026quot; imagename = \u0026quot;dtr.dev.cray.com/cray/cray-uas-sles15:latest\u0026quot; [[results]] default = true image_id = \u0026quot;ff86596e-9699-46e8-9d49-9cb20203df8c\u0026quot; imagename = \u0026quot;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026quot; ncn-m001-pit# cray uas admin config volumes list | grep -e volume_id -e volumename volume_id = \u0026quot;4dc6691e-e7d9-4af3-acde-fc6d308dd7b4\u0026quot; volumename = \u0026quot;broker-sssd-config\u0026quot; volume_id = \u0026quot;55a02475-5770-4a77-b621-f92c5082475c\u0026quot; volumename = \u0026quot;timezone\u0026quot; volume_id = \u0026quot;7aeaf158-ad8d-4f0d-bae6-47f8fffbd1ad\u0026quot; volumename = \u0026quot;munge-key\u0026quot; volume_id = \u0026quot;9fff2d24-77d9-467f-869a-235ddcd37ad7\u0026quot; volumename = \u0026quot;lustre\u0026quot; volume_id = \u0026quot;ea97325c-2b1d-418a-b3b5-3f6488f4a9e2\u0026quot; volumename = \u0026quot;slurm-config\u0026quot; ncn-m001-pit# cray uas admin config classes list | grep -e class_id -e comment class_id = \u0026quot;a623a04a-8ff0-425e-94cc-4409bdd49d9c\u0026quot; comment = \u0026quot;UAI User Class\u0026quot; class_id = \u0026quot;bb28a35a-6cbc-4c30-84b0-6050314af76b\u0026quot; comment = \u0026quot;Non-Brokered UAI User Class\u0026quot;   Create the broker UAI class with the content retrieved in the previous step.\nncn-m001-pit# cray uas admin config classes create --image-id c5dcb261-5271-49b3-9347-afe7f3e31941 --volume-list '4dc6691e-e7d9-4af3-acde-fc6d308dd7b4,55a02475-5770-4a77-b621-f92c5082475c,9fff2d24-77d9-467f-869a-235ddcd37ad7' --uai-compute-network no --public-ip yes --comment \u0026quot;UAI broker class\u0026quot; --uai-creation-class a623a04a-8ff0-425e-94cc-4409bdd49d9c --namespace uas class_id = \u0026quot;74970cdc-9f94-4d51-8f20-96326212b468\u0026quot; comment = \u0026quot;UAI broker class\u0026quot; default = false namespace = \u0026quot;uas\u0026quot; opt_ports = [] priority_class_name = \u0026quot;uai-priority\u0026quot; public_ip = true uai_compute_network = false uai_creation_class = \u0026quot;a623a04a-8ff0-425e-94cc-4409bdd49d9c\u0026quot; [[volume_mounts]] mount_path = \u0026quot;/etc/sssd\u0026quot; volume_id = \u0026quot;4dc6691e-e7d9-4af3-acde-fc6d308dd7b4\u0026quot; volumename = \u0026quot;broker-sssd-config\u0026quot; [volume_mounts.volume_description.secret] default_mode = 384 secret_name = \u0026quot;broker-sssd-conf\u0026quot; [[volume_mounts]] mount_path = \u0026quot;/etc/localtime\u0026quot; volume_id = \u0026quot;55a02475-5770-4a77-b621-f92c5082475c\u0026quot; volumename = \u0026quot;timezone\u0026quot; [volume_mounts.volume_description.host_path] path = \u0026quot;/etc/localtime\u0026quot; type = \u0026quot;FileOrCreate\u0026quot; [[volume_mounts]] mount_path = \u0026quot;/lus\u0026quot; volume_id = \u0026quot;9fff2d24-77d9-467f-869a-235ddcd37ad7\u0026quot; volumename = \u0026quot;lustre\u0026quot; [volume_mounts.volume_description.host_path] path = \u0026quot;/lus\u0026quot; type = \u0026quot;DirectoryOrCreate\u0026quot; [uai_image] default = false image_id = \u0026quot;c5dcb261-5271-49b3-9347-afe7f3e31941\u0026quot; imagename = \u0026quot;dtr.dev.cray.com/cray/cray-uai-broker:latest\u0026quot;   "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/configure_a_default_uai_class_for_legacy_mode/",
	"title": "Configure A Default UAI Class For Legacy Mode",
	"tags": [],
	"description": "",
	"content": "Configure a Default UAI Class for Legacy Mode Using a default UAI class is optional but recommended for any site using the legacy UAI management mode that wants to have some control over UAIs created by users. UAI classes used for this purpose need to have certain minimum configuration in them:\n The image_id field set to identify the image used to construct UAIs The volume_list field set to the list of volumes to mount in UAIs The public_ip field set to true The uai_compute_network flag set to true (if workload management will be used) The default flag set to true to make this the default UAI class  To make UAIs useful, there is a minimum set of volumes that should be defined in the UAS configuration:\n /etc/localtime for default timezone information whatever directory on the host nodes holds persistent end-user storage, typically /lus  In addition to this, there may be volumes defined to support a workload manager (Slurm or PBS Professional) or the Cray Programming Environment (PE) or other packages the full extent of these volumes is outside the scope of this document, but whatever list of these other volumes is needed to get a suitable end-user UAI should be included in the default UAI class configuration.\nExample Minimal Default UAI Class The following is an example set of volumes and an example of how to create a UAI class that would use those volumes for a minimal system:\nncn-m001-pit# cray uas admin config volumes list --format json [ { \u0026quot;mount_path\u0026quot;: \u0026quot;/etc/localtime\u0026quot;, \u0026quot;volume_description\u0026quot;: { \u0026quot;host_path\u0026quot;: { \u0026quot;path\u0026quot;: \u0026quot;/etc/localtime\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;FileOrCreate\u0026quot; } }, \u0026quot;volume_id\u0026quot;: \u0026quot;55a02475-5770-4a77-b621-f92c5082475c\u0026quot;, \u0026quot;volumename\u0026quot;: \u0026quot;timezone\u0026quot; }, { \u0026quot;mount_path\u0026quot;: \u0026quot;/lus\u0026quot;, \u0026quot;volume_description\u0026quot;: { \u0026quot;host_path\u0026quot;: { \u0026quot;path\u0026quot;: \u0026quot;/lus\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;DirectoryOrCreate\u0026quot; } }, \u0026quot;volume_id\u0026quot;: \u0026quot;9fff2d24-77d9-467f-869a-235ddcd37ad7\u0026quot;, \u0026quot;volumename\u0026quot;: \u0026quot;lustre\u0026quot; } ] ncn-m001-pit# cray uas admin config images list [[results]] default = false image_id = \u0026quot;c5dcb261-5271-49b3-9347-afe7f3e31941\u0026quot; imagename = \u0026quot;dtr.dev.cray.com/cray/cray-uai-broker:latest\u0026quot; [[results]] default = false image_id = \u0026quot;c5f6377a-dfc0-41da-89c9-6c88c8a2cda8\u0026quot; imagename = \u0026quot;dtr.dev.cray.com/cray/cray-uas-sles15:latest\u0026quot; [[results]] default = true image_id = \u0026quot;ff86596e-9699-46e8-9d49-9cb20203df8c\u0026quot; imagename = \u0026quot;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026quot; ncn-m001-pit# cray uas admin config classes create --image-id ff86596e-9699-46e8-9d49-9cb20203df8c --volume-list '55a02475-5770-4a77-b621-f92c5082475c,9fff2d24-77d9-467f-869a-235ddcd37ad7' --uai-compute-network yes --public-ip yes --comment \u0026quot;my default legacy mode uai class\u0026quot; --default yes class_id = \u0026quot;e2ea4845-5951-4c79-93d7-186ced8ce8ad\u0026quot; comment = \u0026quot;my default legacy mode uai class\u0026quot; default = true namespace = \u0026quot;user\u0026quot; opt_ports = [] priority_class_name = \u0026quot;uai-priority\u0026quot; public_ip = true uai_compute_network = true [[volume_mounts]] mount_path = \u0026quot;/etc/localtime\u0026quot; volume_id = \u0026quot;55a02475-5770-4a77-b621-f92c5082475c\u0026quot; volumename = \u0026quot;timezone\u0026quot; [volume_mounts.volume_description.host_path] path = \u0026quot;/etc/localtime\u0026quot; type = \u0026quot;FileOrCreate\u0026quot; [[volume_mounts]] mount_path = \u0026quot;/lus\u0026quot; volume_id = \u0026quot;9fff2d24-77d9-467f-869a-235ddcd37ad7\u0026quot; volumename = \u0026quot;lustre\u0026quot; [volume_mounts.volume_description.host_path] path = \u0026quot;/lus\u0026quot; type = \u0026quot;DirectoryOrCreate\u0026quot; [uai_image] default = true image_id = \u0026quot;ff86596e-9699-46e8-9d49-9cb20203df8c\u0026quot; imagename = \u0026quot;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026quot; Example Default UAI Class with Slurm Support The following is an example of a default UAI class configured for Slurm support if Slurm has been installed on the host system:\nncn-m001-pit# cray uas admin config volumes list --format json [ { \u0026quot;mount_path\u0026quot;: \u0026quot;/etc/localtime\u0026quot;, \u0026quot;volume_description\u0026quot;: { \u0026quot;host_path\u0026quot;: { \u0026quot;path\u0026quot;: \u0026quot;/etc/localtime\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;FileOrCreate\u0026quot; } }, \u0026quot;volume_id\u0026quot;: \u0026quot;55a02475-5770-4a77-b621-f92c5082475c\u0026quot;, \u0026quot;volumename\u0026quot;: \u0026quot;timezone\u0026quot; }, { \u0026quot;mount_path\u0026quot;: \u0026quot;/root/slurm_config/munge\u0026quot;, \u0026quot;volume_description\u0026quot;: { \u0026quot;secret\u0026quot;: { \u0026quot;secret_name\u0026quot;: \u0026quot;munge-secret\u0026quot; } }, \u0026quot;volume_id\u0026quot;: \u0026quot;7aeaf158-ad8d-4f0d-bae6-47f8fffbd1ad\u0026quot;, \u0026quot;volumename\u0026quot;: \u0026quot;munge-key\u0026quot; }, { \u0026quot;mount_path\u0026quot;: \u0026quot;/lus\u0026quot;, \u0026quot;volume_description\u0026quot;: { \u0026quot;host_path\u0026quot;: { \u0026quot;path\u0026quot;: \u0026quot;/lus\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;DirectoryOrCreate\u0026quot; } }, \u0026quot;volume_id\u0026quot;: \u0026quot;9fff2d24-77d9-467f-869a-235ddcd37ad7\u0026quot;, \u0026quot;volumename\u0026quot;: \u0026quot;lustre\u0026quot; }, { \u0026quot;mount_path\u0026quot;: \u0026quot;/etc/slurm\u0026quot;, \u0026quot;volume_description\u0026quot;: { \u0026quot;config_map\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;slurm-map\u0026quot; } }, \u0026quot;volume_id\u0026quot;: \u0026quot;ea97325c-2b1d-418a-b3b5-3f6488f4a9e2\u0026quot;, \u0026quot;volumename\u0026quot;: \u0026quot;slurm-config\u0026quot; } ] ncn-m001-pit# cray uas admin config images list [[results]] default = false image_id = \u0026quot;c5dcb261-5271-49b3-9347-afe7f3e31941\u0026quot; imagename = \u0026quot;dtr.dev.cray.com/cray/cray-uai-broker:latest\u0026quot; [[results]] default = false image_id = \u0026quot;c5f6377a-dfc0-41da-89c9-6c88c8a2cda8\u0026quot; imagename = \u0026quot;dtr.dev.cray.com/cray/cray-uas-sles15:latest\u0026quot; [[results]] default = true image_id = \u0026quot;ff86596e-9699-46e8-9d49-9cb20203df8c\u0026quot; imagename = \u0026quot;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026quot; ncn-m001-pit# cray uas admin config classes create --image-id ff86596e-9699-46e8-9d49-9cb20203df8c --volume-list '55a02475-5770-4a77-b621-f92c5082475c,9fff2d24-77d9-467f-869a-235ddcd37ad7,7aeaf158-ad8d-4f0d-bae6-47f8fffbd1ad,ea97325c-2b1d-418a-b3b5-3f6488f4a9e2' --uai-compute-network yes --public-ip yes --comment \u0026quot;my default legacy mode uai class\u0026quot; --default yes class_id = \u0026quot;c0a6dfbc-f74c-4f2c-8c8e-e278ff0e14c6\u0026quot; comment = \u0026quot;my default legacy mode uai class\u0026quot; default = true namespace = \u0026quot;user\u0026quot; opt_ports = [] priority_class_name = \u0026quot;uai-priority\u0026quot; public_ip = true uai_compute_network = true [[volume_mounts]] mount_path = \u0026quot;/etc/localtime\u0026quot; volume_id = \u0026quot;55a02475-5770-4a77-b621-f92c5082475c\u0026quot; volumename = \u0026quot;timezone\u0026quot; [volume_mounts.volume_description.host_path] path = \u0026quot;/etc/localtime\u0026quot; type = \u0026quot;FileOrCreate\u0026quot; [[volume_mounts]] mount_path = \u0026quot;/lus\u0026quot; volume_id = \u0026quot;9fff2d24-77d9-467f-869a-235ddcd37ad7\u0026quot; volumename = \u0026quot;lustre\u0026quot; [volume_mounts.volume_description.host_path] path = \u0026quot;/lus\u0026quot; type = \u0026quot;DirectoryOrCreate\u0026quot; [[volume_mounts]] mount_path = \u0026quot;/root/slurm_config/munge\u0026quot; volume_id = \u0026quot;7aeaf158-ad8d-4f0d-bae6-47f8fffbd1ad\u0026quot; volumename = \u0026quot;munge-key\u0026quot; [volume_mounts.volume_description.secret] secret_name = \u0026quot;munge-secret\u0026quot; [[volume_mounts]] mount_path = \u0026quot;/etc/slurm\u0026quot; volume_id = \u0026quot;ea97325c-2b1d-418a-b3b5-3f6488f4a9e2\u0026quot; volumename = \u0026quot;slurm-config\u0026quot; [volume_mounts.volume_description.config_map] name = \u0026quot;slurm-map\u0026quot; [uai_image] default = true image_id = \u0026quot;ff86596e-9699-46e8-9d49-9cb20203df8c\u0026quot; imagename = \u0026quot;dtr.dev.cray.com/cray/cray-uai-sles15sp1:latest\u0026quot; "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/configure_end-user_uai_classes_for_broker_mode/",
	"title": "Configure End-user UAI Classes For Broker Mode",
	"tags": [],
	"description": "",
	"content": "Configure End-User UAI Classes for Broker Mode Each UAI broker will create and manage a single class of end-user UAIs. Setting up UAI classes for this is similar to Configure a Default UAI Class for Legacy Mode with the following exceptions:\n The public_ip flag for brokered UAI classes should be set to false The default flag for brokered UAI classes may be set to true or false but should, most likely, be set to false.  Everything else should be the same as it would be for a legacy mode UAI class.\n"
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/configure_uais_in_uas/",
	"title": "Configure UAIs In UAS",
	"tags": [],
	"description": "",
	"content": "Configure UAIs in UAS The four main items of UAI configuration in UAS. Links to procedures for listing, adding, examining, updating, and deleting each item.\nOptions for the elements of a UAI are maintained in the UAS configuration. The following can be configured in UAS:\n UAI images Volumes Resource specifications UAI Classes  Only users who are defined as administrators in an HPE Cray EX system and are logged in using the administrative CLI (cray command) can configure UAS. Configure UAS from a LiveCD node or from any system with the administrative CLI installed that can reach the HPE Cray EX API Gateway.\nThe following procedures provide instructions for creating, updating, examining, and removing configuration items from UAS:\n UAI Images:  List Registered UAI Images Register a UAI Image Retrieve UAI Image Registration Information Update a UAI Image Registration Delete a UAI Image Registration   UAS Volumes:  List Volumes Registered in UAS Add a Volume to UAS Obtain the Configuration of a UAS Volume Update a UAS Volume Delete a Volume Configuration   UAI Resource Specifications:  List UAI Resource Specifications Retrieve Resource Specification Details Create a UAI Resource Specification Update a Resource Specification Delete a UAI Resource Specification   UAI Classes:  List Available UAI Classes View a UAI Class Create a UAI Class Modify a UAI Class Delete a UAI Class    "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/add_a_volume_to_uas/",
	"title": "Add A Volume To UAS",
	"tags": [],
	"description": "",
	"content": "Add a Volume to UAS This procedure registers and configures a volume in UAS so that the volume can be mounted in UAIs.\nSee List Volumes Registered in UAS for examples of valid volume configurations. Refer to Elements of a UAI for descriptions of the volume configuration fields and values.\nNote the following caveats about adding volumes to UAS:\n A volume description may specify an underlying directory that is NFS-mounted on the UAI host nodes. Hard-mounted NFS file systems will stop responding indefinitely on references to their mount points if the NFS server fails or becomes unreachable from the UAI host node. This will cause new UAI creation and migration of existing UAIs to stop responding as well until the NFS issue is remedied. Multiple volumes can be configured in UAS with the same mount_path. UAS cannot create a UAI if that UAI has more than one volume specified for a given mount_path. If multiple volumes with the same mount_path exist in the UAS configuration all UAIs must be created using UAI classes that specify a workable subset of volumes. A UAI created without a UAI Class under such a UAS configuration will try to use all configured volumes and creation will fail. The volumename is a string that can describe or name the volume. It must be composed of only lowercase letters, numbers, and dashes ('-'). The volumename also must begin and end with an alphanumeric character. As with UAI images, registering a volume with UAS creates the configuration that will be used to create a UAI. If the underlying object referred to by the volume does not exist at the time the UAI is created, the UAI will, in most cases, wait until the object becomes available before starting up. This will be visible in the UAI state which will eventually move to Waiting  Prerequisites Install and initialize the cray administrative CLI.\nProcedure To create a volume, follow this procedure.\n  Use the cray CLI to create the volume, specifying volumename, mount_path, and volume_description.\nNote difference between the UAS name for the volume type and the Kubernetes name for that type. Kubernetes uses camelCase for its type names, while UAS uses lower_case_with_underscores.\nncn-m001-pit# cray uas admin config volumes create --mount-path \u0026lt;path in UAI\u0026gt; --volume-description '{\u0026quot;\u0026lt;volume-kind\u0026gt;\u0026quot;: \u0026lt;k8s-volume-description\u0026gt;}' --volumename '\u0026lt;string\u0026gt;' For example:\nncn-m001-pit# cray uas admin config volumes create --mount-path /host_files/host_passwd --volume-description '{\u0026quot;host_path\u0026quot;: {\u0026quot;path\u0026quot;: \u0026quot;/etc/passwd\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;FileOrCreate\u0026quot;}}' --volumename 'my-volume-with-passwd-from-the-host-node' The example above will create a directory /host_files in every UAI configured to use this volume and mount the file /etc/passwd from the host node into that directory as a file named host_passwd. Notice the form of the --volume-description argument. It is a JSON string encapsulating an entire volume_description field as shown in the JSON output in the previous section.\n  Perform List Volumes Registered in UAS to verify that the new volume is configured.\nThe new volume appears in the output of the cray uas admin config volumes list command.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/uas_user_and_admin_topics/broker_mode_uai_management/",
	"title": "Broker Mode UAI Management",
	"tags": [],
	"description": "",
	"content": "Broker Mode UAI Management A UAI broker is a special kind of UAI whose job is not to host users directly but to field attempts to reach a UAI, locate or create a UAI for the user making the attempt, and then pass the connection on to the correct UAI. Multiple UAI brokers can be created, each serving a UAI of a different class, making it possible to set up UAIs for varying workflows and environments as needed. The following illustrates a system using the UAI broker mode of UAI management:\nUnlike in the legacy model, in this model users log into their UAIs through the UAI broker. After that, each user is assigned an end-user UAI by the broker and the SSH session is forwarded to the end-user UAI. This is seamless from the user\u0026rsquo;s perspective, as the SSH session is carried through the UAI broker and into the end-user UAI.\nTo make all of this work, the administrator must define at least one UAI class containing the configuration for the end-user UAIs to be created by the UAI broker and one UAI class containing the UAI broker configuration itself. The UAI broker should be configured by the site to permit authentication of users. Refer to the example in Configure a Broker UAI Class for more information. This can be carried out using volumes to place configuration files as needed in the file system namespace of the broker UAI. Finally, once all of this is prepared, the administrator launches the broker UAI, and makes the IP address of the broker UAI available for users to log into.\n"
},
{
	"uri": "/docs-csm/en-11/operations/component_names_xnames/",
	"title": "Component Names (xnames)",
	"tags": [],
	"description": "",
	"content": "Component Names (xnames) Component names (xnames) identify the geolocation for hardware components in the HPE Cray EX system. Every component is uniquely identified by these component names. Some, like the system cabinet number or the CDU number, can be changed by site needs. There is no geolocation encoded within the cabinet number, such as an X-Y coordinate system to relate to the floor layout of the cabinets. Other component names refer to the location within a cabinet and go down to the port on a card or switch or the socket holding a processor or a memory DIMM location.\n   Name Pattern Range Description     s0 n/a Wildcard: Specifies all of a given type of component in the system. Can be used for \u0026ldquo;all nodes\u0026rdquo;, or to refer to the management system NCN cluster by a single logical name.   ncnN N: 1-n Non-compute Node (NCN): A management node in the management plane. Management NCNs are located in standard EIA racks.   all n/a Wildcard: Similar to s0 and can be used to specify all components in a system.   all_comp n/a Wildcard: Specifies all compute nodes.   all_svc n/a Wildcard: Specifies all service or management nodes.   pH.S H: 0-n S: 0-n Partition: A hardware or software partition (hard or soft partition). H specifies a hardware partition; HSN cabling, switches, and so on. The S specifies a software partition. A hard partition can have more than 1 soft partition. A soft partition cannot have more than 1 hard partition. Example: p1.2 is soft partition 2 of hard partition 1.   dD D: 0-999 Coolant Distribution Unit (CDU): 1 CDU for up to 6 cabinets. Example: d3 (CDU 3).   dDwW W: 0-31 Management Switch in a CDU: Example: d3w1 is switch 1 in CDU 3.   xX X: 0-9999 Liquid-cooled Cabinet or Standard Rack: Liquid-cooled cabinets include 8 chassis and do not have a cabinet-level controller; only chassis-level controllers. An standard rack is always considered chassis 0. Examples: x3000 is rack number 3000.   xXdD D: 0-1 Rack-mounted CDU: Example: x1000d0 is CDU 0 for cabinet 1000.   xXmM M: 0-3 Rack PDU Controller (BMC): Controller or BMC for one or more rack PDUs. A primary PDU controller many manage other PDUs. Example: x3000m0 is PDU controller 0, cabinet 3000.   xXmMpP P: 0-7 Rack PDU: managed by a controller. Example: x3000m0p0 is PDU 0, PDU controller 0, cabinet 3000.   xXmMpPjJ J: 1-32 Rack PDU Outlet: Example: x3000m0p0j12 is power outlet 12 on PDU 0, PDU controller 0, rack 3000.   xXmMiI I: 1-3 PDU NIC: The NIC associated with the PDU management controller, not a specific PDU. Example: x3000m0i1 is management NIC 1 of PDU controller 0, cabinet 3000.   xXm0pPvV V: 1-64 PDU Power Connector: Power connectors are connected to node cards or enclosures and also control and monitor power. Example: x3000m0p0v32 is power plug/outlet 32 of cabinet PDU 0, under cab PDU controller 0, rack 3000.   xXeE E: 0-1 CEC: There are Cabinet Environmental Controllers (CEC) per liquid-cooled cabinet. CEC 0 (right) and CEC 1 (left). Example: x1016e0 is the CEC on the right side of cabinet 1016.   xXcC C: 0-7 Chassis: An enclosure within a liquid-cooled cabinet. Standard EIA racks are always considered a single chassis (chassis 0). This component name is used as a component group or prefix and not a single component. Example: x1016c3 is chassis 3 of cabinet 1016.   xXcCbB B: 0 Chassis BMC: Liquid-cooled cabinet chassis management module (CMM) controller (cC). A standard EIA rack is always chassis 0. Example: x1016c4b0 is BMC 0 (cC) for chassis 4, cabinet 1016.   xXcCbBiI I: 0 Chassis BMC NIC: CMM BMC Ethernet NIC. Example: x1000c1b0i0 is NIC 0 of BMC 0, chassis 1, cabinet 1000.   xXcCtT T: 0-2 PSU: Power rectifier (PSU) in the a liquid-cooled chassis rectifier shelf. Three PSUs support a chassis (n+1). Example: x1016c3t2 is PSU 2 for chassis 3, cabinet 1016.   xXcCfF F: 0 CMM FPGA: CMM FPGA. Example: x1016c1f0 is FPGA 0 in chassis 1 CMM, cabinet 1016.   xXcCwW W: 1-48 Management Network Switch: Specifies bottom U-position for the switch. Example: x3000c0w47 is management switch in U47, chassis 0, rack 3000.   xXcCwWjJ J: 1-32 Management Network Switch Connector: Cable connector (port) on a management switch. Example: x3000c0w47j31 is cable connector 31 of switch in U47, chassis 0, rack 3000.   xXcChH H: 1-48 High-level Management Switch Enclosure: Typically spine switch. Example: x3000c0h47 is U47, chassis 0, in rack 3000.   xXcChHsS S: 1-4 High-level Management Network Switch: Typically a spine switch. May be a half-width device specified with a rack U position H and a horizontal space number S. Horizontal space numbers are assigned arbitrarily to physical locations. Example: x3000c0h47s1 is space 1 of U47, chassis 0, rack 3000.   xXcCrR R: 0-64 HSN Switch: Liquid-cooled blade switch slot number or ToR switch bottom U position. Switch blades are numbered 0-7 in a chassis. ToR HSN switches are numbered by bottom U position. Example: x1016c3r6 is switch blade 6 in chassis 3, cabinet 1016. Example: x3000c0r31 is ToR switch in U31, chassis 0, rack 3000.   xXcCrRaA A: 0 HSN Switch ASIC: Example: x3000c0r1a0 is ASIC 0 of ToR switch in U1, chassis 0, of rack 3000. Example: x1016c3r7a0 (ASIC 0 of liquid-cooled switch blade 7, chassis 3, cabinet 1016).   xXcCrRaAlL L: 0-N HSN Switch ASIC Link: The decimal number for the maximum number of links is network-dependent. Example: x1016c0r1a0l25 is link 25 of ASIC 0, switch 1, chassis 0, cabinet 1016).   xXcCrReE E: 0 HSN Switch Submodule: Example: x3000c0r2e0 is HSN switch submodule of ToR switch 2, chassis 0, rack 3000.   xXcCrRbB B: 0 HSN Switch Controller (sC) or BMC: A BMC or embedded controller of a switch blade. Example: x1000c3r4b0 is BMC 0 of switch 4, chassis 3, cabinet 1000. Example: x3000c0r1b0 is BMC 0 of ToR switch in U1, chassis 0, rack 3000.   xXcCrRbBiI I: 0-3 HSN Switch Management NIC: Example: x1016c2r3b0i0 is NIC 0 of controller 0, switch 3, of chassis 2, cabinet 1016.   xXcCrRfF F: 0 HSN Switch Card FPGA: Example: x1016c3r2f0 is FPGA 0 of blade switch 2, chassis 3, cabinet 1016.   xXcCrRtT T: 0 ToR component in a ToR Switch. Example: x3000c0r1t0 ToR switch 0 in U1, chassis 0, cabinet 3000.   xXcCrRjJ J: 1-32 HSN Switch Cable Connector: Example: x1016c3r4j7 is HSN connector 7 in, switch 4, chassis 3, cabinet 1016.   xXcCrRjJpP P: 0-1 HSN Switch Cable Connector Port: Example: x1016c3r4j7p0 is port 0 of HSN connector 7, switch blade 4, chassis 3, cabinet 1016.   xXcCsS S: 0-64 Node Slot or U Position: Liquid-cooled blades are numbered 0-7 in each chassis; a rack system U position specifies the bottom-most U number for the enclosure. An EIA rack is always chassis 0. Example: x1016c1s7 is compute blade 7 in chassis 1, cabinet 1016. Example: x3000c0s24 is node enclosure in U24, chassis 0, of rack 3000.   xXcCsSvV V: 1-2 Power Connector for Rack Node Enclosure: Power connector for an air-cooled node enclosure/blade. There may be one or two power connectors per node. Example: x3000c0s4v1 is power connector 1, server in U4, chassis 0, rack 3000.   xXcCrRvV V: 1-2 Power Connector for ToR HSN Switch: There may be one or two power connectors per ToR HSN switch. Example: x3000c0r4v1 is power connector 1 of ToR switch in U4, chassis 0, rack 3000.   xXcCsSbB B: 0-1 Node Controller or BMC: Liquid-cooled compute blade node card controller (nC), or rack node card BMC. Example: x1016c3s1b0 (node card 0 controller (nC) of compute blade 1, chassis 3, cabinet 1016).   xXcCsSbBiI I: 0-3 Node controller or BMC NIC: NIC associated with a node controller or BMC. Liquid-cooled nC NIC numbers start with 0. Standard rack node card BMC NICs are numbered according to the OEM hardware. Example: x1016c2s1b0i0 is NIC 0 of node card 0 controller (nC), compute blade in slot 1, chassis 3, cabinet 16. Example: x3000c0s24b1i1 is NIC 1 of BMC 1, compute node in rack U-position 24, chassis 0, cabinet 3000.   xXcCsSbBnN N: 0-7 Node: Liquid-cooled node or rack server node. Nodes are numbered 0-N and are children of the parent node controller or BMC. Node names have a bB component which specifies the node controller or BMC number. Component names can support one BMC for several nodes, multiple BMCs for one node, or one BMC per node. Example: x1016c2s3b0n1 is node 1 of node card 0, compute blade in slot 3, chassis 2, cabinet 1016.   xXcCsSeE E: 0 Node Enclosure: Liquid-cooled nodes are located on a node card which includes node card controller (nC). The node card is considered an enclosure. There may be 1 or more node cards in a rack system server or liquid-cooled blade. Rack node enclosures can include multiple subslots inside of a multi-U enclosure. Example: x3000c0s16e0 is node enclosure 0, at U16, chassis 0, cabinet 3000.   xXcCsSbBnNpPx P: 0-3 Node Processor Socket: Example: x1016c2s3b0n1p1 is processor socket 1 of node 1, of node card 0, compute blade in slot 3, chassis 2, cabinet 1016.   xXcCsSbBnNdD D: 0-15 Node DIMM Example: x1016c3s0b0n1d3 is DIMM 3 of node 1, node card 0, compute blade 0, chassis 3, cabinet 1016).   xXcCsSbBnNhH H: 0-3 Node HSN NIC: Example: x1016c3s0b0n1h1 is HSN NIC 1, node 1, node card 0, compute blade in slot 0, chassis 3, cabinet 1016.   xXcCsSbBnNiI I: 1-3 Node Management NIC: Example: x1016c3s0b0n1i1 is node management NIC 1 of node 1, node card 0, compute blade in slot 0, chassis 3, of cabinet 1016.   xXcCsSbBfF F: 0-7 Node Controller FPGA: Node card controller FPGA (FPGA). Example: x16c3s4b1f0 is FPGA 0 of node card 1, compute blade in slot 4, chassis 3, cabinet 1016.   xXcCsSbBnNaA A: 0-7 GPU: Accelerator (GPU) associated with a node. Example: x16c3s0b1n0a1 is accelerator 1, node 0, of node card 1, compute blade 0, of chassis 3, of cabinet 1016.   xXcCsSbBnNgG G: 0-63 Storage Group or Group of Disk Drives for a Node: Example: x1016c3s0b0n1g3 is storage group 3 of node 1, node card 0, compute blade in slot 0, chassis 3, cabinet 1016.   xXcCsSbBnNgGkK K: 0-63 Storage Group Disk: Example: x1016c3s0b0n1g3k1 is disk 1 of storage group 3, node 1, node card 0, of compute blade in slot 0, chassis 3, cabinet 1016.    "
},
{
	"uri": "/docs-csm/en-11/operations/csm_product_management/validate_signed_rpms/",
	"title": "Validate Signed Rpms",
	"tags": [],
	"description": "",
	"content": "Validate Signed RPMs The HPE Cray EX system signs RPMs to provide an extra level of security. Use the following procedure to import a key from either CrayPort or a Kubernetes Secret, and then use that key to validate the RPM package signatures on each node type.\nThe RPMs will vary on compute, application, worker, master, and storage nodes. Check each node type to ensure the RPMs are correctly signed.\nProcedure   Retrieve the signing key required to validate the RPMs.\nUse either the CrayPort or Kubernetes Secret method to find the signing key.\n CrayPort:   Find the signing key. ncn-m001# curl LINK_TO_KEY_IN_CRAYPORT    Kubernetes Secret:    Find the key and write it to a file.\nncn-m001# kubectl -n services get secrets hpe-signing-key -o jsonpath=\u0026#39;{.data.gpg-pubkey}\u0026#39; | base64 -d | tee hpe-signing-key.asc -----BEGIN PGP PUBLIC KEY BLOCK----- Version: GnuPG v2.0.22 (GNU/Linux) mQENBFZp0YMBCADNNhdrR/K7jk6iFh/D/ExEumPSdriJwDUlHY70bkEUChLyRACI QfLEmh0dGoUfFu2Uk8M/RgeGPayJUeO3jeJw/y7JJHvZENJwYjquZKTOza7GLXf6 HyCRanHrEeXeyRffhJlXLf6GvCqYVl9nSvxwSX9raotqMznLY5E1JXIqtfHLrVhJ qHQLiKulpEAHL9pOWamwZKeGbL9M/N6O3LINbHqisiC0EIcV6GIFCLSfCFMODO6C PgkJ/ECVLZEjGDFnSTT0mn5+DveqRUid/+YQejcraKlc3xRUF+qlg4ey+uz0kFzC SFUbKY68Pw6W/dFGrEhfau8A0TnMnIQ4qgLPABEBAAG0P0hld2xldHQgUGFja2Fy ZCBFbnRlcnByaXNlIENvbXBhbnkgUlNBLTIwNDgtMzAgPHNpZ25ocEBocGUuY29t PokBPQQTAQIAJwUCVmnRgwIbLwUJEswDAAYLCQgHAwIGFQgCCQoLAxYCAQIeAQIX gAAKCRDU2uHjnaOfRK5XCACRJLoMQ/nBa7Gna/96inbAHoKM6DUbNramBa1XCTeh KiTxA0bPE3kp7y143jpfOiSGAOTcU0RaCOKk6JMnJJMt60nR4UohVG2lLVtLxT0G H75jCu0nuZQJrKlMh04fJ3zHnqVuOduyUstgmMQ0qVg2lwPTV+KZeY5/eNPHzkcK 75pfos/svDRQNN2LX6qzsVWfAkEN/WdnlZJE76exvA9JsVmNtU3h3PKQTT86W4bb 1MdeDMkX9lDwMCEhClxLVU/sUfj10Kb8CO5+TFimmdqgXXY4BJJsE8STowy67t7Q zECkM4UFVpgcXFrapWW7IniC1OP0c4I+11mnHKCN15DFuQINBFZp0YMQCADo0UHN pIORPOLtaVI2W/aBpOWVlO74HZvMlWKOk+isf8pIKOujivncNZeeVPu2MTT7kOZ6 3Iwuj6B/dBz0hFXkqfzww+ibkhV1NWUx8Gk3FnGm6Ye6VZq2MbYHFMjSMbH3gJNd l76n4wOdwzC8TbLSmfIVxRyf+Uo5GhMrFy/+G28m/WO5nmH/AxKZxOp//NUVxE47 p6Dd2Rqg2IgBfQ99gudh75F/s6RHDYtV+87CsyFyKD7nJW54l/7r9jvvwhO0d89T s37j+bv81AEPYtu17uaRCcfF2B6RtPEdDslZ+J0G14TBsjp53ARh43HmH6BwQ3+4 pyB7QYWwN2ybFCqTAAMFCAC+1JtxaR7TEZsRDNy6ViHH+fHENl7+SB8GTQL7BZXB YgFEtsti+NZpkAAiJ+HXZihgcjCrHPejnlj5Su7dSkveRLHKZbVehvIbiM+LxfNv 7CdxfhLUVPkgPEpiCHGpCHjG/bKyKCL48SDPB5ClUtVu7v05dq/yu4AYaWwU1iix uH9dYQWC1J8pkZX/igHdbD/RYnPMuiil41guTNSWgjzxbOnxEVueaYFKnHdFlqz7 JpzJa10Lm9gEcGmzePVbJH0j8/+1ViwqLhbITq7Gv1S+RkNnewjLM9Vu2R/Fvpzh AUAinTEi5bYPtmVtddZQ94cOFLvh+LrETAC7v4zxvW/ciQElBBgBAgAPBQJWadGD AhsMBQkSzAMAAAoJENTa4eOdo59E6kAIAMC60HIPrr7ztUAF1vmuIdgSMDAjD7y0 UOzCm1L9fuHqeXNc/JQkKbqAv0tMjnRtrt1R13N3qy1qBeUTnG0qxwdHR0jsknHW S/1T24x03XioypowQObeh15PTD/TTAiLherzAWRNqqtf2Yh9Dy2zWLo204FQjK// Apw4IbO28hgYWvIbpFsyPG4WED3uJ7uTnkqdRkNWQl3M3J1GhEycgoXe703hllBP j2iOwecHkFHN2GJjAL67IH2amnp0JqrVy6FwN1fL47lOUfe3AgkjBmBUXT+r0y+e L+aILxdSiFNXn3sqpW2jQnT3r+UOCw5QdOYE8QC2VnJcm0p3bJ+OMVQ= =pzE0 -----END PGP PUBLIC KEY BLOCK-----     Verify that HPE is the issuer of the signed packages.\nReplace the PATH-TO-KEY value in the following command with the path to the signing key.\nncn-m001# rpm -qpi PATH-TO-KEY/hpe-signing-key.asc Name : gpg-pubkey Version : 9da39f44 Release : 5669d183 Architecture: (none) Install Date: Thu 25 Feb 2021 08:58:19 AM CST Group : Public Keys Size : 0 License : pubkey Signature : (none) Source RPM : (none) Build Date : Thu 10 Dec 2015 01:24:51 PM CST Build Host : localhost Relocations : (not relocatable) Packager : Hewlett Packard Enterprise Company RSA-2048-30 \u0026lt;signhp@hpe.com\u0026gt; Summary : gpg(Hewlett Packard Enterprise Company RSA-2048-30 \u0026lt;signhp@hpe.com\u0026gt;) Description : -----BEGIN PGP PUBLIC KEY BLOCK----- Version: rpm-4.11.3 (NSS-3) mQENBFZp0YMBCADNNhdrR/K7jk6iFh/D/ExEumPSdriJwDUlHY70bkEUChLyRACI QfLEmh0dGoUfFu2Uk8M/RgeGPayJUeO3jeJw/y7JJHvZENJwYjquZKTOza7GLXf6 HyCRanHrEeXeyRffhJlXLf6GvCqYVl9nSvxwSX9raotqMznLY5E1JXIqtfHLrVhJ qHQLiKulpEAHL9pOWamwZKeGbL9M/N6O3LINbHqisiC0EIcV6GIFCLSfCFMODO6C PgkJ/ECVLZEjGDFnSTT0mn5+DveqRUid/+YQejcraKlc3xRUF+qlg4ey+uz0kFzC SFUbKY68Pw6W/dFGrEhfau8A0TnMnIQ4qgLPABEBAAG0P0hld2xldHQgUGFja2Fy ZCBFbnRlcnByaXNlIENvbXBhbnkgUlNBLTIwNDgtMzAgPHNpZ25ocEBocGUuY29t PokBPQQTAQIAJwUCVmnRgwIbLwUJEswDAAYLCQgHAwIGFQgCCQoLAxYCAQIeAQIX gAAKCRDU2uHjnaOfRK5XCACRJLoMQ/nBa7Gna/96inbAHoKM6DUbNramBa1XCTeh KiTxA0bPE3kp7y143jpfOiSGAOTcU0RaCOKk6JMnJJMt60nR4UohVG2lLVtLxT0G H75jCu0nuZQJrKlMh04fJ3zHnqVuOduyUstgmMQ0qVg2lwPTV+KZeY5/eNPHzkcK 75pfos/svDRQNN2LX6qzsVWfAkEN/WdnlZJE76exvA9JsVmNtU3h3PKQTT86W4bb 1MdeDMkX9lDwMCEhClxLVU/sUfj10Kb8CO5+TFimmdqgXXY4BJJsE8STowy67t7Q zECkM4UFVpgcXFrapWW7IniC1OP0c4I+11mnHKCN15DFuQINBFZp0YMQCADo0UHN pIORPOLtaVI2W/aBpOWVlO74HZvMlWKOk+isf8pIKOujivncNZeeVPu2MTT7kOZ6 3Iwuj6B/dBz0hFXkqfzww+ibkhV1NWUx8Gk3FnGm6Ye6VZq2MbYHFMjSMbH3gJNd l76n4wOdwzC8TbLSmfIVxRyf+Uo5GhMrFy/+G28m/WO5nmH/AxKZxOp//NUVxE47 p6Dd2Rqg2IgBfQ99gudh75F/s6RHDYtV+87CsyFyKD7nJW54l/7r9jvvwhO0d89T s37j+bv81AEPYtu17uaRCcfF2B6RtPEdDslZ+J0G14TBsjp53ARh43HmH6BwQ3+4 pyB7QYWwN2ybFCqTAAMFCAC+1JtxaR7TEZsRDNy6ViHH+fHENl7+SB8GTQL7BZXB YgFEtsti+NZpkAAiJ+HXZihgcjCrHPejnlj5Su7dSkveRLHKZbVehvIbiM+LxfNv 7CdxfhLUVPkgPEpiCHGpCHjG/bKyKCL48SDPB5ClUtVu7v05dq/yu4AYaWwU1iix uH9dYQWC1J8pkZX/igHdbD/RYnPMuiil41guTNSWgjzxbOnxEVueaYFKnHdFlqz7 JpzJa10Lm9gEcGmzePVbJH0j8/+1ViwqLhbITq7Gv1S+RkNnewjLM9Vu2R/Fvpzh AUAinTEi5bYPtmVtddZQ94cOFLvh+LrETAC7v4zxvW/ciQElBBgBAgAPBQJWadGD AhsMBQkSzAMAAAoJENTa4eOdo59E6kAIAMC60HIPrr7ztUAF1vmuIdgSMDAjD7y0 UOzCm1L9fuHqeXNc/JQkKbqAv0tMjnRtrt1R13N3qy1qBeUTnG0qxwdHR0jsknHW S/1T24x03XioypowQObeh15PTD/TTAiLherzAWRNqqtf2Yh9Dy2zWLo204FQjK// Apw4IbO28hgYWvIbpFsyPG4WED3uJ7uTnkqdRkNWQl3M3J1GhEycgoXe703hllBP j2iOwecHkFHN2GJjAL67IH2amnp0JqrVy6FwN1fL47lOUfe3AgkjBmBUXT+r0y+e L+aILxdSiFNXn3sqpW2jQnT3r+UOCw5QdOYE8QC2VnJcm0p3bJ+OMVQ= =pzE0 -----END PGP PUBLIC KEY BLOCK-----   Import the signing key after validating the issuer.\nncn-m001# rpm --import hpe-singing-key.asc   Search for the signed packages using the version number from the previous step.\nncn-m001# rpm -qa --qf \u0026#39;%{NAME}-%{VERSION}-%{RELEASE} %{SIGGPG:pgpsig}\\n\u0026#39; | grep \u0026#39;9da39f44\u0026#39;   Validate the signature on an RPM.\nThe RPM in this example is csm-install-workarounds-0.1.11-20210504151148_bf748be.src.rpm.\nncn-m001# rpm -Kvv csm-install-workarounds-0.1.11-20210504151148_bf748be.src.rpm D: loading keyring from pubkeys in /var/lib/rpm/pubkeys/*.key D: couldn\u0026#39;t find any keys in /var/lib/rpm/pubkeys/*.key D: loading keyring from rpmdb D: opening db environment /var/lib/rpm cdb:0x401 D: opening db index /var/lib/rpm/Packages 0x400 mode=0x0 D: locked db index /var/lib/rpm/Packages D: opening db index /var/lib/rpm/Name 0x400 mode=0x0 D: read h# 442 Header SHA1 digest: OK (489efff35e604042709daf46fb78611fe90a75aa) D: added key gpg-pubkey-f4a80eb5-53a7ff4b to keyring D: read h# 493 Header SHA1 digest: OK (29ff3649c04c90eb654c1b3b8938e4940ff1fbbd) D: added key gpg-pubkey-4255bf0c-5ec2e252 to keyring D: read h# 494 Header SHA1 digest: OK (e934d6983ae30a7e12c9c1fb6e86abb1c76c69d3) D: added key gpg-pubkey-9da39f44-5669d183 to keyring D: read h# 496 Header SHA1 digest: OK (a93ccf43d5479ff84dc896a576d6f329fd7d723a) D: added key gpg-pubkey-e09422b3-57744e9e to keyring D: read h# 497 Header SHA1 digest: OK (019de42112ea85bfa979968273aafeca8d457936) D: added key gpg-pubkey-fd4bf915-5f573efe to keyring D: Using legacy gpg-pubkey(s) from rpmdb D: Expected size: 36575 = lead(96)+sigs(5012)+pad(4)+data(31463) D: Actual size: 36575 csm-install-workarounds-0.1.11-20210504151148_bf748be.src.rpm: Header V4 RSA/SHA256 Signature, key ID 9da39f44: OK Header SHA1 digest: OK (87c62923c905424eaddac56c5dda7f3b6421d30d) V4 RSA/SHA256 Signature, key ID 9da39f44: OK MD5 digest: OK (130e13f11aaca834408665a93b61a8e4) D: closed db index /var/lib/rpm/Name D: closed db index /var/lib/rpm/Packages D: closed db environment /var/lib/rpm   "
},
{
	"uri": "/docs-csm/en-11/operations/csm_product_management/configure_non-compute_nodes_with_cfs/",
	"title": "Configure Non-compute Nodes With CFS",
	"tags": [],
	"description": "",
	"content": "Configure Non-Compute Nodes with CFS Non-compute node (NCN) personalization applies post-boot configuration to the HPE Cray EX management nodes. Several HPE Cray EX product environments outside of CSM require NCN personalization to function. Consult the manual for each product to configure them on NCNs by referring to the 1.5 HPE Cray EX System Software Getting Started Guide S-8000 on the HPE Customer Support Center.\nThis procedure defines the NCN personalization process for the CSM product using the Configuration Framework Service (CFS).\nSet Up Passwordless SSH This procedure should be run during CSM installation and afterwards whenever the SSH keys need to be changed per site requirements.\nThe goal of passwordless SSH is to enable an easy way for interactive passwordless SSH from and between CSM product environments (management nodes) to downstream managed product environments (COS, UAN, etc), without requiring each downstream environment to create and apply individual changes to NCNs. Passwordless SSH from downstream nodes into CSM management nodes is not intended or supported.\nPasswordless SSH keypairs for the Cray System Management (CSM) are created automatically and maintained with a Kubernetes deployment and staged into Kubernetes secrets (csm-private-key) and ConfigMaps (csm-public-key) in the services namespace. Administrators can use these provided keys, provide their own keys, or use their own solution for authentication.\nThe management of keys on NCNs is achieved by the trust-csm-ssh-keys and passwordless-ssh Ansible roles in the CSM configuration management repository. The SSH keypair is applied to management nodes via NCN personalization.\nChoose one option from the following sections to enable or disable passwordless SSH on NCNs.\nOption 1: Use the CSM-provided SSH Keys The default CSM Ansible plays are already configured to enable Passwordless SSH by default. No further action is necessary before running NCN personalization with CFS.\nOption 2: Provide Custom SSH Keys Administrators may elect to replace the CSM-provided keys with their own custom keys.\n  Replace the private key half:\nncn# kubectl get secret -n services csm-private-key -o json | jq --arg value \u0026#34;$(cat ~/.ssh/id_rsa | base64)\u0026#34; \u0026#39;.data[\u0026#34;value\u0026#34;]=$value\u0026#39; | kubectl apply -f - ~/.ssh/id_rsa is a local file containing a valid SSH private key.\n  Replace the public key half:\nncn# kubectl delete configmap -n services csm-public-key \u0026amp;\u0026amp; \\  cat ~/.ssh/id_rsa.pub | \\  base64 \u0026gt; ./value \u0026amp;\u0026amp; kubectl create configmap --from-file \\  value csm-public-key --namespace services \u0026amp;\u0026amp; rm ./value ~/.ssh/id_rsa.pub is a local file containing a valid public key intended for CSM and downstream products.\n  Passwordless SSH with the provided keys will be setup once NCN personalization runs on the NCNs.\nOption 3: Disable CSM-provided Passwordless SSH Local site security requirements may preclude use of passwordless SSH access. If this is the case, remove or comment out the invocation of the trust-csm-public-keys role in Ansible plays in the configuration repositories of the environments where it is configured. By default, the HPE Cray Operating System (COS) and User Access Node (UAN) configurations enable passwordless SSH. Refer to the following in the documentation for each product stream to change the default configuration:\n COS: Refer to the VCS Configuration section in the Install or Upgrade COS procedure. UAN: Refer to Create UAN Boot Images and UAN Ansible Roles procedures.  Modifying Ansible plays in a configuration repository will require a new commit and subsequent update of the configuration layer associated with the product.\n NOTE: CFS itself does not use the CSM-provided (or user-supplied) SSH keys to make connections between nodes. CFS will continue to function if passwordless SSH is disabled between CSM and other product environments.\n Restore CSM-provided SSH Keys  Use this procedure if switching from custom keys to the default CSM SSH keys only, otherwise it can be skipped.\n The csm-ssh-keys Kubernetes deployment provided by CSM periodically checks the ConfigMap and secret containing the key information. If these entries do not exist, it will recreate them from the default CSM keys. In this case, deleting the associated ConfigMap and secrets will republish them with the default CSM-provided keys.\n Delete the csm-private-key Kubernetes secret. ncn# kubectl delete secret -n services csm-private-key  Delete the csm-public-key Kubernetes configmap. ncn# kubectl delete configmap -n services csm-public-key   Set the Root Password This procedure should be run during CSM installation and afterwards whenever the password needs to be changed per site requirements.\nThe root password is managed on NCNs by using the csm.password Ansible role located in the CSM configuration management repository. Root passwords are set and managed in Vault.\n Set the password in Vault by following steps 1-3 in the Update NCN Passwords procedure. Run NCN personalization.  Run NCN Personalization After completing the previous procedures, apply the configuration to the NCNs by running NCN personalization with CFS.\nPrior to running NCN personalization, gather the following information:\n HTTP clone URL for the configuration repository in VCS Path to the Ansible play to run in the repository Commit ID in the repository for CFS to pull and run on the nodes.     Field Value Description     cloneUrl https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git CSM configuration repo   commit Example: 5081c1ecea56002df41218ee39f6030c3eebdf27 CSM configuration commit hash   name Example: csm-ncn-\u0026lt;version\u0026gt; CSM Configuration layer name   playbook site.yml Default site-wide Ansible playbook for CSM      Retrieve the commit in the repository to use for configuration. If changes have been made to the default branch that was imported during a CSM installation or upgrade, use the commit containing the changes.\n  If no changes have been made, the latest commit on the default branch for this version of CSM should be used. Find the commit in the cray-product-catalog for the current version of CSM. For example:\nncn# kubectl -n services get cm cray-product-catalog -o jsonpath=\u0026#39;{.data.csm}\u0026#39; 1.0.0: configuration: clone_url: https://vcs.SYSTEM_DOMAIN_NAME/vcs/cray/csm-config-management.git commit: 43ecfa8236bed625b54325ebb70916f55884b3a4 import_branch: cray/csm/1.6.12 import_date: 2021-07-28 03:26:01.869501 ssh_url: git@vcs.SYSTEM_DOMAIN_NAME:cray/csm-config-management.git ... The commit will be different for each system and version of CSM. For this example, it is:\n 43ecfa8236bed625b54325ebb70916f55884b3a4    Craft a new configuration layer entry for the new CSM:\n { \u0026quot;name\u0026quot;: \u0026quot;csm-ncn-\u0026lt;version\u0026gt;\u0026quot;, \u0026quot;cloneUrl\u0026quot;: \u0026quot;https://api-gw-service-nmn.local/vcs/cray/csm-config-management.git\u0026quot;, \u0026quot;playbook\u0026quot;: \u0026quot;site.yml\u0026quot;, \u0026quot;commit\u0026quot;: \u0026quot;\u0026lt;retrieved git commit\u0026gt;\u0026quot; }    (Install Only) Follow the procedure in Run NCN Personalization, adding a CSM configuration layer to the NCN personalization using the JSON from step 3.\n  (Upgrade Only) Follow the procedure in Run NCN Personalization, replacing the existing CSM configuration layer to the NCN personalization using the JSON from the step 3.\n   NOTE: The CSM configuration layer MUST be the first layer in the NCN personalization CFS configuration.\n "
},
{
	"uri": "/docs-csm/en-11/operations/csm_product_management/perform_ncn_personalization/",
	"title": "Perform NCN Personalization",
	"tags": [],
	"description": "",
	"content": "Perform NCN Personalization NCN personalization is the process of applying product-specific configuration to NCNs post-boot.\nPrerequisites Prior to running this procedure, gather the following information required by CFS to create a configuration layer:\n HTTP clone URL for the configuration repository in VCS Path to the Ansible play to run in the repository Commit ID in the repository for CFS to pull and run on the nodes  Products may supply multiple plays to run, in which case multiple configuration layers must be created. Consult the manual for each product to configure them on NCNs by referring to the 1.5 HPE Cray EX System Software Getting Started Guide S-8000 on the HPE Customer Support Center.\nProcedure: Perform NCN Personalization Determine if NCN Personalization CFS Configuration Exists If upgrading a product to a new version, an NCN personalization configuration in CFS should already exist. By default, the configuration is named ncn-personalization. If the default name is not used, substitute that name in the steps below.\n Determine if a configuration already exists. ncn# cray cfs configurations describe ncn-personalization --format json \u0026gt; ncn-personalization.json   If the configuration exists, the ncn-personalization.json file will be created and populated with previously defined configuration layers. If it does not exist, the file will be created empty and the command will respond with an error. This error can be ignored.\nAdd Layer(s) to the CFS Configuration CFS executes configuration layers in order. Refer to the 1.5 HPE Cray EX System Software Getting Started Guide S-8000 on the HPE Customer Support Center at https://www.hpe.com/support/ex-gsg to determine if the configuration layer requires special placement in the layer list.\n NOTE: The CSM configuration layer MUST be the first layer in the NCN personalization CFS configuration.\n  Add a configuration layer to the ncn-personalization.json file. Follow the appropriate step based on if an NCN personalization CFS configuration exists:  If the ncn-personalization.json file is empty, overwrite the file with the configuration layer(s) information gathered from the product that is configuring the NCNs. Use the sample file with a single layer as a template. If a CFS configuration exists with one or more layers, add (or replace) the corresponding layer entry(ies) with the configuration layer information gathered for this specific product. For example: ncn# cat ncn-personalization.json { \u0026#34;layers\u0026#34;: [ # ... { \u0026#34;name\u0026#34;: \u0026#34;\u0026lt;product-release-etc\u0026gt;\u0026#34;, \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/\u0026lt;product\u0026gt;-config-management.git\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;site.yml\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;\u0026lt;git commit\u0026gt;\u0026#34; }, # ...  ] }     Create/Update the NCN Personalization CFS Configuration Layer  Upload the configuration file to CFS to update or create the ncn-personalization CFS configuration. ncn# cray cfs configurations update ncn-personalization --file ncn-personalization.json --format json { \u0026#34;lastUpdated\u0026#34;: \u0026#34;2021-07-28T03:26:01Z\u0026#34;, \u0026#34;layers\u0026#34;: [ { ... layer information here ... }, ], \u0026#34;name\u0026#34;: \u0026#34;ncn-personalization\u0026#34; }   Set the Desired Configuration on NCNs   Update the desired configuration for all NCNs.\nncn# for xname in $(cray hsm state components list --role Management--format json | jq -r .Components[].ID) do cray cfs components update --desired-config ncn-personalization --enabled true --format json $xname done After this command is issued, the CFS Batcher service will dispatch a CFS session to configure the NCNs. Since the NCN is now managed by CFS by setting a desired configuration, the same will happen every time the NCN boots.\n  Query the status of the NCN Personalization process. The status will be pending while the node is being configured by CFS, and will change to configured when the configuration has completed.\nncn# export CRAY_FORMAT=json ncn# for xname in $(cray hsm state components list --role Management | jq -r .Components[].ID) do cray cfs components describe $xname | jq -r \u0026#39; .id+\u0026#34; status=\u0026#34;+.configurationStatus\u0026#39; done x3000c0s17b0n0 status=configured x3000c0s19b0n0 status=pending x3000c0s21b0n0 status=configured ... The NCN personalization step is complete and the NCNs are now configured as specified in the ncn-personalization configuration layers when each node\u0026rsquo;s status is configured.\nSee Configuration Management of System Components for more information on setting desired configuration on specific nodes using CFS.\n  Procedure: Re-Run NCN Personalization If no changes have been made to the configuration layers (such as a new layer, different playbook, or new commit made), but NCN personalization needs to be run again, CFS can re-run NCN personalization on specific nodes.\nRe-run the configuration for an NCN by clearing the state of the node. Clearing the node state will cause CFS to reconfigure the node, so long as the desired configuration was set previously.\n  Clear the state of the node using CFS.\nReplace the XNAME value in the following command with the xname of the node being reconfigured.\nncn# cray cfs components update --state \u0026#39;[]\u0026#39; \u0026lt;XNAME\u0026gt;   Clear the error count for the node in CFS.\nReplace the XNAME value in the following command with the xname of the node being reconfigured.\nncn# cray cfs components update --error-count 0 \u0026lt;XNAME\u0026gt;   (Optional) To re-run NCN personalization on all NCNs at once, use the following loop:\nncn# export CRAY_FORMAT=json ncn# for xname in $(cray hsm state components list --role Management | jq -r .Components[].ID) do cray cfs components update --error-count 0 $xname cray cfs components update --state \u0026#39;[]\u0026#39; $xname done   "
},
{
	"uri": "/docs-csm/en-11/operations/csm_product_management/post_install_customizations/",
	"title": "Post-install Customizations",
	"tags": [],
	"description": "",
	"content": "Post-Install Customizations Post-install customizations may be needed as systems scale. These customizations also need to persist across future installs or upgrades. Not all resources can be customized post-install; common scenarios are documented in the following sections.\nThe following is a guide for determining where issues may exist, how to adjust the resources, and how to ensure the changes will persist. Different values may be be needed for systems as they scale.\nKubectl Events OOMKilled Check to see if there are any recent out of memory events.\n  Check kubectl events to see if there are any recent out of memory events.\nncn# kubectl get event -A | grep OOM   Use the Grafana \u0026ldquo;Kubernetes/Compute Resources/Pod\u0026rdquo; Dashboard to view the memory utilization graphs over time for any pod that has been OOMKilled.\n  Prometheus CPUThrottlingHigh Alerts Check Prometheus for recent CPUThrottlingHigh Alerts.\n  From Prometheus (https://prometheus.SYSTEM-NAME_DOMAIN-NAME/), select the Alert tab and scroll down to the alert for CPUThrottlingHigh.\n  Use the Grafana \u0026ldquo;Kubernetes/Compute Resources/Pod\u0026rdquo; Dashboard to view the throttling graphs over time for any pod that is alerting.\n  Grafana Kubernetes/Compute Resources/Pod Dashboard Use Grafana to investigate and analyze CPU Throttling and/or Memory Usage.\n  From Grafana (https://grafana.SYSTEM-NAME_DOMAIN-NAME/) and the Home Dashboard, select the \u0026ldquo;Kubernetes/Compute Resources/Pod\u0026rdquo; Dashboard.\n  Select the datasource, namespace, and pod based on the pod being examined. For example:\ndatasource: default namespace: sysmgmt-health pod: prometheus-cray-sysmgmt-health-promet-prometheus-0   For CPU Throttling   Select the CPU Throttling drop-down to see the CPU Throttling graph for the pod during the selected time (from the top right), and select the container (from the legends under the x axis).\n  The presence of CPU throttling does not always indicate a problem, but if a service is being slow or experiencing latency issues, reviewing the graph and adjusting the resources.limits.cpu can be beneficial. If the pod is being throttled at or near 100% for any period of time, then adjustments are likely needed. If the service\u0026rsquo;s response time is critical, then adjusting the pod\u0026rsquo;s resources to greatly reduce or eliminate any CPU throttling may be required.\n  The resources.requests.cpu are used by the Kubernetes scheduler to decide which node to place the pod on and do not impact CPU Throttling. The resources.limits.cpu can never be lower than the resources.requests.cpu.\n  For Memory Usage   Select the Memory Usage drop-down to see the Memory Usage graph for the pod during the selected time (from the top right), and select the container (from the legends under the x axis).\n  From the Memory Usage graph for the container, determine the steady state memory usage. This is where the resources.requests.memory should be minimally set. But more importantly, determine the spike usage for the container and set the resources.limits.memory based on the spike values with some additional headroom.\n  Common Customization Scenarios  Prerequisite Prometheus Pod is OOMKilled or CPU Throttled Postgres Pods are OOMKilled or CPU Throttled Scale cray-bss Service Postgres PVC Resize  Prerequisite In order to apply post-install customizations to a system, the affected Helm chart must exist on the system so that the same chart version can be redeployed with the desired customizations.\nThis example unpacks the the csm-1.0.0 tarball under /root and lists the Helm charts that are now on your system. Set PATH_TO_RELEASE to the release directory where the Helm directory exists. PATH_TO_RELEASE will be used below when deploying a customization.\nThese unpacked files can be safely removed after the customizations are deployed through loftsman ship in the examples below.\n## This example assumes the csm-1.0.0 release is currently running and the csm-1.0.0.tar.gz has been pulled down under /root ncn# cd /root ncn# tar -xzf csm-1.0.0.tar.gz ncn# rm csm-1.0.0.tar.gz ncn# PATH_TO_RELEASE=/root/csm-1.0.0 ncn# ls $PATH_TO_RELEASE/helm Prometheus Pod is OOMKilled or CPU Throttled Update resources associated with Prometheus in the sysmgmt-health namespace. This example is based on what was needed for a system with 4000 compute nodes. Trial and error may be needed to determine what is best for a given system at scale.\n  Get the current cached customizations.\nncn# kubectl get secrets -n loftsman site-init -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | base64 -d \u0026gt; customizations.yaml   Get the current cached platform manifest.\nncn# kubectl get cm -n loftsman loftsman-platform -o jsonpath=\u0026#39;{.data.manifest\\.yaml}\u0026#39; \u0026gt; platform.yaml   Edit the customizations as desired by adding or updating spec.kubernetes.services.cray-sysmgmt-health.prometheus-operator.prometheus.prometheusSpec.resources.\nncn# yq write -i customizations.yaml \u0026#39;spec.kubernetes.services.cray-sysmgmt-health.prometheus-operator.prometheus.prometheusSpec.resources.requests.cpu\u0026#39; --style=double \u0026#39;2\u0026#39; ncn# yq write -i customizations.yaml \u0026#39;spec.kubernetes.services.cray-sysmgmt-health.prometheus-operator.prometheus.prometheusSpec.resources.requests.memory\u0026#39; \u0026#39;15Gi\u0026#39; ncn# yq write -i customizations.yaml \u0026#39;spec.kubernetes.services.cray-sysmgmt-health.prometheus-operator.prometheus.prometheusSpec.resources.limits.cpu\u0026#39; --style=double \u0026#39;6\u0026#39; ncn# yq write -i customizations.yaml \u0026#39;spec.kubernetes.services.cray-sysmgmt-health.prometheus-operator.prometheus.prometheusSpec.resources.limits.memory\u0026#39; \u0026#39;30Gi\u0026#39;   Check that the customization file has been updated.\nncn# yq read customizations.yaml \u0026#39;spec.kubernetes.services.cray-sysmgmt-health.prometheus-operator.prometheus.prometheusSpec.resources\u0026#39; requests: cpu: \u0026#34;3\u0026#34; memory: 15Gi limits: cpu: \u0026#34;6\u0026#34; memory: 30Gi   Edit the platform.yaml to only include the cray-sysmgmt-health chart and all its current data. (The resources specified above will be updated in the next step and the version may differ, because this is an example).\napiVersion: manifests/v1beta1 metadata: name: platform spec: charts: - name: cray-sysmgmt-health namespace: sysmgmt-health values: . . . version: 0.12.0   Generate the manifest that will be used to redeploy the chart with the modified resources.\nncn# manifestgen -c customizations.yaml -i platform.yaml -o manifest.yaml   Check that the manifest file contains the desired resource settings.\nncn# yq read manifest.yaml \u0026#39;spec.charts.(name==cray-sysmgmt-health).values.prometheus-operator.prometheus.prometheusSpec.resources\u0026#39; requests: cpu: \u0026#34;3\u0026#34; memory: 15Gi limits: cpu: \u0026#34;6\u0026#34; memory: 30Gi   Redeploy the same chart version but with the desired resource settings.\nncn# loftsman ship charts-path ${PATH_TO_RELEASE}/helm --manifest-path ${PWD}/manifest.yaml   Verify the pod restarts and that the desired resources have been applied.\n# Watch the pod prometheus-cray-sysmgmt-health-promet-prometheus-0 restart ncn# watch \u0026#34;kubectl get pods -n sysmgmt-health -l prometheus=cray-sysmgmt-health-promet-prometheus\u0026#34; # It may take 10m for the prometheus-cray-sysmgmt-health-promet-prometheus-0 pod to Terminate - it can be forced deleted if it remains in Terminating state ncn# kubectl delete pod prometheus-cray-sysmgmt-health-promet-prometheus-0 --force --grace-period=0 -n sysmgmt-health # Verify that the resource changes are in place ncn# kubectl get pod prometheus-cray-sysmgmt-health-promet-prometheus-0 -n sysmgmt-health -o json | jq -r \u0026#39;.spec.containers[] | select(.name == \u0026#34;prometheus\u0026#34;).resources\u0026#39;   This step is critical. Store the modified customizations.yaml in the site-init repository in the customer-managed location. If not done, these changes will not persist in future installs or upgrades.\n  Postgres Pods are OOMKilled or CPU Throttled Update resources associated with spire-postgres in the spire namespace. This example is based on what was needed for a system with 4000 compute nodes. Trial and error may be needed to determine what is best for a given system at scale.\nA similar flow can be used to update the resources for cray-sls-postgres, cray-smd-postgres, or gitea-vcs-postgres. Refer to the note at the end of this section for more details.\n  Get the current cached customizations.\nncn# kubectl get secrets -n loftsman site-init -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | base64 -d \u0026gt; customizations.yaml   Get the current cached sysmgmt manifest.\nncn# kubectl get cm -n loftsman loftsman-sysmgmt -o jsonpath=\u0026#39;{.data.manifest\\.yaml}\u0026#39; \u0026gt; sysmgmt.yaml   Edit the customizations as desired by adding or updating spec.kubernetes.services.spire.cray-service.sqlCluster.resources.\nncn# yq write -i customizations.yaml \u0026#39;spec.kubernetes.services.spire.cray-service.sqlCluster.resources.requests.cpu\u0026#39; --style=double \u0026#39;4\u0026#39; ncn# yq write -i customizations.yaml \u0026#39;spec.kubernetes.services.spire.cray-service.sqlCluster.resources.requests.memory\u0026#39; \u0026#39;4Gi\u0026#39; ncn# yq write -i customizations.yaml \u0026#39;spec.kubernetes.services.spire.cray-service.sqlCluster.resources.limits.cpu\u0026#39; --style=double \u0026#39;8\u0026#39; ncn# yq write -i customizations.yaml \u0026#39;spec.kubernetes.services.spire.cray-service.sqlCluster.resources.limits.memory\u0026#39; \u0026#39;8Gi\u0026#39;   Check that the customization file has been updated.\nncn# yq read customizations.yaml \u0026#39;spec.kubernetes.services.spire.cray-service.sqlCluster.resources\u0026#39; requests: cpu: \u0026#34;4\u0026#34; memory: 4Gi limits: cpu: \u0026#34;8\u0026#34; memory: 8Gi   Edit the sysmgmt.yaml to only include the spire chart and all its current data. (The resources specified above will be updated in the next step and the version may differ, because this is an example).\napiVersion: manifests/v1beta1 metadata: name: platform spec: charts: - name: spire namespace: spire values: . . . version: 0.9.1   Generate the manifest that will be used to redeploy the chart with the modified resources.\nncn# manifestgen -c customizations.yaml -i sysmgmt.yaml -o manifest.yaml   Check that the manifest file contains the desired resource settings.\nncn# yq read manifest.yaml \u0026#39;spec.charts.(name==spire).values.cray-service.sqlCluster.resources\u0026#39; requests: cpu: \u0026#34;4\u0026#34; memory: 4Gi limits: cpu: \u0026#34;8\u0026#34; memory: 8Gi   Redeploy the same chart version but with the desired resource settings.\nncn# loftsman ship charts-path ${PATH_TO_RELEASE}/helm --manifest-path ${PWD}/manifest.yaml   Verify the pods restart and that the desired resources have been applied.\nncn# watch \u0026#34;kubectl get pods -n spire -l application=spilo,cluster-name=spire-postgres\u0026#34; ncn# kubectl get pod spire-postgres-0 -n spire -o json | jq -r \u0026#39;.spec.containers[] | select(.name == \u0026#34;postgres\u0026#34;).resources\u0026#39; { \u0026#34;limits\u0026#34;: { \u0026#34;cpu\u0026#34;: \u0026#34;8\u0026#34;, \u0026#34;memory\u0026#34;: \u0026#34;8Gi\u0026#34; }, \u0026#34;requests\u0026#34;: { \u0026#34;cpu\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;memory\u0026#34;: \u0026#34;4Gi\u0026#34; } }   This step is critical. Store the modified customizations.yaml in the site-init repository in the customer-managed location. If not done, these changes will not persist in future installs or upgrades.\n  IMPORTANT: If cray-sls-postgres, cray-smd-postgres, or gitea-vcs-postgres resources need to be adjusted, the same procedure as above can be used with the following changes:\n  cray-sls-postgres\nGet the current cached manifest configmap from: loftsman-core-services Resource path: spec.kubernetes.services.cray-hms-sls.cray-service.sqlCluster.resources\n  cray-smd-postgres\nGet the current cached manifest configmap from: loftsman-core-services Resource path: spec.kubernetes.services.cray-hms-smd.cray-service.sqlCluster.resources\n  gitea-vcs-postgres\nGet the current cached manifest configmap from: loftsman-sysmgmt Resource path: spec.kubernetes.services.gitea.cray-service.sqlCluster.resources\n  Scale cray-bss Service Scale the replica count associated with the cray-bss service in the services namespace. This example is based on what was needed for a system with 4000 compute nodes. Trial and error may be needed to determine what is best for a given system at scale.\n  Get the current cached customizations.\nncn# kubectl get secrets -n loftsman site-init -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | base64 -d \u0026gt; customizations.yaml   Get the current cached sysmgmt manifest.\nncn# kubectl get cm -n loftsman loftsman-sysmgmt -o jsonpath=\u0026#39;{.data.manifest\\.yaml}\u0026#39; \u0026gt; sysmgmt.yaml   Edit the customizations as desired by adding or updating spec.kubernetes.services.cray-hms-bss.cray-service.replicaCount.\nncn# yq write -i customizations.yaml \u0026#39;spec.kubernetes.services.cray-hms-bss.cray-service.replicaCount\u0026#39; \u0026#39;5\u0026#39;   Check that the customization file has been updated.\nncn# yq read customizations.yaml \u0026#39;spec.kubernetes.services.cray-hms-bss.cray-service.replicaCount\u0026#39; 5   Edit the sysmgmt.yaml to only include the cray-hms-bss chart and all its current data. (The replicaCount specified above will be updated in the next step and the version may differ, because this is an example).\napiVersion: manifests/v1beta1 metadata: name: sysmgmt spec: charts: - name: cray-hms-bss namespace: services values: . . . version: 1.5.8   Generate the manifest that will be used to redeploy the chart with the modified resources.\nncn# manifestgen -c customizations.yaml -i sysmgmt.yaml -o manifest.yaml   Check that the manifest file contains the desired resource settings.\nncn# yq read manifest.yaml \u0026#39;spec.charts.(name==cray-hms-bss).values.cray-service.replicaCount\u0026#39; 5   Redeploy the same chart version but with the desired resource settings.\nncn# loftsman ship charts-path ${PATH_TO_RELEASE}/helm --manifest-path ${PWD}/manifest.yaml   Verify the cray-bss pods scale.\n# Watch the `cray-bss` pods scale to 5 and each reach a 2/2 ready state ncn# watch \u0026#34;kubectl get pods -l app.kubernetes.io/instance=cray-hms-bss -n services\u0026#34; NAME READY STATUS RESTARTS AGE cray-bss-fccbc9f7d-7jw2q 2/2 Running 0 82m cray-bss-fccbc9f7d-l524g 2/2 Running 0 93s cray-bss-fccbc9f7d-qwzst 2/2 Running 0 93s cray-bss-fccbc9f7d-sw48b 2/2 Running 0 82m cray-bss-fccbc9f7d-xr26l 2/2 Running 0 82m # Verify that the replicas change is present in the Kubernetes `cray-bss` deployment ncn# kubectl get deployment cray-bss -n services -o json | jq -r \u0026#39;.spec.replicas\u0026#39; 5   This step is critical. Store the modified customizations.yaml in the site-init repository in the customer-managed location. If not done, these changes will not persist in future installs or upgrades.\n  Postgres PVC Resize Increase the PVC volume size associated with cray-smd-postgres cluster in the services namespace. This example is based on what was needed for a system with 4000 compute nodes. Trial and error may be needed to determine what is best for a given system at scale. The PVC size can only ever be increased.\nA similar flow can be used to update the volume size for cray-sls-postgres, gitea-vcs-postgres, or spire-postgres. Refer to the note at the end of this section for more details.\n  Get the current cached customizations.\nncn# kubectl get secrets -n loftsman site-init -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | base64 -d \u0026gt; customizations.yaml   Get the current cached core-services manifest.\nncn# kubectl get cm -n loftsman loftsman-core-services -o jsonpath=\u0026#39;{.data.manifest\\.yaml}\u0026#39; \u0026gt; core-services.yaml   Edit the customizations as desired by adding or updating spec.kubernetes.services.cray-hms-smd.cray-service.sqlCluster.volumeSize.\nncn# yq write -i customizations.yaml \u0026#39;spec.kubernetes.services.cray-hms-smd.cray-service.sqlCluster.volumeSize\u0026#39; \u0026#39;100Gi\u0026#39;   Check that the customization file has been updated.\nncn# yq read customizations.yaml \u0026#39;spec.kubernetes.services.cray-hms-smd.cray-service.sqlCluster.volumeSize\u0026#39; 100Gi   Edit the core-services.yaml to only include the cray-hms-smd chart and all its current data. (The volumeSize specified above will be updated in the next step and the version may differ, because this is an example).\napiVersion: manifests/v1beta1 metadata: name: core-services spec: charts: - name: cray-hms-smd namespace: service values: . . . version: 1.26.20   Generate the manifest that will be used to redeploy the chart with the modified volume size.\nncn# manifestgen -c customizations.yaml -i core-services.yaml -o manifest.yaml   Check that the manifest file contains the desired volume size setting.\nncn# yq read manifest.yaml \u0026#39;spec.charts.(name==cray-hms-smd).values.cray-service.sqlCluster.volumeSize\u0026#39; 100Gi   Redeploy the same chart version but with the desired volume size setting.\nncn# loftsman ship charts-path ${PATH_TO_RELEASE}/helm --manifest-path ${PWD}/manifest.yaml   Verify that the increased volume size has been applied.\nncn# watch \u0026#34;kubectl get postgresql cray-smd-postgres -n services\u0026#34; NAME TEAM VERSION PODS VOLUME CPU-REQUEST MEMORY-REQUEST AGE STATUS cray-smd-postgres cray-smd 11 3 100Gi 500m 8Gi 45m Running   If the status on the above command is SyncFailed instead of Running, refer to Case 1 in the SyncFailed section of Troubleshoot Postgres Database. At this point the Postgres cluster is healthy, but additional steps are required to complete the resize of the Postgres PVCs.\n  This step is critical. Store the modified customizations.yaml in the site-init repository in the customer-managed location. If not done, these changes will not persist in future installs or upgrades.\n  IMPORTANT: If cray-sls-postgres, gitea-vcs-postgres, or spire-postgres volumeSize need to be adjusted, the same procedure as above can be used with the following changes:\n  cray-sls-postgres\nGet the current cached manifest configmap from: loftsman-core-services Resource path: spec.kubernetes.services.cray-hms-sls.cray-service.sqlCluster.volumeSize\n  gitea-vcs-postgres\nGet the current cached manifest configmap from: loftsman-sysmgmt Resource path: spec.kubernetes.services.gitea.cray-service.sqlCluster.volumeSize\n  spire-postgres\nGet the current cached manifest configmap from: loftsman-sysmgmt Resource path: spec.kubernetes.services.spire.cray-service.sqlCluster.volumeSize\n  References To make changes that will not persist across installs or upgrades, see the following references. These procedures will also help to verify and eliminate any issues in the short term. As other resource customizations are needed, contact support to request the feature.\n Reference Determine if Pods are Hitting Resource Limits Reference Increase Pod Resource Limits  "
},
{
	"uri": "/docs-csm/en-11/operations/csm_product_management/remove_artifacts_from_product_installations/",
	"title": "Remove Artifacts From Product Installations",
	"tags": [],
	"description": "",
	"content": "Remove Artifacts from Product Installations Remove product artifacts that were imported from various Cray products. These instructions provide guidance for removing Image Management Service (IMS) images, IMS recipes, and Git repositories present in the Cray Product Catalog from the system.\nThe examples in this procedure show how to remove the product artifacts for the Cray System Management (CSM) product.\nWARNING: If individual Cray products have removal procedures, those instructions supersede this procedure.\nProcedure   View the imported artifacts by printing them from the Cray Product Catalog ConfigMap.\nncn-m001# kubectl get cm cray-product-catalog -n services -o json | jq -r .data.csm 1.0.0: configuration: clone_url: https://vcs.SYSTEM_DOMAIN_NAME/vcs/cray/csm-config-management.git commit: 123264ba75c809c0db7742ea83ff57f713bc1562 import_branch: cray/csm/1.4.5 import_date: 2021-03-12 15:12:49.938936 ssh_url: git@vcs.SYSTEM_DOMAIN_NAME:cray/csm-config-management.git images: cray-shasta-csm-sles15sp1-barebones.x86_64-shasta-1.4: id: 4871cb4a-e055-4131-a228-c0a26f0903cd recipes: cray-shasta-csm-sles15sp1-barebones.x86_64-shasta-1.4: id: 5f5a74e0-108e-4159-9699-47dd2a952205   Remove the imported IMS images using the ID of each image in the images mapping.\nThe example in step 1 includes one image with the id = 4871cb4a-e055-4131-a228-c0a26f0903cd value. Remove the image with the following command:\nncn-m001# cray ims images delete 4871cb4a-e055-4131-a228-c0a26f0903cd   Remove the imported IMS recipes using the ID of each recipe in the recipes mapping.\nThe example in step 1 includes one recipe with the id = 5f5a74e0-108e-4159-9699-47dd2a952205 value. Remove the image with the following command:\nncn-m001# cray ims recipes delete 5f5a74e0-108e-4159-9699-47dd2a952205   Remove the Gitea repositories or branches.\nTo delete a Git branch as specified in the product catalog, follow the external instructions to delete Git remote branches. The branch name is located in the import_branch field.\nIf only one version of the product exists (as in the CSM example), the user can remove the entire repository instead of a single branch. Gitea repositories can be removed via the Gitea web interface or via the Gitea REST API.\nGitea web interface:\n1. Log in to Gitea as the *crayvcs* user. 1. From the dashboard, select the repository to delete based on the name of the repository in the *clone_url* field of the product catalog. 2. Click on \u0026quot;Settings\u0026quot; and scroll to the bottom of the page to the \u0026quot;Danger Zone\u0026quot; section. Follow the instructions to delete the repository.  Gitea REST API:\nRun the following commands on a CSM Kubernetes master or worker node, replacing the name of the repository in the second command.\nncn-m001# VCSPWD=$(kubectl get secret -n services vcs-user-credentials \\ --template={{.data.vcs_password}} | base64 --decode) ncn-m001# curl -X DELETE -u crayvcs:${VCSPWD} \\ https://api-gw-service-nmn.local/vcs/api/v1/repos/cray/{name of repository}   Update the product catalog.\nOnce the images, recipes, and repositories/branches have been removed from the system, update the product catalog to remove the references to them. This is done by editing the cray-product-catalog Kubernetes ConfigMap.\nncn-m001# kubectl edit configmap -n services cray-product-catalog In the editor, delete the entries for the artifacts that were deleted on the system for the specific version of the product. In this example, all artifacts were deleted and only a single product version exists, so the entire entry in the product catalog for the CSM product can be deleted. Save the changes and exit the editor to persist the changes in the ConfigMap.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/access_livecd_usb_device_after_reboot/",
	"title": "Accessing Livecd Usb Device After Reboot",
	"tags": [],
	"description": "",
	"content": "Accessing LiveCD USB Device After Reboot This is an procedure that only applies to the LiveCD USB device after the PIT node has been rebooted.\n USB ONLY If the installation above was done from a Remote ISO.\n After deploying the LiveCD\u0026rsquo;s NCN, the LiveCD USB itself is unharmed and available to an administrator.\nProcedure   Mount and view the USB device.\nncn-m001# mkdir -pv /mnt/{cow,pitdata} ncn-m001# mount -vL cow /mnt/cow ncn-m001# mount -vL PITDATA /mnt/pitdata ncn-m001# ls -ld /mnt/cow/rw/* Example output:\ndrwxr-xr-x 2 root root 4096 Jan 28 15:47 /mnt/cow/rw/boot drwxr-xr-x 8 root root 4096 Jan 29 07:25 /mnt/cow/rw/etc drwxr-xr-x 3 root root 4096 Feb 5 04:02 /mnt/cow/rw/mnt drwxr-xr-x 3 root root 4096 Jan 28 15:49 /mnt/cow/rw/opt drwx------ 10 root root 4096 Feb 5 03:59 /mnt/cow/rw/root drwxrwxrwt 13 root root 4096 Feb 5 04:03 /mnt/cow/rw/tmp drwxr-xr-x 7 root root 4096 Jan 28 15:40 /mnt/cow/rw/usr drwxr-xr-x 7 root root 4096 Jan 28 15:47 /mnt/cow/rw/var   Look at the contents of /mnt/pitdata.\nncn-m001# ls -ld /mnt/pitdata/* Example output:\ndrwxr-xr-x 2 root root 4096 Feb 3 04:32 /mnt/pitdata/configs drwxr-xr-x 14 root root 4096 Feb 3 07:26 /mnt/pitdata/csm-0.7.29 -rw-r--r-- 1 root root 22159328586 Feb 2 22:18 /mnt/pitdata/csm-0.7.29.tar.gz drwxr-xr-x 4 root root 4096 Feb 3 04:25 /mnt/pitdata/data drwx------ 2 root root 16384 Jan 28 15:41 /mnt/pitdata/lost+found drwxr-xr-x 5 root root 4096 Feb 3 04:20 /mnt/pitdata/prep drwxr-xr-x 2 root root 4096 Jan 28 16:07 /mnt/pitdata/static   Unmount the USB device to avoid corruption.\nThe corruption risk is low, but varies if large data use was done to or on the USB.\nncn-m001# umount -v /mnt/cow /mnt/pitdata   Remove the USB device after it has been unmounted.\n  "
},
{
	"uri": "/docs-csm/en-11/operations/csm_product_management/change_passwords_and_credentials/",
	"title": "Change Passwords And Credentials",
	"tags": [],
	"description": "",
	"content": "Change Passwords and Credentials There are many passwords and credentials used in different contexts to manage the system. These can be changed as needed. Their initial settings are documented, so it is recommended to change them during or soon after a CSM software installation.\nSee these topics for more information.\n Manage System Passwords  Keycloak Gitea Grafana Kiali Management Network Switches Redfish Credentials System Controllers (in a Liquid-cooled cabinet)   Update NCN Passwords Change Root Passwords for Compute Nodes Change the Keycloak Admin Password  "
},
{
	"uri": "/docs-csm/en-11/operations/csm_product_management/configure_keycloak_account/",
	"title": "Configure Keycloak Account",
	"tags": [],
	"description": "",
	"content": "Configure Keycloak Account Installation of CSM software includes a default account for administrative access to keycloak.\nDepending on choices made during the installation, there may be a federated connection to an external Identity Provider (IdP), such as an LDAP or AD server, which enables the use of external accounts in keycloak.\nHowever, if the external accounts are not available, then an \u0026ldquo;internal user account\u0026rdquo; could be created in keycloak. Having a usable account in keycloak with administrative authorization enables the use of the cray CLI for many administrative commands, such as those used to Validate CSM Health and general operation of the management services via the API gateway.\nSee System Security and Authentication in the \u0026ldquo;Default Keycloak Realms, Accounts, and Clients\u0026rdquo; section for more information about these topics.\n Certificate Types Change the Keycloak Admin Password Create a Service Account in Keycloak Retrieve the Client Secret for Service Accounts Get a Long-Lived Token for a Service Account Access the Keycloak user Management UI Create Internal User Accounts in the Keycloak Shasta Realm Delete Internal User Accounts in the Keycloak Shasta Realm Remove the Email Mapper from the LDAP User Federation Re-Sync Keycloak Users to Compute Nodes Configure Keycloak for LDAP/AD Authentication Configure the RSA Plugin in Keycloak Preserve Username Capitalization for Users Exported from Keycloak Change the LDAP Server IP Address for Existing LDAP Server Content Change the LDAP Server IP Address for New LDAP Server Content Remove the LDAP User Federation from Keycloak Add LDAP User Federation  "
},
{
	"uri": "/docs-csm/en-11/introduction/differences/",
	"title": "Differences From Previous Release",
	"tags": [],
	"description": "",
	"content": "Differences from Previous Release The most noteworthy changes since the previous release are described here.\nTopics:  New Features Deprecating Features Deprecated Features Other Changes  Details New Features  Scaling improvements for larger systems  BOS CAPMC FAS   New hardware supported in this release:  Compute nodes  Milan-Based Grizzly Peak with A100 40 GB GPU Milan-Based Windom Liquid Cooled System Rome-Based HPE Apollo 6500 XL675d Gen10+ with A100 40 GB GPU Rome-Based HPE Apollo 6500 XL645d Gen10+ with A100 40 GB GPU   User Access Nodes (UANs)  Milan-Based HPE DL 385(v2) Gen10+ Rome-Based HPE DL 385(v1) Gen10     Node consoles are now managed by cray-console-node which is based on conman. HSM now has a v2 REST API PowerDNS authoriative DNS server  Introduces the cray-dns-powerdns, cray-dns-powerdns-postgres, and cray-powerdns-manager pods Supports zone transfer to external DNS servers via AXFR query and DNSSEC Please refer to the DNS overview and PowerDNS Configuration Guide for further information.    Deprecating Features  HSM v1 REST API has been deprecated as of CSM version 0.9.3. The v1 HSM APIs will be removed in the CSM version 1.3 release. Many CAPMC v1 REST API and CLI features are being deprecated as part of CSM version 1.0; Full removal of the deprecated CAPMC features will happen in CSM version 1.3. Further development of CAPMC service or CLI has stopped. CAPMC has entered end-of-life but will still be generally available. CAPMC is going to be replaced with the Power Control Service (PCS) in a future release. The current API/CLI portfolio for CAPMC are being pruned to better align with the future direction of PCS. More information about PCS and the CAPMC transition will be released as part of subsequent CSM releases.  For more information on what features have been deprecated please view the CAPMC swagger doc or read the CAPMC deprecation notice   The Boot Orchestration Service (BOS) API is changing in the upcoming CSM-1.2.0 release:  The --template-body option for the Cray CLI bos command will be deprecated. Performing a GET on the session status for a boot set (i.e. /v1/session/{session_id}/status/{boot_set_name}) currently returns a status code of 201, but instead it should return a status code of 200. This will be corrected to return 200.   PowerDNS will replace Unbound as the authoritative DNS source in CSM version 1.2.  The cray-dns-unbound-manager CronJob will be deprecated in a future release once all DNS records are migrated to PowerDNS. The introduction of PowerDNS and Bifurcated CAN will introduce some node and service naming changes. Please see the PowerDNS migration notice for more information.    Deprecated Features  cray-conman pod. This has been replaced by cray-console-node. The cray-externaldns-coredns, cray-externaldns-etcd, and cray-externaldns-wait-for-etcd pods have been removed. PowerDNS is now the provider of the external DNS service.  Other Changes "
},
{
	"uri": "/docs-csm/en-11/introduction/documentation_conventions/",
	"title": "Documentation Conventions",
	"tags": [],
	"description": "",
	"content": "Documentation Conventions Several conventions have been used in the preparation of this documentation.\n Markdown Format File Formats Typographic Conventions Command Prompt Conventions which describe the context for user, host, directory, chroot environment, or container environment  Markdown Format This documentation is in Markdown format. Although much of it can be viewed with any text editor, a richer experience will come from using a tool which can render the Markdown to show different font sizes, the use of bold and italics formatting, inclusion of diagrams and screen shots as image files, and to follow navigational links within a topic file and to other files.\nThere are many tools which render the Markdown format and provide these advantages. Any Internet search for Markdown tools will provide a long list of these tools. Some of the tools are better than others at displaying the images and allowing you to follow the navigational links.\nFile Formats Some of the installation instructions require updating files in JSON, YAML, or TOML format. These files should be updated with care because some file formats do not accept tab characters for indentation of lines. Only space characters are supported. Refer to online documentation to learn more about the syntax of JSON, YAML, and TOML files. YAML does not support tab characters. The JSON convention is to use four spaces rather than a tab character.\nTypographic Conventions This style indicates program code, reserved words, library functions, command-line prompts, screen output, file/path names, and other software constructs.\n\\ (backslash) At the end of a command line, indicates the Linux shell line continuation character (lines joined by a backslash are parsed as a single line).\nCommand Prompt Conventions Host name and account in command prompts The host name in a command prompt indicates where the command must be run. The account that must run the command is also indicated in the prompt.\n The root or super-user account always has the # character at the end of the prompt Any non-root account is indicated with account@hostname. A non-privileged account is referred to as user.  Node abbreviations The following list contains abbreviations for nodes used below\n CN - compute Node NCN - Non-Compute Node AN - Application Node (special type of NCN) UAN - User Access Node (special type of AN) PIT - Pre-Install Toolkit (initial node used as the inception node during software installation booted from the LiveCD)     Prompt Description     ncn# Run the command as root on any NCN, except an NCN which is functioning as an Application Node (AN), such as a UAN.   ncn-m# Run the command as root on any NCN-M (NCN which is a Kubernetes master node).   ncn-m002# Run the command as root on the specific NCN-M (NCN which is a Kubernetes master node) which has this hostname (ncn-m002).   ncn-w# Run the command as root on any NCN-W (NCN which is a Kubernetes worker node).   ncn-w001# Run the command as root on the specific NCN-W (NCN which is a Kubernetes master node) which has this hostname (ncn-w001).   ncn-s# Run the command as root on any NCN-S (NCN which is a Utility Storage node).   ncn-s003# Run the command as root on the specific NCN-S (NCN which is a Utility Storage node) which has this hostname (ncn-s003).   pit# Run the command as root on the PIT node.   linux# Run the command as root on a Linux host.   uan# Run the command as root on any UAN.   uan01# Run the command as root on hostname uan01.   user@uan\u0026gt; Run the command as any non-root user on any UAN.   cn# Run the command as root on any CN. Note that a CN will have a hostname of the form nid124356, that is \u0026ldquo;nid\u0026rdquo; and a six digit, zero padded number.   hostname# Run the command as root on the specified hostname.   user@hostname\u0026gt; Run the command as any non-root user on the specified hostname.    Command prompt inside chroot If the chroot command is used, the prompt changes to indicate that it is inside a chroot environment on the system.\nhostname# chroot /path/to/chroot chroot-hostname# Command prompt inside Kubernetes pod If executing a shell inside a container of a Kubernetes pod where the pod name is $podName, the prompt changes to indicate that it is inside the pod. Not all shells are available within every pod, this is an example using a commonly available shell.\nncn# kubectl exec -it $podName /bin/sh pod# Command prompt inside image customization session If using SSH to access the image customization environment (pod) during an image customization session, the prompt changes to indicate that it is inside this environment. This example uses $PORT and $HOST as environment variables with specific settings. When using chroot in this context, the prompt will be different than the above chroot example.\nhostname# ssh -p $PORT root@$HOST root@POD# chroot /mnt/image/image-root :/# Directory path in command prompt Example prompts do not include the directory path, because long paths can reduce the clarity of examples. Most of the time, the command can be executed from any directory. When it matters which directory the command is invoked within, the cd command is used to change into the directory, and the directory is referenced with a period (.) to indicate the current directory\nExamples of prompts as they appear on the system:\nhostname:~ # cd /etc hostname:/etc# cd /var/tmp hostname:/var/tmp# ls ./file hostname:/var/tmp# su - user user@hostname:~\u0026gt; cd /usr/bin user hostname:/usr/bin\u0026gt; ./command Examples of prompts as they appear in this publication:\nhostname # cd /etc hostname # cd /var/tmp hostname # ls ./file hostname # su - user user@hostname \u0026gt; cd /usr/bin user@hostname \u0026gt; ./command Command prompts for network switch configuration The prompts when doing network switch configuration can vary widely depending on which vendor switch is being configured and the context of the item being configured on that switch. There may be two levels of user privilege which have different commands available and a special command to enter configuration mode.\nExample of prompts as they appear in this publication:\nEnter \u0026ldquo;setup\u0026rdquo; mode for the switch make and model, for example:\nremote# ssh admin@sw-leaf-001 sw-leaf-001\u0026gt; enable sw-leaf-001# configure terminal sw-leaf-001(conf)# Refer to the switch vendor OEM documentation for more information about configuring a specific switch.\n"
},
{
	"uri": "/docs-csm/en-11/introduction/",
	"title": "Introduction To Installation",
	"tags": [],
	"description": "",
	"content": "Introduction to CSM Installation This document provides an introduction to the Cray System Management (CSM) installation documentation for an HPE Cray EX system.\nTopics:  CSM Overview Scenarios for Shasta v1.5 CSM Product Stream Updates CSM Operational Activities Differences from Previous Release Documentation Conventions  Details CSM Overview The CSM installation prepares and deploys a distributed system across a group of management nodes organized into a Kubernetes cluster which uses Ceph for utility storage. These nodes perform their function as Kubernetes master nodes, Kubernetes worker nodes, or utility storage nodes with the Ceph storage.\nSystem services on these nodes are provided as containerized micro-services packaged for deployment via Helm charts. Kubernetes orchestrates these services and schedules them on Kubernetes worker nodes with horizontal scaling. Horizontal scales increases or decreases the number of services instances demand for them varies, such as when booting many compute nodes or application nodes.\nThere is much more information available in the CSM Overview about the hardware, software, network, and access to these services and components.\nSee CSM Overview\nScenarios for Shasta v1.5 These scenarios for how to get CSM software onto a system are described in Scenarios for Shasta v1.5.\n Installation of CSM software  First time installation of CSM software Reinstall of CSM software   Upgrade from a previous version of CSM software  Note: A migration from Shasta v1.3.x software to Shasta v1.5 software is not supported as a direct action, but is a two step process of first migrating from Shasta v1.3.x to Shasta v1.4 and then following the Upgrade procedure from v1.4 to v1.5.\nSee Scenarios for Shasta v1.5\nCSM Product Stream Updates The software included in the CSM product stream is released in more than one way. The initial product release may be augmented with late-breaking workarounds and documentation updates or hotfixes after the release.\nSee CSM Product Stream Updates\nCSM Operational Activities Procedures which are used during either installation or upgrading of software or in both, but which may also be used for general operation of the system reside here. They are referenced in the context of the installation workflow. For example, updating firmware with FAS or running the CSM health checks.\nSee CSM Operational Activities\nDifferences from Previous Release Significant changes from the previous release of CSM are described.\n New Features Deprecating Features Deprecated Features Other Changes  See Differences from Previous Release\nDocumentation Conventions Several conventions have been used in the preparation of this documentation.\n File Formats Typographic Conventions Command Prompt Conventions which indicate the context for user, host, directory, chroot environment, or container environment  See Documentation Conventions\n"
},
{
	"uri": "/docs-csm/en-11/introduction/scenarios/",
	"title": "Scenarios For Shasta V1.5",
	"tags": [],
	"description": "",
	"content": "Scenarios for Shasta v1.5 There are multiple scenarios for installing CSM software which are described in this documentation with many supporting procedures.\n Scenarios for Shasta v1.5  Installation Upgrade Migration    Installation There are two ways to install the CSM software. There are some differences between a first time install which must create the initial configuration payload and configure the management network switches, whereas a reinstall can reuse a previous configuration payload and skip the configuration of management network switches. The first time install will check and then may update firmware for various components whereas the reinstall will check and indicate that no firmware update is required. There are two different ways to use the LiveCD, either from a RemoteISO or a USB device, which are described in Bootstrap PIT Node. There are a few places where a comment will be made in a procedure for how one of the scenarios needs to do something differently.\n  First time Install\n Prepare Configuration Payload creates the initial configuration payload. Bootstrap PIT Node Configure Management Network Switches    Reinstall\n Prepare Configuration Payload can reuse a previous configuration payload. There may be additional steps to manually wipe disks on the management nodes and do other actions to prepare the management node hardware for the reinstall. Bootstrap PIT Node Can skip the procedure to Configure Management Network Switches    The two paths merge together after configuration of the management network switches to do later actions the same regardless of the starting point in the workflow.\n Deploy Management Nodes Install CSM Services Validate CSM Health Before PIT Node Redeploy Redeploy PIT Node Configure Administrative Access Validate CSM Health Update Firmware with FAS Prepare Compute Nodes  After completion of the firmware update with FAS and the preparation of compute nodes, the CSM product stream has been fully installed and configured. Refer to the 1.5 HPE Cray EX System Software Getting Started Guide S-8000 on the HPE Customer Support Center at https://www.hpe.com/support/ex-gsg for more information on other product streams to be installed and configured after CSM.\nSee Install CSM for the details on the installation process for either a first time install or a reinstall.\nUpgrade The upgrade from Shasta v1.4.2 (including CSM 0.9.3) to Shasta v1.5 (including CSM 1.0) is supported. This process will upgrade the Ceph storage software, then the storage nodes, then the Kubernetes master nodes and worker nodes, and finally the CSM services. The management nodes are upgraded using a rolling upgrade approach which enables management services to continue to function even as one or a few nodes are being upgraded.\nSee Upgrade CSM.\nMigration There is no direct migration from Shasta v1.3.x releases to Shasta v1.5. However, there is a supported path.\n  Migration from v1.3.x to v1.4.0\nThe migration from v1.3.x to v1.4.0 is described in the Shasta v1.4 documentation. Refer to \u0026ldquo;1.3 to 1.4 Install Prerequisites\u0026rdquo; and \u0026ldquo;Collect Data From Healthy Shasta 1.3 System for EX 1.4 Installation\u0026rdquo; in the HPE Cray EX System Installation and Configuration Guide 1.4 S-8000.\n  Upgrade v1.4.x to v1.5\nAn upgrade from the previous release (Shasta v1.4.x) is supported with this release.\nSee Upgrade from 1.4.x to v1.5\n  "
},
{
	"uri": "/docs-csm/en-11/introduction/capmc_deprecation/",
	"title": "CAPMC Deprecation Notice Many Capmc V1 Features Are Being Partially Deprecated",
	"tags": [],
	"description": "",
	"content": "CAPMC Deprecation Notice: many CAPMC v1 features are being partially deprecated Deprecated Features in CSM 1.0 Many CAPMC v1 REST API and CLI features are being deprecated as part of CSM version 1.0; Full removal of the following deprecated CAPMC features will happen in CSM version 1.3. Further development of CAPMC service or CLI has stopped. CAPMC has entered end-of-life but will still be generally available. CAPMC is going to be replaced with the Power Control Service (PCS) in a future release. The current API/CLI portfolio for CAPMC are being pruned to better align with the future direction of PCS. More information about PCS and the CAPMC transition will be released as part of subsequent CSM releases.\nThe API endpoints that remain un-deprecated will remain supported until their \u0026lsquo;phased transition\u0026rsquo; into PCS (e.g. Power Capping is not \u0026lsquo;deprecated\u0026rsquo; and will be supported in PCS; As PCS is developed, CAPMC\u0026rsquo;s Powercapping and PCS\u0026rsquo;s Powercapping will both function, eventually callers of the CAPMC power capping API/CLI will need to will need transition to call PCS as the API will be different.)\nHere is a list of deprecated API (CLI) endpoints:\n node control  /get_node_rules /get_node_status /node_on /node_off /node_reinit   group control  /group_reinit /get_group_status /group_on /group_off   node energy  /get_node_energy /get_node_energy_stats /get_node_energy_counter   system monitor  /get_system_parameters /get_system_power /get_system_power_details   EPO  /emergency_power_off   utilities  /get_nid_map    "
},
{
	"uri": "/docs-csm/en-11/introduction/csm_overview/",
	"title": "Overview",
	"tags": [],
	"description": "",
	"content": "CSM Overview This CSM Overview describes the Cray System Management ecosystem with its hardware, software, and network. It describes how to access these services and components.\nThe CSM installation prepares and deploys a distributed system across a group of management nodes organized into a Kubernetes cluster which uses Ceph for utility storage. These nodes perform their function as Kubernetes master nodes, Kubernetes worker nodes, or utility storage nodes with the Ceph storage.\nSystem services on these nodes are provided as containerized micro-services packaged for deployment via Helm charts. Kubernetes orchestrates these services and schedules them on Kubernetes worker nodes with horizontal scaling. Horizontal scales increases or decreases the number of services instances demand for them varies, such as when booting many compute nodes or application nodes.\nTopics:  System Nodes and Networks Default IP Address Ranges Resilience of System Management Services Access to System Management Services  Details 1. System Nodes and Networks The HPE Cray EX system has two types of nodes:\n Compute Nodes, where high performance computing applications are run, have hostnames in the form of nidXXXXXX, that is, \u0026ldquo;nid\u0026rdquo; followed by six digits. These six digits will be padded with zeroes at the beginning. All other nodes provide supporting functions to these compute nodes. Non-Compute Nodes (NCNs), which carry out system functions and come in many types:  Management nodes in a Kubernetes cluster which host system services.  Kubernetes master nodes, with names in the form of ncn-mXXX. Every system has three or more master nodes. Kubernetes worker nodes, with names in the form of ncn-wXXX. Every system has three or more worker nodes. Utility Storage nodes providing Ceph storage to Kubernetes nodes, with names in the form of ncn-sXXX. Every system has three or more storage nodes.   Application nodes (ANs) which are not part of the Kubernetes management cluster  User Access Nodes (UANs), known by some as login or front-end nodes Other site-defined types:  Gateway nodes Data Mover nodes Visualization nodes        The following system networks connect the devices listed:\n Networks external to the system:  Customer Network (Data Center)  ncn-m001 BMC is connected by the customer network switch to the customer management network ClusterStor System Management Unit (SMU) interfaces User Access Nodes (UANs)     System networks:  Hardware Management Network (HMN)  BMCs for Admin tasks Power distribution units (PDU) Keyboard/video/mouse (KVM)   Node Management Network (NMN)  All NCNs and compute nodes   ClusterStor Management Network  ClusterStor controller management interfaces of all ClusterStor components (SMU, Metadata Management Unit (MMU), and Scalable Storage Unit (SSU))   High-Speed Network (HSN), which connects the following devices:  Kubernetes worker nodes UANs ClusterStor controller data interfaces of all ClusterStor components (SMU, MMU, and SSU)      During initial installation, several of those networks are created with default IP address ranges. See Default IP Address Ranges\nThe network management system (NMS) data model and REST API enable customer sites to construct their own \u0026ldquo;networks\u0026rdquo; of nodes within the high-speed fabric, where a \u0026ldquo;network\u0026rdquo; is a collection of nodes that share a VLAN and an IP subnet.\nThe low-level network management components (switch, DHCP service, ARP service) of the management nodes and ClusterStor interfaces are configured to serve one particular network (the \u0026ldquo;supported network\u0026rdquo;) on the high-speed fabric. As part of the initial installation, the supported network is created to include all of the compute nodes, thereby enabling those compute nodes to access the gateway, user access services, and ClusterStor devices.\nA site may create other networks as well, but it is only the supported network that is served by those devices.\n2. Default IP Address Ranges The initial installation of the system creates default networks with default settings and with no external exposure. These IP address default ranges ensure that no nodes in the system attempt to use the same IP address as a Kubernetes service or pod, which would result in undefined behavior that is extremely difficult to reproduce or debug.\nThe following table shows the default IP address ranges\n   Network IP Address Range     Kubernetes service network 10.16.0.0/12   Kubernetes pod network 10.32.0.0/12   Install Network (MTL) 10.1.0.0/16   Node Management Network (NMN) 10.252.0.0/17   High Speed Network (HSN) 10.253.0.0/16   Hardware Management Network (HMN) 10.254.0.0/17   Mountain NMN Allocate a /22 from this range per liquid cooled cabinet: * cabinet 1 * cabinet 2 * cabinet 3 * \u0026hellip; 10.100.0.0/17 Example IP address in the allocated ranges: * 10.100.0.0/22 * 10.100.4.0/22 * 10.100.8.0/22 * \u0026hellip;   Mountain HMN Allocate a /22 from this range per liquid cooled cabinet: * cabinet 1 * cabinet 2 * cabinet 3 * \u0026hellip; 10.104.0.0/17 Example IP address in the allocated ranges: * 10.104.0.0/22 * 10.104.4.0/22 * 10.104.8.0/22 * \u0026hellip;   River NMN 10.106.0.0/17   River HMN 10.107.0.0/17   Load Balanced NMN     The above values could be modified prior to install if there is a need to ensure that there are no conflicts with customer resources, such as LDAP or license servers. If a customer has more than one HPE Cray EX system, these values can be safely reused across them all. Contact customer support for this site if it is required to change the IP address range for Kubernetes services or pods; for example, if the IP addresses within those ranges must be used for something else. The cluster must be fully reinstalled if either of those ranges are changed.\nThere are several network values and other pieces of system information that are unique to the customer system.\n  IP addresses and the network(s) for ncn-m001 and the BMC on ncn-m001.\n  The main Customer Access Network (CAN) subnet and the two address pools mentioned below need to be part of the main subnet.\nFor more information on the CAN, see Customer Access Network (CAN).\n Subnet for the MetalLB static address pool (can-static-pool), which is used for services that need to be pinned to the same IP address, such as the system DNS service. Subnet for the MetalLB dynamic address pool (can-dynamic-pool), which is used for services such as User Access Instances (UAIs) that can be reached by DNS.    HPE Cray EX Domain: The value of the subdomain that is used to access externally exposed services. For example, if the system is named TestSystem, and the site is example.com, the HPE Cray EX domain would be testsystem.example.com. Central DNS would need to be configured to delegate requests for addresses in this domain to the HPE Cray EX DNS IP address for resolution.\n  HPE Cray EX DNS IP: The IP address used for the HPE Cray EX DNS service. Central DNS delegates the resolution for addresses in the HPE Cray EX Domain to this server. The IP address will be in the can-static-pool subnet.\n  CAN gateway IP address: The IP address assigned to a specific port on the spine switch, which will act as the gateway between the CAN and the rest of the customer\u0026rsquo;s internal networks. This address would be the lasthop route to the CAN network.\n  3. Resilience of System Management Services HPE Cray EX systems are designed so that system management services (SMS) are fully resilient and that there is no single point of failure. The design of the system allows for resiliency in the following ways:\n Three management nodes are configured as Kubernetes master nodes. When one master goes down, operations (such as jobs running across compute nodes) are expected to continue. At least three utility storage nodes provide persistent storage for the services running on the Kubernetes management nodes. When one of the utility storage nodes goes down, operations (such as jobs running across compute nodes) are expected to continue. At least three management nodes are configured as Kubernetes worker nodes. If one of only three Kubernetes worker nodes were to go down, it would be much more difficult for the remaining two worker nodes to handle the total balance of pods. It is less significant to lose one of the worker nodes if the system has more than three worker nodes because there are more worker nodes able to handle the pod load. The state and configuration of the Kubernetes cluster are stored in an etcd cluster distributed across the Kubernetes master nodes. This cluster is also backed up on an interval, and backups are pushed to Ceph Rados Gateway (S3). A micro-service can run on any node that meets the requirements for that micro-service, such as appropriate hardware attributes, which are indicated by Kubernetes labels and taints. All micro-services have shared persistent storage so that they can be restarted on any worker node in the Kubernetes management cluster without losing state.  Kubernetes is designed to ensure that the desired number of deployments of a micro-service are always running on one or more worker nodes. In addition, it ensures that if one worker node becomes unresponsive, the micro-services that were running on it are migrated to another worker node that is up and meets the requirements of those micro-services.\nFor more information about resiliency topics see Resilience of System Management Services.\n4. Access to System Management Services The standard configuration for System Management Services (SMS) is the containerized REST micro-service with a public API. All of the micro-services provide an HTTP interface and are collectively exposed through a single gateway URL. The API gateway for the system is available at a well known URL based on the domain name of the system. It acts as a single HTTPS endpoint for terminating Transport Layer Security (TLS) using the configured certificate authority. All services and the API gateway are not dependent on any single node. This resilient arrangement ensures that services remain available during possible underlying hardware and network failures.\nAccess to individual APIs through the gateway is controlled by a policy-driven access control system. Administrators and users must retrieve a token for authentication before attempting to access APIs through the gateway and present a valid token with each API call. The authentication and authorization decisions are made at the gateway level which prevent unauthorized API calls from reaching the underlying micro-services. For more detail on the process of obtaining tokens and user management, see System Security and Authentication.\nReview the API documentation in the supplied container before attempting to use the API services. This container is generated with the release using the most current API descriptions in OpenAPI 2.0 format. Because this file serves as both an internal definition of the API contract and the external documentation of the API function, it is the most up-to-date reference available.\nThe API Gateway URL for accessing the APIs on a site-specific system is https://api.SYSTEM-NAME.DOMAIN-NAME/apis/.\nThe internal URL from a local console on any of the management nodes is https://api-gw-service-nmn.local/apis.\n"
},
{
	"uri": "/docs-csm/en-11/introduction/powerdns_migration/",
	"title": "PowerDNS Migration Notice",
	"tags": [],
	"description": "",
	"content": "PowerDNS Migration Notice The migration to PowerDNS as the authoritative DNS source and the introduction of Bifurcated CAN (Customer Access Network) will result in some changes to the node and service naming conventions.\nDNS Record Naming Changes Fully qualified domain names will be introduced for all DNS records.\nCanonical name: hostname.network-path.system-name.tld\n hostname - The hostname of the node or service network-path - The network path used to access the node  .nmn - Node Management Network .hmn - Hardware Management Network .hsn - High Speed Network (Slingshot) .can - Customer Access Network .chn - Customer High Speed network .cmn - Customer Management Network   system-name - The customer defined name of the system tld - The top-level domain  Kubernetes services present on the Hardware Management Network (HMN) and Node Management Network (NMN) that needed to be referenced by name used names in the .local domain which will be deprecated in favour of a fully qualified domain name.\nExamples The following examples assume the system was configured with a system-name of shasta and a site-domain of dev.cray.com\n   Old name New name     ncn-w001 ncn-w001.nmn.shasta.dev.cray.com   nid000001-nmn nid000001.nmn.shasta.dev.cray.com   x3000c0s2b0 x3000c0s2b0.hmn.shasta.dev.cray.com   api-gw-service-nmn.local api.nmn.shasta.dev.cray.com   registry.local registry.nmn.shasta.dev.cray.com   packages.local packages.nmn.shasta.dev.cray.com   nexus.shasta.dev.cray.com nexus.cmn.shasta.dev.cray.com   api.shasta.dev.cray.com api.cmn.shasta.dev.cray.com\napi.chn.shasta.dev.cray.com\napi.can.shasta.dev.cray.com    Backwards Compatibility The old service and node names will not be migrated to PowerDNS however they will be maintained in Unbound as local records for the purpose of backwards compatibility. These records will be removed entirely in a future release when the cray-dns-unbound-manager job is deprecated.\n"
},
{
	"uri": "/docs-csm/en-11/install/wipe_ncn_disks_for_reinstallation/",
	"title": "Wipe NCN Disks For Reinstallation",
	"tags": [],
	"description": "",
	"content": "Wipe NCN Disks for Reinstallation This page will detail how disks are wiped and includes workarounds for wedged disks. Any process covered on this page will be covered by the installer.\n Everything in this section should be considered DESTRUCTIVE.\n After following these procedures an NCN can be rebooted and redeployed.\nIdeally the Basic Wipe is enough, and should be tried first. All types of disk wipe can be run from Linux or an initramFS/initrd emergency shell.\nThe following are potential use cases for wiping disks:\n\n Adding a node that is not bare. Adopting new disks that are not bare. Doing a fresh install.  Topics:  Basic Wipe Advanced Wipe Full Wipe  1. Basic Wipe A basic wipe includes wiping the disks and all of the RAIDs. These basic wipe instructions can be executed on any management nodes (master, worker and storage).\n  List the disks for verification.\nncn# ls -1 /dev/sd* /dev/disk/by-label/*   Wipe the disks and the RAIDs.\nncn# wipefs --all --force /dev/sd* /dev/disk/by-label/* If any disks had labels present, output looks similar to the following:\n/dev/sda: 8 bytes were erased at offset 0x00000200 (gpt): 45 46 49 20 50 41 52 54 /dev/sda: 8 bytes were erased at offset 0x6fc86d5e00 (gpt): 45 46 49 20 50 41 52 54 /dev/sda: 2 bytes were erased at offset 0x000001fe (PMBR): 55 aa /dev/sdb: 6 bytes were erased at offset 0x00000000 (crypto_LUKS): 4c 55 4b 53 ba be /dev/sdb: 6 bytes were erased at offset 0x00004000 (crypto_LUKS): 53 4b 55 4c ba be /dev/sdc: 8 bytes were erased at offset 0x00000200 (gpt): 45 46 49 20 50 41 52 54 /dev/sdc: 8 bytes were erased at offset 0x6fc86d5e00 (gpt): 45 46 49 20 50 41 52 54 /dev/sdc: 2 bytes were erased at offset 0x000001fe (PMBR): 55 aa Verify there are no error messages in the output.\nThe wipefs command may fail if no labeled disks are found, which is an indication of a larger problem.\n  2. Advanced Wipe This section is specific to utility storage nodes. An advanced wipe includes deleting the Ceph volumes and then wiping the disks and RAIDs.\n  Delete CEPH volumes.\nncn-s# systemctl stop ceph-osd.target Make sure the OSDs (if any) are not running after running the first command.\nncn-s# ls -1 /dev/sd* /dev/disk/by-label/* ncn-s# vgremove -f -v --select \u0026#39;vg_name=~ceph*\u0026#39;   List the disks for verification.\nncn# ls -1 /dev/sd* /dev/disk/by-label/*   Wipe the disks and RAIDs.\nncn-s# wipefs --all --force /dev/sd* /dev/disk/by-label/* See Basic Wipe section for expected output from the wipefs command.\n  3. Full-Wipe This section is the preferred method for all nodes. A full wipe includes deleting the Ceph volumes (where applicable), stopping the RAIDs, zeroing the disks, and then wiping the disks and RAIDs.\nIMPORTANT: Step 2 is to wipe the Ceph OSD drives. Steps 1, 3, 4, and 5 are for all node types.\n  Reset Kubernetes on each master and worker node.\nThis will stop kubelet, underlying containers, and remove the contents of /var/lib/kubelet.\nNOTE: The recommended order is to do this on the worker nodes, and then the master nodes.\n  For each worker node, run the following:\nncn-mw# kubeadm reset --force   List any containers running in containerd.\nncn-mw# crictl ps CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID 66a78adf6b4c2 18b6035f5a9ce About a minute ago Running spire-bundle 1212 6d89f7dee8ab6 7680e4050386d c8344c866fa55 24 hours ago Running speaker 0 5460d2bffb4d7 b6467c907f063 8e6730a2b718c 3 days ago Running request-ncn-join-token 0 a3a9ca9e1ca78 e8ce2d1a8379f 64d4c06dc3fb4 3 days ago Running istio-proxy 0 6d89f7dee8ab6 c3d4811fc3cd0 0215a709bdd9b 3 days ago Running weave-npc 0 f5e25c12e617e   If there are any running containers from the output of the crictl ps command, stop them.\nncn-mw# crictl stop \u0026lt;container id from the CONTAINER column\u0026gt;   After performing the previous steps for the worker nodes to be wiped, then perform them for the master nodes to be wiped.\n    Delete Ceph Volumes on storage nodes ONLY.\nFor each storage node:\n  Stop Ceph.\n  1.4 or earlier\nncn-s# systemctl stop ceph-osd.target   1.5 or later\nncn-s# cephadm rm-cluster --fsid $(cephadm ls|jq -r \u0026#39;.[0].fsid\u0026#39;) --force     Make sure the OSDs (if any) are not running.\n  1.4 or earlier\nncn-s# ps -ef|grep ceph-osd   1.5 or later\nncn-s# podman ps   Examine the output. There should be no running ceph-osd processes or containers.\n  Remove the VGs.\nncn-s# ls -1 /dev/sd* /dev/disk/by-label/* ncn-s# vgremove -f -v --select \u0026#39;vg_name=~ceph*\u0026#39;     Unmount volumes.\n NOTE Some of the following umount commands may fail or have warnings depending on the state of the NCN. Failures in this section can be ignored and will not inhibit the wipe process.\nNOTE: There is an edge case where the overlay may keep you from unmounting the drive. If this is a rebuild you ignore this.\n   Master nodes.\nncn-m# umount -v /var/lib/etcd /var/lib/sdu   Worker nodes.\nncn-w# umount -v /var/lib/containerd /var/lib/kubelet /var/lib/sdu   Storage nodes.\nncn-s# umount -vf /var/lib/ceph /var/lib/containers /etc/ceph If the umount command is responding with target is busy on the storage node, then try the following:\nncn-s# mount | grep \u0026#34;containers\u0026#34; /dev/mapper/metalvg0-CONTAIN on /var/lib/containers type xfs (rw,noatime,swalloc,attr2,largeio,inode64,allocsize| 32k,noquota) /dev/mapper/metalvg0-CONTAIN on /var/lib/containers/storage/overlay type xfs (rw,noatime,swalloc,attr2,largeio,i| bufs=8,logbsize=32k,noquota) ncn-s# umount -v /var/lib/containers/storage/overlay umount: /var/lib/containers/storage/overlay unmounted ncn-s# umount -v /var/lib/containers umount: /var/lib/containers unmounted     Remove auxiliary LVMs.\n  Stop cray-sdu-rda container if necessary.\nncn# podman ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7741d5096625 registry.local/sdu-docker-stable-local/cray-sdu-rda:1.1.1 /bin/sh -c /usr/s... 6 weeks ago Up 6 weeks ago cray-sdu-rda   If there is a running cray-sdu-rda container in the above output, stop it using the container ID:\nncn# podman stop 7741d5096625 7741d50966259410298bb4c3210e6665cdbd57a82e34e467d239f519ae3f17d4   Remove metal LVM.\nncn# vgremove -f -v --select \u0026#39;vg_name=~metal*\u0026#39;  NOTE Optionally you can run the pvs command and if any drives are still listed, you can remove them with pvremove, but this is rarely needed. Also, if the above command fails or returns a warning about the filesystem being in use, you should ignore the error and proceed to the next step, as this will not inhibit the wipe process.\n     Stop the RAIDs.\nncn# for md in /dev/md/*; do mdadm -S -v $md || echo nope ; done   List the disks for verification.\nncn# ls -1 /dev/sd* /dev/disk/by-label/*   Wipe the disks and RAIDs.\nncn# sgdisk --zap-all /dev/sd* ncn# wipefs --all --force /dev/sd* /dev/disk/by-label/* Note: On worker nodes, it is a known issue that the sgdisk command sometimes encounters a hard hang. If you see no output from the command for 90 seconds, close the terminal session to the worker node, open a new terminal session to it, and complete the disk wipe procedure by running the above wipefs command.\nSee Basic Wipe section for expected output from the wipefs command.\n  "
},
{
	"uri": "/docs-csm/en-11/install/troubleshooting_installation/",
	"title": "Troubleshooting Installation Problems",
	"tags": [],
	"description": "",
	"content": "Troubleshooting Installation Problems The installation of the Cray System Management (CSM) product requires knowledge of the various nodes and switches for the HPE Cray EX system. The procedures in this section should be referenced during the CSM install for additional information on system hardware, troubleshooting, and administrative tasks related to CSM.\nTopics:  Reset root Password on LiveCD Reinstall LiveCD PXE Boot Troubleshooting Wipe NCN Disks for Reinstallation Restart Network Services and Interfaces on NCNs Utility Storage Node Installation Troubleshooting Ceph CSI Troubleshooting Safeguards for CSM NCN Upgrades  Details \n  Reset root Password on LiveCD\nIf the root password on the LiveCD needs to be changed, then this procedure does the reset.\nSee Reset root Password on LiveCD \n  Reinstall LiveCD\nIf a reinstall of the PIT node is needed, the data from the PIT node can be saved to the LiveCD USB and the LiveCD USB can be rebuilt.\nSee Reinstall LiveCD \n  PXE Boot Troubleshooting\nIf a reinstall of the PIT node is needed, the data from the PIT node can be saved to the LiveCD USB and the LiveCD USB can be rebuilt.\nSee PXE Boot Troubleshooting \n  Wipe NCN Disks for Reinstallation\nIf it has been determined an NCN did not properly configure its storage while trying to Deploy Management Nodes during the install, then the storage should be wiped so the node can be redeployed.\nSee Wipe NCN Disks for Reinstallation \n  Restart Network Services and Interfaces on NCNs\nIf an NCN shows any of these problems, the network services and interfaces on that node might need to be restarted.\n Interfaces not showing up IP Addresses not applying Member/children interfaces not being included  See Restart Network Services and Interfaces on NCNs \n  Utility Storage Node Installation Troubleshooting\nIf there is a failure in the creation of Ceph storage on the utility storage nodes for one of these scenarios, the Ceph storage might need to be reinitialized.\n Sometimes a large OSD can be created which is a concatenation of multiple devices, instead of one OSD per device  See Utility Storage Node Installation Troubleshooting \n  Ceph CSI Troubleshooting\nIf there has been a failure to initialize all Ceph CSI components on ncn-s001, then the storage node cloud-init may need to be rerun.\n Verify Ceph CSI Rerun Storage Node cloud-init  See Ceph CSI Troubleshooting \n  Safeguards for CSM NCN Upgrades\nIf a reinstall or upgrade is being done, there might be a reason to use one of these safeguards.\n Preserve Ceph on Utility Storage Nodes Protect RAID Configuration on Management Nodes  See Safeguards for CSM NCN Upgrades\n  "
},
{
	"uri": "/docs-csm/en-11/install/utility_storage_node_installation_troubleshooting/",
	"title": "Utility Storage Installation Troubleshooting",
	"tags": [],
	"description": "",
	"content": "Utility Storage Installation Troubleshooting If there is a failure in the creation of Ceph storage on the utility storage nodes for one of these scenarios, the Ceph storage might need to be reinitialized.\nTopics  Scenario 1 (Shasta v1.4 only) Scenario 2 (Shasta v1.5 only)  Details Scenario 1 (Shasta 1.4 only) IMPORTANT (FOR NODE INSTALLS/REINSTALLS ONLY): If the Ceph install failed please check the following\nncn-s# ceph osd tree ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 83.83459 root default -5 27.94470 host ncn-s001 0 ssd 3.49309 osd.0 up 1.00000 1.00000 4 ssd 3.49309 osd.4 up 1.00000 1.00000 6 ssd 3.49309 osd.6 up 1.00000 1.00000 8 ssd 3.49309 osd.8 up 1.00000 1.00000 10 ssd 3.49309 osd.10 up 1.00000 1.00000 12 ssd 3.49309 osd.12 up 1.00000 1.00000 14 ssd 3.49309 osd.14 up 1.00000 1.00000 16 ssd 3.49309 osd.16 up 1.00000 1.00000 -3 27.94470 host ncn-s002 1 ssd 3.49309 osd.1 down 1.00000 1.00000 3 ssd 3.49309 osd.3 down 1.00000 1.00000 5 ssd 3.49309 osd.5 down 1.00000 1.00000 7 ssd 3.49309 osd.7 down 1.00000 1.00000 9 ssd 3.49309 osd.9 down 1.00000 1.00000 11 ssd 3.49309 osd.11 down 1.00000 1.00000 13 ssd 3.49309 osd.13 down 1.00000 1.00000 15 ssd 3.49309 osd.15 down 1.00000 1.00000 -7 27.94519 host ncn-s003 \u0026lt;--- node where the issue exists 2 ssd 27.94519 osd.2 down 1.00000 1.00000 \u0026lt;--- the problematic VG SSH to the node(s) where the issue exists and do the following:\nncn-s# systemctl stop ceph-osd.target ncn-s# vgremove -f --select \u0026#39;vg_name=~ceph*\u0026#39; # This will take a little bit of time, so do not panic ncn-s# for i in {g..n}; do sgdisk --zap-all /dev/sd$i; done This will vary node to node. Use lsblk to identify all drives available to Ceph\n Manually create OSDs on the problematic nodes\n ncn-s# for i in {g..n}; do ceph-volume lvm create --data /dev/sd$i --bluestore; done ALL THE BELOW WORK WILL BE RUN FROM NCN-S001\n  Verify the /etc/cray/ceph directory is empty. If there are any files there then delete them\n  Put in safeguard\n Edit /srv/cray/scripts/metal/lib.sh Comment out the below lines  22 if [ $wipe == \u0026#39;yes\u0026#39; ]; then 23 ansible osds -m shell -a \u0026#34;vgremove -f --select \u0026#39;vg_name=~ceph*\u0026#39;\u0026#34; 24 fi   Run the cloud init script.\nncn-s001# /srv/cray/scripts/common/storage-ceph-cloudinit.sh   Scenario 2 (Shasta 1.5 only) IMPORTANT (FOR NODE INSTALLS/REINSTALLS ONLY): If the Ceph install failed please check the following\nnncn-s001:~ # ceph osd tree ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 31.43875 root default -3 10.47958 host ncn-s001 2 ssd 1.74660 osd.2 up 1.00000 1.00000 3 ssd 1.74660 osd.3 up 1.00000 1.00000 6 ssd 1.74660 osd.6 up 1.00000 1.00000 9 ssd 1.74660 osd.9 up 1.00000 1.00000 12 ssd 1.74660 osd.12 up 1.00000 1.00000 15 ssd 1.74660 osd.15 up 1.00000 1.00000 -5 10.47958 host ncn-s002 0 ssd 1.74660 osd.0 down 1.00000 1.00000 \u0026lt;-- the bad OSD 4 ssd 1.74660 osd.4 up 1.00000 1.00000 7 ssd 1.74660 osd.7 up 1.00000 1.00000 10 ssd 1.74660 osd.10 up 1.00000 1.00000 13 ssd 1.74660 osd.13 up 1.00000 1.00000 16 ssd 1.74660 osd.16 up 1.00000 1.00000 -7 10.47958 host ncn-s003 1 ssd 1.74660 osd.1 up 1.00000 1.00000 5 ssd 1.74660 osd.5 up 1.00000 1.00000 8 ssd 1.74660 osd.8 up 1.00000 1.00000 11 ssd 1.74660 osd.11 up 1.00000 1.00000 14 ssd 1.74660 osd.14 up 1.00000 1.00000 17 ssd 1.74660 osd.17 up 1.00000 1.00000 Get more information using the host and OSD.\nceph orch ps --daemon-type osd ncn-s002 NAME HOST STATUS REFRESHED AGE VERSION IMAGE NAME IMAGE ID CONTAINER ID osd.0 ncn-s002 running (23h) 7m ago 2d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 98859a09a946 osd.10 ncn-s002 running (23h) 7m ago 2d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 808162b421b8 osd.13 ncn-s002 running (23h) 7m ago 2d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 594d6fd03361 osd.16 ncn-s002 running (23h) 7m ago 2d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 726295e3625f osd.4 ncn-s002 running (23h) 7m ago 2d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c ee1987d99e5a osd.7 ncn-s002 running (23h) 7m ago 2d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 78a89eaef92a   optionally leave off the host name and it will return all the OSD processing the cluster\n  In order to zap a single OSD, it is necessary to gather some information.\nTo list out the devices on that host:\n  ceph orch device ls \nncn-s00[123]:~ # ceph orch device ls ncn-s002 --wide Hostname Path Type Transport RPM Vendor Model Serial Size Health Ident Fault Available Reject Reasons ncn-s002 /dev/sdc ssd Unknown Unknown ATA SAMSUNG MZ7LH1T9 S455NY0M811867 1920G Unknown N/A N/A No locked, LVM detected, Insufficient space (\u0026lt;10 extents) on vgs ncn-s002 /dev/sdd ssd Unknown Unknown ATA SAMSUNG MZ7LH1T9 S455NY0M812407 1920G Unknown N/A N/A No locked, LVM detected, Insufficient space (\u0026lt;10 extents) on vgs ncn-s002 /dev/sde ssd Unknown Unknown ATA SAMSUNG MZ7LH1T9 S455NY0M812406 1920G Unknown N/A N/A No locked, LVM detected, Insufficient space (\u0026lt;10 extents) on vgs ncn-s002 /dev/sdf ssd Unknown Unknown ATA SAMSUNG MZ7LH1T9 S455NY0M812405 1920G Unknown N/A N/A No locked, LVM detected, Insufficient space (\u0026lt;10 extents) on vgs ncn-s002 /dev/sdg ssd Unknown Unknown ATA SAMSUNG MZ7LH1T9 S455NY0M811921 1920G Unknown N/A N/A No locked, LVM detected, Insufficient space (\u0026lt;10 extents) on vgs ncn-s002 /dev/sdh ssd Unknown Unknown ATA SAMSUNG MZ7LH1T9 S455NY0M811873 1920G Unknown N/A N/A No locked, LVM detected, Insufficient space (\u0026lt;10 extents) on vgs   The locked status in the Reject column is likely the result of a wipe failure.\nFind the drive path\ncephadm ceph-volume lvm list ncn-s002:~ # cephadm ceph-volume lvm list Inferring fsid 8f4dd38b-ee84-4d29-8305-1ef24e61a5d8 Using recent Ceph image docker.io/ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d /usr/bin/podman: stdout /usr/bin/podman: stdout /usr/bin/podman: stdout ====== osd.0 ======= /usr/bin/podman: stdout /usr/bin/podman: stdout [block] /dev/ceph-380453cf-4581-4616-b95e-30a8743bece0/osd-data-59bcf0c9-5867-41c3-8e40-2e99232cf8e9 /usr/bin/podman: stdout /usr/bin/podman: stdout block device /dev/ceph-380453cf-4581-4616-b95e-30a8743bece0/osd-data-59bcf0c9-5867-41c3-8e40-2e99232cf8e9 /usr/bin/podman: stdout block uuid 54CjSj-kxEs-df0N-13Vs-miIF-g2KH-sX2UMQ /usr/bin/podman: stdout cephx lockbox secret /usr/bin/podman: stdout cluster fsid 8f4dd38b-ee84-4d29-8305-1ef24e61a5d8 /usr/bin/podman: stdout cluster name ceph /usr/bin/podman: stdout crush device class None /usr/bin/podman: stdout encrypted 0 /usr/bin/podman: stdout osd fsid b2eb119c-4f45-430b-96b0-bad9e8b9aca6 /usr/bin/podman: stdout osd id 0 \u0026lt;-- the OSD number /usr/bin/podman: stdout osdspec affinity /usr/bin/podman: stdout type block /usr/bin/podman: stdout vdo 0 /usr/bin/podman: stdout devices /dev/sdf \u0026lt;--the path /usr/bin/podman: stdout # Shortened output for example To zap a single device:\n ceph orch device zap (hostname) (device path)  ncn-s00[123] ceph orch device zap ncn-s002 /dev/sdf "
},
{
	"uri": "/docs-csm/en-11/install/validate_management_network_cabling/",
	"title": "Validate Management Network Cabling",
	"tags": [],
	"description": "",
	"content": "Validate Management Network Cabling This page is designed to be a guide on how all nodes in a Shasta system are wired to the management network.\nThe Shasta Cabling Diagram (SHCD) for this system describes how the cables connect the nodes to the management network switches and the connections between the different types of management network switches. Having SHCD data which matches how the physical system is cabled will be needed later when preparing the hmn_connections.json file from the SHCD as part of Prepare Configuration Payload procedure and later when doing the Configure Management Network Switches procedure.\n Open the SHCD Look at the Device Diagrams Tab.  There you will see what type of hardware is on the system. Take note of the hardware.   Look at the 25G_10G or 40G_10G tab, this will depend on the SHCD.   Look at this part of the page. The source is either a node or a switch. The destination is usually a switch. The source label info and destination label info indicate the component name (xname) and the port on that component to which the cable is connected. For example, x3000u01s1-j1, means the device in cabinet x3000 at location u01 within the cabinet and in slot s1 with port j1. The physical cable connecting source to destination should be labelled with source label info and destination label info.\nSee Component Names (xnames).\n   Source Source Label Info Destination Label Info Destination     mn01 x3000u01s1-j1 x3000u24L-j1 sw-25g01   mn01 x3000u01s1-j2 x3000u24R-j1 sw-25g02   mn02 x3000u03s1-j1 x3000u24L-j2 sw-25g01   mn02 x3000u03s1-j2 x3000u24R-j2 sw-25g02   mn03 x3000u05s1-j1 x3000u24L-j3 sw-25g01   mn03 x3000u05s1-j2 x3000u24R-j3 sw-25g02   wn01 x3000u07s1-j1 x3000u24L-j4 sw-25g01   wn01 x3000u07s1-j2 x3000u24R-j4 sw-25g02   wn02 x3000u09s1-j1 x3000u24L-j5 sw-25g01   wn02 x3000u09s1-j2 x3000u24R-j5 sw-25g02   wn03 x3000u011s1-j1 x3000u24L-j6 sw-25g01   wn03 x3000u011s1-j2 x3000u24R-j6 sw-25g02   sn01 x3000u013s1-j1 x3000u24L-j7 sw-25g01   sn01 x3000u013s1-j2 x3000u24R-j7 sw-25g02   sn02 x3000u015s1-j1 x3000u24L-j8 sw-25g01   sn02 x3000u015s1-j2 x3000u24R-j8 sw-25g02   sn03 x3000u017s1-j1 x3000u24L-j9 sw-25g01   sn03 x3000u017s1-j2 x3000u24R-j9 sw-25g02   uan01 x3000u027s1-j1 x3000u24L-j10 sw-25g01   uan01 x3000u027s1-j2 x3000u24R-j10 sw-25g02   uan02 x3000u029s1-j1 x3000u24L-j13 sw-25g01   uan02 x3000u029s1-j2 x3000u24R-j13 sw-25g02   uan03 x3000u031s1-j1 x3000u24L-j14 sw-25g01   uan03 x3000u031s1-j2 x3000u24R-j14 sw-25g02      Based on the vendor of the nodes and the name in the first column we can determine how it is supposed to be cabled.\n  We can use mn01 as an example. This is a master node, and in the device diagrams tab it is identified as an HPE DL325 node.\n  Once you have those two pieces of information you can use the Cable Management Network Servers for all nodes listed on the SHCD.\n  UANs and Application Nodes\n  Worker nodes\n  Master nodes\n  Storage nodes\n    Checklist    Hardware Type Step Complete?     UAN/Application Node      Open the SHCD from the system     Go to the Device Diagrams tab, take note of the type of hardware on the system     Depending on the hardware, open either the 25G_10G tab or the 40G_10G tab.     Locate the nodes prefixed with uan or another prefix for application node     Based on the vendor of the node and the name in the first column we can determine how it is supposed to be cabled.     Check cabling against the Cable Management Network Servers If it is cabled incorrectly, contact the team in charge of cabling and request a change.    NCN-Master      Open the SHCD from the system     Go to the Device Diagrams tab, take note of the type of hardware on the system     Depending on the hardware, open either the 25G_10G tab or the 40G_10G tab.     Locate the nodes named mnxx     Based on the vendor of the node and the name in the first column we can determine how it is supposed to be cabled.     Check cabling against the Cable Management Network Servers If it is cabled incorrectly, contact the team in charge of cabling and request a change.    NCN-Worker      Open the SHCD from the system     Go to the Device Diagrams tab, take note of the type of hardware on the system     Depending on the hardware, open either the 25G_10G tab or the 40G_10G tab.     Locate the nodes named wnxx     Based on the vendor of the node and the name in the first column we can determine how it is supposed to be cabled.     Check cabling against the Cable Management Network Servers If it is cabled incorrectly, contact the team in charge of cabling and request a change.    NCN-Storage      Open the SHCD from the system     Go to the Device Diagrams tab, take note of the type of hardware on the system     Depending on the hardware, open either the 25G_10G tab or the 40G_10G tab.     Locate the nodes named snxx     Based on the vendor of the node and the name in the first column we can determine how it is supposed to be cabled.     Check cabling against the Cable Management Network Servers If it is cabled incorrectly, contact the team in charge of cabling and request a change.     "
},
{
	"uri": "/docs-csm/en-11/install/safeguards_for_csm_ncn_upgrades/",
	"title": "Safeguards For",
	"tags": [],
	"description": "",
	"content": "Safeguards for CSM This page covers safe-guards for preventing destructive behaviors on management nodes.\nIf you are reinstalling or upgrading you should run through these safe-guards on a by-case basis:\n Whether or not CEPH should be preserved. Whether or not the RAIDs should be protected.  Safeguard CEPH OSDs   Edit /var/www/ephemeral/configs/data.json and align the following options:\n{ .. // Disables Ceph wipe: \u0026#34;wipe-ceph-osds\u0026#34;: \u0026#34;no\u0026#34; .. } { .. // Restores default behavior: \u0026#34;wipe-ceph-osds\u0026#34;: \u0026#34;yes\u0026#34; .. } pit# vi /var/www/ephemeral/configs/data.json   Quickly toggle yes or no to the file:\n# set wipe-ceph-osds=no pit# sed -i \u0026#39;s/wipe-ceph-osds\u0026#34;: \u0026#34;yes\u0026#34;/wipe-ceph-osds\u0026#34;: \u0026#34;no\u0026#34;/g\u0026#39; /var/www/ephemeral/configs/data.json # set wipe-ceph-osds=yes pit# sed -i \u0026#39;s/wipe-ceph-osds\u0026#34;: \u0026#34;no\u0026#34;/wipe-ceph-osds\u0026#34;: \u0026#34;yes\u0026#34;/g\u0026#39; /var/www/ephemeral/configs/data.json   Activate the new setting:\npit# systemctl restart basecamp   Safeguard RAIDS / BOOTLOADERS / SquashFS / OverlayFS  Edit /var/www/boot/script.ipxe and align the following options as you see them here:    rd.live.overlay.reset=0 will prevent any overlayFS files from being cleared.\n  metal.no-wipe=1 will guard against touching RAIDs, disks, and partitions.\npit# vi /var/www/boot/script.ipxe   "
},
{
	"uri": "/docs-csm/en-11/install/set_gigabyte_node_bmc_to_factory_defaults/",
	"title": "Set Gigabyte Node BMC To Factory Defaults",
	"tags": [],
	"description": "",
	"content": "Set Gigabyte Node BMC to Factory Defaults Prerequisites Use the management scripts and text files to reset Gigabyte BMC to factory default settings. Set the BMC to the factory default settings in the following cases:\n There are problems using the ipmitool command and Redfish does not respond There are problems using the ipmitool command and Redfish is running When BIOS or BMC flash procedures fail using Redfish  Run the do_bmc_factory_default.sh script Run ipmitool -I lanplus -U admin -P password -H BMC_or_CMC_IP mc reset cold and flash it again after 5 minutes seconds   If booted from the PIT node, the required scripts are located in /var/www/fw/river/sh-svr-scripts  Procedure Apply the BMC Factory Command   Create a node.txt file and add the target node information as shown:\nExample node.txt file with two nodes:\n  10.254.1.11 x3000c0s9b0 ncn-w002 10.254.1.21 x3000c0s27b0 uan01 Example `node.txt` file with one node:  10.254.1.11 x3000c0s9b0 ncn-w002  Use Redfish to reset the BMC to factory default, the BMC is running 12.84.01 or later version, run:\nncn-w001# sh do_Redfish_BMC_Factory.sh   Alternatively, use ipmitool to reset the BMC to factory defaults, run:\nncn-w001# sh do_bmc_factory_default.sh   Alternatively, use the power control script, run:\nncn-w001# sh do_bmc_power_control.sh raw 0x32 0x66   After the BMC has Been Reset to Factory Defaults   Wait 5 minutes for BMC and Redfish initialization.\nncn-w001# sleep 300   Add the default login/password to the BMC.\n  ncn-w001# ncn-w001# sh do_bmc_root_account.sh  If BMC is 12.84.01 or later version, skip this step. Otherwise, add the default login/password to Redfish.\nncn-w001# sh do_Redfish_credentials.sh   Make sure the BMC is not in failover mode. Run the script with the read option to check the BMC status:\nncn-w001# sh do_bmc_change_mode_to_manual.sh read --------------------------------------------------- [ BMC: 172.30.48.33 ] =\u0026gt; Manual mode (O) If the BMC displays Failover mode:\n[ BMC: 172.30.48.33 ] ==\u0026gt; Failover mode (X) \u0026lt;== Change the BMC back to manual mode.\nncn-w001# sh do_bmc_change_mode_to_manual.sh change   If the BMC is in a booted management NCN running v1.4+ or v1.3, reapply the static IP address and clear the DHCP address from HSM/KEA. Determine the MAC address in HSM for the DHCP address for the BMC, then delete it from HSM and restart KEA.\n  Reboot or power cycle the target nodes.\nAfter the CMC is Reset to Factory Defaults   Wait 300 seconds for CMC and Redfish initialization, then add the default login/password to the CMC.\nncn-w001# sleep 300 ncn-w001# sh do_bmc_root_account.sh   "
},
{
	"uri": "/docs-csm/en-11/install/shcd_hmn_connections_rules/",
	"title": "Shcd Hmn Tab/hmn Connections Rules",
	"tags": [],
	"description": "",
	"content": "SHCD HMN Tab/HMN Connections Rules Table of contents:  Introduction Compute Node  Dense 4 node chassis - Gigabyte or Intel chassis Single node chassis - Apollo 6500 XL675D Dual node chassis - Apollo 6500 XL645D   Chassis Management Controller (CMC) Management Node  Master Worker Storage   Application Node  Single Node Chassis  Building xnames for nodes in a single application node chassis   Dual Node Chassis  Building xnames for nodes in a dual application node chassis     Columbia Slingshot Switch PDU Cabinet Controller Cooling Door Management Switches  Introduction The HMN tab of the SHCD describes the air-cooled hardware present in the system and how these devices are connected to the Hardware Management Network (HMN). This information is required by CSM to perform hardware discovery and geolocation of air-cooled hardware in the system. The HMN tab may contain other hardware that is not managed by CSM, but is connected to the HMN.\nThe hmn_connections.json file is derived from the HMN tab of a system SHCD, and is one of the seed files required by Cray Site Init (CSI) command to generate configuration files required to install CSM. The hmn_connections.json file is almost a 1 to 1 copy of the right-hand table in the HMN tab of the SHCD. It is an array of JSON objects, and each object represents a row from the HMN tab. Any row that is not understood by CSI will be ignored, this includes any additional devices connected to the HMN that are not managed by CSM.\nThe System Layout Service (SLS) contains data about what hardware is in the system and how it is connected to the HMN network. This data is generated when the CSI tool generates configurations files for system. For air-cooled hardware, SLS will contain the SLS representation of the device and a Management Switch Connector object that describes what device is plugged into a particular management switch port.\nColumn mapping from SHCD to hmn_connections.json:\n   SHCD Column SHCD Column Name hmn_connections Field Description     J20 Source Source Name of the device connected to the HMN network   K20 Rack SourceRack Source Rack, matches regex x\\d+   L20 Location SourceLocation For nodes (Management, Compute, Application), this is bottom most rack slot that the node occupies, and can be extracted by [a-zA-Z]*(\\d+)([a-zA-Z]*). For other device types this is ignored.   M20  SourceSubLocation For compute nodes, this can be L, l, R, r, or blank. For other device types this is ignored.   N20 Parent SourceParent    O20  not used    P20 Port not used    Q20 Destination not used    R20 Rack DestinationRack Rack of the management switch   S20 Location DestinationLocation Rack slot of the management switch   T20  not used    U20 Port DestinationPort Switch port on the management switch     Only J20 needs to have the column name of Source. There are no requirements on what the other columns should be named.\n Some conventions for this document:\n All Source names from the SHCD are lowercased before being processed by the CSI tool. Throughout this document the Field names from the hmn_connections.json file will be used to referenced values from the SHCD. Each device type has an example of how it is represented in the HMN tab of the SHCD, the hmn_connections.json file, and lastly in SLS.  Compute Node The Source field needs to match these conditions to be considered a compute node:\n Has the prefix of:  nid cn   Source field contains ends with an integer that matches this regex: (\\d+$)  This is integer is the Node ID (NID) for the node Each node should have a unique NID value    The following are valid source fields for example:\n nid000001 cn1 cn-01  Depending the type of compute node additional rules may apply. Compute nodes in the follow sections will use the nid prefix.\nDense 4 node chassis - Gigabyte or Intel chassis  Apollo 2000 compute nodes are not currently supported by CSM\n Air-cooled compute nodes are typically in a 2U chassis that contains 4 compute nodes. Each of the compute nodes in the chassis gets its own row in the HMN tab, plus a parent row.\nThe value of the SourceParent field is used to group together the 4 nodes that are contained within the same chassis, and it is used to reference another row in the SHCD HMN table. The referenced SourceParent row is used to determine the rack slot that the compute nodes in occupy.\n The SourceParent row can be a Chassis Management Controller which can be used to control devices underneath it. This device typically will have a connection to the HMN. A Gigabyte CMC is an example of a CMC. If a CMC is not connected to the HMN network, this will prevent CSM services from managing that device. The SourceParent row can be a virtual parent that is used to group the compute nodes together symbolically into a chassis. Does not need to not have a connection to the HMN.  The rack slot that a compute node occupies is determined by the Rack Slot of the SourceParent. The SourceLocation of the parent is the bottom most U of the chassis. The xname that is given to the 4 nodes in the same chassis used by the HMS/SLS services specify that all of the computes are in the same rack U (bottommost U of the chassis)\nThe BMC ordinal for the nodes BMC is derived from the NID of the node, by applying a modulo of 4 plus 1. For example, the node with NID 17 in slot 10 in cabinet 3000 will have the xname of x3000s10b2n0\nSHCD Example 4 compute nodes in the same chassis with a CMC connected to the network. The compute node chassis is located in slot 17 of cabinet 3000, and the compute node BMCs are connected to ports 33-36 in the management leaf switch in slot 14 of cabinet 3000. Port 32 on the leaf switch is for the CMC in the chassis, refer to Chassis Management Controller section for additional details.\n   Source Rack Location  Parent  Port Destination Rack Location  Port     nid000001 x3000 u17 R SubRack-001-CMC - j3 sw-smn01 x3000 u14 - j36   nid000002 x3000 u18 R SubRack-001-CMC - j3 sw-smn01 x3000 u14 - j35   nid000003 x3000 u18 L SubRack-001-CMC - j3 sw-smn01 x3000 u14 - j34   nid000004 x3000 u17 L SubRack-001-CMC - j3 sw-smn01 x3000 u14 - j33   SubRack-001-CMC x3000 u17   - cmc sw-smn01 x3000 u14 - j32     Note that Source names like cn1 and cn-01 are equivalent to the value nid000001\n Example 4 compute nodes in the same chassis without a CMC connected to the HMN network.\n   Source Rack Location  Parent  Port Destination Rack Location  Port     nid000001 x3000 u17 R SubRack-001-CMC - j3 sw-smn01 x3000 u14 - j36   nid000002 x3000 u18 R SubRack-001-CMC - j3 sw-smn01 x3000 u14 - j35   nid000003 x3000 u18 L SubRack-001-CMC - j3 sw-smn01 x3000 u14 - j34   nid000004 x3000 u17 L SubRack-001-CMC - j3 sw-smn01 x3000 u14 - j33   SubRack-001-CMC x3000 u17   -           Note that Source names like cn1 and cn-01 are equivalent to the value nid000001\n HMN Connections Example 4 compute nodes in the same chassis with the a CMC connected to the network. The compute node chassis is located in slot 17 of cabinet 3000, and the compute node BMCs are connected to ports 33-36 in the management leaf switch in slot 14 of cabinet 3000. The SourceParent for the compute nodes SubRack-001-CMC is connected to the port 32 on the leaf switch.\n{\u0026#34;Source\u0026#34;:\u0026#34;nid000001\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u17\u0026#34;,\u0026#34;SourceSubLocation\u0026#34;:\u0026#34;R\u0026#34;,\u0026#34;SourceParent\u0026#34;:\u0026#34;SubRack-001-CMC\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u14\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j36\u0026#34;} {\u0026#34;Source\u0026#34;:\u0026#34;nid000002\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u18\u0026#34;,\u0026#34;SourceSubLocation\u0026#34;:\u0026#34;R\u0026#34;,\u0026#34;SourceParent\u0026#34;:\u0026#34;SubRack-001-CMC\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u14\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j35\u0026#34;} {\u0026#34;Source\u0026#34;:\u0026#34;nid000003\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u18\u0026#34;,\u0026#34;SourceSubLocation\u0026#34;:\u0026#34;L\u0026#34;,\u0026#34;SourceParent\u0026#34;:\u0026#34;SubRack-001-CMC\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u14\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j34\u0026#34;} {\u0026#34;Source\u0026#34;:\u0026#34;nid000004\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u17\u0026#34;,\u0026#34;SourceSubLocation\u0026#34;:\u0026#34;L\u0026#34;,\u0026#34;SourceParent\u0026#34;:\u0026#34;SubRack-001-CMC\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u14\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j33\u0026#34;} {\u0026#34;Source\u0026#34;:\u0026#34;SubRack-001-CMC\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u17\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u14\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j32\u0026#34;}  Note that Source values like cn1 and cn-01 are equivalent to the value nid000001\n Example 4 compute nodes in the same chassis without a CMC connected to the HMN network.\n The SourceParent for the compute nodes SubRack-001-CMC is not connected the HMN network\n {\u0026#34;Source\u0026#34;:\u0026#34;nid000001\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u17\u0026#34;,\u0026#34;SourceSubLocation\u0026#34;:\u0026#34;R\u0026#34;,\u0026#34;SourceParent\u0026#34;:\u0026#34;SubRack-001-CMC\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u14\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j36\u0026#34;} {\u0026#34;Source\u0026#34;:\u0026#34;nid000002\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u18\u0026#34;,\u0026#34;SourceSubLocation\u0026#34;:\u0026#34;R\u0026#34;,\u0026#34;SourceParent\u0026#34;:\u0026#34;SubRack-001-CMC\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u14\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j35\u0026#34;} {\u0026#34;Source\u0026#34;:\u0026#34;nid000003\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u18\u0026#34;,\u0026#34;SourceSubLocation\u0026#34;:\u0026#34;L\u0026#34;,\u0026#34;SourceParent\u0026#34;:\u0026#34;SubRack-001-CMC\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u14\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j34\u0026#34;} {\u0026#34;Source\u0026#34;:\u0026#34;nid000004\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u17\u0026#34;,\u0026#34;SourceSubLocation\u0026#34;:\u0026#34;L\u0026#34;,\u0026#34;SourceParent\u0026#34;:\u0026#34;SubRack-001-CMC\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u14\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j33\u0026#34;} {\u0026#34;Source\u0026#34;:\u0026#34;SubRack-001-CMC\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u17\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34; \u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34; \u0026#34;}  Note that Source values like cn1 and cn-01 are equivalent to the value nid000001\n SLS The CSI tool will generate the following SLS representations compute nodes and their BMC connections to the HMN network.\nCompute node with NID 1:\n  Node\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0s17b1\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0s17b1n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NID\u0026#34;: 1, \u0026#34;Role\u0026#34;: \u0026#34;Compute\u0026#34;, \u0026#34;Aliases\u0026#34;: [ \u0026#34;nid000001\u0026#34; ] } }   Management Switch Connector:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0w14\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0w14j36\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_mgmt_switch_connector\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;MgmtSwitchConnector\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NodeNics\u0026#34;: [ \u0026#34;x3000c0s17b1\u0026#34; ], \u0026#34;VendorName\u0026#34;: \u0026#34;1/1/36\u0026#34; } }  For Aruba leaf switches the VendorName value will be 1/1/36. Dell leaf switches will have value ethernet1/1/36.\n   Compute node with NID 2:\n  Node:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0s17b2\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0s17b2n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NID\u0026#34;: 2, \u0026#34;Role\u0026#34;: \u0026#34;Compute\u0026#34;, \u0026#34;Aliases\u0026#34;: [ \u0026#34;nid000002\u0026#34; ] } }   Management Switch Connector:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0w14\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0w14j35\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_mgmt_switch_connector\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;MgmtSwitchConnector\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NodeNics\u0026#34;: [ \u0026#34;x3000c0s17b2\u0026#34; ], \u0026#34;VendorName\u0026#34;: \u0026#34;1/1/35\u0026#34; } }  For Aruba leaf switches the VendorName value will be 1/1/35. Dell leaf switches will have value ethernet1/1/35.\n   Compute node with NID 3:\n  Node\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0s17b3\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0s17b3n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NID\u0026#34;: 3, \u0026#34;Role\u0026#34;: \u0026#34;Compute\u0026#34;, \u0026#34;Aliases\u0026#34;: [ \u0026#34;nid000003\u0026#34; ] } }   Management Switch Connector:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0w14\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0w14j34\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_mgmt_switch_connector\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;MgmtSwitchConnector\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NodeNics\u0026#34;: [ \u0026#34;x3000c0s17b3\u0026#34; ], \u0026#34;VendorName\u0026#34;: \u0026#34;1/1/34\u0026#34; } }  For Aruba leaf switches the VendorName value will be 1/1/34. Dell leaf switches will have value ethernet1/1/34.\n   Compute node with NID 4:\n  Node\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0s17b4\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0s17b4n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NID\u0026#34;: 4, \u0026#34;Role\u0026#34;: \u0026#34;Compute\u0026#34;, \u0026#34;Aliases\u0026#34;: [ \u0026#34;nid000004\u0026#34; ] } }   Management Switch Connector:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0w14\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0w14j33\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_mgmt_switch_connector\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;MgmtSwitchConnector\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NodeNics\u0026#34;: [ \u0026#34;x3000c0s17b4\u0026#34; ], \u0026#34;VendorName\u0026#34;: \u0026#34;1/1/33\u0026#34; } }  For Aruba leaf switches the VendorName value will be 1/1/33. Dell leaf switches will have value ethernet1/1/33.\n   Single node chassis - Apollo 6500 XL675D A single compute node chassis needs to match these additional conditions:\n No SourceParent defined No SourceSubLocation defined  This convention applies to all compute nodes that have a single node in a chassis, such as the Apollo XL675D.\nSHCD A single chassis node with NID 1 located in slot 2 of cabinet 3000. The node\u0026rsquo;s BMC is connected to port 36 of the management leaf switch in slot 40 of cabinet 3000.\n   Source Rack Location  Parent  Port Destination Rack Location  Port     nid000001 x3000 u02   - j03 sw-smn01 x3000 u40 - j36     Note that Source values like cn1 and cn-01 are equivalent to the value nid000001\n HMN Connections The HMN connections representation for the two SHCD table rows above.\n{\u0026#34;Source\u0026#34;:\u0026#34;nid000001\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u02\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u40\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j36\u0026#34;}  Note that Source values like cn1 and cn-01 are equivalent to the value nid000001\n SLS Compute Node:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0s2b0\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0s2b0n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NID\u0026#34;: 1, \u0026#34;Role\u0026#34;: \u0026#34;Compute\u0026#34;, \u0026#34;Aliases\u0026#34;: [ \u0026#34;nid000001\u0026#34; ] } } Management Switch Connector:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0w40\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0w40j36\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_mgmt_switch_connector\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;MgmtSwitchConnector\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NodeNics\u0026#34;: [ \u0026#34;x3000c0s2b0\u0026#34; ], \u0026#34;VendorName\u0026#34;: \u0026#34;1/1/36\u0026#34; } }  For Aruba leaf switches the VendorName value will be 1/1/36. Dell leaf switches will have value ethernet1/1/36.\n Dual node chassis - Apollo 6500 XL645D Additional matching conditions:\n SourceSubLocation field contains one of: L, l, R, r.  In addition to the top-level compute node naming requirements when they are 2 nodes in a single chassis the SourceSubLocation is required. The SourceSubLocation can contain one of the following values: L, l, R, r. These values are used to determine the BMC ordinal for the node.\n L, l translates into the xname having b1  Such as x3000c0s10b1b0   R, r translates into the xname having b2  Such as x3000c0s10b1b0    This convention applies to all compute nodes that have two nodes in a chassis, such as the Apollo XL645D.\nSHCD A compute node chassis with 2 nodes located in slot 8 of cabinet 3000. NID 1 is on the left side of the chassis, and NID 2 is on the right side. The two node BMCs are connected to ports 37 and 38 of the management leaf switch in slot 40 of cabinet 3000.\n   Source Rack Location  Parent  Port Destination Rack Location  Port     nid000001 x3000 u08 L  - j03 sw-smn01 x3000 u40 - j38   nid000002 x3000 u08 R  - j03 sw-smn01 x3000 u40 - j37     Note that Source values like cn1 and cn-01 are equivalent to the value nid000001\n HMN Connections The HMN connections representation for the two SHCD table rows above.\n{\u0026#34;Source\u0026#34;:\u0026#34;nid000001\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u08\u0026#34;,\u0026#34;SourceSubLocation\u0026#34;:\u0026#34;L\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u40\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j37\u0026#34;} {\u0026#34;Source\u0026#34;:\u0026#34;nid000002\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u08\u0026#34;,\u0026#34;SourceSubLocation\u0026#34;:\u0026#34;R\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u40\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j38\u0026#34;}  Note that Source values like cn1 and cn-01 are equivalent to the value nid000001\n SLS Compute node with NID 1:\n Node: { \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0s8b1\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0s8b1n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NID\u0026#34;: 3, \u0026#34;Role\u0026#34;: \u0026#34;Compute\u0026#34;, \u0026#34;Aliases\u0026#34;: [ \u0026#34;nid000003\u0026#34; ] } }  Management Switch Connector { \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0w40\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0w40j38\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_mgmt_switch_connector\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;MgmtSwitchConnector\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NodeNics\u0026#34;: [ \u0026#34;x3000c0s8b1\u0026#34; ], \u0026#34;VendorName\u0026#34;: \u0026#34;1/1/38\u0026#34; } }  For Aruba leaf switches the VendorName value will be 1/1/38. Dell leaf switches will have value ethernet1/1/38.\n   Compute node with NID 2:\n  Node\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0s8b2\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0s8b2n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NID\u0026#34;: 2, \u0026#34;Role\u0026#34;: \u0026#34;Compute\u0026#34;, \u0026#34;Aliases\u0026#34;: [ \u0026#34;nid000002\u0026#34; ] } }   Management Switch Connectors:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0w40\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0w40j37\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_mgmt_switch_connector\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;MgmtSwitchConnector\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NodeNics\u0026#34;: [ \u0026#34;x3000c0s8b2\u0026#34; ], \u0026#34;VendorName\u0026#34;: \u0026#34;1/1/37\u0026#34; } }  For Aruba leaf switches the VendorName value will be 1/1/37. Dell leaf switches will have value ethernet1/1/37.\n   Chassis Management Controller (CMC)  This is not the same as an RCM (Rack Consolidation Module) that is present in Apollo 2000 chassis.\n Matching conditions:\n This row is referenced as a SourceParent of another row Source field contains cmc or CMC  A Chassis Management Controller is a device which can be used to BMCs underneath it. This device will typically have a connection to the HMN. A Gigabyte CMC is an example of a CMC. If a CMC is not connected to the HMN network, this will prevent CSM services from managing that device.\nThese devices will have the BMC ordinal of 999 for their xnames. Such as x3000c0s10b999.\nSHCD The CMC for the chassis in slot 28 of cabinet 3000 is connected to port 32 of the management leaf switch in slot 22 of cabinet 3000.\n   Source Rack Location  Parent  Port Destination Rack Location  Port     SubRack-002-cmc x3000 u28   - cmc sw-smn01 x3000 u22 - j42    HMN Connections The HMN connections representation for the SHCD table row above.\n{\u0026#34;Source\u0026#34;:\u0026#34;SubRack-002-cmc\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u28\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u22\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j42\u0026#34;} SLS Chassis Management Controller:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0s17b999\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_chassis_bmc\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;ChassisBMC\u0026#34; } Management Switch Connector:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0w14\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0w14j32\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_mgmt_switch_connector\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;MgmtSwitchConnector\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NodeNics\u0026#34;: [ \u0026#34;x3000c0s17b999\u0026#34; ], \u0026#34;VendorName\u0026#34;: \u0026#34;1/1/32\u0026#34; } }  For Aruba leaf switches the VendorName value will be 1/1/32. Dell leaf switches will have value ethernet1/1/32.\n Management Node Master The Source field needs to match both of the following conditions:\n mn prefix Integer number immediately after the prefix, can be padded with 0 characters.  The integer number after the prefix is used to determine the hostname of the master node. For example, mn02 corresponds to host name ncn-m002.\nTypically, the BMC of the first master node is not connected to the HMN network, as its BMC is connected to the site network.\nSHCD Example master node where its BMC is connected to the HMN. The master node is in slot 2 in cabinet 3000, and its BMC is connected to port 25 in the management leaf switch in slot 14 of cabinet 3000.\n   Source Rack Location  Parent  Port Destination Rack Location  Port     mn02 x3000 u02 -   j3 sw-smn01 x3000 u14 - j25    Example master node where its BMC is connected to the site network:\n   Source Rack Location  Parent  Port Destination Rack Location  Port     mn01 x3000 u01 -   j3         HMN Connections Example master node where its BMC is connected to the HMN:\n{\u0026#34;Source\u0026#34;:\u0026#34;mn02\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u02\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u14\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j25\u0026#34;} Example master node where its BMC is connected to the site network, and no connection to the HMN:\n{\u0026#34;Source\u0026#34;:\u0026#34;mn01\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u01\u0026#34;}  The following is also equivalent to a master node with not connection to the HMN. The values DestinationRack, DestinationLocation, DestinationPort can all contain whitespace and it is still considered to have no connection the HMN.\n{\u0026#34;Source\u0026#34;:\u0026#34;mn01\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u01\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34; \u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34; \u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34; \u0026#34;}  SLS Management Master Node:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0s2b0\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0s2b0n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NID\u0026#34;: 100008, \u0026#34;Role\u0026#34;: \u0026#34;Management\u0026#34;, \u0026#34;SubRole\u0026#34;: \u0026#34;Master\u0026#34;, \u0026#34;Aliases\u0026#34;: [ \u0026#34;ncn-m002\u0026#34; ] } } Management Switch Connector:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0w14\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0w14j25\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_mgmt_switch_connector\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;MgmtSwitchConnector\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NodeNics\u0026#34;: [ \u0026#34;x3000c0s2b0\u0026#34; ], \u0026#34;VendorName\u0026#34;: \u0026#34;1/1/25\u0026#34; } }  For Aruba leaf switches the VendorName value will be 1/1/25. Dell leaf switches will have value ethernet1/1/25.\n Worker The Source field needs to match both of the following conditions:\n wn prefix Integer number immediately after the prefix, can be padded with 0 characters.  The integer number after the prefix is used to determine the hostname of the master node. For example, wn01 corresponds to host name ncn-w001.\nSHCD The worker node is in slot 4 of cabinet 3000, and its BMC is connected to port 48 of management leaf switch in slot 14 of cabinet 3000.\n   Source Rack Location  Parent  Port Destination Rack Location  Port     wn01 x3000 u04 -   j3 sw-smn01 x3000 u14 - j48    HMN Connections The HMN connections representation for the SHCD table row above.\n{\u0026#34;Source\u0026#34;:\u0026#34;wn01\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u04\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u14\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j48\u0026#34;} SLS Management Worker Node:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0s4b0\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0s4b0n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NID\u0026#34;: 100006, \u0026#34;Role\u0026#34;: \u0026#34;Management\u0026#34;, \u0026#34;SubRole\u0026#34;: \u0026#34;Worker\u0026#34;, \u0026#34;Aliases\u0026#34;: [ \u0026#34;ncn-w001\u0026#34; ] } } Management Switch Connector:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0w14\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0w14j48\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_mgmt_switch_connector\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;MgmtSwitchConnector\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NodeNics\u0026#34;: [ \u0026#34;x3000c0s4b0\u0026#34; ], \u0026#34;VendorName\u0026#34;: \u0026#34;1/1/48\u0026#34; } }  For Aruba leaf switches the VendorName value will be 1/1/48. Dell leaf switches will have value ethernet1/1/48∂.\n Storage The Source field needs to match both of the following conditions:\n sn prefix Integer number immediately after the prefix, can be padded with 0 characters.  The integer number after the prefix is used to determine the hostname of the master node. For example, sn01 corresponds to host name ncn-s001.\nSHCD The storage node is in slot 4 of cabinet 3000, and its BMC is connected to port 29 of management leaf switch in slot 14 of cabinet 3000.\n   Source Rack Location  Parent  Port Destination Rack Location  Port     sn01 x3000 u07 -   j3 sw-smn01 x3000 u14 - j29    HMN Connections The HMN connections representation for the two SHCD table rows above.\n{\u0026#34;Source\u0026#34;:\u0026#34;sn01\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u07\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u14\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j29\u0026#34;} SLS Management Storage Node:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0s7b0\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0s7b0n0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_node\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NID\u0026#34;: 100003, \u0026#34;Role\u0026#34;: \u0026#34;Management\u0026#34;, \u0026#34;SubRole\u0026#34;: \u0026#34;Storage\u0026#34;, \u0026#34;Aliases\u0026#34;: [ \u0026#34;ncn-s001\u0026#34; ] } } Management Switch Connector:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0w14\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0w14j29\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_mgmt_switch_connector\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;MgmtSwitchConnector\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NodeNics\u0026#34;: [ \u0026#34;x3000c0s7b0\u0026#34; ], \u0026#34;VendorName\u0026#34;: \u0026#34;1/1/29\u0026#34; } }  For Aruba leaf switches the VendorName value will be 1/1/29. Dell leaf switches will have value ethernet1/1/29.\n Application Node The Source field needs to match these conditions to be considered an application node:\n Has the prefix of:  uan gn ln   The naming conventions for application nodes can be unique to a system. Refer to the application node config procedure for the process to to adding additional Source name prefixes for application nodes.\n   Single Node Chassis A single application node chassis needs to match these additional conditions:\n No SourceParent defined No SourceSubLocation defined  This convention applies to all application nodes that have a single node in a chassis.\nSHCD Example application node is in slot 4 of cabinet 3000, and its BMC is connected to port 25 of management leaf switch in slot 14 of cabinet 3000.\n   Source Rack Location  Parent  Port Destination Rack Location  Port     uan01 x3000 u04 -   j3 sw-smn01 x3000 u14 - j25    HMN Connections The HMN connections representation for the SHCD table row above.\n{\u0026#34;Source\u0026#34;:\u0026#34;uan01\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u04\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u14\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j25\u0026#34;} Building xnames for nodes in a single application node chassis The xname format for nodes takes the form of xXcCsSbBnN:\n xX: where X is the Cabinet or Rack identification number. cC: where C is the chassis identification number. This should be 0. sS: where S is the lowest slot the node chassis occupies. bB: where B is the ordinal of the node BMC. This should be 0. nN: where N is the ordinal of the node This should be 0.  For example, if an application node is in slot 4 of cabinet 3000, then it would have x3000c0s4b0n0 as its xname.\nDual Node Chassis Additional matching conditions:\n SourceSubLocation field contains one of: L, l, R, r.  In addition to the top-level compute node naming requirements when they are 2 nodes in a single chassis the SourceSubLocation is required. The SourceSubLocation can contain one of the following values: L, l, R, r. These values are used to determine the BMC ordinal for the node.\n L, l translates into the xname having b1  Such as x3000c0s10b1b0   R, r translates into the xname having b2  Such as x3000c0s10b1b0    This convention applies to all application nodes that have two nodes in a single chassis.\nSHCD A application node chassis with 2 nodes located in slot 8 of cabinet 3000. uan01 is on the left side of the chassis, and uan02 is on the right side. The two node BMCs are connected to ports 37 and 38 of the management leaf switch in slot 40 of cabinet 3000.\n   Source Rack Location  Parent  Port Destination Rack Location  Port     uan01 x3000 u08 L  - j03 sw-smn01 x3000 u40 - j38   uan02 x3000 u08 R  - j03 sw-smn01 x3000 u40 - j37     Note that Source values like cn1 and cn-01 are equivalent to the value nid000001\n HMN Connections The HMN connections representation for the two SHCD table rows above.\n{\u0026#34;Source\u0026#34;:\u0026#34;uan01\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u08\u0026#34;,\u0026#34;SourceSubLocation\u0026#34;:\u0026#34;L\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u40\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j37\u0026#34;} {\u0026#34;Source\u0026#34;:\u0026#34;uan02\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u08\u0026#34;,\u0026#34;SourceSubLocation\u0026#34;:\u0026#34;R\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u40\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j38\u0026#34;} Building xnames for nodes in a dual application node chassis The xname format for nodes takes the form of xXcCsSbBnN:\n xX: where X is the Cabinet or Rack identification number. cC: where C is the chassis identification number. This should be 0. sS: where S is the lowest slot the node chassis occupies. bB: where B is the ordinal of the node BMC.  If the SourceSubLocation is L or l, then this should be 1. If the SourceSubLocation is R or r, then this should be 2.   nN: where N is the ordinal of the node This should be 0.  For example:\n If an application node is in slot 8 of cabinet 3000 with a SourceSubLocation of L, then it would have x3000c0s8b1n0 as its xname. If an application node is in slot 8 of cabinet 3000 with a SourceSubLocation of R, then it would have x3000c0s8b2n0 as its xname.  Columbia Slingshot Switch The Source field needs to matching one of the following conditions:\n Prefixed with: sw-hsn Equal to columbia or Columbia  The following are examples of valid matches:\n sw-hsn01 Columbia columbia  SHCD A Columbia Slingshot Switch in slot 42 of cabinet 3000. Its BMC is connected to port 45 of the leaf switch in slot 38 of cabinet 3000.\n   Source Rack Location  Parent  Port Destination Rack Location  Port     sw-hsn01 x3000 u42 -   j3 sw-smn01 x3000 u38 - j45     Note that Source values like Columbia or columbia are also valid.\n HMN Connections The HMN connections representation for the SHCD table row above.\n{\u0026#34;Source\u0026#34;:\u0026#34;sw-hsn01\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u42\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u38\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j45\u0026#34;} SLS Router BMC:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0r42b0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_rtr_bmc\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;RouterBMC\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;Username\u0026#34;: \u0026#34;vault://hms-creds/x3000c0r42b0\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;vault://hms-creds/x3000c0r42b0\u0026#34; } } Management Switch Connector:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0w38\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0w38j45\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_mgmt_switch_connector\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;MgmtSwitchConnector\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NodeNics\u0026#34;: [ \u0026#34;x3000c0r42b0\u0026#34; ], \u0026#34;VendorName\u0026#34;: \u0026#34;1/1/45\u0026#34; } }  For Aruba leaf switches the VendorName value will be 1/1/45. Dell leaf switches will have value ethernet1/1/45.\n PDU Cabinet Controller The Source field for a PDU Cabinet Controller needs to match the following regex (x\\d+p|pdu)(\\d+). This regex matches the following 2 patterns:\n xXpP where X is the cabinet number, and P is the ordinal of the PDU controller in the cabinet pduP where P is the ordinal of the PDU controller in the cabinet  The following are examples of valid matches:\n x3000p0 pdu0  A PDU Cabinet Controller is the device that is connected to the HMN network and manages PDU underneath it.\nSHCD PDU controller for cabinet 3000 is connected port 41 of the leaf switch in slot 38 of cabinet 3000.\n   Source Rack Location  Parent  Port Destination Rack Location  Port     x3000p0 x3000  -   i0 sw-smn01 x3000 u38 - j41    Alternative naming convention for the same HMN connection.\n   Source Rack Location  Parent  Port Destination Rack Location  Port     pdu0 x3000 pdu0 -   j01 sw-smn01 x3000 u40 - j48    HMN Connections The HMN connections representation for the first SHCD table above.\n{\u0026#34;Source\u0026#34;:\u0026#34;x3000p0\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34; \u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u38\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j41\u0026#34;} The HMN connections representation for alternative naming convention.\n{\u0026#34;Source\u0026#34;:\u0026#34;pdu0\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;pdu0\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u38\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j41\u0026#34;} SLS Cabinet PDU Controller:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000m0\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_cab_pdu_controller\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;CabinetPDUController\u0026#34; } Management Switch Connector:\n{ \u0026#34;Parent\u0026#34;: \u0026#34;x3000c0w38\u0026#34;, \u0026#34;Xname\u0026#34;: \u0026#34;x3000c0w38j41\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;comptype_mgmt_switch_connector\u0026#34;, \u0026#34;Class\u0026#34;: \u0026#34;River\u0026#34;, \u0026#34;TypeString\u0026#34;: \u0026#34;MgmtSwitchConnector\u0026#34;, \u0026#34;ExtraProperties\u0026#34;: { \u0026#34;NodeNics\u0026#34;: [ \u0026#34;x3000m0\u0026#34; ], \u0026#34;VendorName\u0026#34;: \u0026#34;1/1/41\u0026#34; } }  For Aruba leaf switches the VendorName value will be 1/1/41. Dell leaf switches will have value ethernet1/1/41.\n Cooling Door The Source field for a Cooling door contains door.\nCooling doors in an air-cooled cabinet are not currently supported by CSM software and are ignored.\nSHCD Cooling door for cabinet 3000 is connected to port 27 of the leaf switch in slot 36 of cabinet 3000.\n   Source Rack Location  Parent  Port Destination Rack Location  Port     x3000door-Motiv x3000  -   j1 sw-smn04 x3000 u36 - j27    HMN Connections The HMN connections representation for the SHCD table row above.\n{\u0026#34;Source\u0026#34;:\u0026#34;x3000door-Motiv\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34; \u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u36\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j27\u0026#34;} SLS Cooling doors are not currently supported by HMS services, and are not present in SLS.\nManagement Switches The Source Field matches has one of the following prefixes: * sw-agg * sw-25g * sw-40g * sw-100g * sw-smn\nAny management switch that is found in the HMN tab of the SHCD will be ignored by CSI.\nSHCD Management switch in slot 12 of cabinet 3000, its management port is connected to port 41 of the leaf management switch in slot 14 of cabinet 3000.\n   Source Rack Location  Parent  Port Destination Rack Location  Port     sw-25g01 x3000 u12 -   j1 sw-smn01 x3000 u14 - j41    HMN Connections The HMN connections representation for the SHCD table row above.\n{\u0026#34;Source\u0026#34;:\u0026#34;sw-25g01\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u12\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u14\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j41\u0026#34;} SLS The Management switches in SLS are not populated by hmn_connections.json, instead from switch_metadata.csv.\n"
},
{
	"uri": "/docs-csm/en-11/install/switch_pxe_boot_from_onboard_nic_to_pcie/",
	"title": "Switch PXE Boot From Onboard NIC To Pcie",
	"tags": [],
	"description": "",
	"content": "Switch PXE Boot from Onboard NIC to PCIe This page details how to migrate NCNs from using their onboard NICs for PXE booting to booting over the PCIe cards.\n Enabling UEFI PXE Mode  Mellanox  Print current UEFI and SR-IOV State Setting Expected Values High-Speed Network Obtaining Mellanox Tools   QLogic FastLinq  Kernel Modules     Disabling/Removing On-Board Connections  This applies to Newer systems (Spring 2020 or newer) where onboard NICs are still used.\nThis presents a need for migration for systems still using the legacy, preview topology. Specifically, systems with onboard connections to their leaf switches and NCNs need to disable/remove that connection.\nThis onboard NCN port came from before spine-switches were added to the shasta-network topology. The onboard connection was responsible for every network (MTL/NMN/HMN/CAN) and was the sole driver of PXE booting for. Now, NCNs use bond interfaces and spine switches for those networks, however some older systems still have this legacy connection to their leaf switches and solely use it for PXE booting. This NIC is not used during runtime, and NCNs in this state should enable PXE within their PCIe devices' OpROMs and disable/remove this onboard connection.\nEnabling UEFI PXE Mode Mellanox This uses the Mellanox CLI Tools for configuring UEFI PXE from the Linux command line.\nOn any NCN (using 0.0.10 k8s, or 0.0.8 Ceph; anything built on ncn-0.0.21 or higher) can run this to begin interacting with Mellanox cards: If you are recovering NCNs with an earlier image without the Mellanox tools, please refer to the section on the bottom of the Mellanox this segment.\nncn# mst start Now mst status and other commands like mlxfwmanager or mlxconfig will work, and devices required for these commands will be created in /dev/mst.\nPrint current UEFI and SR-IOV State  UEFI: all boots are UEFI, this needs to be enabled for access to the UEFI OpROM for configuration and for usage of UEFI firmwares. SR_IOV: This is currently DISABLED because it can attribute to longer POSTs on HPE blades (Gen10+, i.e. DL325 or DL385) with Mellanox ConnectX-5 PCIe cards. The technology is not yet enabled for virtualization usage, but may be in the future.\n Use this snippet to display device name and current UEFI PXE state.\nncn# mst status for MST in $(ls /dev/mst/*); do mlxconfig -d ${MST} q | egrep \u0026#34;(Device|EXP_ROM|SRIOV_EN)\u0026#34; done Setting Expected Values Use this snippet to enable and dump UEFI PXE state.\nfor MST in $(ls /dev/mst/*); do echo ${MST} mlxconfig -d ${MST} -y set EXP_ROM_UEFI_x86_ENABLE=1 mlxconfig -d ${MST} -y set EXP_ROM_PXE_ENABLE=1 mlxconfig -d ${MST} -y set SRIOV_EN=0 mlxconfig -d ${MST} q | egrep \u0026#34;EXP_ROM\u0026#34; done High-Speed Network For worker nodes with High-Speed network attachments, the PXE and SR-IOV features should be disabled.\n  Run mlxfwmanager to probe and dump your Mellanox PCIe cards\nncn# mlxfwmanager   Find the device path for the HSN card, assuming it is a ConnectX-5 or other 100GB card this should be easy to pick out.\n  Run this, swapping the MST variable for your actual card path\n# Set UEFI to YES ncn# MST=/dev/mst/mt4119_pciconf1 ncn# mlxconfig -d ${MST} -y set EXP_ROM_UEFI_ARM_ENABLE=0 ncn# mlxconfig -d ${MST} -y set EXP_ROM_UEFI_x86_ENABLE=0 ncn# mlxconfig -d ${MST} -y set EXP_ROM_PXE_ENABLE=0 ncn# mlxconfig -d ${MST} -y set SRIOV_EN=0 ncn# mlxconfig -d ${MST} q | egrep \u0026#34;EXP_ROM\u0026#34;   Your Mellanox HSN card is now neutralized, and will only be usable in a booted system.\nObtaining Mellanox Tools mft is installed in 1.4 NCN images, for 1.3 systems they will need to obtain the tools by hand:\nlinux# wget https://www.mellanox.com/downloads/MFT/mft-4.15.1-9-x86_64-rpm.tgz linux# tar -xzvf mft-4.15.1-9-x86_64-rpm.tgz linux# cd mft-4.15.1-9-x86_64-rpm/RPMS linux# rpm -ivh ./mft-4.15.1-9.x86_64.rpm linux# cd linux# mst start QLogic FastLinq These should already be configured for PXE booting.\nKernel Modules KMP modules for Qlogic are installed:\n qlgc-fastlinq-kmp-default qlgc-qla2xxx-kmp-default  See [#casm-triage][2] if this is not the case.\nDisabling or Removing On-Board Connections The onboard connection can be disabled a few ways, short of removing the physical connection one may shutdown the switchport as well.\nIf you can remove the physical connection, this is preferred and can be done so after enabling PXE on the PCIe cards.\nIf you want to disable the connection, you will need to log in to your respective leaf switch.\n Connect to the leaf switch using serial or SSH connections.  Select one of the connection options below. The IP addresses and device names may vary in the commands below. ```bash # SSH over METAL MANAGEMENT pit# ssh admin@10.1.0.4 # SSH over NODE MANAGEMENT pit# ssh admin@10.252.0.4 # SSH over HARDWARE MANAGEMENT pit# ssh admin@10.254.0.4\n# or.. serial (device name will vary). pit# minicom -b 115200 -D /dev/tty.USB1 ```  Enter configuration mode sw-leaf-001\u0026gt; configure terminal sw-leaf-001(config)#\u0026gt;  Disable the NCN interfaces. Check your SHCD for reference before continuing so that the interfaces which are connected to management NCNs are being changes. Ports 2 to 10 are commonly the master, worker, and storage nodes when there are 3 of each. Some systems may have more worker nodes or utility storage nodes, or may be racked and cabled differently. sw-leaf-001(config)#\u0026gt; interface range 1/1/2-1/1/10 sw-leaf-001(config)#\u0026gt; shutdown sw-leaf-001(config)#\u0026gt; write memory   You can enable them again at anytime by switching the shutdown command out for no shutdown.\n"
},
{
	"uri": "/docs-csm/en-11/install/redeploy_pit_node/",
	"title": "Redeploy Pit Node",
	"tags": [],
	"description": "",
	"content": "Redeploy PIT Node The following procedure contains information for rebooting and deploying the management node that is currently hosting the LiveCD. This assists with remote-console setup to aid in observing the reboot. At the end of this procedure, the LiveCD will no longer be active. The node it was using will join the Kubernetes cluster as the final of three master nodes forming a quorum.\nImportant: While the node is rebooting, it will be available only through Serial-over-LAN and local terminals. This procedure entails deactivating the LiveCD, meaning the LiveCD and all of its resources will be unavailable.\nTopics:\n Required Services Notice of Danger Hand-Off  Start Hand-Off   Reboot Enable NCN Disk Wiping Safeguard Configure DNS and NTP on each BMC Next Topic  Details 1. Required Services These services must be healthy before the reboot of the LiveCD can take place. If the health checks executed in the previous installation step completed successfully (Validate CSM Health), the following services will be healthy and ready for reboot of the LiveCD:\nRequired Platform Services:\n Utility Storage (Ceph) cray-bss cray-dhcp-kea cray-dns-unbound cray-ipxe cray-sls cray-tftp  2. Notice of Danger  An administrator is strongly encouraged to be mindful of pitfalls during this segment of the CSM install. The steps below do contain warnings themselves, but overall there are risks:\n  SSH will cease to work when the LiveCD reboots; the serial console will need to be leveraged\n  Rebooting a remoteISO will dump all running changes on the PIT node; USBs are accessible after the install\n  The NCN will never wipe a USB device during installation\n     Prior to shutting down the PIT, learning the CAN IP addresses of the other NCNs will be a benefit if troubleshooting is required  This procedure entails deactivating the LiveCD, meaning the LiveCD and all of its resources will be unavailable.\n 3. Hand-Off The steps in this guide will ultimately walk an administrator through loading hand-off data and rebooting the node. This will assist with remote-console setup, for observing the reboot.\nAt the end of these steps, the LiveCD will be no longer active. The node it was using will join the Kubernetes cluster as the final of three master nodes forming a quorum.\n3.1 Start Hand-Off   Start a new typescript (quit ) (Run this on the PIT node as root, the prompts are removed for easier copy-paste; this step is only useful as a whole)\n  Exit the current typescript if one has arrived here from the prior pages:\npit# exit pit# popd   Start the new script\nmkdir -pv /var/www/ephemeral/prep/admin pushd /var/www/ephemeral/prep/admin script -af csm-livecd-reboot.$(date +%Y-%m-%d).txt export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39;     Follow the workaround instructions for the livecd-pre-reboot breakpoint.\n  Check for workarounds in the /opt/cray/csm/workarounds/livecd-pre-reboot directory. If there are any workarounds in that directory, run those when the workaround instructs. Timing is critical to ensure properly loaded data, so run them only when indicated. Instructions are in the README files.\n# Example pit# ls /opt/cray/csm/workarounds/livecd-pre-reboot If there is a workaround here, the output looks similar to the following:\nCASMINST-435   Upload SLS file.\n Note the system name environment variable SYSTEM_NAME must be set\n pit# csi upload-sls-file --sls-file /var/www/ephemeral/prep/${SYSTEM_NAME}/sls_input_file.json Expected output looks similar to the following:\n2021/02/02 14:05:15 Retrieving S3 credentials ( sls-s3-credentials ) for SLS 2021/02/02 14:05:15 Uploading SLS file: /var/www/ephemeral/prep/eniac/sls_input_file.json 2021/02/02 14:05:15 Successfully uploaded SLS Input File.   Get a token to use for authenticated communication with the gateway.\n NOTE api-gw-service-nmn.local is legacy, and will be replaced with api-gw-service.nmn.\n pit# export TOKEN=$(curl -k -s -S -d grant_type=client_credentials \\  -d client_id=admin-client \\  -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\  https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39;)   Upload NCN boot artifacts into S3.\n  Set variables\nIMPORTANT: The variables you set depend on whether or not you customized the default NCN images. The most common procedures that involve customizing the images are Configuring NCN Images to Use Local Timezone and Changing NCN Image Root Password and SSH Keys. The two paths forward are listed below:\n  If you did not customize the NCN images, set the following variables (this is the default path):\npit# export CSM_RELEASE=csm-x.y.z pit# export artdir=/var/www/ephemeral/${CSM_RELEASE}/images pit# export k8sdir=$artdir/kubernetes pit# export cephdir=$artdir/storage-ceph   If you customized the NCN images, set the following variables:\npit# export artdir=/var/www/ephemeral/data pit# export k8sdir=$artdir/k8s pit# export cephdir=$artdir/ceph     After setting the variables above per your situation, run:\npit# csi handoff ncn-images \\ --k8s-kernel-path $k8sdir/*.kernel \\ --k8s-initrd-path $k8sdir/initrd.img*.xz \\ --k8s-squashfs-path $k8sdir/*.squashfs \\ --ceph-kernel-path $cephdir/*.kernel \\ --ceph-initrd-path $cephdir/initrd.img*.xz \\ --ceph-squashfs-path $cephdir/*.squashfs Running this command will output a block that looks like this at the end:\nYou should run the following commands so the versions you just uploaded can be used in other steps: export KUBERNETES_VERSION=x.y.z export CEPH_VERSION=x.y.z   Run the export commands listed at the end of the output from the previous step.\n    Upload the same data.json file we used to BSS, our Kubernetes cloud-init DataSource.\nIf you have made any changes to this file as a result of any customizations or workarounds, use the path to that file instead. This step will prompt for the root password of the NCNs.\npit# csi handoff bss-metadata --data-file /var/www/ephemeral/configs/data.json || echo \u0026#34;ERROR: csi handoff bss-metadata failed\u0026#34;   Patch the metadata for the CEPH nodes to have the correct run commands:\npit# python3 /usr/share/doc/csm/scripts/patch-ceph-runcmd.py   Ensure the DNS server value is correctly set to point toward Unbound at 10.92.100.225.\npit# csi handoff bss-update-cloud-init --set meta-data.dns-server=10.92.100.225 --limit Global   Upload the bootstrap information; note this denotes information that should always be kept together in order to fresh-install the system again.\n  Log in; setup passwordless SSH to the PIT node by copying ONLY the public keys from ncn-m002 and ncn-m003 to the PIT (do not setup passwordless SSH from the PIT or the key will have to be securely tracked or expunged if using a USB installation).\npit# CSM_RELEASE=$(basename $(ls -d /var/www/ephemeral/csm*/ | head -n 1)) pit# echo \u0026#34;${CSM_RELEASE}\u0026#34; # these will prompt for a password: pit# ssh ncn-m002 cat /root/.ssh/id_rsa.pub \u0026gt;\u0026gt; /root/.ssh/authorized_keys pit# ssh ncn-m003 cat /root/.ssh/id_rsa.pub \u0026gt;\u0026gt; /root/.ssh/authorized_keys pit# chmod 600 /root/.ssh/authorized_keys   Run this to create the backup; in one swoop, log in to ncn-m002 and ncn-m003 and pull the files off the PIT. This runs rsync with specific parameters; partial, non-verbose, and progress.\npit# ssh ncn-m002 CSM_RELEASE=$(basename $(ls -d /var/www/ephemeral/csm*/ | head -n 1)) \\ \u0026#34;mkdir -pv /metal/bootstrap rsync -e \u0026#39;ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null\u0026#39; -rltD -P --delete pit.nmn:/var/www/ephemeral/prep /metal/bootstrap/ rsync -e \u0026#39;ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null\u0026#39; -rltD -P --delete pit.nmn:/var/www/ephemeral/${CSM_RELEASE}/cray-pre-install-toolkit*.iso /metal/bootstrap/\u0026#34; pit# ssh ncn-m003 CSM_RELEASE=$(basename $(ls -d /var/www/ephemeral/csm*/ | head -n 1)) \\ \u0026#34;mkdir -pv /metal/bootstrap rsync -e \u0026#39;ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null\u0026#39; -rltD -P --delete pit.nmn:/var/www/ephemeral/prep /metal/bootstrap/ rsync -e \u0026#39;ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null\u0026#39; -rltD -P --delete pit.nmn:/var/www/ephemeral/${CSM_RELEASE}/cray-pre-install-toolkit*.iso /metal/bootstrap/\u0026#34;     List ipv4 boot options using efibootmgr:\npit# efibootmgr | grep -Ei \u0026#34;ip(v4|4)\u0026#34;   Set and trim the boot order on the PIT node.\nIn Deploy Management Nodes.\n  Tell the node to PXE boot on the next boot \u0026hellip; use efibootmgr to set next boot device to the first PXE boot option. This step assumes the boot order was set up by the immediate, previous step.\npit# efibootmgr -n $(efibootmgr | grep -Ei \u0026#34;ip(v4|4)\u0026#34; | awk \u0026#39;{print $1}\u0026#39; | head -n 1 | tr -d Boot*) | grep -i bootnext BootNext: 0014   Collect a backdoor login. Fetch the CAN IP address for ncn-m002 for a backdoor during the reboot of ncn-m001.\n  Get the IP\npit# ssh ncn-m002 \u0026#39;ip a show vlan007 | grep inet\u0026#39; Expected output (values may differ):\ninet 10.102.11.13/24 brd 10.102.11.255 scope global vlan007 inet6 fe80::1602:ecff:fed9:7820/64 scope link   Log in from another external machine to verify SSH is up and running for this session.\nexternal# ssh root@10.102.11.13 ncn-m002#    Keep this terminal active as it will enable kubectl commands during the bring-up of the new NCN. If the reboot successfully deploys the LiveCD, this terminal can be exited.\n  POINT OF NO RETURN The next step will wipe the underlying nodes disks clean, it will ignore USB devices. RemoteISOs are at risk here, even though a backup has been performed of the PIT node, we cannot simply boot back to the same state. This is the last step before rebooting the node.\n   Wipe the disks on the PIT node.\n WARNING : USER ERROR Do not assume to wipe the first three disks (e.g. sda, sdb, and sdc), they float and are not pinned to any physical disk layout. Choosing the wrong ones may result in wiping the USB device, the USB device can only be wiped by operators at this point in the install. The USB device are never wiped by the CSM installer.\n   Select disks to wipe; SATA/NVME/SAS\npit# md_disks=\u0026#34;$(lsblk -l -o SIZE,NAME,TYPE,TRAN | grep -E \u0026#39;(sata|nvme|sas)\u0026#39; | sort -h | awk \u0026#39;{print \u0026#34;/dev/\u0026#34; $2}\u0026#39;)\u0026#34;   Sanity check; print disks into typescript or console\npit# echo $md_disks Expected output looks similar to the following:\n/dev/sda /dev/sdb /dev/sdc   Wipe. This is irreversible.\npit# wipefs --all --force $md_disks If any disks had labels present, output looks similar to the following:\n/dev/sda: 8 bytes were erased at offset 0x00000200 (gpt): 45 46 49 20 50 41 52 54 /dev/sda: 8 bytes were erased at offset 0x6fc86d5e00 (gpt): 45 46 49 20 50 41 52 54 /dev/sda: 2 bytes were erased at offset 0x000001fe (PMBR): 55 aa /dev/sdb: 6 bytes were erased at offset 0x00000000 (crypto_LUKS): 4c 55 4b 53 ba be /dev/sdb: 6 bytes were erased at offset 0x00004000 (crypto_LUKS): 53 4b 55 4c ba be /dev/sdc: 8 bytes were erased at offset 0x00000200 (gpt): 45 46 49 20 50 41 52 54 /dev/sdc: 8 bytes were erased at offset 0x6fc86d5e00 (gpt): 45 46 49 20 50 41 52 54 /dev/sdc: 2 bytes were erased at offset 0x000001fe (PMBR): 55 aa If there was any wiping done, output should appear similar to the snippet above. If this is re-run, there may be no output or an ignorable error.\n    If you wish to preserve your conman console logs for the other NCNs, this is your last chance to do so. They will be lost after rebooting. They are located in /var/log/conman on the PIT node.\n  Quit the typescript session with the exit command and copy the file (csm-livecd-reboot.\u0026lt;date\u0026gt;.txt) to a location on another server for reference later.\npit# exit   Optionally, setup conman or serial console, if not already on, from any laptop or other system with network connectivity to the cluster.\nexternal# script -a boot.livecd.$(date +%Y-%m-%d).txt external# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39; external# SYSTEM_NAME=eniac external# USERNAME=root external# export IPMI_PASSWORD=changeme external# ipmitool -I lanplus -U $USERNAME -E -H ${SYSTEM_NAME}-ncn-m001-mgmt chassis power status external# ipmitool -I lanplus -U $USERNAME -E -H ${SYSTEM_NAME}-ncn-m001-mgmt sol activate   4. Reboot   Reboot the LiveCD.\npit# reboot   The node should boot, acquire its hostname (i.e. ncn-m001), and run cloud-init.\n NOTE: If the nodes has PXE boot issues, such as getting PXE errors or not pulling the ipxe.efi binary, see PXE boot troubleshooting\n  NOTE: If ncn-m001 did not run all the cloud-init scripts, the following commands need to be run (but only in that circumstance).\n   Run the following commands:\nncn-m001# cloud-init clean ncn-m001# cloud-init init ncn-m001# cloud-init modules -m init ncn-m001# cloud-init modules -m config ncn-m001# cloud-init modules -m final     Once cloud-init has completed successfully, log in and start a typescript (the IP address used here is the one we noted for ncn-m002 in an earlier step).\nexternal# ssh root@10.102.11.13 ncn-m002# pushd /metal/bootstrap/prep/admin ncn-m002# script -af csm-verify.$(date +%Y-%m-%d).txt ncn-m002# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39; ncn-m002# ssh ncn-m001   If the pre-NCN deployment password change method was not used, then the root password on ncn-m001 needs to be changed now. Run passwd on ncn-m001 and complete the prompts.\nncn-m001# passwd   Run kubectl get nodes to see the full Kubernetes cluster.\n NOTE If the new node fails to join the cluster after running other cloud-init items, please refer to the handoff\n ncn-m001# kubectl get nodes Expected output looks similar to the following:\nNAME STATUS ROLES AGE VERSION ncn-m001 Ready master 7s v1.18.6 ncn-m002 Ready master 4h40m v1.18.6 ncn-m003 Ready master 4h38m v1.18.6 ncn-w001 Ready \u0026lt;none\u0026gt; 4h39m v1.18.6 ncn-w002 Ready \u0026lt;none\u0026gt; 4h39m v1.18.6 ncn-w003 Ready \u0026lt;none\u0026gt; 4h39m v1.18.6   Restore and verify the site link. It will be necessary to restore the ifcfg-lan0 file from either manual backup taken during the prior \u0026ldquo;Hand-Off\u0026rdquo; step or re-mount the USB and copy it from the prep directory to /etc/sysconfig/network/.\nncn-m001# SYSTEM_NAME=eniac ncn-m001# rsync ncn-m002:/metal/bootstrap/prep/${SYSTEM_NAME}/pit-files/ifcfg-lan0 /etc/sysconfig/network/ ncn-m001# wicked ifreload lan0 ncn-m001# wicked ifstatus lan0 lan0 up link: #32, state up, mtu 1500 type: bridge, hwaddr 90:e2:ba:0f:11:c2 config: compat:suse:/etc/sysconfig/network/ifcfg-lan0 leases: ipv4 static granted addr: ipv4 172.30.53.88/20 [static]   Run ip a to show our lan0 IP address, verify the site link.\nncn-m001# ip a show lan0   Run ip a to show our VLANs, verify they all have IP addresses.\nncn-m001# ip a show vlan002 ncn-m001# ip a show vlan004 ncn-m001# ip a show vlan007   Run ip r to show our default route is via the CAN/vlan007.\nncn-m001# ip r show default   Verify we do not have a metal bootstrap IP.\nncn-m001# ip a show bond0   Verify zypper repositories are empty and all remote SUSE repositories are disabled.\nncn-m001# rm -v /etc/zypp/repos.d/* \u0026amp;\u0026amp; zypper ms --remote --disable   Install the latest documentation and workaround packages. This will require external access.\nIf this machine does not have direct Internet access, these RPMs will need to be externally downloaded and then copied to the system.\nImportant: In an earlier step, the CSM release plus any patches, workarounds, or hotfixes were downloaded to a system using the instructions in Check for Latest Workarounds and Documentation Updates. Use that set of RPMs rather than downloading again.\nlinux# wget https://storage.googleapis.com/csm-release-public/shasta-1.5/docs-csm/docs-csm-latest.noarch.rpm linux# wget https://storage.googleapis.com/csm-release-public/shasta-1.5/csm-install-workarounds/csm-install-workarounds-latest.noarch.rpm linux# scp -p docs-csm-*rpm csm-install-workarounds-*rpm ncn-m001:/root linux# ssh ncn-m001 ncn-m001# rpm -Uvh docs-csm-latest.noarch.rpm ncn-m001# rpm -Uvh csm-install-workarounds-latest.noarch.rpm   Follow the workaround instructions for the livecd-post-reboot breakpoint.\n  Now exit the typescript and relocate the backup over to ncn-m001, thus removing the need to track ncn-m002 as yet-another bootstrapping agent. This is required to facilitate reinstallations, because it pulls the preparation data back over to the documented area (ncn-m001).\nncn-m001# exit ncn-m002# exit # typescript exited ncn-m002# rsync -rltDv -P /metal/bootstrap ncn-m001:/metal/ ncn-m002# rm -rfv /metal/bootstrap ncn-m002# exit   5. Enable NCN Disk Wiping Safeguard  The next steps require csi from the installation media. csi will not be provided on an NCN otherwise because it is used for Cray installation and bootstrap. The CSI binary is compiled against the NCN base, simply fetching it from the bootable media will suffice.\n   SSH back into ncn-m001, or restart a local console and resume the typescript\nncn-m001# script -af /metal/bootstrap/prep/admin/csm-verify.$(date +%Y-%m-%d).txt ncn-m001# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39;   Obtain access to CSI\nncn-m001# mkdir -pv /mnt/livecd /mnt/rootfs /mnt/sqfs ncn-m001# mount -v /metal/bootstrap/cray-pre-install-toolkit-*.iso /mnt/livecd/ ncn-m001# mount -v /mnt/livecd/LiveOS/squashfs.img /mnt/sqfs/ ncn-m001# mount -v /mnt/sqfs/LiveOS/rootfs.img /mnt/rootfs/ ncn-m001# cp -pv /mnt/rootfs/usr/bin/csi /tmp/csi ncn-m001# /tmp/csi version ncn-m001# umount -vl /mnt/sqfs /mnt/rootfs /mnt/livecd   Authenticate with the cluster\nncn-m001# export TOKEN=$(curl -k -s -S -d grant_type=client_credentials \\ -d client_id=admin-client \\ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39;)   IN-PLACE WORKAROUND Set the wipe safeguard to allow safe net-reboots. This will set the safeguard on all NCNs.\nncn-m001# /tmp/csi handoff bss-update-param --set metal.no-wipe=1    CSI NOTE /tmp/csi will delete itself on the next reboot. The /tmp directory is tmpfs and runs in memory, it normally will not persist on restarts.\n 6. Configure DNS and NTP on each BMC  NOTE If your system is Gigabyte or Intel hardware, skip this section.\n Perform the following steps on every NCN except ncn-m001.\n  Set environment variables. Make sure to set the appropriate value for the IPMI_PASSWORD variable.\nncn# export IPMI_PASSWORD=changeme ncn# export USERNAME=root   Disable DHCP and configure NTP on the BMC using data from cloud-init.\nncn# /opt/cray/csm/scripts/node_management/set-bmc-ntp-dns.sh ilo -H \u0026#34;$(hostname)-mgmt\u0026#34; -S -n   Configure DNS on the BMC using data from cloud-init.\nncn# /opt/cray/csm/scripts/node_management/set-bmc-ntp-dns.sh ilo -H \u0026#34;$(hostname)-mgmt\u0026#34; -d   Show the settings of the BMC, if desired:\nncn# /opt/cray/csm/scripts/node_management/set-bmc-ntp-dns.sh ilo -H \u0026#34;$(hostname)-mgmt\u0026#34; -s   Next Topic After completing this procedure, the next step is to configure administrative access.\n See Configure Administrative Access  "
},
{
	"uri": "/docs-csm/en-11/install/reinstall_livecd/",
	"title": "Reinstall Livecd",
	"tags": [],
	"description": "",
	"content": "Reinstall LiveCD Setup a re-install of LiveCD on a node using the previous configuration.\n  Backup to the data partition:\npit# mkdir -pv /var/www/ephemeral/backup pit# pushd /var/www/ephemeral/backup pit# tar -czvf \u0026#34;dnsmasq-data-$(date \u0026#39;+%Y-%m-%d_%H-%M-%S\u0026#39;).tar.gz\u0026#34; /etc/dnsmasq.* pit# tar -czvf \u0026#34;network-data-$(date \u0026#39;+%Y-%m-%d_%H-%M-%S\u0026#39;).tar.gz\u0026#34; /etc/sysconfig/network/* pit# cp -pv /etc/hosts ./ pit# popd pit# umount -v /var/www/ephemeral   Unplug the USB device.\nThe USB device should now contain all the information already loaded, as well as the backups of the initialized files.\n  Plug the device into a new machine, or make a backup on the booted NCN. Make a snapshot of the USB device.\nmylinuxpc\u0026gt; mount -v /dev/disk/by-label/PITDATA /mnt mylinuxpc\u0026gt; tar -czvf --exclude *.squashfs \\ \u0026#34;install-data-$(date \u0026#39;+%Y-%m-%d_%H-%M-%S\u0026#39;).tar.gz\u0026#34; /mnt/ mylinuxpc\u0026gt; umount -v /dev/disk/by-label/PITDATA   Follow the steps in the \u0026ldquo;Boot LiveCD\u0026rdquo; procedure in the HPE Cray EX System Installation and Configuration Guide S-8000. The new tar.gz file can be stored anywhere, and can be used to reinitialize the LiveCD.\n  The new tar.gz file you made can be stored anywhere, and can be used to reinit the liveCD. Follow the directions in Bootstrap PIT Node from LiveCD USB and then return here and move onto the next step.\n  Delete the existing content on the USB device and create a new LiveCD on that same USB device.\nOnce the install-data partition is created, it can be remounted and can be used to restore the backup.\nmylinuxpc\u0026gt; mount -v /dev/disk/by-label/PITDATA /mnt mylinuxpc\u0026gt; tar -xzvf $(ls -ltR *.tar.gz | head -n 1) mylinuxpc\u0026gt; ls -R /mnt The tarball should have extracted everything into the install-data partition.\n  Retrieve the SquashFS artifacts. The artifacts can be retrieved at the following locations:\n /mnt/var/www/ephemeral/k8s/ /mnt/var/www/ephemeral/ceph/    Attach the USB to a Cray non-compute node and reboot into the USB device.\n  Once booted into the USB device, restore network configuration, dnsmasq, and ensure the pods are started.\n STOP AND INSPECT ANY FAILURE IN ANY OF THESE COMMANDS\n pit# tar -xzvf /var/www/ephemeral/backup/dnsmasq*.tar.gz pit# tar -xzvf /var/www/ephemeral/backup/network*.tar.gz pit# systemctl restart wicked wickedd-nanny pit# systemctl restart dnsmasq pit# systemctl start basecamp nexus   The LiveCD is now re-installed with the previous configuration.\n  "
},
{
	"uri": "/docs-csm/en-11/install/reset_root_password_on_livecd/",
	"title": "Reset Root Password On Livecd",
	"tags": [],
	"description": "",
	"content": "Reset root Password on LiveCD It may become desirable to clear the password on the LiveCD.\nThe root password is preserved within the COW partition at cow:rw/etc/shadow. This is the modified copy of the /etc/shadow file used by the operating system.\nIf a site/user needs to reset/clear the password for root, they can mount their USB on another machine and remove this file from the COW partition. When next booting from the USB it will reinitialize to an empty password for root, and again at next login it will require the password to be changed.\nClearing the password (macOS or Linux):\nmypc:~ \u0026gt; mount -vL cow /mnt mypc:~ \u0026gt; sudo rm -fv /mnt/rw/etc/shadow mypc:~ \u0026gt; umount -v /mnt "
},
{
	"uri": "/docs-csm/en-11/install/restart_network_services_and_interfaces_on_ncns/",
	"title": "Restart Network Services And Interfaces On NCNs",
	"tags": [],
	"description": "",
	"content": "Restart Network Services and Interfaces on NCNs Interfaces within the network stack can be reloaded or reset to fix wedged interfaces. The NCNs have network device names set during first boot. The names vary based on the available hardware. For more information, see NCN Networking. Any process covered on this page will be covered by the installer.\nThe use cases for resetting services:\n Interfaces not showing up IP Addresses not applying Member/children interfaces not being included  Topics:  Restart Network Services and Interfaces))) Command Reference  Check interface status (up/down/broken) Show routing and status for all devices Print real devices ( ignore no-device ) Show the currently enabled network service (Wicked or Network Manager)    Restart Network Services There are a few daemons that make up the SUSE network stack. The following are sorted by safest to touch relative to keeping an SSH connection up.\n  wickedd.service: The daemons handling each interface. Resetting this clears stale configuration. This command restarts the wickedd service without reconfiguring the network interfaces.\nncn# systemctl restart wickedd   wicked.service: The overarching service for spawning daemons and manipulating interface configuration. Resetting this reloads daemons and configuration. This command restarts the wicked service which will respawns daemons and reconfigure the network.\nncn# systemctl restart wicked   network.service: Responsible for network configuration per interface; This does not reload wicked. This command restarts the network interface configuration, but leaves wicked daemons alone.\n NOTE: Commonly the problem exists within wicked. This is a last resort in the event the configuration is so bad wicked cannot handle it.\n # Restart the network interface configuration, but leaves wicked daemons alone. ncn# systemctl restart network   Command Reference   Check interface status (up/down/broken)\nncn# wicked ifstatus   Show routing and status for all devices\nncn# wicked ifstatus --verbose all lo up link: #1, state up type: loopback control: persistent config: compat:suse:/etc/sysconfig/network/ifcfg-lo, uuid: 6ad37e59-72d7-5988-9675-93b8df96d9f6 leases: ipv4 static granted leases: ipv6 static granted addr: ipv4 127.0.0.1/8 scope host label lo [static] addr: ipv6 ::1/128 scope host [static] route: ipv6 ::1/128 type unicast table main scope universe protocol kernel priority 256 em1 device-unconfigured link: #2, state down, mtu 1500 type: ethernet, hwaddr a4:bf:01:48:1f:dc config: none em2 device-unconfigured link: #3, state down, mtu 1500 type: ethernet, hwaddr a4:bf:01:48:1f:dd config: none mgmt0 enslaved link: #4, state up, mtu 9000, master bond0 type: ethernet, hwaddr b8:59:9f:f9:1c:8e control: none config: compat:suse:/etc/sysconfig/network/ifcfg-mgmt0, uuid: 7175c041-ee2b-5ce2-a4d7-67fa6cb94a17 mgmt1 device-unconfigured link: #5, state up, mtu 9000, master bond0 type: ethernet, hwaddr b8:59:9f:f9:1c:8e config: none bond0 device-unconfigured link: #6, state up, mtu 9000 type: bond, mode ieee802-3ad, hwaddr b8:59:9f:f9:1c:8e config: none addr: ipv6 fe80::ba59:9fff:fef9:1c8e/64 scope link route: ipv6 fe80::/64 type unicast table main scope universe protocol kernel priority 256 vlan002 device-unconfigured link: #7, state up, mtu 9000 type: vlan bond0[2], hwaddr b8:59:9f:f9:1c:8e config: none addr: ipv4 10.252.2.2/17 brd 10.252.2.2 scope universe label vlan002 addr: ipv6 fe80::ba59:9fff:fef9:1c8e/64 scope link route: ipv4 0.0.0.0/0 via 10.252.1.1 dev vlan002 type unicast table main scope universe protocol boot route: ipv4 10.252.0.0/17 type unicast table main scope link protocol kernel pref-src 10.252.2.2 route: ipv6 fe80::/64 type unicast table main scope universe protocol kernel priority 256 vlan004 device-unconfigured link: #8, state up, mtu 9000 type: vlan bond0[4], hwaddr b8:59:9f:f9:1c:8e config: none addr: ipv4 10.254.2.2/17 brd 10.254.2.2 scope universe label vlan004 addr: ipv6 fe80::ba59:9fff:fef9:1c8e/64 scope link route: ipv4 10.254.0.0/17 type unicast table main scope link protocol kernel pref-src 10.254.2.2 route: ipv6 fe80::/64 type unicast table main scope universe protocol kernel priority 256 vlan007 device-unconfigured link: #9, state up, mtu 9000 type: vlan bond0[7], hwaddr b8:59:9f:f9:1c:8e config: none addr: ipv4 10.102.9.12/24 brd 10.102.9.12 scope universe label vlan007 addr: ipv6 fe80::ba59:9fff:fef9:1c8e/64 scope link route: ipv4 10.102.9.0/24 type unicast table main scope link protocol kernel pref-src 10.102.9.12 route: ipv6 fe80::/64 type unicast table main scope universe protocol kernel priority 256 eth0 no-device   Print real devices ( ignore no-device )\nncn# wicked show --verbose all   Show the currently enabled network service (Wicked or Network Manager)\nncn# systemctl show -p Id network.service Id=wicked.service   "
},
{
	"uri": "/docs-csm/en-11/install/prepare_management_nodes/",
	"title": "Prepare Management Nodes",
	"tags": [],
	"description": "",
	"content": "Prepare Management Nodes The procedures described on this page are being done before any node is booted with the Cray Pre-Install Toolkit. When the PIT node is referenced during these procedures, it means the node that will be be booted as the PIT node.\nTopics:  Quiesce Compute and Application Nodes Disable DHCP Service (if any management nodes are booted) Wipe Disks on Booted Nodes Power Off Booted Nodes Set Node BMCs to DHCP Wipe USB Device on PIT Node (Only if switching from USB LiveCD method to RemoteISO LiveCD method) Power Off PIT Node  Quiesce compute nodes and application nodes.  Skip this section if compute nodes and application nodes are not booted\n The compute nodes and application nodes depend on the management nodes to provide services for their runtime environment.\n CPS to project the operating system image or the CPE image or the Analytics image cray-dns-unbound (internal system DNS) cray-kea (DHCP leases) Access to the API gateway for node heartbeats  While the reinstall process happens, these nodes would not be able to function normally. As part of the reinstall, they will be rebooted with new boot images and configuration.\nSee Shut Down and Power Off Compute and User Access Nodes.\nDisable DHCP Service  Skip this section if none of the management nodes are booted\n If doing a reinstall and any of the management nodes are booted, then the DHCP service will need to be disabled before powering off management nodes.\nRuntime DHCP services interfere with the LiveCD\u0026rsquo;s bootstrap nature to provide DHCP leases to BMCs. To remove edge cases, disable the run-time cray-dhcp-kea pod.\nScale the deployment from either the LiveCD or any Kubernetes node\nncn# kubectl scale -n services --replicas=0 deployment cray-dhcp-kea Wipe Disks on Booted Nodes  Skip this section if none of the management nodes are booted\n If any of the management nodes are booted with Linux, then they have previous installations data on them which should be wiped.\n REQUIRED If the above is true, then for each management node, excluding ncn-m001, log in and do a full wipe of the of the node.\nSee full wipe from Wipe NCN Disks for Reinstallation\n Power Off Booted Nodes  Skip this section if none of the management nodes are booted\n Power each NCN off using ipmitool from ncn-m001 (or the booted LiveCD if reinstalling an incomplete install).\n  Shut down from LiveCD (pit)\npit# export USERNAME=root pit# export IPMI_PASSWORD=changeme pit# conman -q | grep mgmt | grep -v m001 | xargs -t -i ipmitool -I lanplus -U $USERNAME -E -H {} power off Check the power status to confirm the nodes have powered off.\npit# conman -q | grep mgmt | grep -v m001 | xargs -t -i ipmitool -I lanplus -U $USERNAME -E -H {} power status   Shut down from ncn-m001\nncn-m001# export USERNAME=root ncn-m001# export IPMI_PASSWORD=changeme ncn-m001# grep ncn /etc/hosts | grep mgmt | grep -v m001 | sort -u | awk \u0026#39;{print $2}\u0026#39; | xargs -t -i ipmitool -I lanplus -U $USERNAME -E -H {} power off Check the power status to confirm the nodes have powered off.\nncn-m001# grep ncn /etc/hosts | grep mgmt | grep -v m001 | sort -u | awk \u0026#39;{print $2}\u0026#39; | xargs -t -i ipmitool -I lanplus -U $USERNAME -E -H {} power status   Set Node BMCs to DHCP Set the BMCs on the management nodes to DHCP.\n NOTE During the install of the management nodes their BMCs get set to static IP addresses. The installation expects these BMCs to be set back to DHCP before proceeding.\n   Set the LAN variable.\n  If you have Intel nodes, set it to 3.\nlinux# export LAN=3   For non-Intel nodes, set it to 1.\nlinux# export LAN=1     Set the BMCs to DHCP.\n  from the LiveCD (pit):\n NOTE This step uses the old statics.conf on the system in case CSI changes IP addresses:\n pit# export USERNAME=root pit# export IPMI_PASSWORD=changeme pit# for h in $( grep mgmt /etc/dnsmasq.d/statics.conf | grep -v m001 | awk -F \u0026#39;,\u0026#39; \u0026#39;{print $2}\u0026#39; ) do echo \u0026#34;Setting $hto DHCP\u0026#34; ipmitool -U $USERNAME -I lanplus -H $h -E lan set $LAN ipsrc dhcp done Verify the BMCs have been set to DHCP:\npit# for h in $( grep mgmt /etc/dnsmasq.d/statics.conf | grep -v m001 | awk -F \u0026#39;,\u0026#39; \u0026#39;{print $2}\u0026#39; ) do printf \u0026#34;$h: \u0026#34; ipmitool -U $USERNAME -I lanplus -H $h -E lan print $LAN | grep Source done  If an error similar to the following occurs, it means that the BMC is no longer reachable by its IP.\n10.254.1.5: Error: Unable to establish IPMI v2 / RMCP+ session  The timing of this change can vary based on the hardware, so if the IP address of any BMC can still be reached after running the above commands then run the following. A BMC is considered reachable if it can still be pinged by its IP address or hostname (such as ncn-w001-mgmt).\npit# for h in $( grep mgmt /etc/dnsmasq.d/statics.conf | grep -v m001 | awk -F \u0026#39;,\u0026#39; \u0026#39;{print $2}\u0026#39; ) do printf \u0026#34;$h: \u0026#34; ipmitool -U $USERNAME -I lanplus -H $h -E mc reset cold done   from ncn-m001:\n NOTE This step uses to the /etc/hosts file on ncn-m001 to determine the IP addresses of the BMCs:\n ncn-m001# export USERNAME=root ncn-m001# export IPMI_PASSWORD=changeme ncn-m001# for h in $( grep ncn /etc/hosts | grep mgmt | grep -v m001 | awk \u0026#39;{print $2}\u0026#39; ) do echo \u0026#34;Setting $hto DHCP\u0026#34; ipmitool -U $USERNAME -I lanplus -H $h -E lan set $LAN ipsrc dhcp done Verify the BMCs have been set to DHCP:\nncn-m001# for h in $( grep ncn /etc/hosts | grep mgmt | grep -v m001 | awk \u0026#39;{print $2}\u0026#39; ) do printf \u0026#34;$h: \u0026#34; ipmitool -U $USERNAME -I lanplus -H $h -E lan print $LAN | grep Source done  If an error similar to the following occurs, it means that the BMC is no longer reachable by its IP.\nncn-w001-mgmt: Error: Unable to establish IPMI v2 / RMCP+ session  The timing of this change can vary based on the hardware, so if the IP address of any BMC can still be reached after running the above commands then run the following. A BMC is considered reachable if it can still be pinged by its IP address or hostname (such as ncn-w001-mgmt).\nncn-m001# for h in $( grep ncn /etc/hosts | grep mgmt | grep -v m001 | awk \u0026#39;{print $2}\u0026#39; ) do printf \u0026#34;$h: \u0026#34; ipmitool -U $USERNAME -I lanplus -H $h -E mc reset cold done     Wipe USB Device on PIT Node If intending to boot the PIT node from the Remote ISO and there is a USB device which was previously used with LiveCD data, it should be wiped to avoid having two devices with disk labels claiming to be the LiveCD. Alternatively, the USB device could be removed from the PIT node.\n  If not removing the USB device from ncn-m001, then wipe its USB storage with the following command:\nncn-m001# wipefs --all --force /dev/disk/by-label/cow /dev/disk/by-label/PITDATA /dev/disk/by-label/BOOT /dev/disk/by-label/CRAYLIVE   Power Off PIT Node  Skip this step if you are planning to use this node as a staging area to create the USB LiveCD.\n Shut down the LiveCD or ncn-m001 node.\nncn-m001# poweroff Next Topic After completing this procedure the next step is to bootstrap the PIT node.\n See Bootstrap PIT Node  "
},
{
	"uri": "/docs-csm/en-11/install/prepare_site_init/",
	"title": "Prepare Site Init",
	"tags": [],
	"description": "",
	"content": "Prepare Site Init These procedures guide administrators through setting up the site-init directory which contains important customizations for various products. The appendix is informational only; it does not include any default install procedures.\nTopics:  Background Create and Initialize Site-Init Directory Create Baseline System Customizations Generate Sealed Secrets Version Control Site-Init Files  Push to a Remote Repository   Patch cloud-init with the CA Customer-Specific Customizations  Details 1. Background The shasta-cfg directory included in CSM includes relatively static, installation-centric artifacts such as:\n Cluster-wide network configuration settings required by Helm Charts deployed by product stream Loftsman Manifests Sealed Secrets Sealed Secret Generate Blocks \u0026ndash; a form of plain-text input that renders to a Sealed Secret Helm Chart value overrides that are merged into Loftsman Manifests by product stream installers  2. Create and Initialize Site-Init Directory   Create directory /mnt/pitdata/prep/site-init:\nlinux# mkdir -pv /mnt/pitdata/prep/site-init linux# cd /mnt/pitdata/prep/site-init   Set CSM_RELEASE and SYSTEM_NAME variables, if not already set:\nlinux# CSM_RELEASE=csm-x.y.z linux# SYSTEM_NAME=eniac   Initialize /mnt/pitdata/prep/site-init from CSM:\nlinux# /mnt/pitdata/${CSM_RELEASE}/shasta-cfg/meta/init.sh /mnt/pitdata/prep/site-init   The yq tool used in the following procedures is available under /mnt/pitdata/prep/site-init/utils/bin once the SHASTA-CFG repo has been cloned\nlinux# alias yq=\u0026#34;/mnt/pitdata/${CSM_RELEASE}/shasta-cfg/utils/bin/$(uname | awk \u0026#39;{print tolower($0)}\u0026#39;)/yq\u0026#34;   3. Create Baseline System Customizations The following steps update /mnt/pitdata/prep/site-init/customizations.yaml with system-specific customizations.\n  Ensure system-specific settings generated by CSI are merged into customizations.yaml:\nlinux# yq merge -xP -i /mnt/pitdata/prep/site-init/customizations.yaml \u0026lt;(yq prefix -P \u0026#34;/mnt/pitdata/prep/${SYSTEM_NAME}/customizations.yaml\u0026#34; spec)   Set the cluster name:\nlinux# yq write -i /mnt/pitdata/prep/site-init/customizations.yaml spec.wlm.cluster_name \u0026#34;$SYSTEM_NAME\u0026#34;   Make a backup copy of /mnt/pitdata/prep/site-init/customizations.yaml:\nlinux# cp -v /mnt/pitdata/prep/site-init/customizations.yaml /mnt/pitdata/prep/site-init/customizations.yaml.prepassword   Review the configuration to generate these sealed secrets in customizations.yaml in the site-init directory:\n spec.kubernetes.sealed_secrets.cray_reds_credentials spec.kubernetes.sealed_secrets.cray_meds_credentials spec.kubernetes.sealed_secrets.cray_hms_rts_credentials  Replace the Password references with values appropriate for your system. See Edit customizations.yaml, replacing the Password references with values appropriate for your system. See the Decrypting Sealed Secrets for Review section of Sealed Secrets Procedures if you need to examine credentials from prior installs.\n  Review the changes that you made:\nlinux# diff /mnt/pitdata/prep/site-init/customizations.yaml /mnt/pitdata/prep/site-init/customizations.yaml.prepassword   Validate that REDS/MEDS/RTS sealed secrets contain valid JSON using jq:\nValidate REDS credentials (used by the REDS and HMS discovery services, targeting River Redfish BMC endpoints and management switches).\nlinux# yq read /mnt/pitdata/prep/site-init/customizations.yaml \u0026#39;spec.kubernetes.sealed_secrets.cray_reds_credentials.generate.data[0].args.value\u0026#39; | jq linux# yq read /mnt/pitdata/prep/site-init/customizations.yaml \u0026#39;spec.kubernetes.sealed_secrets.cray_reds_credentials.generate.data[1].args.value\u0026#39; | jq  NOTE: For vault_redfish_defaults, the only entry used is \u0026lsquo;{\u0026ldquo;Cray\u0026rdquo;: {\u0026ldquo;Username\u0026rdquo;: \u0026ldquo;root\u0026rdquo;, \u0026ldquo;Password\u0026rdquo;: \u0026ldquo;XXXX\u0026rdquo;}\u0026rsquo; Make sure it is specified as shown, with the \u0026lsquo;Cray\u0026rsquo; key. This key is not used in any of the other credential specifications. Make sure Username and Password entries are correct.\n Validate MEDS credentials (used by the MEDS service, targeting Redfish BMC endpoints). Make sure Username and Password entries are correct.\nlinux# yq read /mnt/pitdata/prep/site-init/customizations.yaml \u0026#39;spec.kubernetes.sealed_secrets.cray_meds_credentials.generate.data[0].args.value\u0026#39; | jq Validate RTS credentials (used by the Redfish Translation Service, targeting River Redfish BMC endpoints and PDU controllers). Make sure Username and Password entries are correct.\nlinux# yq read /mnt/pitdata/prep/site-init/customizations.yaml \u0026#39;spec.kubernetes.sealed_secrets.cray_hms_rts_credentials.generate.data[0].args.value\u0026#39; | jq linux# yq read /mnt/pitdata/prep/site-init/customizations.yaml \u0026#39;spec.kubernetes.sealed_secrets.cray_hms_rts_credentials.generate.data[1].args.value\u0026#39; | jq   To customize the PKI Certificate Authority (CA) used by the platform, see Certificate_authority.\n IMPORTANT The CA may not be modified after install.\n   To federate Keycloak with an upstream LDAP:\nIn the example below, the LDAP server has the hostname dcldap2.us.cray.com and is using the port 636.\nlinux# export LDAP=dcldap2.us.cray.com linux# export PORT=636   If LDAP requires TLS (recommended), update the cray-keycloak sealed secret value by supplying a base64 encoded Java KeyStore (JKS) that contains the CA certificate that signed the LDAP server\u0026rsquo;s host key. The password for the JKS file must be password. Administrators may use the keytool command from the openjdk:11-jre-slim container image packaged with CSM to create a JKS file that includes a PEM-encoded CA certificate to verify the LDAP host(s) as follows:\nLoad the openjdk container image:\n NOTE Requires a properly configured Docker or Podman environment.\n linux# /mnt/pitdata/${CSM_RELEASE}/hack/load-container-image.sh dtr.dev.cray.com/library/openjdk:11-jre-slim Create (or update) cert.jks with the PEM-encoded CA certificate for an LDAP host:\n IMPORTANT Replace \u0026lt;ca-cert.pem\u0026gt; and \u0026lt;alias\u0026gt; as appropriate.\n linux# podman run --rm -v \u0026#34;$(pwd):/data\u0026#34; dtr.dev.cray.com/library/openjdk:11-jre-slim keytool \\ -importcert -trustcacerts -file /data/\u0026lt;ca-cert.pem\u0026gt; -alias \u0026lt;alias\u0026gt; -keystore /data/certs.jks \\ -storepass password -noprompt For example, create the certs.jks.b64 file as follows:\n  Get the issuer certificate for the LDAP server at port 636. Use openssl s_client to connect and show the certificate chain returned by the LDAP host:\nlinux# openssl s_client -showcerts -connect $LDAP:${PORT} \u0026lt;/dev/null Either manually extract (i.e., cut/paste) the issuer\u0026rsquo;s certificate into cacert.pem or try the following commands to create it automatically.\n NOTE The following commands were verified using OpenSSL version 1.1.1d and use the -nameopt RFC2253 option to ensure consistent formatting of distinguished names (DNs). Unfortunately, older versions of OpenSSL may not support -nameopt on the s_client command or may use a different default format. As a result, your mileage may vary; however, you should be able to extract the issuer certificate manually from the output of the above openssl s_client example if the following commands are unsuccessful.\n   Observe the issuer\u0026rsquo;s DN:\nlinux# openssl s_client -showcerts -nameopt RFC2253 -connect $LDAP:${PORT} \u0026lt;/dev/null 2\u0026gt;/dev/null | grep issuer= | sed -e \u0026#39;s/^issuer=//\u0026#39; Expected output would include a line similar to this:\nemailAddress=dcops@hpe.com,CN=Data Center,OU=HPC/MCS,O=HPE,ST=WI,C=US   Extract the issuer\u0026rsquo;s certificate using awk:\n NOTE The issuer DN is properly escaped as part of the awk pattern below. If the value you are using is different, be sure to escape it properly!\n linux# openssl s_client -showcerts -nameopt RFC2253 -connect $LDAP:${PORT} \u0026lt;/dev/null 2\u0026gt;/dev/null | \\  awk \u0026#39;/s:emailAddress=dcops@hpe.com,CN=Data Center,OU=HPC\\/MCS,O=HPE,ST=WI,C=US/,/END CERTIFICATE/\u0026#39; | \\  awk \u0026#39;/BEGIN CERTIFICATE/,/END CERTIFICATE/\u0026#39; \u0026gt; cacert.pem     Verify issuer\u0026rsquo;s certificate was properly extracted and saved in cacert.pem:\nlinux# cat cacert.pem Expected output looks like:\n-----BEGIN CERTIFICATE----- MIIDvTCCAqWgAwIBAgIUYxrG/PrMcmIzDuJ+U1Gh8hpsU8cwDQYJKoZIhvcNAQEL BQAwbjELMAkGA1UEBhMCVVMxCzAJBgNVBAgMAldJMQwwCgYDVQQKDANIUEUxEDAO BgNVBAsMB0hQQy9NQ1MxFDASBgNVBAMMC0RhdGEgQ2VudGVyMRwwGgYJKoZIhvcN AQkBFg1kY29wc0BocGUuY29tMB4XDTIwMTEyNDIwMzM0MVoXDTMwMTEyMjIwMzM0 MVowbjELMAkGA1UEBhMCVVMxCzAJBgNVBAgMAldJMQwwCgYDVQQKDANIUEUxEDAO BgNVBAsMB0hQQy9NQ1MxFDASBgNVBAMMC0RhdGEgQ2VudGVyMRwwGgYJKoZIhvcN AQkBFg1kY29wc0BocGUuY29tMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKC AQEAuBIZkKitHHVQHymtaQt4D8ZhG4qNJ0cTsLhODPMtVtBjPZp59e+PWzbc9Rj5 +wfjLGteK6/fNJsJctWlS/ar4jw/xBIPMk5pg0dnkMT2s7lkSCmyd9Uib7u6y6E8 yeGoGcb7I+4ZI+E3FQV7zPact6b17xmajNyKrzhBGEjYucYJUL5iTgZ6a7HOZU2O aQSXe7ctiHBxe7p7RhHCuKRrqJnxoohakloKwgHHzDLFQzX/5ADp1hdJcduWpaXY RMBu6b1mhmwo5vmc+fDnfUpl5/X4i109r9VN7JC7DQ5+JX8u9SHDGLggBWkrhpvl bNXMVCnwnSFfb/rnmGO7rdJSpwIDAQABo1MwUTAdBgNVHQ4EFgQUVg3VYExUAdn2 WE3e8Xc8HONy/+4wHwYDVR0jBBgwFoAUVg3VYExUAdn2WE3e8Xc8HONy/+4wDwYD VR0TAQH/BAUwAwEB/zANBgkqhkiG9w0BAQsFAAOCAQEAWLDQLB6rrmK+gwUY+4B7 0USbQK0JkLWuc0tCfjTxNQTzFb75PeH+GH21QsjUI8VC6QOAAJ4uzIEV85VpOQPp qjz+LI/Ej1xXfz5ostZQu9rCMnPtVu7JT0B+NV7HvgqidTfa2M2dw9yUYS2surZO 8S0Dq3Bi6IEhtGU3T8ZpbAmAp+nNsaJWdUNjD4ECO5rAkyA/Vu+WyMz6F3ZDBmRr ipWM1B16vx8rSpQpygY+FNX4e1RqslKhoyuzXfUGzyXux5yhs/ufOaqORCw3rJIx v4sTWGsSBLXDsFM3lBgljSAHfmDuKdO+Qv7EqGzCRMpgSciZihnbQoRrPZkOHUxr NA== -----END CERTIFICATE-----   Create certs.jks:\nlinux# podman run --rm -v \u0026#34;$(pwd):/data\u0026#34; dtr.dev.cray.com/library/openjdk:11-jre-slim keytool -importcert \\ -trustcacerts -file /data/cacert.pem -alias cray-data-center-ca -keystore /data/certs.jks \\ -storepass password -noprompt   Create certs.jks.b64 by base-64 encoding certs.jks:\nlinux# base64 certs.jks \u0026gt; certs.jks.b64 Then, inject and encrypt certs.jks.b64 into customizations.yaml:\nlinux# cat \u0026lt;\u0026lt;EOF | yq w - \u0026#39;data.\u0026#34;certs.jks\u0026#34;\u0026#39; \u0026#34;$(\u0026lt;certs.jks.b64)\u0026#34; | \\ yq r -j - | /mnt/pitdata/prep/site-init/utils/secrets-encrypt.sh | \\ yq w -f - -i /mnt/pitdata/prep/site-init/customizations.yaml \u0026#39;spec.kubernetes.sealed_secrets.cray-keycloak\u0026#39; { \u0026#34;kind\u0026#34;: \u0026#34;Secret\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;keycloak-certs\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;services\u0026#34;, \u0026#34;creationTimestamp\u0026#34;: null }, \u0026#34;data\u0026#34;: {} } EOF   Update the keycloak_users_localize sealed secret with the appropriate value for ldap_connection_url.\nSet ldap_connection_url in customizations.yaml:\nFor example:\nlinux# yq write -i /mnt/pitdata/prep/site-init/customizations.yaml \\ \u0026#39;spec.kubernetes.sealed_secrets.keycloak_users_localize.generate.data.(args.name==ldap_connection_url).args.value\u0026#39; \u0026#34;ldaps://$LDAP\u0026#34; On success, review the keycloak_users_localize sealed secret.\nlinux# yq read /mnt/pitdata/prep/site-init/customizations.yaml spec.kubernetes.sealed_secrets.keycloak_users_localize Expected output is similar to:\n   generate: name: keycloak-users-localize data: - type: static args: name: ldap_connection_url value: ldaps://dcldap2.us.cray.com    Configure the ldapSearchBase and localRoleAssignments settings for the cray-keycloak-users-localize chart in customizations.yaml.\nSet ldapSearchBase in customizations.yaml:\n IMPORTANT Replace \u0026lt;search-base\u0026gt; as appropriate.\n linux# yq write -i /mnt/pitdata/prep/site-init/customizations.yaml spec.kubernetes.services.cray-keycloak-users-localize.ldapSearchBase \u0026#39;\u0026lt;search-base\u0026gt;\u0026#39; Set localRoleAssignments that map to admin and/or user roles for both shasta and cray clients in customizations.yaml:\n IMPORTANT Replace \u0026lt;admin-group\u0026gt; and \u0026lt;user-group\u0026gt; as appropriate. Also add other assignments as desired.\n linux# yq write -s - -i /mnt/pitdata/prep/site-init/customizations.yaml \u0026lt;\u0026lt;EOF - command: update path: spec.kubernetes.services.cray-keycloak-users-localize.localRoleAssignments value: - {\u0026#34;group\u0026#34;: \u0026#34;\u0026lt;admin-group\u0026gt;\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;shasta\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;\u0026lt;admin-group\u0026gt;\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;cray\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;\u0026lt;user-group\u0026gt;\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;shasta\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;\u0026lt;user-group\u0026gt;\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;cray\u0026#34;} EOF For example, if you wanted to set the search-base and localRoleAssignments to look like this:\nldapSearchBase: \u0026#34;dc=dcldap,dc=dit\u0026#34; localRoleAssignments: - {\u0026#34;group\u0026#34;: \u0026#34;criemp\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;shasta\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;criemp\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;cray\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;craydev\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;shasta\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;craydev\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;cray\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;shasta_admins\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;shasta\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;shasta_admins\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;cray\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;shasta_users\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;shasta\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;shasta_users\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;cray\u0026#34;} Then set ldapSearchBase in customizations.yaml:\nlinux# yq write -i /mnt/pitdata/prep/site-init/customizations.yaml spec.kubernetes.services.cray-keycloak-users-localize.ldapSearchBase \u0026#39;dc=dcldap,dc=dit\u0026#39; And then set localRoleAssignments in customizations.yaml:\nlinux# yq write -s - -i /mnt/pitdata/prep/site-init/customizations.yaml \u0026lt;\u0026lt;EOF - command: update path: spec.kubernetes.services.cray-keycloak-users-localize.localRoleAssignments value: - {\u0026#34;group\u0026#34;: \u0026#34;criemp\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;shasta\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;criemp\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;cray\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;craydev\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;shasta\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;craydev\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;cray\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;shasta_admins\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;shasta\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;shasta_admins\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;cray\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;shasta_users\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;shasta\u0026#34;} - {\u0026#34;group\u0026#34;: \u0026#34;shasta_users\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;cray\u0026#34;} EOF On success, review the cray-keycloak-users-localize values.\nlinux# yq read /mnt/pitdata/prep/site-init/customizations.yaml spec.kubernetes.services.cray-keycloak-users-localize Expected output looks similar to:\nsealedSecrets: - '{{ kubernetes.sealed_secrets.keycloak_users_localize | toYaml }}' localRoleAssignments: - {\u0026quot;group\u0026quot;: \u0026quot;criemp\u0026quot;, \u0026quot;role\u0026quot;: \u0026quot;admin\u0026quot;, \u0026quot;client\u0026quot;: \u0026quot;shasta\u0026quot;} - {\u0026quot;group\u0026quot;: \u0026quot;criemp\u0026quot;, \u0026quot;role\u0026quot;: \u0026quot;admin\u0026quot;, \u0026quot;client\u0026quot;: \u0026quot;cray\u0026quot;} - {\u0026quot;group\u0026quot;: \u0026quot;craydev\u0026quot;, \u0026quot;role\u0026quot;: \u0026quot;admin\u0026quot;, \u0026quot;client\u0026quot;: \u0026quot;shasta\u0026quot;} - {\u0026quot;group\u0026quot;: \u0026quot;craydev\u0026quot;, \u0026quot;role\u0026quot;: \u0026quot;admin\u0026quot;, \u0026quot;client\u0026quot;: \u0026quot;cray\u0026quot;} - {\u0026quot;group\u0026quot;: \u0026quot;shasta_admins\u0026quot;, \u0026quot;role\u0026quot;: \u0026quot;admin\u0026quot;, \u0026quot;client\u0026quot;: \u0026quot;shasta\u0026quot;} - {\u0026quot;group\u0026quot;: \u0026quot;shasta_admins\u0026quot;, \u0026quot;role\u0026quot;: \u0026quot;admin\u0026quot;, \u0026quot;client\u0026quot;: \u0026quot;cray\u0026quot;} - {\u0026quot;group\u0026quot;: \u0026quot;shasta_users\u0026quot;, \u0026quot;role\u0026quot;: \u0026quot;user\u0026quot;, \u0026quot;client\u0026quot;: \u0026quot;shasta\u0026quot;} - {\u0026quot;group\u0026quot;: \u0026quot;shasta_users\u0026quot;, \u0026quot;role\u0026quot;: \u0026quot;user\u0026quot;, \u0026quot;client\u0026quot;: \u0026quot;cray\u0026quot;} ldapSearchBase: dc=dcldap,dc=dit     Configure the Unbound DNS resolver.\n  If a valid DNS server was defined using the CSI --site-dns option then no further action is required and the default configuration will suffice.\nDefault configuration:\ncray-dns-unbound: domain_name: '{{ network.dns.external }}' forwardZones: - name: \u0026quot;.\u0026quot; forwardIps: - \u0026quot;{{ network.netstaticips.system_to_site_lookups }}\u0026quot; If there is no requirement to resolve external hostnames or no upstream DNS server then remove the DNS forwarding configuration from the cray-dns-unbound service.\n  Remove the forwardZones configuration for the cray-dns-unbound service:\nlinux# yq delete -i /mnt/pitdata/prep/site-init/customizations.yaml spec.kubernetes.services.cray-dns-unbound.forwardZones   1. Review the `cray-dns-unbound` values. ```bash linux# yq read /mnt/pitdata/prep/site-init/customizations.yaml spec.kubernetes.services.cray-dns-unbound ``` Expected output is: ``` domain_name: '{{ network.dns.external }}' ``` \u0026gt; **`IMPORTANT`** **Do not** remove the `domain_name` entry, it is required for Unbound to forward requests to PowerDNS correctly.    Configure PowerDNS zone transfer and DNSSEC (optional)\n  If zone transfer is to be configured review customizations.yaml and ensure the primary_server, secondary_servers, and notify_zones values are set correctly.\n  If DNSSEC is to be used then add the desired keys into the dnssec SealedSecret.\n  Please see the PowerDNS Configuration Guide for more information.\n  4. Generate Sealed Secrets Secrets are stored in customizations.yaml as SealedSecret resources (i.e., encrypted secrets) which are deployed by specific charts and decrypted by the Sealed Secrets operator. But first, those secrets must be seeded generated and encrypted.\n  Load the zeromq container image required by Sealed Secret Generators:\n NOTE Requires a properly configured Docker or Podman environment.\n linux# /mnt/pitdata/${CSM_RELEASE}/hack/load-container-image.sh dtr.dev.cray.com/zeromq/zeromq:v4.0.5   Re-encrypt existing secrets:\nlinux# /mnt/pitdata/prep/site-init/utils/secrets-reencrypt.sh /mnt/pitdata/prep/site-init/customizations.yaml \\ /mnt/pitdata/prep/site-init/certs/sealed_secrets.key /mnt/pitdata/prep/site-init/certs/sealed_secrets.crt   Generate secrets:\nlinux# /mnt/pitdata/prep/site-init/utils/secrets-seed-customizations.sh \\ /mnt/pitdata/prep/site-init/customizations.yaml Expected output looks similar to:\nCreating Sealed Secret keycloak-certs Generating type static_b64... Creating Sealed Secret keycloak-master-admin-auth Generating type static... Generating type static... Generating type randstr... Generating type static... Creating Sealed Secret cray_reds_credentials Generating type static... Generating type static... Creating Sealed Secret cray_meds_credentials Generating type static... Creating Sealed Secret cray_hms_rts_credentials Generating type static... Generating type static... Creating Sealed Secret vcs-user-credentials Generating type randstr... Generating type static... Creating Sealed Secret generated-platform-ca-1 Generating type platform_ca... Creating Sealed Secret pals-config Generating type zmq_curve... Generating type zmq_curve... Creating Sealed Secret munge-secret Generating type randstr... Creating Sealed Secret slurmdb-secret Generating type static... Generating type static... Generating type randstr... Generating type randstr... Creating Sealed Secret keycloak-users-localize Generating type static...   5. Version Control Site-Init Files Setup /mnt/pitdata/prep/site-init as a Git repository in order to manage the baseline configuration during initial system installation.\n  Initialize /mnt/pitdata/prep/site-init as a Git repository:\nlinux# cd /mnt/pitdata/prep/site-init linux# git init .   (Optional) WARNING If production system or operational security is a concern, do NOT store the sealed secret private key in Git; instead, store the sealed secret key outside of Git in a secure offline system. To ensure these sensitive keys are not accidentally committed, configure .gitignore to ignore files under the certs directory:\nlinux# echo \u0026#34;certs/\u0026#34; \u0026gt;\u0026gt; .gitignore   Stage site-init files to be committed:\nlinux# git add -A   Review what will be committed:\nlinux# git status   Commit all the above changes as the baseline configuration:\nlinux# git commit -m \u0026#34;Baseline configuration for $(/mnt/pitdata/${CSM_RELEASE}/lib/version.sh)\u0026#34;   5.1 Push to a Remote Repository It is strongly recommended that the site-init repository be maintained off-cluster. Add a remote repository and push the baseline configuration on master branch to a corresponding remote branch.\n6. Patch cloud-init with the CA NOTE Skip this if using a USB LiveCD. These steps are done elsewhere in that procedure.\nUsing csi on a generated site-init directory\u0026hellip;\n  Patch the CA certificate from the shasta-cfg:\npit# csi patch ca \\ --cloud-init-seed-file /var/www/ephemeral/configs/data.json \\ --customizations-file /var/www/ephemeral/prep/site-init/customizations.yaml \\ --sealed-secret-key-file /var/www/ephemeral/prep/site-init/certs/sealed_secrets.key   To assure it picks up the new meta-data:\npit# systemctl restart basecamp   Unmount the shim from earlier if one was used (for users of the Bootstrap LiveCD Remote ISO):\npit# umount -v /mnt/pitdata   7. Customer-Specific Customizations Customer-specific customizations are any changes on top of the baseline configuration to satisfy customer-specific requirements. It is recommended that customer-specific customizations be tracked on branches separate from the mainline in order to make them easier to manage.\nApply any customer-specific customizations by merging the corresponding branches into master branch of /mnt/pitdata/prep/site-init.\nWhen considering merges, and especially when resolving conflicts, carefully examine differences to ensure all changes are relevant. For example, when applying a customer-specific customization used in a prior version, be sure the change still makes sense. It is common for options to change as new features are introduced and bugs are fixed.\n"
},
{
	"uri": "/docs-csm/en-11/install/pxe_boot_troubleshooting/",
	"title": "PXE Boot Troubleshooting",
	"tags": [],
	"description": "",
	"content": "PXE Boot Troubleshooting This page is designed to cover various issues that arise when trying to PXE boot nodes in a Shasta system.\n Configuration required for PXE booting Switch Configuration  Aruba Configuration Mellanox Configuration   Next steps  Restart BSS Restart KEA Missing BSS Data    In order for PXE booting to work successfully, the management network switches need to be configured correctly.\nConfiguration required for PXE booting To successfully PXE boot nodes, the following is required.\n The IP helper-address must be configured on VLAN 1,2,4,7. This will be where the layer 3 gateway exists (spine or aggregation) The virtual-IP/VSX/MAGP IP address must be configured on VLAN 1,2,4,7. There must be a static route pointing to the TFTP server (Aruba Only). ncn-m001 needs an active gateway on VLAN1 this can be identified from MTL.yaml generated from CSI. ncn-m001 needs an IP helper-address on VLAN1 pointing to 10.92.100.222.  snippet of MTL.yaml\n name: network_hardware net-name: MTL vlan_id: 0 comment: \u0026quot; gateway: 10.1.0.1 Switch Configuration Aruba Configuration Check the configuration for interface vlan x This configuration will be the same on BOTH Switches (except the ip address). You will see that there is an active-gateway and ip helper-address configured.\nsw-spine-001(config)# int vlan 1,2,4,7 sw-spine-001(config-if-vlan-\u0026lt;1,2,4,7\u0026gt;)# show run current-context Ouput\ninterface vlan 1 ip mtu 9198 ip address 10.1.0.2/16 active-gateway ip mac 12:00:00:00:6b:00 active-gateway ip 10.1.0.1 ip helper-address 10.92.100.222 interface vlan 2 vsx-sync active-gateways ip mtu 9198 ip address 10.252.0.2/17 active-gateway ip mac 12:01:00:00:01:00 active-gateway ip 10.252.0.1 ip helper-address 10.92.100.222 ip ospf 1 area 0.0.0.0 interface vlan 4 vsx-sync active-gateways ip mtu 9198 ip address 10.254.0.2/17 active-gateway ip mac 12:01:00:00:01:00 active-gateway ip 10.254.0.1 ip helper-address 10.94.100.222 ip ospf 1 area 0.0.0.0 interface vlan 7 ip mtu 9198 ip address 10.103.11.1/24 active-gateway ip mac 12:01:00:00:01:00 active-gateway ip 10.103.11.111 ip helper-address 10.92.100.222 If any of this configuration is missing, you will need to update it to BOTH switches.\nsw-spine-002# conf t sw-spine-002(config)# int vlan 1 sw-spine-002(config-if-vlan)# ip helper-address 10.92.100.222 sw-spine-002(config-if-vlan)# active-gateway ip mac 12:01:00:00:01:00 sw-spine-002(config-if-vlan)# active-gateway ip 10.1.0.1 sw-spine-002# conf t sw-spine-002(config)# int vlan 2 sw-spine-002(config-if-vlan)# ip helper-address 10.92.100.222 sw-spine-002(config-if-vlan)# active-gateway ip mac 12:01:00:00:01:00 sw-spine-002(config-if-vlan)# active-gateway ip 10.252.0.1 sw-spine-002# conf t sw-spine-002(config)# int vlan 4 sw-spine-002(config-if-vlan)# ip helper-address 10.94.100.222 sw-spine-002(config-if-vlan)# active-gateway ip mac 12:01:00:00:01:00 sw-spine-002# conf t sw-spine-002(config)# int vlan 7 sw-spine-002(config-if-vlan)# ip helper-address 10.92.100.222 sw-spine-002(config-if-vlan)# active-gateway ip mac 12:01:00:00:01:00 sw-spine-002(config-if-vlan)# active-gateway ip xxxxxxx sw-spine-002(config-if-vlan)# write mem Verify the route to the TFTP server is in place. This is a static route to get to the TFTP server via a worker node. You can get the worker node IP address from NMN.yaml from CSI-generated data.\n - ip_address: 10.252.1.9 name: ncn-w001 comment: x3000c0s4b0n0 aliases: sw-spine-002(config)# show ip route static output\n Displaying ipv4 routes selected for forwarding '[x/y]' denotes [distance/metric] 0.0.0.0/0, vrf default via 10.103.15.209, [1/0], static 10.92.100.60/32, vrf default via 10.252.1.7, [1/0], static You can see that the route is 10.92.100.60/32 via 10.252.1.7 with 10.252.1.7 being the worker node.\nIf that static route is missing you will need to add it.\nsw-spine-001(config)# ip route 10.92.100.60/32 10.252.1.7 Mellanox Configuration Check the configuration for interface vlan 1 This configuration will be the same on BOTH Switches (except the ip address). You will see that there is magp and ip dhcp relay configured.\nsw-spine-001 [standalone: master] # show run int vlan 1 output\ninterface vlan 1 interface vlan 1 ip address 10.1.0.2/16 primary interface vlan 1 ip dhcp relay instance 2 downstream interface vlan 1 magp 1 interface vlan 1 magp 1 ip virtual-router address 10.1.0.1 interface vlan 1 magp 1 ip virtual-router mac-address 00:00:5E:00:01:01 If this configuration is missing, you will need to add it to BOTH switches.\nsw-spine-001 [standalone: master] # conf t sw-spine-001 [standalone: master] (config) # interface vlan 1 magp 1 sw-spine-001 [standalone: master] (config interface vlan 1 magp 1) # ip virtual-router address 10.1.0.1 sw-spine-001 [standalone: master] (config interface vlan 1 magp 1) # ip virtual-router mac-address 00:00:5E:00:01:01 sw-spine-001 [standalone: master] # conf t sw-spine-001 [standalone: master] (config) # ip dhcp relay instance 2 vrf default sw-spine-001 [standalone: master] (config) # ip dhcp relay instance 2 address 10.92.100.222 sw-spine-001 [standalone: master] (config) # interface vlan 2 ip dhcp relay instance 2 downstream You can then verify the VLAN 1 MAGP configuration.\nsw-spine-001 [standalone: master] # show magp 1 output\n MAGP 1: Interface vlan: 1 Admin state : Enabled State : Master Virtual IP : 10.1.0.1 Virtual MAC : 00:00:5E:00:01:01 Verify the DHCP relay configuration\nsw-spine-001 [standalone: master] (config) # show ip dhcp relay instance 2 output\n VRF Name: default DHCP Servers: 10.92.100.222 DHCP relay agent options: always-on : Disabled Information Option: Disabled UDP port : 67 Auto-helper : Disabled ------------------------------------------- Interface Label Mode ------------------------------------------- vlan1 N/A downstream vlan2 N/A downstream vlan7 N/A downstream Verify that the route to the TFTP server and the route for the ingress gateway are available.\nsw-spine-001 [standalone: master] # show ip route 10.92.100.60  Flags: F: Failed to install in H/W B: BFD protected (static route) i: BFD session initializing (static route) x: protecting BFD session failed (static route) c: consistent hashing p: partial programming in H/W VRF Name default: ------------------------------------------------------------------------------------------------------ Destination Mask Flag Gateway Interface Source AD/M ------------------------------------------------------------------------------------------------------ default 0.0.0.0 c 10.101.15.161 eth1/12 static 1/1 10.92.100.60 255.255.255.255 c 10.252.0.5 vlan2 bgp 200/0 c 10.252.0.6 vlan2 bgp 200/0 c 10.252.0.7 vlan2 bgp 200/0 sw-spine-001 [standalone: master] # show ip route 10.92.100.71  Flags: F: Failed to install in H/W B: BFD protected (static route) i: BFD session initializing (static route) x: protecting BFD session failed (static route) c: consistent hashing p: partial programming in H/W VRF Name default: ------------------------------------------------------------------------------------------------------ Destination Mask Flag Gateway Interface Source AD/M ------------------------------------------------------------------------------------------------------ default 0.0.0.0 c 10.101.15.161 eth1/12 static 1/1 10.92.100.71 255.255.255.255 c 10.252.0.5 vlan2 bgp 200/0 c 10.252.0.6 vlan2 bgp 200/0 c 10.252.0.7 vlan2 bgp 200/0 If these routes are missing please see Update BGP Neighbors.\nNext steps If your configuration looks good, and you are still not able to PXE boot there are some other things to try.\nRestart BSS If while watching an NCN boot attempt you see the following output on the console during PXE (specifically the 404 error at the bottom):\nhttps://api-gw-service-nmn.local/apis/bss/boot/v1/bootscript...X509 chain 0x6d35c548 added X509 0x6d360d68 \u0026#34;eniac.dev.cray.com\u0026#34; X509 chain 0x6d35c548 added X509 0x6d3d62e0 \u0026#34;Platform CA - L1 (a0b073c8-5c9c-4f89-b8a2-a44adce3cbdf)\u0026#34; X509 chain 0x6d35c548 added X509 0x6d3d6420 \u0026#34;Platform CA (a0b073c8-5c9c-4f89-b8a2-a44adce3cbdf)\u0026#34; EFITIME is 2021-02-26 21:55:04 HTTP 0x6d35da88 status 404 Not Found   Rollout a restart of the BSS deployment from any other NCN (likely ncn-m002 if you are executing the ncn-m001 reboot):\nncn-m002# kubectl -n services rollout restart deployment cray-bss deployment.apps/cray-bss restarted   Wait for this command to return (it will block showing status as the pods are refreshed):\nncn-m002# # kubectl -n services rollout status deployment cray-bss Waiting for deployment \u0026#34;cray-bss\u0026#34; rollout to finish: 1 out of 3 new replicas have been updated... Waiting for deployment \u0026#34;cray-bss\u0026#34; rollout to finish: 1 out of 3 new replicas have been updated... Waiting for deployment \u0026#34;cray-bss\u0026#34; rollout to finish: 1 out of 3 new replicas have been updated... Waiting for deployment \u0026#34;cray-bss\u0026#34; rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \u0026#34;cray-bss\u0026#34; rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \u0026#34;cray-bss\u0026#34; rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \u0026#34;cray-bss\u0026#34; rollout to finish: 1 old replicas are pending termination... Waiting for deployment \u0026#34;cray-bss\u0026#34; rollout to finish: 1 old replicas are pending termination... deployment \u0026#34;cray-bss\u0026#34; successfully rolled out   Reboot the NCN that failed to PXE boot.\n  Restart KEA In some cases rebooting the KEA pod has resolved PXE issues.\n  Get KEA pod\nncn-m002# kubectl get pods -n services | grep kea cray-dhcp-kea-6bd8cfc9c5-m6bgw 3/3 Running 0 20h   Delete KEA Pod\nncn-m002# kubectl delete pods -n services cray-dhcp-kea-6bd8cfc9c5-m6bgw   Missing BSS Data If the PXE boot is giving 404 errors, this could be because the necessary information is not in BSS. The information is uploaded into BSS with the csi handoff bss-metadata and csi handoff bss-update-cloud-init commands in the Redeploy PIT Node procedure. If these commands failed or were skipped accidentally, this will cause the ncn-m001 PXE boot to fail.\nIn that case, use the following recovery procedure.\n  Reboot to the PIT.\n  If using a USB PIT:\n  Reboot the PIT node, watching the console as it boots.\n  Manually stop it at the boot menu.\n  Select the USB device for the boot.\n  Once booted, log in and mount the data partition.\npit# mount -vL PITDATA     If using a remote ISO PIT, follow the Bootstrap LiveCD Remote ISO procedure up through (and including) the Set Up The Site Link step.\n    Set variables for the system name, the CAN IP address for ncn-m002. the Kubernetes version, and the Ceph version.\nThe CAN IP address for ncn-m002 is obtained at this step of the Redeploy PIT Node procedure.\nThe Kubernetes and Ceph versions are from the output of the csi handoff ncn-images command in the Redeploy PIT Node procedure. If needed, the typescript file from that procedure should be on ncn-m002 in the /metal/bootstrap/prep/admin directory.\nBe sure to substitute the correct values for your system in the commands below.\npit# SYSTEM_NAME=eniac pit# CAN_IP_NCN_M002=a.b.c.d pit# export KUBERNETES_VERSION=m.n.o pit# export CEPH_VERSION=x.y.z   If using a remote ISO PIT, run the following commands to finish configuring the network and copy files.\nSkip these steps if using a USB PIT.\n  Run the following command to copy files from ncn-m002 to the PIT node.\npit# scp -p ${CAN_IP_NCN_M002}:/metal/bootstrap/prep/${SYSTEM_NAME}/pit-files/* /etc/sysconfig/network/   Apply the network changes.\npit# wicked ifreload all pit# systemctl restart wickedd-nanny \u0026amp;\u0026amp; sleep 5   Copy data.json from ncn-m002 to the PIT node.\npit# mkdir -p /var/www/ephemeral/configs pit# scp ${CAN_IP_NCN_M002}:/metal/bootstrap/prep/${SYSTEM_NAME}/basecamp/data.json /var/www/ephemeral/configs     Copy Kubernetes config file from ncn-m002.\npit# mkdir -pv ~/.kube pit# scp ${CAN_IP_NCN_M002}:/etc/kubernetes/admin.conf ~/.kube/config   Set DNS to use unbound.\npit# echo \u0026#34;nameserver 10.92.100.225\u0026#34; \u0026gt; /etc/resolv.conf   Export the API token.\npit# export TOKEN=$(curl -k -s -S -d grant_type=client_credentials \\  -d client_id=admin-client \\  -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=\u0026#39;{.data.client-secret}\u0026#39; | base64 -d` \\  https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r \u0026#39;.access_token\u0026#39;)   Re-run the BSS handoff commands from the Redeploy PIT Node procedure.\nWARNING: These commands should never be run from a node other than the PIT node or ncn-m001\npit# csi handoff bss-metadata --data-file /var/www/ephemeral/configs/data.json || echo \u0026#34;ERROR: csi handoff bss-metadata failed\u0026#34; pit# csi handoff bss-update-cloud-init --set meta-data.dns-server=10.92.100.225 --limit Global   Perform the BSS Restart and the KEA Restart procedures.\n  Reboot the PIT node.\n  "
},
{
	"uri": "/docs-csm/en-11/install/",
	"title": "Install",
	"tags": [],
	"description": "",
	"content": "Install CSM Installation of the CSM product stream has many steps in multiple procedures which should be done in a specific order. Information about the HPE Cray EX system and the site is used to prepare the configuration payload. The initial node used to bootstrap the installation process is called the PIT node because the Pre-Install Toolkit (PIT) is installed there. Once the management network switches have been configured, the other management nodes can be deployed with an operating system and the software to create a Kubernetes cluster utilizing Ceph storage. The CSM services provide essential software infrastructure including the API gateway and many micro-services with REST APIs for managing the system. Once administrative access has been configured, the installation of CSM software can be validated with health checks before doing operational tasks like the checking and updating of firmware on system components or the preparation of compute nodes. Once the CSM installation has completed, other product streams for the HPE Cray EX system can be installed.\nTopics:  Validate Management Network Cabling Prepare Configuration Payload Prepare Management Nodes Bootstrap PIT Node Configure Management Network Switches Collect MAC Addresses for NCNs Deploy Management Nodes Install CSM Services Validate CSM Health Before PIT Node Redeploy Redeploy PIT Node Configure Administrative Access Validate CSM Health Configure Prometheus Alert Notifications Update Firmware with FAS Prepare Compute Nodes Next Topic Troubleshooting Installation Problems  The topics in this chapter need to be done as part of an ordered procedure so are shown here with numbered topics.\nNote: If problems are encountered during the installation, some topics have their own troubleshooting sections, but there is also a general troubleshooting topic.\nDetails \n  Validate Management Network Cabling\nThe cabling should be validated between the nodes and the management network switches. The information in the Shasta Cabling Diagram (SHCD) can be used to confirm the cables which physically connect components of the system. Having the data in the SHCD which matches the physical cabling will be needed later in both Prepare Configuration Payload and Configure Management Network Switches.\nSee Validate Management Network Cabling\nNote: If a reinstall or fresh install of this software release is being done on this system and the management network cabling has already been validated, then this topic could be skipped and instead move to Prepare Configuration Payload \n  Prepare Configuration Payload\nInformation gathered from a site survey is needed to feed into the CSM installation process, such as system name, system size, site network information for the CAN, site DNS configuration, site NTP configuration, network information for the node used to bootstrap the installation. Much of the information about the system hardware is encapsulated in the SHCD (Shasta Cabling Diagram), which is a spreadsheet prepared by HPE Cray Manufacturing to assemble the components of the system and connect appropriately labeled cables.\nSee Prepare Configuration Payload \n  Prepare Management Nodes\nSome preparation of the management nodes might be needed before starting an install or reinstall. The preparation includes checking and updating the firmware on the PIT node, quiescing the compute nodes and application nodes, scaling back DHCP on the management nodes, wiping the storage on the management nodes, powering off the management nodes, and possibly powering off the PIT node.\nSee Prepare Management Nodes \n  Bootstrap PIT Node\nThe Pre-Install Toolkit (PIT) node needs to be bootstrapped from the LiveCD. There are two media available to bootstrap the PIT node\u0026ndash;the RemoteISO or a bootable USB device. The recommended media is the RemoteISO because it does not require any physical media to prepare. However, remotely mounting an ISO on a BMC does not work smoothly for nodes from all vendors. It is recommended to try the RemoteISO first.\nUse one of these procedures to bootstrap the PIT node from the LiveCD.\n Bootstrap Pit Node from LiveCD Remote ISO (recommended)  Gigabyte BMCs should not use the RemoteISO method. Intel BMCs should not use the RemoteISO method.   Bootstrap Pit Node from LiveCD USB (fallback)  Using the LiveCD USB method requires a USB 3.0 device with at least 1TB of space to create a bootable LiveCD. \n  Configure Management Network Switches\nNow that the PIT node has been booted with the LiveCD environment and CSI has generated the switch IP addresses, the management network switches can be configured. This procedure will configure the spine switches, aggregation switches (if present), CDU switches (if present), and the leaf switches.\nSee Configure Management Network Switches\nNote: If a reinstall of this software release is being done on this system and the management network switches have already been configured, then this topic could be skipped and instead move to Collect MAC Addresses for NCNs \n  Collect MAC Addresses for NCNs\nNow that the PIT node has been booted with the LiveCD and the management network switches have been configured, the actual MAC addresses for the management nodes can be collected. This process will include repetition of some of the steps done up to this point because csi config init will need to be run with the proper MAC addresses.\nSee Collect MAC Addresses for NCNs\nNote: If a reinstall of this software release is being done on this system and the ncn_metadata.csv file already had valid MAC addresses for both BMC and node interfaces before csi config init was run, then this topic could be skipped and instead move to Deploy Management Nodes.\nNote: If a first time install of this software release is being done on this system and the ncn_metadata.csv file already had valid MAC addresses for both BMC and node interfaces before csi config init was run, then this topic could be skipped and instead move to Deploy Management Nodes. \n  Deploy Management Nodes\nNow that the PIT node has been booted with the LiveCD and the management network switches have been configured, the other management nodes can be deployed. This procedure will boot all of the management nodes, initialize Ceph storage on the storage nodes, and start the Kubernetes cluster on all of the worker nodes and the master nodes, except for the PIT node. The PIT node will join Kubernetes after it is rebooted later in Redeploy PIT Node.\nSee Deploy Management Nodes\n\n  Install CSM Services\nNow that deployment of management nodes is complete with initialized Ceph storage and a running Kubernetes cluster on all worker and master nodes, except the PIT node, the CSM services can be installed. The Nexus repository will be populated with artifacts; containerized CSM services will be installed; and a few other configuration steps taken.\nSee Install CSM Services \n  Validate CSM Health Before PIT Node Redeploy\nAfter installing all of the CSM services now would be an good time to validate the health of the management nodes and all CSM services. The advantage in doing it now is that if there are any problems detected with the core infrastructure or the nodes, it is easy to rewind the installation to Deploy Management Nodes because the PIT node has not yet been redeployed.\nAfter installing all of the CSM services, wait at least 15 minutes to let the various Kubernetes resources get initialized and started before trying to validate CSM health. Because there are a number of dependencies between them, some services are not expected to work immediately after the install script completes. Some of the time waiting can be spent preparing the cray CLI.\nNote: If doing the CSM validation at this point, some of the tests which use the \u0026lsquo;cray\u0026rsquo; CLI will fail until these two procedures have been done. These tests, such as Hardware State Manager Discovery Validation, Booting the CSM Barebones Image on compute nodes, or the UAS/UAI Tests can be skipped until after the PIT node has been redeployed.\nTo enable the \u0026lsquo;cray\u0026rsquo; CLI, these two procedures could be done now.\n Optional Configure Keycloak Account Optional Configure the Cray Command Line Interface (cray CLI)  To run the CSM health checks now, see Validate CSM Health \n  Redeploy PIT Node\nNow that all CSM services have been installed and the CSM health checks completed, with the possible exception of Booting the CSM Barebones Image and the UAS/UAI tests, the PIT node can be rebooted to leave the LiveCD environment and assume its intended role as one the Kubernetes master nodes.\nSee Redeploy PIT Node \n  Configure Administrative Access\nNow that all of the CSM services have been installed and the PIT node has been redeployed, administrative access can be prepared. This may include configuring Keycloak with a local Keycloak account or confirming Keycloak is properly federating LDAP or other Identity Provider (IdP), initializing the \u0026lsquo;cray\u0026rsquo; CLI for administrative commands, locking the management nodes from accidental actions such as firmware updates by FAS or power actions by CAPMC, configuring the CSM layer of configuration by CFS in NCN personalization,and configuring the node BMCs (node controllers) for nodes in liquid cooled cabinets.\nSee Configure Administrative Access \n  Validate CSM Health\nNow that all management nodes have joined the Kubernetes cluster, CSM services have been installed, and administrative access has been enabled, the health of the management nodes and all CSM services should be validated. There are no exceptions to running the tests\u0026ndash;all can be run now.\nThis CSM health validation can also be run at other points during the system lifecycle, such as when replacing a management node, checking the health after a management node has rebooted because of a crash, as part of doing a full system power down or power up, or after other types of system maintenance.\nSee Validate CSM Health \n  Configure Prometheus Alert Notifications\nNow that CSM has been installed and health has been validated, if the system management health monitoring tools and specifically, Prometheus, are found to be useful, email notifications can be configured for specific alerts defined in Prometheus. Prometheus upstream documentation can be leveraged for an Alert Notification Template Reference as well as Notification Template Examples. Currently supported notification types include Slack, Pager Duty, email, or a custom integration via a generic webhook interface.\nSee Configure Prometheus Email Alert Notifications for example configuration of an email alert notification for Postgres replication alerts that are defined on the system. \n  Update Firmware with FAS\nNow that all management nodes and CSM services have been validated as healthy, the firmware on other components in the system can be checked and updated. The Firmware Action Service (FAS) communicates with many devices on the system. FAS can be used to update the firmware for all of the devices it communicates with at once, or specific devices can be targeted for a firmware update.\nSee Update Firmware with FAS \n  Prepare Compute Nodes\nAfter completion of the firmware update with FAS, compute nodes can be prepared. Some compute node types have special preparation steps, but most compute nodes are ready to be used now.\nThese compute node types require preparation.\n HPE Apollo 6500 XL645d Gen10 Plus  See Prepare Compute Nodes \n  Next Topic\nAfter completion of the firmware update with FAS and the preparation of compute nodes, the CSM product stream has been fully installed and configured. Refer to the 1.5 HPE Cray EX System Software Getting Started Guide S-8000 on the HPE Customer Support Center at https://www.hpe.com/support/ex-gsg for more information on other product streams to be installed and configured after CSM. \n  Troubleshooting Installation Problems\nThe installation of the Cray System Management (CSM) product requires knowledge of the various nodes and switches for the HPE Cray EX system. The procedures in this section should be referenced during the CSM install for additional information on system hardware, troubleshooting, and administrative tasks related to CSM. See Troubleshooting Installation Problems\n  "
},
{
	"uri": "/docs-csm/en-11/install/install_csm_services/",
	"title": "Install Services",
	"tags": [],
	"description": "",
	"content": "Install CSM Services This procedure will install CSM applications and services into the CSM Kubernetes cluster.\n Node: Check the information in Known Issues before starting this procedure to be warned about possible problems.\n Topics:  Initialize Bootstrap Registry Create Site-Init Secret Deploy Sealed Secret Decryption Key Deploy CSM Applications and Services Setup Nexus Set NCNs to use Unbound Apply Pod Priorities Apply After Sysmgmt Manifest Workarounds Known Issues  install.sh known issues Setup Nexus known issues   Next Topic  Details 1. Initialize Bootstrap Registry  NOTE The bootstrap registry runs in a default Nexus configuration, which is started and populated in this section. It only exists during initial CSM install on the PIT node in order to bootstrap CSM services. Once CSM install is completed and the PIT node is rebooted as an NCN, the bootstrap Nexus no longer exists.\n   Verify that Nexus is running:\npit# systemctl status nexus   Verify that Nexus is ready. (Any HTTP response other than 200 OK indicates Nexus is not ready.)\npit# curl -sSif http://localhost:8081/service/rest/v1/status/writable Expected output looks similar to the following:\nHTTP/1.1 200 OK Date: Thu, 04 Feb 2021 05:27:44 GMT Server: Nexus/3.25.0-03 (OSS) X-Content-Type-Options: nosniff Content-Length: 0   Load the skopeo image installed by the cray-nexus RPM:\npit# podman load -i /var/lib/cray/container-images/cray-nexus/skopeo-stable.tar quay.io/skopeo/stable   Use skopeo sync to upload container images from the CSM release:\npit# export CSM_RELEASE=csm-x.y.z pit# podman run --rm --network host -v /var/www/ephemeral/${CSM_RELEASE}/docker/dtr.dev.cray.com:/images:ro quay.io/skopeo/stable sync \\ --scoped --src dir --dest docker --dest-tls-verify=false --dest-creds admin:admin123 /images localhost:5000  NOTE As the bootstrap Nexus uses the default configuration, the above command uses the default admin credentials (admin user with password admin123) in order to upload to the bootstrap registry, which is listening on localhost:5000.\n   2. Create Site-Init Secret The site-init secret in the loftsman namespace makes /var/www/ephemeral/prep/site-init/customizations.yaml available to product installers. The site-init secret should only be updated when the corresponding customizations.yaml data is changed, such as during system installation or upgrade. Create the site-init secret to contain /var/www/ephemeral/prep/site-init/customizations.yaml:\npit# kubectl create secret -n loftsman generic site-init --from-file=/var/www/ephemeral/prep/site-init/customizations.yaml Expected output looks similar to the following:\nsecret/site-init created  NOTE If the site-init secret already exists then kubectl will error with a message similar to:\nError from server (AlreadyExists): secrets \u0026quot;site-init\u0026quot; already exists In this case, delete the site-init secret and recreate it.\n  First delete it:\npit# kubectl delete secret -n loftsman site-init Expected output looks similar to the following:\nsecret \u0026quot;site-init\u0026quot; deleted   Then recreate it:\npit# kubectl create secret -n loftsman generic site-init --from-file=/var/www/ephemeral/prep/site-init/customizations.yaml Expected output looks similar to the following:\nsecret/site-init created     WARNING If for some reason the system customizations need to be modified to complete product installation, administrators must first update customizations.yaml in the site-init Git repository, which may no longer be mounted on any cluster node, and then delete and recreate the site-init secret as shown below.\nTo read customizations.yaml from the site-init secret:\nncn# kubectl get secrets -n loftsman site-init -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | base64 -d \u0026gt; customizations.yaml To delete the site-init secret:\nncn# kubectl -n loftsman delete secret site-init To recreate the site-init secret:\nncn# kubectl create secret -n loftsman generic site-init --from-file=customizations.yaml  3. Deploy Sealed Secret Decryption Key Deploy the corresponding key necessary to decrypt sealed secrets:\npit# /var/www/ephemeral/prep/site-init/deploy/deploydecryptionkey.sh An error similar to the following may occur when deploying the key:\nError from server (NotFound): secrets \u0026quot;sealed-secrets-key\u0026quot; not found W0304 17:21:42.749101 29066 helpers.go:535] --dry-run is deprecated and can be replaced with --dry-run=client. secret/sealed-secrets-key created Restarting sealed-secrets to pick up new keys No resources found This is expected and can safely be ignored.\n4. Deploy CSM Applications and Services Run install.sh to deploy CSM applications services. This command may take 25 minutes or more to run.\n NOTE install.sh requires various system configuration which are expected to be found in the locations used in proceeding documentation; however, it needs to know SYSTEM_NAME in order to find metallb.yaml and sls_input_file.json configuration files.\nSome commands will also need to have the CSM_RELEASE variable set.\nVerify that the SYSTEM_NAME and CSM_RELEASE environment variables are set:\npit# echo $SYSTEM_NAME pit# echo $CSM_RELEASE If they are not set, perform the following:\npit# export SYSTEM_NAME=eniac pit# export CSM_RELEASE=csm-x.y.z  pit# cd /var/www/ephemeral/$CSM_RELEASE pit# ./install.sh On success, install.sh will output OK to stderr and exit with status code 0, e.g.:\npit# ./install.sh ... + CSM applications and services deployed install.sh: OK In the event that install.sh does not complete successfully, consult the known issues below to resolve potential problems and then try running install.sh again.\nIMPORTANT: If you have to re-run install.sh to re-deploy failed ceph-csi provisioners you must make sure to delete the jobs that have not completed. These are left there for investigation on failure. They are automatically removed on a successful deployment.\npit# kubectl get jobs NAME COMPLETIONS DURATION AGE cray-ceph-csi-cephfs 0/1 3m35s cray-ceph-csi-rbd 0/1 8m36s  If these jobs exist then kubectl delete job \u0026lt;jobname\u0026gt; before running install.sh again.\n 5. Setup Nexus Run ./lib/setup-nexus.sh to configure Nexus and upload CSM RPM repositories, container images, and Helm charts. This command may take 20 minutes or more to run.\npit# ./lib/setup-nexus.sh On success, setup-nexus.sh will output to OK on stderr and exit with status code 0, e.g.:\npit# ./lib/setup-nexus.sh ... + Nexus setup complete setup-nexus.sh: OK In the event of an error, consult the known issues below to resolve potential problems and then try running setup-nexus.sh again. Note that subsequent runs of setup-nexus.sh may report FAIL when uploading duplicate assets. This is ok as long as setup-nexus.sh outputs setup-nexus.sh: OK and exits with status code 0.\n6. Set Management NCNs to use Unbound First, verify that SLS properly reports all management NCNs in the system:\npit# ./lib/list-ncns.sh On success, each management NCN will be output, e.g.:\npit# ./lib/list-ncns.sh + Getting admin-client-auth secret + Obtaining access token + Querying SLS ncn-m001 ncn-m002 ncn-m003 ncn-s001 ncn-s002 ncn-s003 ncn-w001 ncn-w002 ncn-w003 If any management NCNs are missing from the output, take corrective action before proceeding.\nNext, run lib/set-ncns-to-unbound.sh to SSH to each management NCN and update /etc/resolv.conf to use Unbound as the nameserver.\npit# ./lib/set-ncns-to-unbound.sh  NOTE If passwordless SSH is not configured, the administrator will have to enter the corresponding password as the script attempts to connect to each NCN.\n On success, the nameserver configuration in /etc/resolv.conf will be printed for each management NCN, e.g.,:\npit# ./lib/set-ncns-to-unbound.sh + Getting admin-client-auth secret + Obtaining access token + Querying SLS + Updating ncn-m001 Password: ncn-m001: nameserver 127.0.0.1 ncn-m001: nameserver 10.92.100.225 + Updating ncn-m002 Password: ncn-m002: nameserver 10.92.100.225 + Updating ncn-m003 Password: ncn-m003: nameserver 10.92.100.225 + Updating ncn-s001 Password: ncn-s001: nameserver 10.92.100.225 + Updating ncn-s002 Password: ncn-s002: nameserver 10.92.100.225 + Updating ncn-s003 Password: ncn-s003: nameserver 10.92.100.225 + Updating ncn-w001 Password: ncn-w001: nameserver 10.92.100.225 + Updating ncn-w002 Password: ncn-w002: nameserver 10.92.100.225 + Updating ncn-w003 Password: ncn-w003: nameserver 10.92.100.225  NOTE The script connects to ncn-m001 which will be the PIT node, whose password may be different from that of the other NCNs.\n 7. Apply Pod Priorities Run the add_pod_priority.sh script to create and apply a pod priority class to services critical to CSM. This will give these services a higher priority than others to ensure they get scheduled by Kubernetes in the event that resources are limited on smaller deployments.\npit# /usr/share/doc/csm/upgrade/1.0/scripts/upgrade/add_pod_priority.sh Creating csm-high-priority-service pod priority class priorityclass.scheduling.k8s.io/csm-high-priority-service configured Patching cray-postgres-operator deployment in services namespace deployment.apps/cray-postgres-operator patched Patching cray-postgres-operator-postgres-operator-ui deployment in services namespace deployment.apps/cray-postgres-operator-postgres-operator-ui patched Patching istio-operator deployment in istio-operator namespace deployment.apps/istio-operator patched Patching istio-ingressgateway deployment in istio-system namespace deployment.apps/istio-ingressgateway patched . . . After running the add_pod_priority.sh script, the affected pods will be restarted as the pod priority class is applied to them.\n8. Apply After Sysmgmt Manifest Workarounds Follow the workaround instructions for the after-sysmgmt-manifest breakpoint.\n9. Known Issues 9.1 install.sh known issues The install.sh script changes cluster state and should not simply be rerun in the event of a failure without careful consideration of the specific error. It may be possible to resume installation from the last successful command executed by install.sh, but administrators will need to appropriately modify install.sh to pick up where the previous run left off. (Note: The install.sh script runs with set -x, so each command will be printed to stderr prefixed with the expanded value of PS4, namely, + .)\nThe following error may occur when running ./install.sh:\n+ csi upload-sls-file --sls-file /var/www/ephemeral/prep/eniac/sls_input_file.json 2021/10/05 18:42:58 Retrieving S3 credentials ( sls-s3-credentials ) for SLS 2021/10/05 18:42:58 Unable to SLS S3 secret from k8s:secrets \u0026quot;sls-s3-credentials\u0026quot; not found   Verify the sls-s3-credentials secret exists in the default namespace:\npit# kubectl get secret sls-s3-credentials NAME TYPE DATA AGE sls-s3-credentials Opaque 7 28d   Check for running sonar-sync jobs. If there are no sonar-sync jobs, then wait for one to complete. The sonar-sync cronjob is responsible for copying the sls-s3-credentials secret from the default to services namespaces.\npit# kubectl -n services get pods -l cronjob-name=sonar-sync NAME READY STATUS RESTARTS AGE sonar-sync-1634322840-4fckz 0/1 Completed 0 73s sonar-sync-1634322900-pnvl6 1/1 Running 0 13s   Verify the sls-s3-credentials secret now exists in the services namespaces.\npit# kubectl -n services get secret sls-s3-credentials NAME TYPE DATA AGE sls-s3-credentials Opaque 7 20s   Running install.sh again is expected to succeed.\n  9.2 Setup Nexus known issues Known potential issues with suggested fixes are listed in Troubleshoot Nexus.\n10. Next Topic After completing this procedure the next step is to redeploy the PIT node.\n See Validate CSM Health Before PIT Node Redeploy  "
},
{
	"uri": "/docs-csm/en-11/install/prepare_compute_nodes/",
	"title": "Prepare Compute Nodes",
	"tags": [],
	"description": "",
	"content": "Prepare Compute nodes Some compute nodes types have special preparation steps, but most compute nodes are ready to be used now These nodes have an additional procedure before they can be booted.\nTopics:  Configure HPE Apollo 6500 XL645d Gen10 Plus Compute Nodes  Prerequisites The time for Gigabyte compute nodes is synced with the rest of the system. See Update the Gigabyte Server BIOS Time.\nDetails 1. Configure HPE Apollo 6500 XL645d Gen10 Plus Compute Nodes The HPE Apollo 6500 XL645d Gen10 Plus compute node uses a NIC/shared iLO network port. The NIC is also referred to as the Embedded LOM (LAN On Motherboard) and is available to the booted OS. This shared port is plugged into a port on the TOR Ethernet switch designated for the Hardware Management Network (HMN) causing the NIC to get an IP address assigned to it from the wrong pool. To prevent this from happening, the iLO VLAN tag needs to be configured for VLAN 4 and the switch port the NIC/shared iLO is plugged into needs to be configured to allow only VLAN 4 traffic. This prevents the NIC from communicating over the switch, and it will no longer DHCP an IP address.\nThis procedure needs to be done for each HPE Apollo 6500 XL645d node managed by CSM software.\n  Gather Information\nExample using x3000c0s30b0n0 as the target component xname\nncn-m001:~ # cray hsm inventory ethernetInterfaces list --component-id \\ x3000c0s30b0n0 --format json | jq '.[]|select((.IPAddresses|length)\u0026gt;0)' { \u0026quot;ID\u0026quot;: \u0026quot;6805cabbc182\u0026quot;, \u0026quot;Description\u0026quot;: \u0026quot;, \u0026quot;MACAddress\u0026quot;: \u0026quot;68:05:ca:bb:c1:82\u0026quot;, \u0026quot;LastUpdate\u0026quot;: \u0026quot;2021-04-19T22:15:00.523621Z\u0026quot;, \u0026quot;ComponentID\u0026quot;: \u0026quot;x3000c0s30b0n0\u0026quot;, \u0026quot;Type\u0026quot;: \u0026quot;Node\u0026quot;, \u0026quot;IPAddresses\u0026quot;: [ { \u0026quot;IPAddress\u0026quot;: \u0026quot;10.252.1.21\u0026quot; } ] } { \u0026quot;ID\u0026quot;: \u0026quot;9440c938f7b4\u0026quot;, \u0026quot;Description\u0026quot;: \u0026quot;, \u0026quot;MACAddress\u0026quot;: \u0026quot;94:40:c9:38:f7:b4\u0026quot;, \u0026quot;LastUpdate\u0026quot;: \u0026quot;2021-05-07T18:37:59.239924Z\u0026quot;, \u0026quot;ComponentID\u0026quot;: \u0026quot;x3000c0s30b0n0\u0026quot;, \u0026quot;Type\u0026quot;: \u0026quot;Node\u0026quot;, \u0026quot;IPAddresses\u0026quot;: [ { \u0026quot;IPAddress\u0026quot;: \u0026quot;10.254.1.38\u0026quot; } ] } NOTE: the data might not exist in HSM for the node which is fine because if it isn\u0026rsquo;t there then the data isn\u0026rsquo;t needed to clean up KEA later.\nMake a note of the ID, MACAddress, and IPAddress of the entry that has the 10.254 address listed. Those will be used later to clean up kea and Hardware State Manager (HSM). There may not be a 10.254 address list, that is fine, continue on to the next step.\n\n  Configure the iLO to use VLAN 4\n Connect to BMC WebUI and log in with standard root credentials Click on iLO Shared Network Port on left menu Make a note of the MAC Address under the Information section, that will be needed later. Click on General on the top menu Under NIC Settings move slider to Enable VLAN In the VLAN Tag box, enter 4 Click Apply Click Reset iLO when it appears Click Yes, Reset when it appears on the right After accepting the BMC restart, connection to the BMC will be lost until the switch port reconfiguration is performed.    Configure the switch port for the iLO to use VLAN 4\n  Find the port and the switch the iLO is plugged into using the SHCD.\n  ssh to the switch and log in with standard admin credentials. Refer to /etc/hosts for exact hostname.\n  Verify the MAC on the port.\nExample using port number 46.\nsw-leaf01# show mac-address-table | include 1/1/46 94:40:c9:38:08:c7 4 dynamic 1/1/46 Make sure MAC address returned for that port matches the MAC address noted in step 2.3 from the Information section of the WebUI\nIf the MAC is not correct, double check the server cabling and SHCD then start this section over.\n  Configure the port if the MAC is correct.\nExample using port number 46.\nsw-leaf01# configure t sw-leaf01(config)# int 1/1/46 sw-leaf01(config-if)# vlan trunk allowed 4 sw-leaf01(config-if)# write mem Copying configuration: [Success] sw-leaf01(config-if)# exit sw-leaf01(config)# exit sw-leaf01# show running-config interface 1/1/46 interface 1/1/46 no shutdown mtu 9198 description dl645d no routing vlan trunk native 1 vlan trunk allowed 4 spanning-tree bpdu-guard spanning-tree port-type admin-edge exit After a few minutes the switch will be configured and access to the WebUI will be regained.\n    Clear bad MAC and IP address out of kea\nSkip this step if there was no MAC and IP address found in step 1.\nExample using the MAC and IP address from step 1.\nncn-m001:~ # curl -s -k -H \u0026quot;Authorization: Bearer ${TOKEN}\u0026quot; -X POST -H \\ \u0026quot;Content-Type: application/json\u0026quot; -d '{\u0026quot;command\u0026quot;: \u0026quot;lease4-del\u0026quot;, \\ \u0026quot;service\u0026quot;: [ \u0026quot;dhcp4\u0026quot; ], \u0026quot;arguments\u0026quot;: {\u0026quot;hw-address\u0026quot;: \u0026quot;94:40:c9:38:f7:b4\u0026quot;, \\ \u0026quot;ip-address\u0026quot;: \u0026quot;10.254.1.38\u0026quot;}}' https://api-gw-service-nmn.local/apis/dhcp-kea [ { \u0026quot;result\u0026quot;: 0, \u0026quot;text\u0026quot;: \u0026quot;IPv4 lease deleted.\u0026quot; } ]   Clear bad ID out of HSM\nSkip this step if there was no ID found in step 1.\nExample using the ID from step 1.\nncn-m001:~ # cray hsm inventory ethernetInterfaces delete 9440c938f7b4 { \u0026quot;code\u0026quot;: 0, \u0026quot;message\u0026quot;: \u0026quot;deleted 1 entry\u0026quot; } Everything is now configured and the node is ready to be rebooted.\n  Next Topic After completing the preparation for compute management nodes, the CSM product stream has been fully installed and configured. Check the next topic.\nSee Next Topic\n"
},
{
	"uri": "/docs-csm/en-11/install/prepare_configuration_payload/",
	"title": "Prepare Configuration Payload",
	"tags": [],
	"description": "",
	"content": "Prepare Configuration Payload The configuration payload consists of the information which must be known about the HPE Cray EX system so it can be passed to the csi (Cray Site Init) program during the CSM installation process.\nInformation gathered from a site survey is needed to feed into the CSM installation process, such as system name, system size, site network information for the CAN, site DNS configuration, site NTP configuration, network information for the node used to bootstrap the installation. More detailed component level information about the system hardware is encapsulated in the SHCD (Shasta Cabling Diagram), which is a spreadsheet prepared by HPE Cray Manufacturing to assemble the components of the system and connect appropriately labeled cables.\nHow the configuration payload is prepared depends on whether this is a first time installation of CSM software on this system or the CSM software is being reinstalled. The reinstall scenario has the advantage of being able to use the configuration payload from the previous CSM installation and an extra configuration file which that installation generated.\nTopics:  Command Line Configuration Payload Configuration Payload Files First Time Install Reinstall Next Topic  Details Command Line Configuration Payload This information from a site survey can be given to the csi command as command line arguments. The information is shown here to explain what data is needed. It will not be used until moving to the procedure Bootstrap PIT Node\nThe air-cooled cabinet is known to csi as a river cabinet. The liquid-cooled cabinets are either mountain or hill (if a TDS system).\nFor more description of these settings and the default values, see Default IP Address Ranges\n   CSI option Information     \u0026ndash;bootstrap-ncn-bmc-user root Administrative account for the management node BMCs   \u0026ndash;bootstrap-ncn-bmc-pass changeme Password for bootstrap-ncn-bmc-user account   \u0026ndash;system-name eniac Name of the HPE Cray EX system   \u0026ndash;mountain-cabinets 4 Number of Mountain cabinets, but this could also be in cabinets.yaml   \u0026ndash;starting-mountain-cabinet 1000 Starting Mountain cabinet   \u0026ndash;hill-cabinets 0 Number of Hill cabinets, but this could also be in cabinets.yaml   \u0026ndash;river-cabinets 1 Number of River cabinets, but this could also be in cabinets.yaml   \u0026ndash;can-cidr 10.103.11.0/24 IP subnet for the CAN assigned to this system   \u0026ndash;can-external-dns 10.103.11.113 IP address on CAN for this system\u0026rsquo;s DNS server   \u0026ndash;can-gateway 10.103.11.1 Virtual IP address for the CAN (on the spine switches)   \u0026ndash;can-static-pool 10.103.11.112/28 MetalLB static pool on CAN   \u0026ndash;can-dynamic-pool 10.103.11.128/25 MetalLB dynamic pool on CAN   \u0026ndash;hmn-cidr 10.254.0.0/17 Override the default cabinet IPv4 subnet for River HMN   \u0026ndash;nmn-cidr 10.252.0.0/17 Override the default cabinet IPv4 subnet for River NMN   \u0026ndash;hmn-mtn-cidr 10.104.0.0/17 Override the default cabinet IPv4 subnet for Mountain HMN   \u0026ndash;nmn-mtn-cidr 10.100.0.0/17 Override the default cabinet IPv4 subnet for Mountain NMN   \u0026ndash;ntp-pool time.nist.gov External NTP server for this system to use   \u0026ndash;site-domain dev.cray.com Domain name for this system   \u0026ndash;site-ip 172.30.53.79/20 IP address and netmask for the PIT node lan0 connection   \u0026ndash;site-gw 172.30.48.1 Gateway for the PIT node to use   \u0026ndash;site-nic p1p2 NIC on the PIT node to become lan0   \u0026ndash;site-dns 172.30.84.40 Site DNS servers to be used by the PIT node   \u0026ndash;install-ncn-bond-members p1p1,p10p1 NICs on each management node to become bond0   \u0026ndash;application-node-config-yaml application_node_config.yaml Name of application_node_config.yaml   \u0026ndash;cabinets-yaml cabinets.yaml Name of cabinets.yaml   \u0026ndash;bgp-peers aggregation Override the default BGP peers, using aggregation switches instead of spines   \u0026ndash;primary-server-name primary Desired name for the primary DNS server   \u0026ndash;secondary-servers \u0026quot; Comma seperated list of FQDN/IP for all DNS servers to be notified on DNS zone update   \u0026ndash;notify-zones \u0026quot; A comma separated list of DNS zones to transfer     This is a long list of options. It can be helpful to create a Bash script file to call the csi command with all of these options, and then edit that file to adjust the values for the particular system being installed. The bootstrap-ncn-bmc-user and bootstrap-ncn-bmc-pass must match what is used for the BMC account and its password for the management nodes. Set site parameters (site-domain, site-ip, site-gw, site-nic, site-dns) for the information which connects ncn-m001 (the PIT node) to the site. The site-nic is the interface on this node connected to the site. There are other interfaces possible, but the install-ncn-bond-members are typically:  p1p1,p10p1 for HPE nodes p1p1,p1p2 for Gigabyte nodes p801p1,p801p2 for Intel nodes   The starting cabinet number for each type of cabinet (for example, starting-mountain-cabinet) has a default that can be overridden. See the csi config init --help output for more information. An override to default cabinet IPv4 subnets can be made with the hmn-mtn-cidr and nmn-mtn-cidr parameters. Several parameters (can-gateway, can-cidr, can-static-pool, can-dynamic-pool) describe the CAN (Customer Access network). The can-gateway is the common gateway IP address used for both spine switches and commonly referred to as the Virtual IP address for the CAN. The can-cidr is the IP subnet for the CAN assigned to this system. The can-static-pool and can-dynamic-pool are the MetalLB address static and dynamic pools for the CAN. The can-external-dns is the static IP address assigned to the DNS instance running in the cluster to which requests the cluster subdomain will be forwarded. The can-external-dns IP address must be within the can-static-pool range. Set ntp-pool to a reachable NTP server. The application_node_config.yaml file is required. It is used to describe the mapping between prefixes in hmn_connections.csv and HSM subroles. This file also defines aliases application nodes. For details, see Create Application Node YAML. For systems that use non-sequential cabinet id numbers, use cabinets-yaml to include the cabinets.yaml file. This file can include information about the starting ID for each cabinet type and number of cabinets which have separate command line options, but is a way to specify explicitly the id of every cabinet in the system. See Create Cabinets YAML. The PowerDNS zone transfer arguments primary-server-name, secondary-servers, and notify-zones are optional unless zone transfer is being configured. For more information see the PowerDNS Configuration Guide  Configuration Payload Files A few configuration files are needed for the installation of Shasta v1.5. These are all provided to the csi command during the installation process.\n   Filename Source Information     cabinets.yaml SHCD The number and type of air-cooled and liquid-cooled cabinets. cabinet IDs, and VLAN numbers   application_node_config.yaml SHCD The number and type of application nodes with mapping from the name in the SHCD to the desired hostname   hmn_connections.json SHCD The network topology for HMN of the entire system   ncn_metadata.csv SHCD, other The number of master, worker, and storage nodes and MAC address information for BMC and bootable NICs   switch_metadata.csv SHCD Inventory of all spine, aggregation, CDU, and leaf switches    Although some information in these files can be populated from site survey information, the SHCD prepared by HPE Cray Manufacturing is the best source of data for hmn_connections.json. The ncn_metadata.csv does require collection of MAC addresses from the management nodes because that information is not present in the SHCD.\ncabinets.yaml The cabinets.yaml file describes the type of cabinets in the system, the number of each type of cabinet, and the starting cabinet ID for every cabinet in the system. This file can be used to indicate that a system has non-contiguous cabinet ID numbers or non-standard VLAN numbers.\nThe xnames used in the other files should fit within the cabinet ids defined by the starting cabinet id for River cabinets (modified by the number of cabinets). It is OK for management nodes not to be in x3000 (as the first River cabinet), but they must be in one of the River cabinets. For example, x3000 with 2 cabinets would mean x3000 or x3001 should have all management nodes.\nSee Create Cabinets YAML for instructions about creating this file.\napplication_node_config.yaml The application_node_config.yaml file controls how the csi config init command finds and treats application nodes discovered in the hmn_connections.json file when building the SLS Input file.\nDifferent node prefixes in the SHCD can be identified as Application nodes. Each node prefix can be mapped to a specific HSM sub role. These sub roles can then be used as the targets of Ansible plays run by CFS to configure these nodes. The xname for each Application node can be assigned one or more hostname aliases.\nSee Create Application Node YAML for instructions about creating this file.\nhmn_connections.json The hmn_connections.json file is extracted from the HMN tab of the SHCD spreadsheet. The CSM release includes the hms-shcd-parser container which can be used on the PIT node booted from the LiveCD (RemoteISO or USB device) or a Linux system to do this extraction. Although some information in these files can be populated from site survey information, the SHCD prepared by HPE Cray Manufacturing is the best source of data for hmn_connections.json.\nNo action is required to create this file at this point, and it will be created when the PIT node is bootstrapped.\nncn_metadata.csv The information in the ncn_metadata.csv file identifies each of the management nodes, assigns the function as a master, worker, or storage node, and provides the MAC address information needed to identify the BMC and the NIC which will be used to boot the node.\nFor each management node, the xname, role, and subrole can be extracted from the SHCD. However, the rest of the MAC address information needs to be collected another way. Collect as much information as possible before the PIT node is booted from the LiveCD and then get the rest later when directed. See the scenarios which enable partial data collection below in First Time Install.\nSee Create NCN Metadata CSV for instructions about creating this file.\nswitch_metadata.csv The switch_metadata.csv file is manually created to include information about all spine, aggregation, CDU, and leaf switches in the system. None of the Slingshot switches for the HSN should be included in this file.\nSee Create Switch Metadata CSV for instructions about creating this file.\nFirst Time Install The process to install for the first time must collect the information needed to create these files.\n  Collect data for cabinets.yaml\nSee Create Cabinets YAML for instructions about creating this file.\n  Collect data for application_node_config.yaml\nSee Create Application Node YAML for instructions about creating this file.\n  Collect data for ncn_metadata.csv\nSee Create NCN Metadata CSV for instructions about creating this file.\n  Collect data for switch_metadata.csv\nSee Create Switch Metadata CSV for instructions about creating this file.\n  Reinstall The process to reinstall must have the configuration payload files available.\n  Collect Payload for Reinstall\n  These files from a previous install are needed to do a reinstall.\n application_node_config.yaml (if used previously) cabinets.yaml (if used previously) hmn_connections.json ncn_metadata.csv switch_metadata.csv system_config.yaml  If the system_config.yaml is not available, then a reinstall cannot be done. Switch to the install process and generate any of the other files for the Configuration Payload Files which are missing.\n  The command line options used to call csi config init are not needed.\nWhen doing a reinstall, all of the command line options which had been given to csi config init during the previous installation will be found inside the system_config.yaml file. This simplifies the reinstall process.\nWhen you are ready to bootstrap the LiveCD, it will indicate when to run this command without any extra command line options. It will expect to find all of the above files in the current working directory.\nlinux# csi config init     Next Topic After completing this procedure the next step is to prepare the management nodes.\n See Prepare Management Nodes  "
},
{
	"uri": "/docs-csm/en-11/install/create_hmn_connections_json/",
	"title": "Create Hmn Connections Json",
	"tags": [],
	"description": "",
	"content": "Create HMN Connections JSON About this task The following procedure shows the process for generating the hmn_connections.json from the system\u0026rsquo;s SHCD Excel document. This process is typically needed when generating the hmn_connections.json file for a new system, or regenerating it when system\u0026rsquo;s SHCD is changed (specifically the HMN tab). The hms-shcd-parser tool can be used to generate the hmn_connections.json file.\nThe SHCD/HMN Connections Rules document explains the expected naming conventions and rules for the HMN tab of the SHCD, and the hmn_connections.json file.\nPrerequisites  SHCD Excel file for your system Podman is available  Podman is available on the CSM LiveCD, and is installed onto a NCN when being used as an environment to create the CSM PIT in the Bootstrap PIT Node from LiveCD USB or Bootstrap Pit Node from LiveCD Remote ISO procedures.\n   Procedure   Inspect the HMN tab of the SHCD to verify that it does not have unexpected data in columns J through U in row 20 or below. If any unexpected data is present in this region of the HMN tab it will end up in the generated hmn_connections.json, and needs to be removed before generating the hmn_connections.json file. Unexpected data is anything other than HMN cabling information, such as another table placed below the HMN cabling information. Any data above row 20 will not interfere when generating hmn_connections.json.\nFor example, below is an example a unexpected table present underneath HMN cabling information in rows 26 to 29. Note the HMN cabling information was truncated for brevity.   Load the hms-shcd-parser container image from the CSM release distribution into Podman.\n The CSM_RELEASE environment variable is expected to to be set from the Bootstrap PIT Node from LiveCD USB or Bootstrap Pit Node from LiveCD Remote ISO procedures.\nIt is expected that current directory contains the directory of the extracted CSM release tarball.\n Determine the version of the hms-shcd-parser container image:\nlinux# SHCD_PARSER_VERSION=$(realpath ./${CSM_RELEASE}/docker/dtr.dev.cray.com/cray/hms-shcd-parser* | egrep -o \u0026#39;[0-9]+\\.[0-9]+\\.[0-9]+$\u0026#39;) linux# echo $SHCD_PARSER_VERSION Load the hms-shcd-parser container image into Podman:\nlinux# ./${CSM_RELEASE}/hack/load-container-image.sh dtr.dev.cray.com/cray/hms-shcd-parser:$SHCD_PARSER_VERSION   Copy over the system\u0026rsquo;s SHCD over the machine being used to prepare the hmn_connections.json file.\n  Set environment to point to the system\u0026rsquo;s SHCD Excel file:\n Note: Make sure to quote the SHCD file path if there are spaces in the document\u0026rsquo;s filename.\n linux# export SHCD_FILE=\u0026#34;/path/to/systems/SHCD.xlsx\u0026#34;   Generate the hmn_connections.json file from the SHCD. This will either create or overwrite the hmn_connections.json file in the current directory:\nlinux# podman run --rm -it --name hms-shcd-parser -v \u0026#34;$(realpath \u0026#34;$SHCD_FILE\u0026#34;)\u0026#34;:/input/shcd_file.xlsx -v \u0026#34;$(pwd)\u0026#34;:/output dtr.dev.cray.com/cray/hms-shcd-parser:$SHCD_PARSER_VERSION   "
},
{
	"uri": "/docs-csm/en-11/install/create_ncn_metadata_csv/",
	"title": "Create NCN Metadata Csv",
	"tags": [],
	"description": "",
	"content": "Create NCN Metadata CSV The information in the ncn_metadata.csv file identifies each of the management nodes, assigns the function as a master, worker, or storage node, and provides the MAC address information needed to identify the BMC and the NIC which will be used to boot the node.\nSome of the data in the ncn_metadata.csv can be found in the SHCD. However, the hardest data to collect is the MAC addresses for the node\u0026rsquo;s BMC, the node\u0026rsquo;s bootable network interface, and the pair of network interfaces which will become the bonded interface bond0.\nTopics:  Introduction  LACP Bonding   PXE or BOOTSTRAP MAC Sample ncn_metadata.csv Collection of MAC Addresses  Details Introduction Each of the management nodes is represented as a row in the ncn_metadata.csv file.\nFor example:\nXname,Role,Subrole,BMC MAC,Bootstrap MAC,Bond0 MAC0,Bond0 MAC1 x3000c0s9b0n0,Management,Storage,94:40:c9:37:77:26,14:02:ec:d9:76:88,14:02:ec:d9:76:88,94:40:c9:5f:b6:92 For each management node, the xname, role, and subrole can be extracted from the SHCD. However, the rest of the MAC address information needs to be collected another way.\nCheck the description for component names while mapping names between the SHCD and your ncn_metadata.csv file. See Component Names (xnames).\nThere are two interesting parts to the NCN metadata file:\n The MAC of the BMC The MAC(s) of the shasta-network interface(s)  The \u0026ldquo;shasta-network interface\u0026rdquo; is the interfaces, one or more, that comprise the NCNs' LACP link-aggregation ports.\nLACP Bonding NCNs may have one or more bond interfaces, which may be comprised from one or more physical interfaces. The preferred default configuration is two physical network interfaces per bond. The number of bonds themselves depends on your systems network topology.\nFor example, systems with 4 network interfaces on a given node could configure either of these permutations (for redundancy minimums within Shasta cluster):\n one bond with 4 interfaces (bond0) two bonds with 2 interfaces each (bond0 and bond1)  For more information, see NCN Networking page for NCNs.\nPXE or BOOTSTRAP MAC In general this refers to the interface to be used when the node attempts to PXE boot. This varies between vintages of systems; systems before \u0026ldquo;Spring 2020\u0026rdquo; often booted NCNs with onboard NICs, newer systems boot over their PCIe cards.\nIf the system is booting over PCIe then the \u0026ldquo;bootstrap MAC\u0026rdquo; and the \u0026ldquo;bond0 MAC0\u0026rdquo; will be identical. If the system is booting over onboard NICs then the \u0026ldquo;bootstrap MAC\u0026rdquo; and the \u0026ldquo;bond0 MAC0\u0026rdquo; will be different.\n Other Nomenclature\n  \u0026ldquo;BOND MACS\u0026rdquo; are the MAC addresses for the physical interfaces that your node will use for the various VLANs. BOND0 MAC0 and BOND0 MAC1 should not be on the same physical network card to establish redundancy for failed chips. On the other hand, if any nodes' capacity prevents it from being redundant, then MAC1 and MAC0 will still produce a valid configuration if they do reside on the same physical chip/card. The BMC MAC is the exclusive, dedicated LAN for the onboard BMC. It should not be swapped with any other device.  Sample ncn_metadata.csv The following are sample rows from a ncn_metadata.csv file:\n  Use case: NCN with a single PCIe card (1 card with 2 ports):\n Notice how the MAC address for Bond0 MAC0 and Bond0 MAC1 are only off by 1, which indicates that they are on the same 2 port card.\n Xname,Role,Subrole,BMC MAC,Bootstrap MAC,Bond0 MAC0,Bond0 MAC1 x3000c0s6b0n0,Management,Worker,94:40:c9:37:77:b8,14:02:ec:da:bb:00,14:02:ec:da:bb:00,14:02:ec:da:bb:01   Use case: NCN with a dual PCIe cards (2 cards with 2 ports each for 4 ports total):\n Notice how the MAC address for Bond0 MAC0 and Bond0 MAC1 have a difference greater than 1, which indicates that they are on not on the same 2 port same card.\n Xname,Role,Subrole,BMC MAC,Bootstrap MAC,Bond0 MAC0,Bond0 MAC1 x3000c0s9b0n0,Management,Storage,94:40:c9:37:77:26,14:02:ec:d9:76:88,14:02:ec:d9:76:88,94:40:c9:5f:b6:92   Example ncn_metadata.csv file for a system that has been configured as follows:\n Management NCNs are configured to boot over the PCIe NICs Master and Storage management NCNs have two 2 port PCIe cards Worker management NCNs have one 2 port PCIe card   Because the NCNs have been configured to boot over their PCIe NICs, the Bootstrap MAC and Bond0 MAC0 columns have the same value.\n IMPORTANT: Mind the index for each group of nodes (3, 2, 1\u0026hellip;. ; not 1, 2, 3). If storage nodes are ncn-s001 x3000c0s7b0n0, ncn-s002 x3000c0s8b0n0, ncn-s003 x3000c0s9b0n0, then their portion of the file would be ordered x3000c0s9b0n0, x3000c0s8b0n0, x3000c0s7b0n0.\nXname,Role,Subrole,BMC MAC,Bootstrap MAC,Bond0 MAC0,Bond0 MAC1 x3000c0s9b0n0,Management,Storage,94:40:c9:37:77:26,14:02:ec:d9:76:88,14:02:ec:d9:76:88,94:40:c9:5f:b6:92 x3000c0s8b0n0,Management,Storage,94:40:c9:37:87:5a,14:02:ec:d9:7b:c8,14:02:ec:d9:7b:c8,94:40:c9:5f:b6:5c x3000c0s7b0n0,Management,Storage,94:40:c9:37:0a:2a,14:02:ec:d9:7c:88,14:02:ec:d9:7c:88,94:40:c9:5f:9a:a8 x3000c0s6b0n0,Management,Worker,94:40:c9:37:77:b8,14:02:ec:da:bb:00,14:02:ec:da:bb:00,14:02:ec:da:bb:01 x3000c0s5b0n0,Management,Worker,94:40:c9:35:03:06,14:02:ec:d9:76:b8,14:02:ec:d9:76:b8,14:02:ec:d9:76:b9 x3000c0s4b0n0,Management,Worker,94:40:c9:37:67:60,14:02:ec:d9:7c:40,14:02:ec:d9:7c:40,14:02:ec:d9:7c:41 x3000c0s3b0n0,Management,Master,94:40:c9:37:04:84,14:02:ec:d9:79:e8,14:02:ec:d9:79:e8,94:40:c9:5f:b5:cc x3000c0s2b0n0,Management,Master,94:40:c9:37:f9:b4,14:02:ec:da:b8:18,14:02:ec:da:b8:18,94:40:c9:5f:a3:a8 x3000c0s1b0n0,Management,Master,94:40:c9:37:87:32,14:02:ec:da:b9:98,14:02:ec:da:b9:98,14:02:ec:da:b9:99 Collection of MAC Addresses Collect as much information as possible for the ncn_metadata.csv file before the PIT node is booted from the LiveCD and then get the rest later when directed. Having dummy MAC addresses, such as de:ad:be:ef:00:00, in the ncn_metadata.csv file is acceptable until the point during the install at which the management network switches have been configured and the PIT node can be used to collect the information. The correct MAC addresses are needed before attempting to boot the management nodes with their real image in Deploy Management Nodes\n  If the nodes are booted to Linux, then the data can be collected by ipmitool lan print for the BMC MAC, and the ip address command for the other NICs. This is rarely the case for a first time install. The PIT node examples of using these two commands could be extrapolated for other nodes which are booted to Linux. See the PIT node examples in Collecting BMC MAC Addresses and Collecting NCN MAC Addresses.\n  If the nodes are powered up and there is SSH access to the spine and leaf switches, it is possible to collect information from the spine and leaf switches.\n The BMC MAC address can be collected from the switches using knowledge about the cabling of the HMN from the SHCD. See Collecting BMC MAC Addresses. The node MAC addresses cannot be collected until after the PIT node has booted from the LiveCD. At that point, a partial boot of the management nodes can be done to collect the remaining information from the conman console logs on the PIT node using the Procedure: iPXE Consoles    If the nodes are powered up and there is no SSH access to the spine and leaf switches, it is possible to connect to the spine and leaf switches using the method described in Connect to Switch over USB-Serial Cable.\n The BMC MAC address can be collected from the switches using knowledge about the cabling of the HMN from the SHCD. See Collecting BMC MAC Addresses. The node MAC addresses cannot be collected until after the PIT node has booted from the LiveCD. At that point, a partial boot of the management nodes can be done to collect the remaining information from the conman console logs on the PIT node using the Procedure: iPXE Consoles    In all other cases, the full information needed for ncn_metadata.csv will not be available for collection until after the PIT node has been booted from the LiveCD. Having incorrect MAC addresses in the ncn_metadata.csv file as placeholders is acceptable until the point during the install at which the management network switches have been configured and the PIT node can be used to collect the information.\n At that point in the installation workflow, you will be directed to see Collect MAC Addresses for NCNs.    Unless your system does not use or does not have onboard NICs on the management nodes, then this topic may be necessary before constructing the ncn_metadata.csv file.\n Switch PXE Boot from Onboard NIC to PCIe    "
},
{
	"uri": "/docs-csm/en-11/install/create_switch_metadata_csv/",
	"title": "Create Switch Metadata Csv",
	"tags": [],
	"description": "",
	"content": "Create Switch Metadata CSV This page provides directions on constructing the switch_metadata.csv file.\nThis file is manually created to include information about all spine, leaf, CDU, and aggregation switches in the system. None of the Slingshot switches for the HSN should be included in this file.\nThe file should have the following format, in ascending order by Xname:\nSwitch Xname,Type,Brand d0w1,CDU,Dell d0w2,CDU,Dell x3000c0w38,Leaf,Dell x3000c0w36,Leaf,Dell x3000c0h33s1,Spine,Mellanox x3000c0h34s1,Spine,Mellanox The above file would lead to this pairing between component name and hostname:\n   Hostname Component Name     sw-spine-001 x3000c0h33s1   sw-spine-002 x3000c0h34s1   sw-leaf-001 x3000c0w38   sw-leaf-002 x3000c0w36   sw-cdu-001 d0w1   sw-cdu-002 d0w2    The hostname\u0026rsquo;s are automatically generated in ascending order by switch type.\nThe Brand name of the management switches can be determined from one of two places. The Device Diagrams or River Device Diagrams tab of the SHCD has pictures and diagrams of the components of the system including the management network switches. This will have a long name which shows the part number and the vendor name. The Rack Layout or River Rack Layout tab shows the part number in the context of its location within the cabinet.\n   Part Number Brand     Aruba 8320 48P 1G/10GBASE-T and 6P 40G QSFP with X472 (JL481A) Aruba   Aruba 8325-23C 32-port 100G QSFP+/QSFP28 (JL627A) Aruba   CS-XGE40-MLNX-2100-16 Mellanox   HPE Aruba 6300M - switch - 48 ports - managed - rack-mountable Aruba   JL625A - Aruba 8325-48Y8C BF 6 F 2 PS Bdl Aruba   XC-XGE-48P-DL2 Ethernet switch (Dell S3048-ON) Dell   XC-XGT-48P-DL2 Ethernet switch (Dell S4048-ON) Dell    There may be other switches in a specific SHCD, but the general guidelines for any abbreviations are that MLNX or MLX is for Mellanox and DL is for Dell. All other switches are HPE Aruba switches.\nRequirements For this you will need:\n The SHCD for your system Check the description for component names while mapping names between the SHCD and your switch_metadata.csv file. See Component Names (xnames).  Format Spine and aggregation switches use the format xXcChHsS. Leaf switches use xXcCwW. CDU switches use dDwW.\nReference Diagram for Subsequent Sections  Diagram of a cabinet with side-by-side switches in SHCD.\n Directions  In your SHCD, identify your switches. Look for:  The slot number(s) for the leaf switches (usually 48-port switches)  In the above diagram this is x3000u22   The slot number(s) for the spine switches  In the above diagram this is x3000u23R and x3000u23L (two side-by-side switches) Newer side-by-side switches use slot numbers of s1 and s2 instead of R and L     Each spine or aggregation switch will follow this format: xXcChHsS  This format also applies to CDU switches that are in a River cabinet that make connections to an adjacent Hill cabinet.\n  xX : where \u0026ldquo;X\u0026rdquo; is the River cabinet identification number (the figure above is \u0026ldquo;3000\u0026rdquo;) cC : where \u0026ldquo;C\u0026rdquo; is the chassis identification number. This should be \u0026ldquo;0\u0026rdquo;. hH : where \u0026ldquo;H\u0026rdquo; is the slot number in the cabinet (height) sS : where \u0026ldquo;S\u0026rdquo; is the horizontal space number'   Each leaf switch will follow this format: xXcCwW:  xX : where \u0026ldquo;X\u0026rdquo; is the River cabinet identification number (the figure above is \u0026ldquo;3000\u0026rdquo;) cC : where \u0026ldquo;C\u0026rdquo; is the chassis identification number. This should be \u0026ldquo;0\u0026rdquo;. wW : where \u0026ldquo;W\u0026rdquo; is the slot number in the cabinet (height)   Each CDU switch will follow this format: dDwW:  If a CDU switch is in a River cabinet, then follow the naming convention in step 2 instead.\n  dD : where \u0026ldquo;D\u0026rdquo; is the Coolant Distribution Unit (CDU) wW : where \u0026ldquo;W\u0026rdquo; is the management switch in a CDU   Each item in the file is either of type Aggregation, CDU, Leaf, or Spine. Each line in the file must denote the Brand, either Dell, Mellanox, or Aruba. Create the switch_metadata.csv file with this information.  linux# vi switch_metadata.csv See the example files below for reference.\nExamples  Use case: 2 Aruba CDU Switches, 2 Aruba leaf switches, 4 Aruba aggregation switches, and 2 Aruba spine switches\n pit# cat example_switch_metadata.csv Switch Xname,Type,Brand d0w1,CDU,Aruba d0w2,CDU,Aruba x3000c0w31,Leaf,Aruba x3000c0w32,Leaf,Aruba x3000c0h33s1,Aggregation,Aruba x3000c0h34s1,Aggregation,Aruba x3000c0h35s1,Aggregation,Aruba x3000c0h36s1,Aggregation,Aruba x3000c0h37s1,Spine,Aruba x3000c0h38s1,Spine,Aruba  Use case: 2 Dell CDU switches, 2 Dell leaf switches, and 2 Mellanox spine switches:\n Switch Xname,Type,Brand d0w1,CDU,Dell d0w2,CDU,Dell x3000c0w36,Leaf,Dell x3000c0w38,Leaf,Dell x3000c0h33s1,Spine,Mellanox x3000c0h34s1,Spine,Mellanox  Use case: 2 Dell leaf switches and 2 Mellanox switches in the same slot number:\n pit# cat example_switch_metadata.csv Switch Xname,Type,Brand x3000c0w38,Leaf,Dell x3000c0w36,Leaf,Dell x3000c0h33s1,Spine,Mellanox x3000c0h33s2,Spine,Mellanox "
},
{
	"uri": "/docs-csm/en-11/install/deploy_management_nodes/",
	"title": "Deploy Management Nodes",
	"tags": [],
	"description": "",
	"content": "Deploy Management Nodes The following procedure deploys Linux and Kubernetes software to the management NCNs. Deployment of the nodes starts with booting the storage nodes followed by the master nodes and worker nodes together. After the operating system boots on each node, there are some configuration actions which take place. Watching the console or the console log for certain nodes can help to understand what happens and when. When the process completes for all nodes, the Ceph storage is initialized and the Kubernetes cluster is created and ready for a workload. The PIT node will join Kubernetes after it is rebooted later in Redeploy PIT Node.\nTiming of Deployments The timing of each set of boots varies based on hardware. Nodes from some manufacturers will POST faster than others or vary based on BIOS setting. After powering on a set of nodes, an administrator can expect a healthy boot session to take about 60 minutes depending on the number of storage and worker nodes.\nTopics:  Prepare for Management Node Deployment  Tokens and IPMI Password Apply NCN Pre-Boot Workarounds Ensure Time Is Accurate Before Deploying NCNs   Update Management Node Firmware Deploy Management Nodes  Deploy Workflow Deploy Check LVM on Masters and Workers Check for Unused Drives on Utility Storage Nodes Apply NCN Post-Boot Workarounds   Configure after Management Node Deployment  LiveCD Cluster Authentication BGP Routing Install Tests and Test Server on NCNs   Validate Management Node Deployment  Validation Optional Validation   Next Topic  1. Prepare for Management Node Deployment Preparation of the environment must be done before attempting to deploy the management nodes.\n1.1 Tokens and IPMI Password   Define shell environment variables that will simplify later commands to deploy management nodes.\nNotice that one of them is the IPMI_PASSWORD. Replace changeme with the real root password for BMCs.\npit# export mtoken=\u0026#39;ncn-m(?!001)\\w+-mgmt\u0026#39; pit# export stoken=\u0026#39;ncn-s\\w+-mgmt\u0026#39; pit# export wtoken=\u0026#39;ncn-w\\w+-mgmt\u0026#39; pit# export USERNAME=root pit# export IPMI_PASSWORD=changeme Throughout the guide, simple one-liners can be used to query status of expected nodes. If the shell or environment is terminated, these environment variables should be re-exported.\nExamples:\nCheck power status of all NCNs.\npit# grep -oP \u0026#34;($mtoken|$stoken|$wtoken)\u0026#34; /etc/dnsmasq.d/statics.conf | sort -u | xargs -t -i ipmitool -I lanplus -U $USERNAME -E -H {} power status Power off all NCNs.\npit# grep -oP \u0026#34;($mtoken|$stoken|$wtoken)\u0026#34; /etc/dnsmasq.d/statics.conf | sort -u | xargs -t -i ipmitool -I lanplus -U $USERNAME -E -H {} power off   1.2 Apply NCN Pre-Boot Workarounds There will be post-boot workarounds as well.\nFollow the workaround instructions for the before-ncn-boot breakpoint.\n1.3 Ensure Time Is Accurate Before Deploying NCNs NOTE: If you wish to use a timezone other than UTC, instead of step 1 below, follow this procedure for setting a local timezone, then proceed to step 2.\n  Ensure that the PIT node has the current and correct time.\nThe time can be inaccurate if the system has been powered off for a long time, or, for example, the CMOS was cleared on a Gigabyte node. See Clear Gigabyte CMOS.\n This step should not be skipped\n Check the time on the PIT node to see whether it matches the current time:\npit# date \u0026quot;+%Y-%m-%d %H:%M:%S.%6N%z\u0026quot; If the time is inaccurate, set the time manually.\npit# timedatectl set-time \u0026quot;2019-11-15 00:00:00\u0026quot; Run the NTP script:\npit# /root/bin/configure-ntp.sh This ensures that the PIT is configured with an accurate date/time, which will be properly propagated to the NCNs during boot.\n  Ensure the current time is set in BIOS for all management NCNs.\n If each NCN is booted to the BIOS menu, you can check and set the current UTC time.\n pit# export USERNAME=root pit# export IPMI_PASSWORD=changeme Repeat the following process for each NCN.\n  Start an IPMI console session to the NCN.\npit# bmc=ncn-w001-mgmt # Change this to be each node in turn. pit# conman -j $bmc   Using another terminal to watch the console, boot the node to BIOS.\npit# bmc=ncn-w001-mgmt # Change this to be each node in turn. pit# ipmitool -I lanplus -U $USERNAME -E -H $bmc chassis bootdev bios pit# ipmitool -I lanplus -U $USERNAME -E -H $bmc chassis power off pit# sleep 10 pit# ipmitool -I lanplus -U $USERNAME -E -H $bmc chassis power on  For HPE NCNs the above process will boot the nodes to their BIOS, but the menu is unavailable through conman as the node is booted into a graphical BIOS menu.\nTo access the serial version of the BIOS setup. Perform the ipmitool steps above to boot the node. Then in conman press ESC+9 key combination when you see the following messages in the console. This will open a menu you use to enter the BIOS via conman.\nFor access via BIOS Serial Console: Press 'ESC+9' for System Utilities Press 'ESC+0' for Intelligent Provisioning Press 'ESC+!' for One-Time Boot Menu Press 'ESC+@' for Network Boot For HPE NCNs the date configuration menu can be found at the following path: System Configuration -\u0026gt; BIOS/Platform Configuration (RBSU) -\u0026gt; Date and Time\nAlternatively for HPE NCNs you can log in to the BMC\u0026rsquo;s web interface and access the HTML5 console for the node to interact with the graphical BIOS. From the administrators own machine create a SSH tunnel (-L creates the tunnel, and -N prevents a shell and stubs the connection):\nlinux# bmc=ncn-w001-mgmt # Change this to be each node in turn. linux# ssh -L 9443:$bmc:443 -N root@eniac-ncn-m001 Opening a web browser to https://localhost:9443 will give access to the BMC\u0026rsquo;s web interface.\n   When the node boots, you will be able to use the conman session to see the BIOS menu to check and set the time to current UTC time. The process varies depending on the vendor of the NCN.\n  After you have verified the correct time, power off the NCN.\n  Repeat the above process for each NCN.\n  2. Update Management Node Firmware The management nodes are expected to have certain minimum firmware installed for BMC, node BIOS, and PCIe card firmware. Where possible, the firmware should be updated prior to install. Some firmware can be updated during or after the installation, but it is better to meet the minimum NCN firmware requirement before starting.\n  (optional) Check these BIOS settings on management nodes NCN BIOS.\n This is optional, the BIOS settings (or lack thereof) do not prevent deployment. The NCN Installation will work with the CMOS' default BIOS. There may be settings that facilitate the speed of deployment, but they may be tuned at a later time.\n  NOTE The BIOS tuning will be automated, further reducing this step.\n   The firmware on the management nodes should be checked for compliance with the minimum required version and updated, if necessary, at this point.\n WARNING: Gigabyte NCNs running BIOS version C20 can become unusable when Shasta 1.5 is installed. This is a result of a bug in the Gigabyte firmware. This bug has not been observed in BIOS version C17.\nA key symptom of this bug is that the NCN will not PXE boot and will instead fall through to the boot menu, despite being configure to PXE boot. This behavior will persist until the failing node\u0026rsquo;s CMOS is cleared.\n  See Clear Gigabyte CMOS.    3. Deploy Management Nodes Deployment of the nodes starts with booting the storage nodes first. Then, the master nodes and worker nodes should be booted together. After the operating system boots on each node, there are some configuration actions which take place. Watching the console or the console log for certain nodes can help to understand what happens and when. When the process is complete for all nodes, the Ceph storage will have been initialized and the Kubernetes cluster will be created ready for a workload.\n3.1 Deploy Workflow The configuration workflow described here is intended to help understand the expected path for booting and configuring. See the actual steps below for the commands to deploy these management NCNs.\n Start watching the consoles for ncn-s001 and at least one other storage node Boot all storage nodes at the same time  The first storage node ncn-s001 will boot and then starts a loop as ceph-ansible configuration waits for all other storage nodes to boot The other storage nodes boot and become passive. They will be fully configured when ceph-ansible runs to completion on ncn-s001   Once ncn-s001 notices that all other storage nodes have booted, ceph-ansible will begin Ceph configuration. This takes several minutes. Once ceph-ansible has finished on ncn-s001, then ncn-s001 waits for ncn-m002 to create /etc/kubernetes/admin.conf. Start watching the consoles for ncn-m002, ncn-m003, and at least one worker node Boot master nodes (ncn-m002 and ncn-m003) and all worker nodes at the same time  The worker nodes will boot and wait for ncn-m002 to create the /etc/cray/kubernetes/join-command-control-plane so they can join Kubernetes The third master node ncn-m003 boots and waits for ncn-m002 to create the /etc/cray/kubernetes/join-command-control-plane so it can join Kubernetes The second master node ncn-m002 boots, runs the kubernetes-cloudinit.sh which will create /etc/kubernetes/admin.conf and /etc/cray/kubernetes/join-command-control-plane, then waits for the storage node to create etcd-backup-s3-credentials   Once ncn-s001 notices that ncn-m002 has created /etc/kubernetes/admin.conf, then ncn-s001 waits for any worker node to become available. Once each worker node notices that ncn-m002 has created /etc/cray/kubernetes/join-command-control-plane, then it will join the Kubernetes cluster.  Now ncn-s001 should notice this from any one of the worker nodes and move forward with creation of ConfigMaps and running the post-Ceph playbooks (s3, OSD pools, quotas, etc.)   Once ncn-s001 creates etcd-backup-s3-credentials during the ceph-rgw-users role which is one of the last roles after Ceph has been set up, then ncn-m001 notices this and moves forward  NOTE: If several hours have elapsed between storage and master nodes booting, or if there were issues PXE booting master nodes, the cloud init script on ncn-s001 may not complete successfully. This can cause the /var/log/cloud-init-output.log on master node(s) to continue to output the following message:\n[ 1328.351558] cloud-init[8472]: Waiting for storage node to create etcd-backup-s3-credentials secret\u0026hellip;\nIn this case, the following script is safe to be executed again on ncn-s001:\nncn-s001# /srv/cray/scripts/common/storage-ceph-cloudinit.sh\nAfter this script finishes, the secrets will be created and the cloud-init script on the master node(s) should complete.\n   3.2 Deploy   Change the default root password and SSH keys\n If you want to avoid using the default install root password and SSH keys for the NCNs, follow the NCN image customization steps in Change NCN Image Root Password and SSH Keys\n This step is strongly encouraged for all systems.\n  Create boot directories for any NCN in DNS:\n This will create folders for each host in /var/www, allowing each host to have their own unique set of artifacts; kernel, initrd, SquashFS, and script.ipxe bootscript.\n pit# /root/bin/set-sqfs-links.sh   Customize boot scripts for any out-of-baseline NCNs\n kubernetes-worker nodes with more than 2 small disks need to make adjustments to prevent bare-metal etcd creation A brief overview of what is expected is here, in disk plan of record / baseline    Run the BIOS Baseline script to apply a configs to BMCs. The script will apply helper configs to facilitate more deterministic network booting on any NCN port. The script depends on\npit# /root/bin/bios-baseline.sh   Set each node to always UEFI Network Boot, and ensure they are powered off\npit# grep -oP \u0026#34;($mtoken|$stoken|$wtoken)\u0026#34; /etc/dnsmasq.d/statics.conf | sort -u | xargs -t -i ipmitool -I lanplus -U $USERNAME -E -H {} chassis bootdev pxe options=efiboot,persistent pit# grep -oP \u0026#34;($mtoken|$stoken|$wtoken)\u0026#34; /etc/dnsmasq.d/statics.conf | sort -u | xargs -t -i ipmitool -I lanplus -U $USERNAME -E -H {} power off  NOTE:The NCN boot order is further explained in NCN Boot Workflow.\n   Validate that the LiveCD is ready for installing NCNs.\n Observe the output of the checks and note any failures, then remediate them.\n pit# csi pit validate --livecd-preflight  Note: This check sometimes leaves the terminal in a state where input is not echoed to the screen. If this happens, running the reset command will correct it. Note: You can ignore any errors about not being able resolve arti.dev.cray.com.\n   Print the consoles available to you:\npit# conman -q Expected output looks similar to the following:\nncn-m001-mgmt ncn-m002-mgmt ncn-m003-mgmt ncn-s001-mgmt ncn-s002-mgmt ncn-s003-mgmt ncn-w001-mgmt ncn-w002-mgmt ncn-w003-mgmt  IMPORTANT This is the administrator\u0026rsquo;s last chance to run NCN pre-boot workarounds (the before-ncn-boot breakpoint).\n  NOTE: All consoles are located at /var/log/conman/console* \n   Boot the Storage Nodes\n  Boot all storage nodes except ncn-s001:\npit# grep -oP $stoken /etc/dnsmasq.d/statics.conf | grep -v \u0026#34;ncn-s001-\u0026#34; | sort -u | xargs -t -i ipmitool -I lanplus -U $USERNAME -E -H {} power on   Wait approximately 1 minute.\n  Boot ncn-s001:\npit# ipmitool -I lanplus -U $USERNAME -E -H ncn-s001-mgmt power on     Wait. Observe the installation through ncn-s001-mgmt\u0026rsquo;s console:\nPrint the console name:\npit# conman -q | grep s001 Expected output looks similar to the following:\nncn-s001-mgmt Then join the console:\npit# conman -j ncn-s001-mgmt From there an administrator can witness console-output for the cloud-init scripts.\nNOTE: Watch the storage node consoles carefully for error messages. If any are seen, consult Ceph-CSI Troubleshooting\nNOTE: If the nodes have PXE boot issues (e.g. getting PXE errors, not pulling the ipxe.efi binary), see PXE boot troubleshooting\nNOTE: If other issues arise, such as cloud-init (e.g. NCNs come up to Linux with no hostname), see the CSM workarounds for fixes around mutual symptoms. If there is a workaround here, the output will look similar to the following.\n pit# ls /opt/cray/csm/workarounds/after-ncn-boot CASMINST-1093\n   Wait for storage nodes before booting Kubernetes master nodes and worker nodes.\nNOTE: Once all storage nodes are up and the message ...sleeping 5 seconds until /etc/kubernetes/admin.conf appears on ncn-s001\u0026rsquo;s console, it is safe to proceed with booting the Kubernetes master nodes and worker nodes\npit# grep -oP \u0026#34;($mtoken|$wtoken)\u0026#34; /etc/dnsmasq.d/statics.conf | sort -u | xargs -t -i ipmitool -I lanplus -U $USERNAME -E -H {} power on   Stop watching the console from ncn-s001.\nType the ampersand character and then the period character to exit from the conman session on ncn-s001.\n\u0026amp;. pit#   Wait. Observe the installation through ncn-m002-mgmt\u0026rsquo;s console:\nPrint the console name:\npit# conman -q | grep m002 Expected output looks similar to the following:\nncn-m002-mgmt Then join the console:\npit# conman -j ncn-m002-mgmt NOTE: If the nodes have PXE boot issues (e.g. getting PXE errors, not pulling the ipxe.efi binary) see PXE boot troubleshooting\nNOTE: If one of the master nodes seems hung waiting for the storage nodes to create a secret, check the storage node consoles for error messages. If any are found, consult CEPH CSI Troubleshooting\nNOTE: If other issues arise, such as cloud-init (e.g. NCNs come up to Linux with no hostname) see the CSM workarounds for fixes around mutual symptoms. If there is a workaround here, the output will look similar to the following.\n pit# ls /opt/cray/csm/workarounds/after-ncn-boot CASMINST-1093    Refer to timing of deployments. It should take no more than 60 minutes for the kubectl get nodes command to return output indicating that all the master nodes and worker nodes aside from the PIT node booted from the LiveCD are Ready:\npit# ssh ncn-m002 ncn-m002# kubectl get nodes -o wide Expected output looks similar to the following:\nNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME ncn-m002 Ready master 14m v1.18.6 10.252.1.5 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP2 5.3.18-24.43-default containerd://1.3.4 ncn-m003 Ready master 13m v1.18.6 10.252.1.6 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP2 5.3.18-24.43-default containerd://1.3.4 ncn-w001 Ready \u0026lt;none\u0026gt; 6m30s v1.18.6 10.252.1.7 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP2 5.3.18-24.43-default containerd://1.3.4 ncn-w002 Ready \u0026lt;none\u0026gt; 6m16s v1.18.6 10.252.1.8 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP2 5.3.18-24.43-default containerd://1.3.4 ncn-w003 Ready \u0026lt;none\u0026gt; 5m58s v1.18.6 10.252.1.12 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP2 5.3.18-24.43-default containerd://1.3.4   Stop watching the console from ncn-m002.\nType the ampersand character and then the period character to exit from the conman session on ncn-m002.\n\u0026amp;. pit#   3.3 Check LVM on Masters and Workers 3.3.1 Run The Check Run the following command on the PIT node to validate that the expected LVM labels are present on disks on the master and worker nodes. When it prompts you for a password, enter the password for ncn-m002.\npit# /usr/share/doc/csm/install/scripts/check_lvm.sh 3.3.2 Expected Check Output Expected output looks something like\nWhen prompted, please enter the NCN password for ncn-m002 Warning: Permanently added 'ncn-m002,10.252.1.11' (ECDSA) to the list of known hosts. Password: Checking ncn-m002... ncn-m002: OK Checking ncn-m003... Warning: Permanently added 'ncn-m003,10.252.1.10' (ECDSA) to the list of known hosts. Warning: Permanently added 'ncn-m003,10.252.1.10' (ECDSA) to the list of known hosts. ncn-m003: OK Checking ncn-w001... Warning: Permanently added 'ncn-w001,10.252.1.9' (ECDSA) to the list of known hosts. Warning: Permanently added 'ncn-w001,10.252.1.9' (ECDSA) to the list of known hosts. ncn-w001: OK Checking ncn-w002... Warning: Permanently added 'ncn-w002,10.252.1.8' (ECDSA) to the list of known hosts. Warning: Permanently added 'ncn-w002,10.252.1.8' (ECDSA) to the list of known hosts. ncn-w002: OK Checking ncn-w003... Warning: Permanently added 'ncn-w003,10.252.1.7' (ECDSA) to the list of known hosts. Warning: Permanently added 'ncn-w003,10.252.1.7' (ECDSA) to the list of known hosts. ncn-w003: OK SUCCESS: LVM checks passed on all master and worker NCNs If the check fails for any nodes, the problem must be resolved before continuing. See LVM Check Failure Recovery.\n3.3.3 Manual LVM Check Procedure If needed, the LVM checks can be performed manually on the master and worker nodes.\n  Manual check on master nodes:\nncn-m# blkid -L ETCDLVM /dev/sdc   Manual check on worker nodes:\nncn-w# blkid -L CONLIB /dev/sdb2 ncn-w# blkid -L CONRUN /dev/sdb1 ncn-w# blkid -L K8SLET /dev/sdb3   The manual checks are considered successful if all of the blkid commands report a disk device (such as /dev/sdc \u0026ndash; the particular device is unimportant). If any of the lsblk commands return no output, then the check is a failure. Any failures must be resolved before continuing. See the following section for details on how to do so.\n3.3.3 LVM Check Failure Recovery If there are LVM check failures, then the problem must be resolved before continuing with the install.\n  If any master node has the problem, then you must wipe and redeploy all of the NCNs before continuing the installation:\n Wipe each worker node using the \u0026lsquo;Basic Wipe\u0026rsquo; section of Wipe NCN Disks for Reinstallation. Wipe each master node (except ncn-m001 because it is the PIT node) using the \u0026lsquo;Basic Wipe\u0026rsquo; section of Wipe NCN Disks for Reinstallation. Wipe each storage node using the \u0026lsquo;Full Wipe\u0026rsquo; section of Wipe NCN Disks for Reinstallation. Return to the Set each node to always UEFI Network Boot, and ensure they are powered off step of the Deploy Management Nodes section above.    If only worker nodes have the problem, then you must wipe and redeploy the affected worker nodes before continuing the installation:\n Wipe each affected worker node using the \u0026lsquo;Basic Wipe\u0026rsquo; section of Wipe NCN Disks for Reinstallation. Power off each affected worker node. Return to the Boot the Master and Worker Nodes step of the Deploy Management Nodes section above.  Note: The ipmitool command will give errors trying to power on the unaffected nodes, since they are already powered on \u0026ndash; this is expected and not a problem.      3.4 Check for Unused Drives on Utility Storage Nodes  IMPORTANT: Do the following if NCNs are Gigabyte hardware. IMPORTANT: the cephadm may output this warning \u0026ldquo;WARNING: The same type, major and minor should not be used for multiple devices.\u0026rdquo;. You can ignore this warning.\n Option 1 If you have OSDs on each node (ceph osd tree can show this), then you have all your nodes in Ceph. That means you can utilize the orchestrator to look for the devices.\n  Get the number of OSDs in the cluster.\nncn-s# ceph -f json-pretty osd stat |jq .num_osds 24 IMPORTANT: If the returned number of OSDs is equal to total_osds calculated, then you can skip the following steps. If not, then please proceed with the below additional checks and remediation steps.\n  Compare your number of OSDs to your output which should resemble the example below. The number of drives will depend on the server hardware.\n NOTE: If your Ceph cluster is large and has a lot of nodes, you can specify a node after the below command to limit the results.\n ncn-s# ceph orch device ls Hostname Path Type Serial Size Health Ident Fault Available ncn-s001 /dev/sda ssd PHYF015500M71P9DGN 1920G Unknown N/A N/A No ncn-s001 /dev/sdb ssd PHYF016500TZ1P9DGN 1920G Unknown N/A N/A No ncn-s001 /dev/sdc ssd PHYF016402EB1P9DGN 1920G Unknown N/A N/A No ncn-s001 /dev/sdd ssd PHYF016504831P9DGN 1920G Unknown N/A N/A No ncn-s001 /dev/sde ssd PHYF016500TV1P9DGN 1920G Unknown N/A N/A No ncn-s001 /dev/sdf ssd PHYF016501131P9DGN 1920G Unknown N/A N/A No ncn-s001 /dev/sdi ssd PHYF016500YB1P9DGN 1920G Unknown N/A N/A No ncn-s001 /dev/sdj ssd PHYF016500WN1P9DGN 1920G Unknown N/A N/A No ncn-s002 /dev/sda ssd PHYF0155006W1P9DGN 1920G Unknown N/A N/A No ncn-s002 /dev/sdb ssd PHYF0155006Z1P9DGN 1920G Unknown N/A N/A No ncn-s002 /dev/sdc ssd PHYF015500L61P9DGN 1920G Unknown N/A N/A No ncn-s002 /dev/sdd ssd PHYF015502631P9DGN 1920G Unknown N/A N/A No ncn-s002 /dev/sde ssd PHYF0153000G1P9DGN 1920G Unknown N/A N/A No ncn-s002 /dev/sdf ssd PHYF016401T41P9DGN 1920G Unknown N/A N/A No ncn-s002 /dev/sdi ssd PHYF016504C21P9DGN 1920G Unknown N/A N/A No ncn-s002 /dev/sdj ssd PHYF015500GQ1P9DGN 1920G Unknown N/A N/A No ncn-s003 /dev/sda ssd PHYF016402FP1P9DGN 1920G Unknown N/A N/A No ncn-s003 /dev/sdb ssd PHYF016401TE1P9DGN 1920G Unknown N/A N/A No ncn-s003 /dev/sdc ssd PHYF015500N51P9DGN 1920G Unknown N/A N/A No ncn-s003 /dev/sdd ssd PHYF0165010Z1P9DGN 1920G Unknown N/A N/A No ncn-s003 /dev/sde ssd PHYF016500YR1P9DGN 1920G Unknown N/A N/A No ncn-s003 /dev/sdf ssd PHYF016500X01P9DGN 1920G Unknown N/A N/A No ncn-s003 /dev/sdi ssd PHYF0165011H1P9DGN 1920G Unknown N/A N/A No ncn-s003 /dev/sdj ssd PHYF016500TQ1P9DGN 1920G Unknown N/A N/A No If you have devices that are \u0026ldquo;Available = Yes\u0026rdquo; and they are not being automatically added, you may have to zap that device.\nIMPORTANT: Prior to zapping any device please ensure it is not being used.\n  Check to see if the number of devices is less than the number of listed drives or your output from step 1.\nncn-s# ceph orch device ls|grep dev|wc -l 24 If the numbers are equal, then you may need to fail your ceph-mgr daemon to get a fresh inventory.\nncn-s# ceph mgr fail $(ceph mgr dump | jq -r .active_name) Give it 5 minutes then re-check ceph orch device ls to see if the drives are still showing as available. If so, then proceed to the next step.\n  ssh to the host and look at lsblk output and check against the device from the above ceph orch device ls\nncn-s# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT loop0 7:0 0 4.2G 1 loop / run/ rootfsbase loop1 7:1 0 30G 0 loop └─live-overlay-pool 254:8 0 300G 0 dm loop2 7:2 0 300G 0 loop └─live-overlay-pool 254:8 0 300G 0 dm sda 8:0 0 1.8T 0 disk └─ceph--0a476f53--8b38--450d--8779--4e587402f8a8-osd--data--b620b7ef--184a--46d7--9a99--771239e7a323 254:7 0 1.8T 0 lvm  If it has an LVM volume like above, then it may be in use and you should do the option 2 check below to make sure we can wipe the drive.    Option 2   Log into each ncn-s node and check for unused drives.\nncn-s# cephadm shell -- ceph-volume inventory IMPORTANT: The cephadm command may output this warning WARNING: The same type, major and minor should not be used for multiple devices.. You can ignore this warning.\nThe field available would be True if Ceph sees the drive as empty and can be used, e.g.:\nDevice Path Size rotates available Model name /dev/sda 447.13 GB False False SAMSUNG MZ7LH480 /dev/sdb 447.13 GB False False SAMSUNG MZ7LH480 /dev/sdc 3.49 TB False False SAMSUNG MZ7LH3T8 /dev/sdd 3.49 TB False False SAMSUNG MZ7LH3T8 /dev/sde 3.49 TB False False SAMSUNG MZ7LH3T8 /dev/sdf 3.49 TB False False SAMSUNG MZ7LH3T8 /dev/sdg 3.49 TB False False SAMSUNG MZ7LH3T8 /dev/sdh 3.49 TB False False SAMSUNG MZ7LH3T8 Alternatively, just dump the paths of available drives:\nncn-s# cephadm shell -- ceph-volume inventory --format json-pretty | jq -r \u0026#39;.[]|select(.available==true)|.path\u0026#39;   Wipe the drive ONLY after you have confirmed the drive is not being used by the current Ceph cluster via options 1, 2, or both.\n The following example wipes drive /dev/sdc on ncn-s002. You should replace these values with the appropriate ones for your situation.\n ncn-s# ceph orch device zap ncn-s002 /dev/sdc --force   Add unused drives.\nncn-s# cephadm shell -- ceph-volume lvm create --data /dev/sd\u0026lt;drive to add\u0026gt; --bluestore   More information can be found at the cephadm reference page.\n3.5 Apply NCN Post-Boot Workarounds Follow the workaround instructions for the after-ncn-boot breakpoint.\n4. Configure after Management Node Deployment After the management nodes have been deployed, configuration can be applied to the booted nodes.\n4.1 LiveCD Cluster Authentication The LiveCD needs to authenticate with the cluster to facilitate the rest of the CSM installation.\n  Copy the Kubernetes config to the LiveCD to be able to use kubectl as cluster administrator.\n This will always be whatever node is the first-master-hostname in your /var/www/ephemeral/configs/data.json | jq file. If you are provisioning your HPE Cray EX system from ncn-m001, then you can expect to fetch these from ncn-m002.\n pit# mkdir -v ~/.kube pit# scp ncn-m002.nmn:/etc/kubernetes/admin.conf ~/.kube/config   4.2 BGP Routing After the NCNs are booted, the BGP peers will need to be checked and updated if the neighbor IP addresses are incorrect on the switches. Follow the steps below and see Check and Update BGP Neighbors for more details on the BGP configuration.\n  Make sure the SYSTEM_NAME variable is set to name of your system.\npit# export SYSTEM_NAME=eniac   Determine the IP address of the worker NCNs.\npit# grep -B1 \u0026#34;name: ncn-w\u0026#34; /var/www/ephemeral/prep/${SYSTEM_NAME}/networks/NMN.yaml   Determine the IP addresses for the switches that are peering.\npit# grep peer-address /var/www/ephemeral/prep/${SYSTEM_NAME}/metallb.yaml   Run the script appropriate for your switch hardware vendor:\n  If you have Mellanox switches, run the BGP helper script.\nThe BGP helper script requires three parameters: the IP address of switch 1, the IP address of switch 2, and the path to the to CSI generated network files.\n The IP addresses used should be Node Management Network IP addresses (NMN). These IP addresses will be used for the BGP Router-ID. The path to the CSI generated network files must include CAN.yaml, HMN.yaml, HMNLB.yaml, NMNLB.yaml, and NMN.yaml. The path must include the SYSTEM_NAME.  The IP addresses in this example should be replaced by the IP addresses of the switches.\npit# /usr/local/bin/mellanox_set_bgp_peers.py 10.252.0.2 10.252.0.3 /var/www/ephemeral/prep/${SYSTEM_NAME}/networks/   If you have Aruba switches, run CANU.\nCANU requires three parameters: the IP address of switch 1, the IP address of switch 2, and the path to the to directory containing the file sls_input_file.json\nThe IP addresses in this example should be replaced by the IP addresses of the switches.\npit# canu -s 1.5 config bgp --ips 10.252.0.2,10.252.0.3 --csi-folder /var/www/ephemeral/prep/${SYSTEM_NAME}/     Do the following steps for each of the switch IP addresses that you found previously:\n  Log in to the switch as the admin user:\npit# ssh admin@\u0026lt;switch_ip_address\u0026gt;   Clear the BGP peering sessions by running the following commands. You should see either \u0026ldquo;arubanetworks\u0026rdquo; or \u0026ldquo;Mellanox\u0026rdquo; in the first output you see when you log in to the switch.\n Aruba: clear bgp * Mellanox: First run enable, then run clear ip bgp all    Wait about 10 seconds, then check the status of the BGP peering sessions.\n Aruba: show bgp ipv4 unicast summary Mellanox: show ip bgp summary  You should see a neighbor for each of the workers NCN IP addresses found above. If it is an Aruba switch, you will also see a neighbor for the other switch of the pair that are peering.\nAt this point the peering sessions with the worker IP addresses should be in IDLE, CONNECT, or ACTIVE state (not ESTABLISHED). This is due to the MetalLB speaker pods not being deployed yet.\nYou should see that the MsgRcvd and MsgSent columns for the worker IP addresses are 0.\n  Check the BGP config to verify that the NCN neighbors are configured as passive.\n  Aruba:\n# show run bgp The passive neighbor configuration is required, which looks similar to neighbor 10.252.1.7 passive\nEXAMPLE OUTPUT\nsw-spine-001# show run bgp router bgp 65533 bgp router-id 10.252.0.2 maximum-paths 8 distance bgp 20 70 neighbor 10.252.0.3 remote-as 65533 neighbor 10.252.1.7 remote-as 65533 neighbor 10.252.1.7 passive neighbor 10.252.1.8 remote-as 65533 neighbor 10.252.1.8 passive neighbor 10.252.1.9 remote-as 65533 neighbor 10.252.1.9 passive   Mellanox:\n# show run protocol bgp The passive neighbor configuration is required, which looks similar to router bgp 65533 vrf default neighbor 10.252.1.7 transport connection-mode passive\nEXAMPLE OUTPUT\nprotocol bgp router bgp 65533 vrf default router bgp 65533 vrf default router-id 10.252.0.2 force router bgp 65533 vrf default maximum-paths ibgp 32 router bgp 65533 vrf default neighbor 10.252.1.7 remote-as 65533 router bgp 65533 vrf default neighbor 10.252.1.7 route-map ncn-w003 router bgp 65533 vrf default neighbor 10.252.1.8 remote-as 65533 router bgp 65533 vrf default neighbor 10.252.1.8 route-map ncn-w002 router bgp 65533 vrf default neighbor 10.252.1.9 remote-as 65533 router bgp 65533 vrf default neighbor 10.252.1.9 route-map ncn-w001 router bgp 65533 vrf default neighbor 10.252.1.7 transport connection-mode passive router bgp 65533 vrf default neighbor 10.252.1.8 transport connection-mode passive router bgp 65533 vrf default neighbor 10.252.1.9 transport connection-mode passive     Repeat the previous steps for the remaining switch IP addresses.\n    If the neighbor IP addresses do not match the worker NCN IP addresses:\n  Run the script appropriate for your switch hardware vendor:\n  If you have Mellanox switches, run the BGP helper script.\nThe BGP helper script requires three parameters: the IP address of switch 1, the IP addresss of switch 2, and the path to the to CSI generated network files.\n The IP addresses used should be Node Management Network IP addresses (NMN). These IP addresses will be used for the BGP Router-ID. The path to the CSI generated network files must include CAN.yaml, HMN.yaml, HMNLB.yaml, NMNLB.yaml, and NMN.yaml. The path must include the SYSTEM_NAME.  The IP addresses in this example should be replaced by the IP addresses of the switches.\npit# /usr/local/bin/mellanox_set_bgp_peers.py 10.252.0.2 10.252.0.3 /var/www/ephemeral/prep/${SYSTEM_NAME}/networks/   If you have Aruba switches, run CANU.\nCANU requires three parameters: the IP address of switch 1, the IP addresss of switch 2, and the path to the to directory containing the file sls_input_file.json\nThe IP addresses in this example should be replaced by the IP addresses of the switches.\npit# canu -s 1.5 config bgp --ips 10.252.0.2,10.252.0.3 --csi-folder /var/www/ephemeral/prep/${SYSTEM_NAME}/     Repeat the previous BGP check procedure on each switch.\n    4.4 Install Tests and Test Server on NCNs pit# export CSM_RELEASE=csm-x.y.z pit# pushd /var/www/ephemeral pit# ${CSM_RELEASE}/lib/install-goss-tests.sh pit# popd 5. Validate Management Node Deployment Do all of the validation steps. The optional validation steps are manual steps which could be skipped.\n5.1 Validation The following csi pit validate commands will run a series of remote tests on the other nodes to validate they are healthy and configured correctly.\nObserve the output of the checks and note any failures, then remediate them.\n  Check the storage nodes.\npit# csi pit validate --ceph | tee csi-pit-validate-ceph.log Once that command has finished, check the last line of output to see the results of the tests.\nExample last line of output:\nTotal Tests: 7, Total Passed: 7, Total Failed: 0, Total Execution Time: 1.4226 seconds If the test total line reports any failed tests, look through the full output of the test in csi-pit-validate-ceph.log to see which node had the failed test and what the details are for that test.\nNote: Please see Utility Storage to help resolve any failed tests.\n  Check the master and worker nodes.\nNote: Throughout the output of the csi pit validate command there will be a test total for each node where the tests run. Be sure to check all of them and not just the final one. (a grep command is provided to help with this)\npit# csi pit validate --k8s | tee csi-pit-validate-k8s.log Once that command has finished, the following will extract the test totals reported for each node:\npit# grep \u0026#34;Total\u0026#34; csi-pit-validate-k8s.log Example output for a system with 5 master and worker nodes (other than the PIT node):\nTotal Tests: 16, Total Passed: 16, Total Failed: 0, Total Execution Time: 0.3072 seconds Total Tests: 16, Total Passed: 16, Total Failed: 0, Total Execution Time: 0.2727 seconds Total Tests: 12, Total Passed: 12, Total Failed: 0, Total Execution Time: 0.2841 seconds Total Tests: 12, Total Passed: 12, Total Failed: 0, Total Execution Time: 0.3622 seconds Total Tests: 12, Total Passed: 12, Total Failed: 0, Total Execution Time: 0.2353 seconds If these total lines report any failed tests, look through the full output of the test to see which node had the failed test and what the details are for that test.\n WARNING If there are failures for tests with names like \u0026ldquo;Worker Node CONLIB FS Label\u0026rdquo;, then manual tests should be run on the node which reported the failure. See Manual LVM Check Procedure. If the manul tests fail, then the problem must be resolved before continuing to the next step. See LVM Check Failure Recovery.\n   Ensure that weave has not become split-brained.\nRun the following command on each member of the Kubernetes cluster (master nodes and worker nodes) to ensure that weave is operating as a single cluster:\nncn# weave --local status connections | grep failed If you see messages like IP allocation was seeded by different peers, then weave looks to have become split-brained. At this point, it is necessary to wipe the NCNs and start the PXE boot again:\n Wipe the NCNs using the \u0026lsquo;Basic Wipe\u0026rsquo; section of Wipe NCN Disks for Reinstallation. Return to the \u0026lsquo;Boot the Storage Nodes\u0026rsquo; step of Deploy Management Nodes section above.    5.2 Optional Validation  Verify all nodes have joined the cluster  Check that the status of kubernetes nodes is Ready.\nncn# kubectl get nodes If one or more nodes are not in the Ready state, the following command can be run to get additional information:\nncn# kubectl describe node \u0026lt;node-name\u0026gt; #for example, ncn-m001 Verify etcd is running outside Kubernetes on master nodes  On each kubernetes master node, check the status of the etcd service and ensure it is Active/Running:\nncn-m# systemctl status etcd.service Verify that all the pods in the kube-system namespace are running  Check that pods listed are in the Running or Completed state.\nncn# kubectl get pods -o wide -n kube-system Verify that the ceph-csi requirements are in place Ceph CSI Troubleshooting  Important Checkpoint  Before you move on, this is the last point where you will be able to rebuild nodes without having to rebuild the PIT node. So take time to double check both the cluster and the validation test results\n Next Topic After completing the deployment of the management nodes, the next step is to install the CSM services.\nSee Install CSM Services\n"
},
{
	"uri": "/docs-csm/en-11/install/configure_mellanox_spine_switch/",
	"title": "Configure Mellanox Spine Switch",
	"tags": [],
	"description": "",
	"content": "Configure Mellanox Spine Switch This page describes how Mellanox spine switches are configured.\nDepending on the size of the HPE Cray EX system, the spine switches will serve different purposes. On TDS systems, the NCNs will plug directly into the spine switches. On larger systems with aggregation switches, the spine switches will provide connection between the aggregation switches.\nPrerequisites  One connection between the switches is used for the Inter switch link (ISL). Connectivity to the switch is established.  Here is an example snippet about a spine switch from the SHCD.\nThe ISL ports are port 32 on both spine switches.\n   Source Source Label Info Destination Label Info Destination Description     sw-100g01 x3105u40-j32 x3105u41-j32 sw-100g02 100g-1m-DAC    Configure VLAN Cray Site Init (CSI) generates the IP addresses used by the system, below are samples only. The VLAN information is located in the network YAML files. The following are examples.\n  Verify the spine switches have VLAN interfaces in NMN, HMN, and CAN networks.\nExample NMN.yaml:\npit# cat /var/www/ephemeral/prep/${SYSTEM_NAME}/networks/NMN.yaml SNIPPET - ip_address: 10.252.0.2 name: sw-spine-001 comment: x3000c0h12s1 aliases: [] - ip_address: 10.252.0.3 name: sw-spine-002 comment: x3000c0h13s1 aliases: [] name: network_hardware net-name: NMN vlan_id: 2 comment: \u0026#34; gateway: 10.252.0.1 Example HMN.yaml:\npit# cat /var/www/ephemeral/prep/${SYSTEM_NAME}/networks/HMN.yaml SNIPPET - ip_address: 10.254.0.2 name: sw-spine-001 comment: x3000c0h12s1 aliases: [] - ip_address: 10.254.0.3 name: sw-spine-002 comment: x3000c0h13s1 aliases: [] name: network_hardware net-name: HMN vlan_id: 4 comment: \u0026#34; gateway: 10.254.0.1 Example CAN.yaml:\npit# cat /var/www/ephemeral/prep/${SYSTEM_NAME}/networks/CAN.yaml SNIPPET - ip_address: 10.102.11.2 name: can-switch-1 comment: \u0026#34; aliases: [] - ip_address: 10.102.11.3 name: can-switch-2 comment: \u0026#34; aliases: [] net-name: CAN vlan_id: 7 comment: \u0026#34; gateway: 10.102.11.1 The following is an example of spine switch IP addressing based on the network .yaml files from above.\n   VLAN Spine01 Spine02 Purpose     2 10.252.0.2/17 10.252.0.3/17 River Node Management   4 10.254.0.2/17 10.254.0.3/17 River Hardware Management   7 10.102.11.2/24 10.102.11.3/24 Customer Access      Configure MAGP MAGP setup for Mellanox spine switches. This should be set for every VLAN interface (1,2,4,7,10).\nSee https://community.mellanox.com/s/article/howto-configure-magp-on-mellanox-switches for more information.\n  Enable MAGP protocol.\nsw-spine-001 \u0026amp; sw-spine-002 (config)# protocol magp   Configure DHCP IP-Helpers will reside on VLANs 1,2,4, and 7.\n  Add DHCP configuration.\n## DHCP relay configuration ## ip dhcp relay instance 2 vrf default ip dhcp relay instance 4 vrf default ip dhcp relay instance 2 address 10.92.100.222 ip dhcp relay instance 4 address 10.94.100.222 interface vlan 1 ip dhcp relay instance 2 downstream interface vlan 2 ip dhcp relay instance 2 downstream interface vlan 4 ip dhcp relay instance 4 downstream interface vlan 7 ip dhcp relay instance 2 downstream   Configure OSPF OSPF is a dynamic routing protocol used to exchange routes. It provides reachability from the MTN networks to NMN/Kubernetes networks. The router-id used here is the NMN IP address (VLAN 2 IP).\n  Configure OSPF.\nsw-spine-001 \u0026amp; sw-spine-002 (config)# protocol ospf router ospf 1 vrf default interface vlan 2 ip ospf area 0.0.0.2 interface vlan 4 ip ospf area 0.0.0.4 interface vlan 2 ip ospf priority 254 interface vlan 4 ip ospf priority 254   Set the NMN VLAN configuration.\nsw-spine-001(config)# vlan 2 interface vlan 2 interface vlan 2 ip address 10.252.0.2/17 primary interface vlan 2 ipv4 port access-group nmn-hmn interface vlan 2 ip ospf area 0.0.0.2 interface vlan 2 ip ospf priority 254 interface vlan 2 ip dhcp relay instance 2 downstream interface vlan 2 magp 2 interface vlan 2 magp 2 ip virtual-router address 10.252.0.1 interface vlan 2 magp 2 ip virtual-router mac-address 00:00:5E:00:01:02 sw-spine-002(config)# vlan 2 interface vlan 2 interface vlan 2 ip address 10.252.0.3/17 primary interface vlan 2 ipv4 port access-group nmn-hmn interface vlan 2 ip ospf area 0.0.0.2 interface vlan 2 ip ospf priority 254 interface vlan 2 ip dhcp relay instance 2 downstream interface vlan 2 magp 2 interface vlan 2 magp 2 ip virtual-router address 10.252.0.1 interface vlan 2 magp 2 ip virtual-router mac-address 00:00:5E:00:01:02   Set the HMN VLAN configuration.\nsw-spine-001(config)# vlan 4 interface vlan 4 interface vlan 4 ip address 10.254.0.2/17 primary interface vlan 4 ipv4 port access-group nmn-hmn interface vlan 4 ip ospf area 0.0.0.4 interface vlan 4 ip ospf priority 254 interface vlan 4 ip dhcp relay instance 4 downstream interface vlan 4 magp 4 interface vlan 4 magp 4 ip virtual-router address 10.254.0.1 interface vlan 4 magp 4 ip virtual-router mac-address 00:00:5E:00:01:04 sw-spine-002(config)# vlan 4 interface vlan 4 interface vlan 4 ip address 10.254.0.3/17 primary interface vlan 4 ipv4 port access-group nmn-hmn interface vlan 4 ip ospf area 0.0.0.4 interface vlan 4 ip ospf priority 254 interface vlan 4 ip dhcp relay instance 4 downstream interface vlan 4 magp 4 interface vlan 4 magp 4 ip virtual-router address 10.254.0.1 interface vlan 4 magp 4 ip virtual-router mac-address 00:00:5E:00:01:04 exit   Set the CAN VLAN configuration.\nsw-spine-001(config)# vlan 7 interface vlan 7 ip address 10.101.8.2/24 primary interface vlan 7 ip dhcp relay instance 2 downstream interface vlan 7 magp 7 interface vlan 7 magp 7 ip virtual-router address 10.101.8.1 interface vlan 7 magp 7 ip virtual-router mac-address 00:00:5E:00:01:07 sw-spine-002(config)# vlan 7 interface vlan 7 ip address 10.101.8.3/24 primary interface vlan 7 ip dhcp relay instance 2 downstream interface vlan 7 magp 7 interface vlan 7 magp 7 ip virtual-router address 10.101.8.1 interface vlan 7 magp 7 ip virtual-router mac-address 00:00:5E:00:01:07   Configure MLAG These two ports are cabled between the Mellanox switches.\nSpine01 (config) # protocol mlag (config) # interface port-channel 100 (config) # interface ethernet 1/14 channel-group 100 mode active (config) # interface ethernet 1/13 channel-group 100 mode active (config) # interface ethernet 1/13 dcb priority-flow-control mode on force (config) # interface ethernet 1/14 dcb priority-flow-control mode on force (config) # vlan 4000 (config) # interface vlan 4000 (config) # interface port-channel 100 ipl 1 (config) # interface port-channel 100 dcb priority-flow-control mode on force (config interface vlan 4000) # ip address 192.168.255.254 255.255.255.252 (config interface vlan 4000) # ipl 1 peer-address 192.168.255.253 (config) # mlag system-mac 00:00:5E:00:01:5D (config) # no mlag shutdown Spine02 (config) # protocol mlag (config) # interface port-channel 100 (config) # interface ethernet 1/14 channel-group 100 mode active (config) # interface ethernet 1/13 channel-group 100 mode active (config) # interface ethernet 1/13 dcb priority-flow-control mode on force (config) # interface ethernet 1/14 dcb priority-flow-control mode on force (config) # vlan 4000 (config) # interface vlan 4000 (config) # interface port-channel 100 ipl 1 (config) # interface port-channel 100 dcb priority-flow-control mode on force (config interface vlan 4000) # ip address 192.168.255.253 255.255.255.252 (config interface vlan 4000) # ipl 1 peer-address 192.168.255.254 (config) # mlag system-mac 00:00:5E:00:01:5D (config) # no mlag shutdown Adding MLAG ports (these ports go to NCNs/UANs/switch downlinks).\nSpine01 (config) # int mlag-port-channel 1 (config interface mlag-port-channel 1) # mtu 9216 force (config interface mlag-port-channel 1) # switchport mode hybrid (config interface mlag-port-channel 1) # no shutdown (config interface mlag-port-channel 1) # lacp-individual enable force (config interface mlag-port-channel 1) # switchport hybrid allowed-vlan add 2 (config interface mlag-port-channel 1) # switchport hybrid allowed-vlan add 4 (config interface mlag-port-channel 1) # switchport hybrid allowed-vlan add 7 (config interface mlag-port-channel 1) # switchport hybrid allowed-vlan add 10 Spine02 NOTE: \u0026lsquo;lacp fallback\u0026rsquo; is only on one of the Spines. Disable \u0026ldquo;lacp-individual enable force\u0026rdquo; on Spine02 if it was set previously.\n(config) # int mlag-port-channel 1 (config interface mlag-port-channel 1) # mtu 9216 force (config interface mlag-port-channel 1) # switchport mode hybrid (config interface mlag-port-channel 1) # no shutdown (config interface mlag-port-channel 1) # no lacp-individual enable force (config interface mlag-port-channel 1) # switchport hybrid allowed-vlan add 2 (config interface mlag-port-channel 1) # switchport hybrid allowed-vlan add 4 (config interface mlag-port-channel 1) # switchport hybrid allowed-vlan add 7 (config interface mlag-port-channel 1) # switchport hybrid allowed-vlan add 10 Add ports to the MLAG after it is created:\n(config) # interface ethernet 1/1 (config interface ethernet 1/1) # mlag-channel-group 1 mode active (config interface ethernet 1/1) # interface ethernet 1/1 speed 40G force (config interface ethernet 1/1) # interface ethernet 1/1 mtu 9216 force Configuration with Recommended MLAG-VIP cable.\n This is recommended by Mellanox but not required. Its purpose is to prevent \u0026ldquo;split brain\u0026rdquo; which is where both spines think they are the active gateway. It requires an RJ45 cable between the mgmt0 ports on both switches. https://community.mellanox.com/s/article/how-to-configure-mlag-on-mellanox-switches#jive_content_id_MLAG_VIP  NOTE: Replace the SYSTEM_NAME value that is part of the mlag-vip name in the following examples with the actual name of the system.\nSpine01 no interface mgmt0 dhcp interface mgmt0 ip address 192.168.255.241 /29 no mlag shutdown mlag system-mac 00:00:5E:00:01:5D mlag-vip SYSTEM_NAME-mlag-domain ip 192.168.255.242 /29 force Spine02 no interface mgmt0 dhcp interface mgmt0 ip address 192.168.255.243 /29 no mlag shutdown mlag system-mac 00:00:5E:00:01:5D mlag-vip SYSTEM_NAME-mlag-domain ip 192.168.255.242 /29 force Verify the mlag-vip:\nsw-spine-001 [SYSTEM_NAME-mlag-domain: master] # show mlag-vip MLAG-VIP: MLAG group name: SYSTEM_NAME-mlag-domain MLAG VIP address: 192.168.255.242/29 Active nodes: 2 ---------------------------------------------------------------------------------- Hostname VIP-State IP Address ---------------------------------------------------------------------------------- sw-spine-001 master 192.168.255.241 sw-spine-002 standby 192.168.255.243 Configure ACL These ACLs are designed to block traffic from the Node Management Network (NMN) to and from the Hardware Management Network (HMN).\n  Create the access list.\nNOTE: The following are examples only. The IP addresses below need to match what was generated by CSI.\nsw-spine-001 \u0026amp; sw-spine-002 (config)# sw-spine-001(config) # ipv4 access-list nmn-hmn sw-spine-001(config ipv4 access-list nmn-hmn) # bind-point rif sw-spine-001(config ipv4 access-list nmn-hmn) # seq-number 10 deny ip 10.252.0.0 mask 255.255.128.0 10.254.0.0 mask 255.255.128.0 sw-spine-001(config ipv4 access-list nmn-hmn) # seq-number 20 deny ip 10.252.0.0 mask 255.255.128.0 10.104.0.0 mask 255.252.0.0 sw-spine-001(config ipv4 access-list nmn-hmn) # seq-number 30 deny ip 10.254.0.0 mask 255.255.128.0 10.252.0.0 mask 255.255.128.0 sw-spine-001(config ipv4 access-list nmn-hmn) # seq-number 40 deny ip 10.254.0.0 mask 255.255.128.0 10.100.0.0 mask 255.252.0.0 sw-spine-001(config ipv4 access-list nmn-hmn) # seq-number 50 deny ip 10.100.0.0 mask 255.252.0.0 10.254.0.0 mask 255.255.128.0 sw-spine-001(config ipv4 access-list nmn-hmn) # seq-number 60 deny ip 10.100.0.0 mask 255.252.0.0 10.104.0.0 mask 255.252.0.0 sw-spine-001(config ipv4 access-list nmn-hmn) # seq-number 70 deny ip 10.104.0.0 mask 255.252.0.0 10.252.0.0 mask 255.255.128.0 sw-spine-001(config ipv4 access-list nmn-hmn) # seq-number 80 deny ip 10.104.0.0 mask 255.252.0.0 10.100.0.0 mask 255.252.0.0 sw-spine-001(config ipv4 access-list nmn-hmn) # seq-number 90 permit ip any any sw-spine-001(config ipv4 access-list nmn-hmn) # exit   Apply ACL to VLANs.\nsw-spine-001(config) # interface vlan 2 ipv4 port access-group nmn-hmn sw-spine-001(config) # interface vlan 4 ipv4 port access-group nmn-hmn   Configure Spanning-Tree Spanning-tree will need to be applied to each MAGP pair. Spine01 will have a lower priority making it the root bridge. Spanning-tree configuration has not changed from 1.3 to 1.5.\n  Apply the following configuration to the Mellanox spine switches.\nThis is an example of a switch-to-switch connection.\nsw-spine-001 \u0026amp; sw-spine-002 (config)# spanning-tree mode rpvst spanning-tree port type edge default interface ethernet 1/13-1/14 spanning-tree port type network interface ethernet 1/15/1-1/15/2 spanning-tree port type network interface mlag-port-channel 113 spanning-tree port type network interface mlag-port-channel 151-152 spanning-tree port type network interface ethernet 1/13-1/14 spanning-tree guard root interface ethernet 1/15/1-1/15/2 spanning-tree guard root interface mlag-port-channel 113 spanning-tree guard root interface mlag-port-channel 151-152 spanning-tree guard root spanning-tree port type edge bpdufilter default spanning-tree port type edge bpduguard default spanning-tree vlan 1-2 priority 0 spanning-tree vlan 4 priority 0 spanning-tree vlan 7 priority 0 spanning-tree vlan 10 priority 0   Configure NTP The IP addresses used here will be the first three worker nodes on the NMN network. These can be found in NMN.yaml.\n  Get current NTP configuration.\nsw-spine-001 [standalone: master] (config) # show running-config | include ntp no ntp server 10.252.1.9 disable ntp server 10.252.1.9 keyID 0 no ntp server 10.252.1.9 trusted-enable ntp server 10.252.1.9 version 4 no ntp server 10.252.1.10 disable ntp server 10.252.1.10 keyID 0 no ntp server 10.252.1.10 trusted-enable ntp server 10.252.1.10 version 4 no ntp server 10.252.1.11 disable ntp server 10.252.1.11 keyID 0 no ntp server 10.252.1.11 trusted-enable ntp server 10.252.1.11 version 4   Delete any current NTP configuration.\nsw-spine-001# configure terminal sw-spine-001 [standalone: master] (config) # no ntp server 10.252.1.9 sw-spine-001 [standalone: master] (config) # no ntp server 10.252.1.10 sw-spine-001 [standalone: master] (config) # no ntp server 10.252.1.11   Add new NTP server configuration.\nsw-spine-001 [standalone: master] (config) # ntp server 10.252.1.12 sw-spine-001 [standalone: master] (config) # ntp server 10.252.1.13 sw-spine-001 [standalone: master] (config) # ntp server 10.252.1.14   Verify NTP status.\nsw-spine-001 [standalone: master] # show ntp NTP is administratively : enabled NTP Authentication administratively: disabled NTP server role : enabled Clock is synchronized: Reference: 10.252.1.14 Offset : -0.056 ms Active servers and peers: 10.252.1.12: Conf Type : serv Status : candidat(+) Stratum : 4 Offset(msec) : -0.119 Ref clock : 10.252.1.4 Poll Interval (sec): 128 Last Response (sec): 107 Auth state : none 10.252.1.13: Conf Type : serv Status : candidat(+) Stratum : 4 Offset(msec) : -0.059 Ref clock : 10.252.1.4 Poll Interval (sec): 128 Last Response (sec): 96 Auth state : none 10.252.1.14: Conf Type : serv Status : sys.peer(*) Stratum : 4 Offset(msec) : -0.056 Ref clock : 10.252.1.4 Poll Interval (sec): 128 Last Response (sec): 118 Auth state : none   Configure DNS   Configure DNS.\nThis will point to the unbound DNS server.\nsw-spine-001 \u0026amp; sw-spine-002 (config)# ip name-server 10.92.100.225   Verify the configuration.\nsw-spine-002 [standalone: master] # show ip dhcp relay Instance ID 2: VRF Name: default DHCP Servers: 10.92.100.222 DHCP relay agent options: always-on : Disabled Information Option: Disabled UDP port : 67 Auto-helper : Disabled ------------------------------------------- Interface Label Mode ------------------------------------------- vlan1 N/A downstream vlan2 N/A downstream vlan7 N/A downstream Instance ID 4: VRF Name: default DHCP Servers: 10.94.100.222 DHCP relay agent options: always-on : Disabled Information Option: Disabled UDP port : 67 Auto-helper : Disabled ------------------------------------------- Interface Label Mode ------------------------------------------- vlan4 N/A downstream   Save Configuration   Save the changes made during this configuration session.\nsw-spine-001(config)# exit sw-spine-001# write memory   Show Running Configuration   Show the current configuration\nsw-spine-001# show running-config   "
},
{
	"uri": "/docs-csm/en-11/install/connect_to_switch_over_usb_serial_cable/",
	"title": "Connect To Switch Over Usb-serial Cable",
	"tags": [],
	"description": "",
	"content": "Connect to Switch over USB-Serial Cable In the event that network plumbing is lacking, down, or unconfigured for procuring devices then it is recommended to use the Serial/COM ports on the spine and leaf switches.\nThis guide will instruct the user on procuring MAC addresses for the NCNs metadata files with the serial console.\nMileage may vary, as some obstacles such as BAUDRATE and terminal usage vary per manufacturer.\nCommon Manufacturers (click any for links to support/document portals)\n Aruba Dell Mellanox  Setup / Connection This will involve using a preferred utility (minicom, screen, or cu) for connecting to your switches console.\n At this time it is assumed you have connected your USB-DB-9 or USB-RJ-45 cable between your switch and your NCN.\n screen screen /dev/ttyUSB1 screen /dev/ttyUSB1 115200 minicom minicom -b 9600 -D /dev/ttyUSB1 minicom -b 115200 -D /dev/ttyUSB1 cu cu -l /dev/ttyUSB1 -s 115200 Debugging Connections Tip : Mellanox On Mellanox switches if the console is not responding when opened try holding CTRL + R (or control + R for macOS) to initiate a screen refresh. This should take 5-10 seconds.\nTip : No USB TTY Device If you cannot see your device in /dev/tty* then follow dmesg -w and try reseating your USB cable (unplug the end in the NCN, and plug it back in). Observe the dmesg -w output, does it show errors pertaining to USB? The cable may be bad, or you may need to reboot.\nAdditional External References  USB-B to RJ-45 rs232 Cable USB-B to USB-C adapter  "
},
{
	"uri": "/docs-csm/en-11/install/create_application_node_config_yaml/",
	"title": "Create Application Node Config Yaml",
	"tags": [],
	"description": "",
	"content": "Create Application Node Config YAML This topic provides directions on constructing the application_node_config.yaml file. This file controls how the csi config init command finds and treats application nodes discovered in the hmn_connections.json file when generating configuration files for the system.\n Requirements Background Directions  Requirements The application_node_config.yaml file can be constructed from information from one of the following sources:\n The SHCD Excel spreadsheet for your system The hmn_connections.json file generated from the system\u0026rsquo;s SHCD.  Background SHCD and hmn_connections.json The HMN tab of the SHCD describes the air-cooled hardware present in the system and how these devices are connected to the Hardware Management Network (HMN). This information is required by CSM to perform hardware discovery and geolocation of air-cooled hardware in the system. The HMN tab may contain other hardware that is not managed by CSM, but is connected to the HMN.\nThe hmn_connections.json file is derived from the HMN tab of a system SHCD, and is one of the seed files required by Cray Site Init (CSI) command to generate configuration files required to install CSM. The hmn_connections.json file is almost a 1 to 1 copy of the right-hand table in the HMN tab of the SHCD. It is an array of JSON objects, and each object represents a row from the HMN tab. Any row that is not understood by CSI will be ignored, this includes any additional devices connected to the HMN that are not managed by CSM.\nFor a detailed mapping between the data in the SHCD and the equivalent information the hmn_connections.json file, see Introduction to SHCD HMN Connections Rules.\nWhat is a source name? The source name is the Source field in each row or element of the hmn_connections.json, and this name of the device that is being connected to the HMN network. From this source name the csi config init command can infer the type of hardware that is connected to the HMN network (Node BMC, PDU, HSN Switch BMC, etc\u0026hellip;).\nExample hmn_connections.json row representing an application node with SourceName uan01 in cabinet x3000 in slot 19. Its BMC is connected to port 47 of the management leaf switch in x3000 in slot 14.\n{ \u0026#34;Source\u0026#34;: \u0026#34;uan01\u0026#34;, \u0026#34;SourceRack\u0026#34;: \u0026#34;x3000\u0026#34;, \u0026#34;SourceLocation\u0026#34;: \u0026#34;u19\u0026#34;, \u0026#34;DestinationRack\u0026#34;: \u0026#34;x3000\u0026#34;, \u0026#34;DestinationLocation\u0026#34;: \u0026#34;u14\u0026#34;, \u0026#34;DestinationPort\u0026#34;: \u0026#34;j37\u0026#34; } Directions   Create a file called application_node_config.yaml with the contents below. This is a base application node config file for CSI that does not add any additional prefixes, HSM SubRole mappings, or aliases.\n--- # Additional application node prefixes to match in the hmn_connections.json file prefixes: [] # Additional HSM SubRoles prefix_hsm_subroles: {} # Application Node aliases aliases: {}   Identify application nodes present in hmn_connections.json or the HMN tab of the system\u0026rsquo;s SHCD. In general, everything in the HMN tab of the SHCD or hmn_connections.json file that does not follow the SHCD/HMN Connections Rules should be considered an application node, unless it is a KVM.\nIf the hmn_connections.json file is available, then the following command can be used to show the HMN rows that are application nodes.\nlinux# cat hmn_connections.json | jq -rc \u0026#39;.[] | select(.Source | test(\u0026#34;^((mn|wn|sn|nid|cn|cn\\\\-|pdu)\\\\d+|.*(cmc|rcm|kvm|door).*|x\\\\d+p\\\\d*|sw-.+|columbia$)\u0026#34;; \u0026#34;i\u0026#34;) | not)\u0026#39;  Example hmn_connections.json output:\n{\u0026#34;Source\u0026#34;:\u0026#34;gateway01\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u29\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u32\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j42\u0026#34;}, {\u0026#34;Source\u0026#34;:\u0026#34;login02\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u28\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u32\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j43\u0026#34;} {\u0026#34;Source\u0026#34;:\u0026#34;lnet01\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u27\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u32\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j41\u0026#34;} {\u0026#34;Source\u0026#34;:\u0026#34;vn01\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u25\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u32\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j40\u0026#34;}, {\u0026#34;Source\u0026#34;:\u0026#34;uan01\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u23\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u32\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j39\u0026#34;}, If the HMN Connections.json file is not available, then the HMN tab of SHCD spreadsheet will need to be used instead. The table below is equivalent to the example hmn_connections.json output above.\n     Source (J20) Rack (K20) Location (L20) (M20) Parent (N20) (O20) Port (P20) Destination (Q20) Rack (R20) Location (S20) (T20) Port (U20)     gateway01 x3000 u29   - j3 sw-smn01 x3000 u32 - j42   login02 x3000 u28   - j3 sw-smn01 x3000 u32 - j43   lnet01 x3000 u27   - j3 sw-smn01 x3000 u32 - j41   vn01 x3000 u25   - j3 sw-smn01 x3000 u32 - j40   uan01 x3000 u23   - j3 sw-smn01 x3000 u32 - j39     Add additional Application node prefixes\nThe prefixes field is an array of strings, that augments the list of source name prefixes that are treated as application nodes. By default csi config init only looks for application nodes that have source names that start with uan, gn, and ln. If your system contains application nodes that fall outside of those source name prefixes you will need to add additional prefixes to application_node_config.yaml. These additional prefixes will be used in addition to the default prefixes.\nTo add an additional prefix append a new string element to the prefixes array.\n Note: The command csi config init does a case insensitive check for whether a source name contains an application node prefix. For example, the prefix uan will match uan, Uan, and UAN.\n From the example hmn_connections.json output the following additional prefixes are required:\n# Additional application node prefixes to match in the hmn_connections.json file prefixes: - gateway - login - lnet - vn   Add HSM SubRoles for Application node prefixes\nThe prefix_hsm_subroles field mapping application node prefix (string) to the applicable Hardware State Manager (HSM) SubRole (string) for the application nodes. All applications nodes have the HSM Role of Application, and the SubRole value can be used to label what type of the application node it is (such as UAN, Gateway, LNETRouter, etc\u0026hellip;).\nBy default, the csi config init command will use the following SubRoles for application nodes:\n   Prefix HSM SubRole     uan UAN   ln UAN   gn Gateway    To add additional HSM SubRole for a given prefix add a new mapping under the prefix_hsm_subroles field. Where the key is the application node prefix and the value is the HSM SubRole.\nValid HSM SubRoles values are: Worker, Master, Storage, UAN, Gateway, LNETRouter, Visualization, and UserDefined.\nFrom the example hmn_connections.json output the following additional prefix HSM SubRole mappings are required:\n# Additional HSM SubRoles prefix_hsm_subroles: login: UAN lnet: LNETRouter gateway: Gateway vn: Visualization   Add Application node aliases\nThe aliases field is an map of xnames (strings) to an array of aliases (strings).\n For guidance on building application node xnames follow one of the following:\n Building xnames for nodes in a single application node chassis Building xnames for nodes in a dual application node chassis   By default, the csi config init command does not set the ExtraProperties.Alias field for application nodes in the SLS input file.\nFor each application node add its alias mapping under the aliases field. Where the key is the xname of the application node, and the value is an array of aliases (strings) which allows for one or more aliases to be specified for an application node.\nFrom the example hmn_connections.json output the following application node aliases are required:\n# Application Node aliases aliases: x3113c0s29b0n0: [\u0026#34;gateway01\u0026#34;] x3113c0s28b0n0: [\u0026#34;login02\u0026#34;] x3113c0s27b0n0: [\u0026#34;lnet01\u0026#34;] x3113c0s25b0n0: [\u0026#34;visualization01\u0026#34;, \u0026#34;vn02\u0026#34;] x3113c0s23b0n0: [\u0026#34;uan01\u0026#34;]  The ordering of xnames under aliases does not matter.\n   Final information in the example application_node_config.yaml built from the example hmn_connections.json output.\n--- # Additional application node prefixes to match in the hmn_connections.json file prefixes: - gateway - login - lnet - vn # Additional HSM SubRoles prefix_hsm_subroles: login: UAN lnet: LNETRouter gateway: Gateway vn: Visualization # Application Node aliases aliases: x3113c0s29b0n0: [\u0026#34;gateway01\u0026#34;] x3113c0s28b0n0: [\u0026#34;login02\u0026#34;] x3113c0s27b0n0: [\u0026#34;lnet01\u0026#34;] x3113c0s25b0n0: [\u0026#34;visualization01\u0026#34;, \u0026#34;vn02\u0026#34;] x3113c0s23b0n0: [\u0026#34;uan01\u0026#34;]   "
},
{
	"uri": "/docs-csm/en-11/install/create_cabinets_yaml/",
	"title": "Create Cabinets Yaml",
	"tags": [],
	"description": "",
	"content": "Create Cabinets YAML This page provides directions on constructing the optional cabinets.yaml file. This file lists cabinet ids for any systems with non-contiguous cabinet id numbers and controls how the csi config init command treats cabinet ids.\nThis file is manually created and follows this format. For each \u0026ldquo;type\u0026rdquo; of cabinet can have several fields: total_number of cabinets of this type, starting_id for this cabinet type, and a list of the ids.\n--- cabinets: - type: hill total_number: 2 starting_id: 9000 - type: mountain total_number: 4 starting_id: 1000 cabinets: - id: 1000 nmn-vlan: 2000 hmn-vlan: 3000 - id: 1001 nmn-vlan: 2001 hmn-vlan: 3001 - id: 1002 - id: 1003 - type: river total_number: 4 starting_id: 3000 In the above example file, there are 2 Hill cabinets that will be automatically numbered as 9000 and 9001. The Mountain cabinets appear in 3 groupings of four ids. The River cabinets are non-contiguous in 4 separated ids.\nA system will Hill cabinets can have 1 to 4 cabinet ids. There is no limit on the number of Mountain or River cabinets.\nWhen the above cabinets.yaml file is used, csi will ignore any command-line argument to csi config init for starting-mountain-cabinet, starting-river-cabinet, starting-hill-cabinet, mountain-cabinets, river-cabinets, or hill-cabinets.\n"
},
{
	"uri": "/docs-csm/en-11/install/configure_dell_leaf_switch/",
	"title": "Configure Dell Leaf Switch",
	"tags": [],
	"description": "",
	"content": "Configure Dell Leaf Switch This page describes how Dell leaf switches are configured.\nLeaf switches are located in air-cooled cabinets and provide connectivity to components in those cabinets.\nPrerequisites  Connectivity to the switch is established. There are two uplinks from the switch to the upstream switch; this is can be an aggregation switch or a spine switch.  Here are example snippets from a leaf switch in the SHCD.\n   Source Source Label Info Destination Label Info Destination Description     sw-smn01 x3000u40-j49 x3105u38-j47 sw-25g01 25g-15m-LC-LC   sw-smn01 x3000u40-j50 x3105u39-j47 sw-25g02 25g-15m-LC-LC    The uplinks are port 49 and 50 on the leaf. They connect to Aggregation switch 1 and 2 on port 47.\nConfigure Uplink The uplink ports are the ports connecting the leaf switches to the upstream switch. Set the description to indicate the appropriate switch names and ports from the SHCD.\ninterface port-channel100 description vertex_to_sw-40g0x no shutdown switchport mode trunk switchport access vlan 1 switchport trunk allowed vlan 2,4,7,10 mtu 9216 interface ethernet1/1/51 description to:sw-40g02_x3000u34-j15:from:sw-smn01_x3000u38-j51 no shutdown channel-group 100 mode active no switchport mtu 9216 speed 10000 flowcontrol receive off flowcontrol transmit off ! interface ethernet1/1/52 description to:sw-40g01_x3000u33-j15:from:sw-smn01_x3000u38-j52 no shutdown channel-group 100 mode active no switchport mtu 9216 speed 10000 flowcontrol receive off flowcontrol transmit off Configure VLAN Cray Site Init (CSI) generates the IP addresses used by the system, below are samples only. The VLAN information is located in the network YAML files. The following are examples.\n  Verify the leaf switches have VLAN interfaces in Node Management Network (NMN) and the Hardware Management Network (HMN).\nExample NMN.yaml:\npit# cat /var/www/ephemeral/prep/${SYSTEM_NAME}/networks/NMN.yaml SNIPPET - ip_address: 10.252.0.4 name: sw-leaf-001 comment: x3000c0w14 aliases: [] name: network_hardware net-name: NMN vlan_id: 2 comment: \u0026#34; gateway: 10.252.0.1 Example HMN.yaml:\npit# cat /var/www/ephemeral/prep/${SYSTEM_NAME}/networks/HMN.yaml SNIPPET - ip_address: 10.254.0.4 name: sw-leaf-001 comment: x3000c0w14 aliases: [] name: network_hardware net-name: HMN vlan_id: 4 comment: \u0026quot; gateway: 10.254.0.1   Set the NMN VLAN configuration.\nsw-leaf-001(config)# vlan 2 interface vlan2 description RIVER_NMN ip address 10.252.0.4/17 exit   Set the HMN VLAN configuration:\nsw-leaf-001(config)# vlan 4 interface vlan4 description RIVER_HMN ip address 10.254.0.4/17 exit   Configure SNMP   Configure SNMP.\nThis configuration is required for hardware discovery of the HPE Cray EX system.\nsnmp-server group cray-reds-group 3 noauth read cray-reds-view snmp-server user testuser cray-reds-group 3 auth md5 testpass1 priv des testpass2 snmp-server view cray-reds-view 1.3.6.1.2 included   Configure ACL These ACLs are designed to block traffic from the node management network to and from the hardware management network.\n  Create the access list.\nNOTE: The following are examples only. The IP addresses below need to match what was generated by CSI.\nsw-leaf-001(config)# seq 10 deny ip 10.252.0.0/17 10.254.0.0/17 seq 20 deny ip 10.252.0.0/17 10.104.0.0/14 seq 30 deny ip 10.254.0.0/17 10.252.0.0/17 seq 40 deny ip 10.254.0.0/17 10.100.0.0/14 seq 50 deny ip 10.100.0.0/14 10.254.0.0/17 seq 60 deny ip 10.100.0.0/14 10.104.0.0/14 seq 70 deny ip 10.104.0.0/14 10.252.0.0/17 seq 80 deny ip 10.104.0.0/14 10.100.0.0/14   Apply ACL to VLANs.\nsw-leaf-001(config)# interface vlan2 ip access-group nmn-hmn in ip access-group nmn-hmn out interface vlan4 ip access-group nmn-hmn in ip access-group nmn-hmn out interface vlan2000 ip access-group nmn-hmn in ip access-group nmn-hmn out interface vlan3000 ip access-group nmn-hmn in ip access-group nmn-hmn out   Configure Spanning-Tree Spanning-tree is used to protect the network against layer2 loops. Dell switches should have these settings for spanning-tree using bpduguard and not bpdufilter.\n  Enable spanning-tree for these VLANs.\nsw-leaf-001(config)# spanning-tree vlan 1-2,4,7,10 priority 61440 spanning-tree vlan 4 enable   Apply the following configuration to Dell leaf switches.\nsw-leaf-001(config)# interface ethernet1/1/2 no shutdown switchport mode trunk switchport access vlan 1 switchport trunk allowed vlan 2,4,7,10 mtu 9216 flowcontrol receive on flowcontrol transmit off no spanning-tree bpdufilter spanning-tree bpduguard enable spanning-tree port type edge   Configure OSPF OSPF is a dynamic routing protocol used to exchange routes. It provides reachability from the leaf switch to Kubernetes. The router-id used here is the NMN IP address (VLAN 2 IP).\n  Configure OSPF.\nsw-leaf-001(config)# router ospf 1 router-id 10.252.0.x interface vlan2 ip ospf 1 area 0.0.0.2 interface vlan4 ip ospf 1 area 0.0.0.4   Configure NTP The IP addresses used are the first three worker nodes on the NMN network. These can be found in NMN.yaml.\n  Get current NTP configuration.\nsw-leaf-001# show running-configuration | grep ntp ntp server 10.252.1.12 ntp server 10.252.1.13 ntp server 10.252.1.14 prefer   Delete any current NTP configuration.\nsw-leaf-001# configure terminal sw-leaf-001(config)# no ntp server 10.252.1.12 sw-leaf-001(config)# no ntp server 10.252.1.13 sw-leaf-001(config)# no ntp server 10.252.1.14   Add new NTP server configuration.\nntp server 10.252.1.10 prefer ntp server 10.252.1.11 ntp server 10.252.1.12 ntp source vlan 2   Verify NTP status.\nsw-leaf-001# show ntp associations remote refid st t when poll reach delay offset jitter ============================================================================== *10.252.1.10 10.252.1.4 4 u 52 64 3 0.420 -0.262 0.023 10.252.1.11 10.252.1.4 4 u 51 64 3 0.387 -0.225 0.043 10.252.1.12 10.252.1.4 4 u 48 64 3 0.399 -0.222 0.050 * master (synced), # master (unsynced), + selected, - candidate, ~ configured   Configure DNS   Configure DNS.\nThis will point to the unbound DNS server.\nsw-leaf-001(config)# ip name-server 10.92.100.225   Configure Flow Control sw-leaf01(config)# interface range ethernet 1/1/1-1/1/48 sw-leaf01(conf-range-eth1/1/1-1/1/48)# flowcontrol receive on sw-leaf01(conf-range-eth1/1/1-1/1/48)# flowcontrol transmit off sw-leaf01(conf-range-eth1/1/1-1/1/48)# end sw-leaf01(config)# interface range ethernet 1/1/51-1/1/52 sw-leaf01(conf-if-eth1/1/51-1/1/52)# flowcontrol receive off sw-leaf01(conf-if-eth1/1/51-1/1/52)# flowcontrol transmit off sw-leaf01(conf-if-eth1/1/51-1/1/52)# end Configure Edge Port Ports that need to be on the HMN, including BMCs/PDUs:\nsw-leaf-001(config)# interface 1/1/35 no shutdown no routing vlan access 4 spanning-tree bpdu-guard spanning-tree port-type admin-edge Ports that need to be on the NMN, including air-cooled compute nodes:\nsw-leaf-001(config)# interface 1/1/35 no shutdown no routing vlan access 2 spanning-tree bpdu-guard spanning-tree port-type admin-edge Disable iSCSI Disable iSCSI in the configuration.\nsw-leaf-001 \u0026amp; sw-leaf-002 (config)# no iscsi enable Save Configuration To save a configuration:\nsw-leaf-001(config)# exit sw-leaf-001# write memory Show Running Configuration To display the running configuration:\nsw-leaf-001# show running-config "
},
{
	"uri": "/docs-csm/en-11/install/configure_management_network/",
	"title": "Configure Management Network Switches",
	"tags": [],
	"description": "",
	"content": "Configure Management Network Switches HPE Cray EX systems can have network switches in many roles: spine switches, leaf switches, aggregation switches, and CDU switches. Newer systems have HPE Aruba switches, while older systems have Dell and Mellanox switches. Switch IP addresses are generated by Cray Site Init (CSI).\nThe configuration steps are different for these switch vendors. The switch configuration procedures for HPE Aruba will be grouped separately from the switch configuration procedures for other vendors.\nIt is assumed that the administrator configuring the Management Network has a basic understanding of networking protocols (STP, VLAN, OSPF, LAG/MLAG), and knows how to configure network equipment. It is also assumed that they understand and know how to read an SHCD file.\nBefore configuring/reconfiguring any switches, make sure to get the current running config and save that in case you need to revert the config.\nSave the output of the following:\n# show run Topics:  HPE Aruba switch configuration Dell and Mellanox switch configuration Next Topic  Details HPE Aruba Switch Configuration The management network switches should be configured in this order: Spine, Aggregation (if present), CDU (if present), and Leaf. Only systems with liquid-cooled cabinets will have the CDU switches. Only systems with many nodes in air-cooled cabinets will have Aggregation switches.\n Configure Aruba Spine Switch Configure Aruba Aggregation Switch (if present) Configure Aruba CDU Switch (if present) Configure Aruba Leaf Switch Update Management Network Firmware Workaround for known mac-learning issue with 8325.  Dell and Mellanox Switch Configuration The management network switches should be configured in this order: Spine, Aggregation (if present), CDU (if present), and Leaf. Only systems with liquid-cooled cabinets will have the CDU switches. Only systems with many nodes in air-cooled cabinets will have Aggregation switches.\nOn a typical system, the Mellanox switches are Spine switches and the Dell switches are used for Aggregation, CDU, and Leaf switches.\n Configure Mellanox Spine Switch Configure Dell Aggregation Switch (if present) Configure Dell CDU Switch (if present) Configure Dell Leaf Switch Update Management Network Firmware  Next Topic After completing this procedure, the next step is to collect MAC Addresses for the management nodes using the PIT node and the management network switches configured in this procedure.\n See Collect Mac Addresses  "
},
{
	"uri": "/docs-csm/en-11/install/configure_dell_cdu_switch/",
	"title": "Configure Dell Cdu Switch",
	"tags": [],
	"description": "",
	"content": "Configure Dell CDU switch This page describes how Dell CDU switches are configured.\nCDU switches are located in liquid-cooled cabinets and provide connectivity to MTN (Mountain) components. CDU switches act as leaf switches in the architecture. They run in a high availability pair and use VLT to provide redundancy.\nPrerequisites  Two uplinks from each CDU switch to the upstream switch, this is normally a spine switch. Connectivity to the switch is established. Three connections between the switches, two of these are used for the VLT interconnect (VLTi), and one used for the keepalive. The VLTi uses two 100gb links between the switches.  Here are example snippets from a CDU switch in the SHCD.\nThe uplinks are port 51 and 52 on both CDU switches. These go to sw-100g01 and sw-100g2 which are spine 1 and 2.\nThe ISL are ports 49 and 50 on both CDU switches.\nThe Keepalive is port 48.\nNOTE: The following are only examples; installation and cabling may vary. This information is on the 25G_10G tab of the SHCD spreadsheet.\n   Source Source Label Info Destination Label Info Destination Description Notes     sw-100g01 x3105u40-j01 d100u01-j51 d100sw1 100g-30m-AOC    sw-100g02 x3105u41-j01 d100u01-j52 d100sw1 100g-30m-AOC    sw-100g01 x3105u40-j02 d100u02-j51 d100sw2 100g-30m-AOC    sw-100g02 x3105u41-j02 d100u02-j52 d100sw2 100g-30m-AOC    d100sw1 d100u01-j48 d100u02-j48 d100sw2 6ft keepalive   d100sw1 d100u01-j49 d100u02-j49 d100sw2 100g-1m-DAC    d100sw1 d100u01-j50 d100u02-j50 d100sw2 100g-1m-DAC     Configure VLT Configure VLAN Cray Site Init (CSI) generates the IP addresses used by the system, below are samples only. The VLAN information is located in the network YAML files. The following are examples.\n  Verify the CDU switches have VLAN interfaces in the NMN, HMN, NMN_MTN, and HMN_MTN.\nExample NMN.yaml:\npit# cat /var/www/ephemeral/prep/${SYSTEM_NAME}/networks/NMN.yaml SNIPPET - ip_address: 10.252.0.5 name: sw-cdu-001 comment: d0w1 aliases: [] - ip_address: 10.252.0.6 name: sw-cdu-002 comment: d0w2 aliases: [] name: network_hardware net-name: NMN vlan_id: 2 comment: \u0026#34; gateway: 10.252.0.1 Example HMN.yaml:\npit# cat /var/www/ephemeral/prep/${SYSTEM_NAME}/networks/HMN.yaml SNIPPET - ip_address: 10.254.0.5 name: sw-cdu-001 comment: d0w1 aliases: [] - ip_address: 10.254.0.6 name: sw-cdu-002 comment: d0w2 aliases: [] name: network_hardware net-name: HMN vlan_id: 4 comment: \u0026#34; gateway: 10.254.0.1 Example NMN_MTN.yaml:\npit# cat /var/www/ephemeral/prep/${SYSTEM_NAME}/networks/NMN_MTN.yaml full_name: Mountain Node Management Network cidr: 10.100.0.0/17 subnets: - full_name: \u0026#34; cidr: ip: 10.100.0.0 mask: - 255 - 255 - 252 - 0 ip_reservations: [] name: cabinet_9000 net-name: \u0026#34; vlan_id: 2000 comment: \u0026#34; gateway: 10.100.0.1 _: \u0026#34; dns_server: \u0026#34; iprange-start: 10.100.0.10 iprange-end: 10.100.3.254 name: NMN_MTN Example HMN_MTN.yaml:\npit# cat /var/www/ephemeral/prep/${SYSTEM_NAME}/networks/HMN_MTN.yaml full_name: Mountain Hardware Management Network cidr: 10.104.0.0/17 subnets: - full_name: \u0026#34; cidr: ip: 10.104.0.0 mask: - 255 - 255 - 252 - 0 ip_reservations: [] name: cabinet_9000 net-name: \u0026#34; vlan_id: 3000 comment: \u0026#34; gateway: 10.104.0.1 _: \u0026#34; dns_server: \u0026#34; iprange-start: 10.104.0.10 iprange-end: 10.104.3.254 name: HMN_MTN NOTE: CSI does not yet generate IP addresses for the CDU switches on VLANs HMN_MTN and NMN_MTN.\n The first CDU switch in the pair will always have an IP address ending in .2 on the HMN_MTN and NMN_MTN networks. The second CDU switch in the pair will always have an IP address ending in .3 on the HMN_MTN and NMN_MTN networks. Both CDU MTN VLAN IP addresses will be at the beginning of the subnet. The gateway will always end in .1 and will be at the beginning of the subnet. Every Mountain Cabinet will get its own HMN and NMN VLAN.  The following is an example of CDU switch IP addressing based on the network .yaml files from above.\n   VLAN CDU1 CDU2 Purpose     2 10.252.0.5/17 10.252.0.6/17 River Node Management   4 10.254.0.5/17 10.254.0.6/17 River Hardware Management   2000 10.100.0.2/22 10.100.0.3/22 Mountain Node Management   3000 10.104.0.2/22 10.104.0.3/22 Mountain Hardware Management    If the system has additional Mountain Cabinets the VLANs will look like the following. This is an example of a system with 3 cabinets.\n   VLAN CDU1 CDU2 Purpose     2000 10.100.0.2/22 10.100.0.3/22 Mountain Node Management   3000 10.104.0.2/22 10.104.0.3/22 Mountain Hardware Management   2001 10.100.4.2/22 10.100.4.3/22 Mountain Node Management   3001 10.104.4.2/22 10.104.4.3/22 Mountain Hardware Management   2002 10.100.8.2/22 10.100.8.3/22 Mountain Node Management   3002 10.104.8.2/22 10.104.8.3/22 Mountain Hardware Management      View the output of the SHCD.\nThe components in the x1000 cabinet would get their own NMN and HMN VLAN and components in the x1001 would also get their own NMN and HMN VLAN. The CECs will be on the HMN VLAN of that cabinet.\n  Configure the VLANs on the switches.\nNMN MTN VLAN configuration:\nsw-cdu-001(config)# interface vlan2000 mode L3 description CAB_1000_MTN_NMN no shutdown ip address 10.100.0.2/22 ip helper-address 10.92.100.222 ! vrrp-group 20 virtual-address 10.100.0.1 sw-cdu-002(config)# interface vlan2000 mode L3 description CAB_1000_MTN_NMN no shutdown ip address 10.100.0.3/22 ip helper-address 10.92.100.222 ! vrrp-group 20 virtual-address 10.100.0.1 HMN MTN VLAN configuration:\nsw-cdu-001(config)# interface vlan3000 mode L3 description CAB_1000_MTN_HMN no shutdown ip address 10.104.0.2/22 ip helper-address 10.94.100.222 ! vrrp-group 30 virtual-address 10.104.0.1 sw-cdu-002(config)# interface vlan3000 mode L3 description CAB_1000_MTN_HMN no shutdown ip address 10.104.0.3/22 ip helper-address 10.94.100.222 ! vrrp-group 30 virtual-address 10.104.0.1   Configure Uplink The uplink ports are the ports connecting the CDU switches to the upstream switch, most likely a spine switch.\nConfigure ACL These ACLs are designed to block traffic from the NMN to and from the HMN.\n  Create the access list.\nNOTE: The following are examples only. The IP addresses below need to match what was generated by CSI.\nsw-cdu-001 \u0026amp; sw-cdu-002 (config)# ip access-list nmn-hmn seq 10 deny ip 10.252.0.0/17 10.254.0.0/17 seq 20 deny ip 10.252.0.0/17 10.104.0.0/14 seq 30 deny ip 10.254.0.0/17 10.252.0.0/17 seq 40 deny ip 10.254.0.0/17 10.100.0.0/14 seq 50 deny ip 10.100.0.0/14 10.254.0.0/17 seq 60 deny ip 10.100.0.0/14 10.104.0.0/14 seq 70 deny ip 10.104.0.0/14 10.252.0.0/17 seq 80 deny ip 10.104.0.0/14 10.100.0.0/14 seq 90 permit ip any any   Apply ACL to VLANs.\nsw-cdu-001 \u0026amp; sw-cdu-002 (config)# interface vlan2 ip access-group nmn-hmn in ip access-group nmn-hmn out interface vlan4 ip access-group nmn-hmn in ip access-group nmn-hmn out interface vlan2000 ip access-group nmn-hmn in ip access-group nmn-hmn out interface vlan3000 ip access-group nmn-hmn in ip access-group nmn-hmn out   Configure Spanning-Tree Spanning-tree is used to protect the network against layer2 loops. Dell switches should have these settings for spanning-tree using bpduguard and not bpdufilter.\n  Enable spanning-tree for these VLANs.\nsw-cdu-001 \u0026amp; sw-cdu-002 (config)# spanning-tree vlan 1-2,4,4091 priority 61440   Ensure that no ports have bpduguard enabled.\nsw-cdu-001 \u0026amp; sw-cdu-002 (config)# interface ethernet 1/1/x no spanning-tree bpdufilter spanning-tree bpduguard enable   Add BPDUguard to ports going to CMMs.\ninterface port-channel1 description CMM_CAB_1000 no shutdown switchport mode trunk switchport access vlan 2000 switchport trunk allowed vlan 3000,4091 mtu 9216 vlt-port-channel 1 spanning-tree bpduguard enable   Configure OSPF OSPF is a dynamic routing protocol used to exchange routes. It provides reachability from the MTN networks to NMN/Kubernetes networks. The router-id used here is the NMN IP address (VLAN 2 IP).\nsw-cdu-001 \u0026amp; sw-cdu-002 (config)# router ospf 1 router-id 10.252.0.x interface vlan2 ip ospf 1 area 0.0.0.2 interface vlan4 ip ospf 1 area 0.0.0.4 interface vlan2000 ip ospf 1 area 0.0.0.2 ip ospf passive interface vlan3000 ip ospf 1 area 0.0.0.4 ip ospf passive Configure NTP The IP addresses used are the first three worker nodes on the NMN network. These can be found in NMN.yaml.\n  Get current NTP configuration.\nsw-cdu-001# show running-configuration | grep ntp ntp server 10.252.1.12 ntp server 10.252.1.13 ntp server 10.252.1.14 prefer   Delete any current NTP configuration.\nsw-cdu-001# configure terminal sw-cdu-001(config)# no ntp server 10.252.1.12 sw-cdu-001(config)# no ntp server 10.252.1.13 sw-cdu-001(config)# no ntp server 10.252.1.14   Add new NTP server configuration.\nntp server 10.252.1.10 prefer ntp server 10.252.1.11 ntp server 10.252.1.12 ntp source vlan 2   Verify NTP status.\nsw-cdu-001# show ntp associations remote refid st t when poll reach delay offset jitter ============================================================================== *10.252.1.10 10.252.1.4 4 u 52 64 3 0.420 -0.262 0.023 10.252.1.11 10.252.1.4 4 u 51 64 3 0.387 -0.225 0.043 10.252.1.12 10.252.1.4 4 u 48 64 3 0.399 -0.222 0.050 * master (synced), # master (unsynced), + selected, - candidate, ~ configured   Configure DNS   Configure DNS.\nThis will point to the unbound DNS server.\nsw-cdu-001 \u0026amp; sw-cdu-002 (config)# ip name-server 10.92.100.225   Configure SNMP   Configure SNMP.\nsw-cdu-001 \u0026amp; sw-cdu-002 (config)# snmp-server group cray-reds-group 3 noauth read cray-reds-view snmp-server user testuser cray-reds-group 3 auth md5 testpass1 priv des testpass2 snmp-server view cray-reds-view 1.3.6.1.2 included   Downlink Port Configuration Configure Flow Control Configure LAG for CMMs  This requires updated CMM firmware. (version 1.4.20) See v1.4 Admin Guide for details on updating CMM firmware A static LAG will be configured on the CDU switches. The CDU switches have two cables (10Gb RJ45) connecting to each CMM. This configuration offers increased throughput and redundancy. The CEC will not need to be programmed in order to support the LAG configuration as it was required in previous versions. The updated firmware takes care of this.    Configure ports going to CMM switches.\nThe VLANs used are the cabinet VLANs that are generated from CSI. The description should be changed to match the cabinet number.\nsw-cdu-001 \u0026amp; sw-cdu-002 (config)# interface port-channel1 description CMM_CAB_1000 no shutdown switchport mode trunk switchport access vlan 2000 switchport trunk allowed vlan 3000,4091 mtu 9216 vlt-port-channel 1 sw-cdu-001 \u0026amp; sw-cdu-002 (config)# interface ethernet1/1/1 description CMM_CAB_1000 no shutdown channel-group 1 mode on no switchport mtu 9216 flowcontrol receive on flowcontrol transmit on   CEC Port Configuration The VLAN used here is generated from CSI. It is the HMN_MTN VLAN that is assigned to that cabinet.\nsw-cdu-001 \u0026amp; sw-cdu-002 (config)# interface ethernet1/1/50 description CEC_CAB_1003_alt no shutdown switchport access vlan 3003 flowcontrol receive off flowcontrol transmit off spanning-tree bpduguard enable spanning-tree port type edge Disable iSCSI Disable iSCSI in the configuration.\nsw-cdu-001 \u0026amp; sw-cdu-002 (config)# no iscsi enable Save Configuration To save the configuration:\nsw-cdu-001(config)# exit sw-cdu-001# write memory Show Running Configuration To display the running configuration:\nsw-cdu-001# show running-config "
},
{
	"uri": "/docs-csm/en-11/install/configure_aruba_spine_switch/",
	"title": "Configure Aruba Spine Switch",
	"tags": [],
	"description": "",
	"content": "Configure Aruba Spine Switch This page describes how Aruba spine switches are configured.\nDepending on the size of the HPE Cray EX system, the spine switches will serve different purposes. On TDS systems, the NCNs will plug directly into the spine switches. On larger systems with aggregation switches, the spine switches will provide connection between the aggregation switches.\nSwitch models used: JL635A Aruba 8325-48Y8C and JL636A Aruba 8325-32C\nThey run in a high availability pair and use VSX to provide redundancy.\nPrerequisites  Three connections between the switches, two of these are used for the Inter switch link (ISL), and one used for the keepalive. Connectivity to the switch is established. The Configure Aruba Management Network Base procedure has been run. The ISL uses 100GB ports and the keepalive will be a 100GB port on the JL636A and a 25GB port on the JL635A.  The following is an example snippet from a spine switch on the SHCD.\nThe ISL ports are 31 and 32 on both spine switches. The keepalive is port 27.\n   Source Source Label Info Destination Label Info Destination Description Notes     sw-100g01 x3105u40-j32 x3105u41-j32 sw-100g02 100g-1m-DAC    sw-100g01 x3105u40-j31 x3105u41-j31 sw-100g02 100g-1m-DAC    sw-100g01 x3105u40-j27 x3105u41-j27 sw-100g02 100g-1m-DAC keepalive    Configure VSX   Create the keepalive VRF on both switches.\nsw-spine-001 \u0026amp; sw-spine-002 (config)# vrf keepalive   Setup the keepalive link.\nThis will require a unique IP address on both switches. The IP address is in its own VRF so this address will not be reachable from anywhere besides the spine pair.\nsw-spine-001(config)# int 1/1/27 no shutdown vrf attach keepalive description VSX keepalive ip address 192.168.255.0/31 sw-spine-002(config)# int 1/1/27 no shutdown vrf attach keepalive description VSX keepalive ip address 192.168.255.1/31   Create the ISL lag on both switches.\nsw-spine-001 \u0026amp; sw-spine-002 (config)# interface lag 256 no shutdown description ISL link no routing vlan trunk native 1 tag vlan trunk allowed all lacp mode active   Add the ISL ports to the LAG; these are two of the ports connected between the switches.\nsw-spine-001 \u0026amp; sw-spine-002 (config)# int 1/1/31-1/1/32 no shutdown mtu 9198 lag 99   Create the VSX instance and setup the keepalive link.\nsw-spine-001(config)# no ip icmp redirect vsx system-mac 02:01:00:00:01:00 inter-switch-link lag 99 role primary keepalive peer 192.168.255.1 source 192.168.255.0 vrf keepalive linkup-delay-timer 600 vsx-sync vsx-global sw-spine-002(config)# no ip icmp redirect vsx system-mac 02:01:00:00:01:00 inter-switch-link lag 99 role secondary keepalive peer 192.168.255.0 source 192.168.255.1 vrf keepalive linkup-delay-timer 600 vsx-sync vsx-global   Verify there is an Established VSX session.\nsw-spine-001 # show vsx brief ISL State : In-Sync Device State : Sync-Primary Keepalive State : Keepalive-Established Device Role : secondary Number of Multi-chassis LAG interfaces : 0   Configure VLAN Cray Site Init (CSI) generates the IP addresses used by the system, below are samples only. The VLAN information is located in the network YAML files. The following are examples.\n  View the spine switch VLAN interfaces in NMN, HMN, and CAN networks.\nExample NMN.yaml:\npit# cat /var/www/ephemeral/prep/${SYSTEM_NAME}/networks/NMN.yaml SNIPPET - ip_address: 10.252.0.2 name: sw-spine-001 comment: x3000c0h12s1 aliases: [] - ip_address: 10.252.0.3 name: sw-spine-002 comment: x3000c0h13s1 aliases: [] name: network_hardware net-name: NMN vlan_id: 2 comment: \u0026#34; gateway: 10.252.0.1 Example HMN.yaml:\npit# cat /var/www/ephemeral/prep/${SYSTEM_NAME}/networks/HMN.yaml SNIPPET - ip_address: 10.254.0.2 name: sw-spine-001 comment: x3000c0h12s1 aliases: [] - ip_address: 10.254.0.3 name: sw-spine-002 comment: x3000c0h13s1 aliases: [] name: network_hardware net-name: HMN vlan_id: 4 comment: \u0026#34; gateway: 10.254.0.1 Example CAN.yaml:\npit# cat /var/www/ephemeral/prep/${SYSTEM_NAME}/networks/CAN.yaml SNIPPET - ip_address: 10.102.11.2 name: can-switch-1 comment: \u0026#34; aliases: [] - ip_address: 10.102.11.3 name: can-switch-2 comment: \u0026#34; aliases: [] net-name: CAN vlan_id: 7 comment: \u0026#34; gateway: 10.102.11.1 The following is an example of spine switch IP addressing based on the network .yaml files from above.\n   VLAN Spine01 Spine02 Purpose     2 10.252.0.2/17 10.252.0.3/17 River Node Management   4 10.254.0.2/17 10.254.0.3/17 River Hardware Management   7 10.102.11.2/24 10.102.11.3/24 Customer Access      Set the NMN VLAN configuration.\nsw-spine-001(config)# vlan 2 interface vlan2 vsx-sync active-gateways ip address 10.252.0.2/17 active-gateway ip mac 12:01:00:00:01:00 active-gateway ip 10.252.0.1 ip mtu 9198 ip helper-address 10.92.100.222 exit sw-spine-002(config)# vlan 2 interface vlan2 vsx-sync active-gateways ip address 10.252.0.3/17 active-gateway ip mac 12:01:00:00:01:00 active-gateway ip 10.252.0.1 ip mtu 9198 ip helper-address 10.92.100.222 exit   Set the HMN VLAN configuration.\nsw-spine-001(config)# vlan 4 interface vlan4 vsx-sync active-gateways ip address 10.254.0.2/17 active-gateway ip mac 12:01:00:00:01:00 active-gateway ip 10.254.0.1 ip mtu 9198 ip helper-address 10.94.100.222 exit sw-spine-002(config)# vlan 4 interface vlan4 vsx-sync active-gateways ip address 10.254.0.3/17 active-gateway ip mac 12:01:00:00:01:00 active-gateway ip 10.254.0.1 ip mtu 9198 ip helper-address 10.94.100.222 exit   Set the CAN VLAN configuration.\nsw-spine-001(config)# interface vlan 7 vsx-sync active-gateways ip mtu 9198 ip address 10.102.11.2/24 active-gateway ip mac 12:01:00:00:01:00 active-gateway ip 10.102.11.1 ip helper-address 10.92.100.222 sw-spine-002(config)# interface vlan 7 vsx-sync active-gateways ip mtu 9198 ip address 10.102.11.3/24 active-gateway ip mac 12:01:00:00:01:00 active-gateway ip 10.102.11.1 ip helper-address 10.92.100.222   Configure Uplink The uplink ports are the ports connecting the spine switches to the downstream switches, these switches can be aggregation, leaf, or spine switches.\n  Create the LAG.\nsw-spine-001 \u0026amp; sw-spine-002 (config)# interface lag 1 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed all lacp mode active exit   Add ports to the LAG.\nsw-spine-001 \u0026amp; sw-spine-002 (config)# interface 1/1/1 - 1/1/2 no shutdown mtu 9198 lag 1 exit   Configure ACL These ACLs are designed to block traffic from the node management network to and from the hardware management network.\n  Create the access list.\nNOTE: these are examples only, the IP addresses below need to match what was generated by CSI.\nsw-spine-001 \u0026amp; sw-spine-002 (config)# access-list ip nmn-hmn 10 deny any 10.252.0.0/255.255.128.0 10.254.0.0/255.255.128.0 20 deny any 10.252.0.0/255.255.128.0 10.104.0.0/255.252.0.0 30 deny any 10.254.0.0/255.255.128.0 10.252.0.0/255.255.128.0 40 deny any 10.254.0.0/255.255.128.0 10.100.0.0/255.252.0.0 50 deny any 10.100.0.0/255.252.0.0 10.254.0.0/255.255.128.0 60 deny any 10.100.0.0/255.252.0.0 10.104.0.0/255.252.0.0 70 deny any 10.104.0.0/255.252.0.0 10.252.0.0/255.255.128.0 80 deny any 10.104.0.0/255.252.0.0 10.100.0.0/255.252.0.0 90 permit any any any   Apply ACL to VLANs.\nsw-spine-001 \u0026amp; sw-spine-002 (config)# vlan 2 name RVR_NMN apply access-list ip nmn-hmn in apply access-list ip nmn-hmn out vlan 4 name RVR_HMN apply access-list ip nmn-hmn in apply access-list ip nmn-hmn out   Configure Spanning-Tree   Apply the following configuration to Aruba spine switches.\nsw-spine-001 \u0026amp; sw-spine-002 (config)# spanning-tree mode rpvst spanning-tree spanning-tree priority 7 spanning-tree vlan 1,2,4,7   Configure OSPF OSPF is a dynamic routing protocol used to exchange routes. It provides reachability from the MTN networks to NMN/Kubernetes networks. The router-id used here is the NMN IP address. (VLAN 2 IP)\n  Configure OSPF.\nsw-spine-001 \u0026amp; sw-spine-002 (config)# router ospf 1 router-id 10.252.0.x interface vlan2 ip ospf 1 area 0.0.0.2 interface vlan4 ip ospf 1 area 0.0.0.4 redistribute bgp   Configure NTP   Configure NTP.\nThe IP addresses used are the first three worker nodes on the NMN. These can be found in NMN.yaml.\nsw-spine-001 \u0026amp; sw-spine-002 (config)# ntp server 10.252.1.7 ntp server 10.252.1.8 ntp server 10.252.1.9 ntp enable   Configure DNS   Configure DNS.\nThis will point to the unbound DNS server.\nsw-spine-001 \u0026amp; sw-spine-002 (config)# ip dns server-address 10.92.100.225   Configure Edge Port Edge ports are connected to non-compute nodes (NCNs).\n  Set the worker and master node configuration.\nRefer to Cable Management Network Servers for cabling specs.\nsw-spine-001 \u0026amp; sw-spine-002 (config)# interface lag 4 multi-chassis no shutdown description w001 no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge exit sw-spine-001 \u0026amp; sw-spine-002 (config)# interface 1/1/7 no shutdown mtu 9198 lag 4 exit   Set the Aruba Storage port configuration (future use).\nThese will be configured, but the ports will be shut down until needed. These are OCP and PCIe port 2 on storage nodes.\nsw-spine-001 \u0026amp; sw-spine-002 (config)# interface 1/1/7 shutdown mtu 9198 lag 4 exit   Set the Aruba LAG configuration.\nsw-spine-001 \u0026amp; sw-spine-002 (config)# interface lag 4 multi-chassis shutdown no routing vlan access 10 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge exit   Configure User Access/Login/Application Node Port  One connection will go to a NMN(VLAN2) access port; this is where the UAN will pxe boot and communicate with internal systems (see SHCD for UAN cabling). ONE OF THESE PORTS IS SHUTDOWN. One Bond (two connections) will be going to the MLAG/VSX pair of switches. This will be a TRUNK port for the CAN connection.    Set the Aruba UAN NMN configuration.\nOne port is shutdown.\nsw-spine-001 (config)# interface 1/1/16 no shutdown mtu 9198 no routing vlan access 2 spanning-tree bpdu-guard spanning-tree port-type admin-edge exit sw-spine-002 (config)# interface 1/1/16 shutdown mtu 9198 no routing vlan access 2 spanning-tree bpdu-guard spanning-tree port-type admin-edge exit   Set the Aruba UAN CAN configuration.\nPort configuration is the same on both switches.\nsw-spine-001 \u0026amp; sw-spine-002 (config)# interface lag 17 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 7 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge exit sw-spine-001 \u0026amp; sw-spine-002 (config)# interface 1/1/17 no shutdown mtu 9198 lag 17   Save Configuration To save the configuration:\nsw-spine-001(config)# exit sw-spine-001# write memory Show Running Configuration To display the running configuration:\nsw-spine-001# show running-config "
},
{
	"uri": "/docs-csm/en-11/install/configure_dell_aggregation_switch/",
	"title": "Configure Dell Aggregation Switch",
	"tags": [],
	"description": "",
	"content": "Configure Dell Aggregation Switch This page describes how Dell aggregation switches are configured.\nManagement nodes and Application nodes will be plugged into aggregation switches.\nThey run in a high availability pair and use VLT to provide redundancy.\nPrerequisites   Three connections between the switches, two of these are used for the Inter switch link (ISL), and one used for the keepalive.\n  Connectivity to the switch is established.\n  The ISL uses 100GB ports and the keepalive will be a 25 GB port.\nHere is an example snippet from an aggregation switch on the 25G_10G tab of the SHCD spreadsheet.\n   Source Source Label Info Destination Label Info Destination Description Notes     sw-25g01 x3105u38-j49 x3105u39-j49 sw-25g02 100g-1m-DAC    sw-25g01 x3105u38-j50 x3105u39-j50 sw-25g02 100g-1m-DAC    sw-25g01 x3105u38-j53 x3105u39-j53 sw-25g02 100g-1m-DAC keepalive      Configure VLT Configure VLAN Cray Site Init (CSI) generates the IP addresses used by the system, below are samples only. The VLAN information is located in the network YAML files. Below are examples.\n  Verify the aggregation switches have VLAN interfaces in Node Management Network (NMN) and Hardware Management Network (HMN).\nExample NMN.yaml:\npit# cat /var/www/ephemeral/prep/${SYSTEM_NAME}/networks/NMN.yaml SNIPPET - ip_address: 10.252.0.4 name: sw-agg-001 comment: x3000c0h12s1 aliases: [] - ip_address: 10.252.0.5 name: sw-agg-002 comment: x3000c0h13s1 aliases: [] name: network_hardware net-name: NMN vlan_id: 2 comment: \u0026#34; gateway: 10.252.0.1 Example HMN.yaml:\npit# cat /var/www/ephemeral/prep/${SYSTEM_NAME}/networks/HMN.yaml SNIPPET - ip_address: 10.254.0.4 name: sw-agg-001 comment: x3000c0h12s1 aliases: [] - ip_address: 10.254.0.5 name: sw-agg-002 comment: x3000c0h13s1 aliases: [] name: network_hardware net-name: HMN vlan_id: 4 comment: \u0026#34; gateway: 10.254.0.1 The following is an example of aggregation switch IP addressing based on the network .yaml files from above.\n   VLAN Agg01 Agg02 Purpose     2 10.252.0.4/17 10.252.0.5/17 River Node Management   4 10.254.0.4/17 10.254.0.5/17 River Hardware Management      Set the NMN VLAN configuration.\nsw-agg-001(config)# vlan 2 interface vlan2 vsx-sync active-gateways ip address 10.252.0.2/17 ip mtu 9198 exit sw-agg-002(config)# vlan 2 interface vlan2 ip address 10.252.0.4/17 ip mtu 9198 exit   Set the HMN VLAN configuration.\nsw-agg-001(config)# vlan 4 interface vlan4 vsx-sync active-gateways ip address 10.254.0.4/17 ip mtu 9198 exit sw-agg-002(config)# vlan 4 interface vlan4 vsx-sync active-gateways ip address 10.254.0.5/17 ip mtu 9198 exit   Configure Uplink The uplink ports are the ports connecting the aggregation switches to the spine switches.\nConfigure ACL These ACLs are designed to block traffic from the NMN to and from the HMN.\n  Create the access list.\nNOTE: these are examples only, the IP addresses below need to match what was generated by CSI.\nsw-agg-001 \u0026amp; sw-agg-002 (config)# access-list ip nmn-hmn seq 10 deny ip 10.252.0.0/17 10.254.0.0/17 seq 20 deny ip 10.252.0.0/17 10.104.0.0/14 seq 30 deny ip 10.254.0.0/17 10.252.0.0/17 seq 40 deny ip 10.254.0.0/17 10.100.0.0/14 seq 50 deny ip 10.100.0.0/14 10.254.0.0/17 seq 60 deny ip 10.100.0.0/14 10.104.0.0/14 seq 70 deny ip 10.104.0.0/14 10.252.0.0/17 seq 80 deny ip 10.104.0.0/14 10.100.0.0/14 90 permit any any any   Apply ACL to VLANs.\nsw-agg-001 \u0026amp; sw-agg-002 (config)# interface vlan2 ip access-group nmn-hmn in ip access-group nmn-hmn out interface vlan4 ip access-group nmn-hmn in ip access-group nmn-hmn out interface vlan2000 ip access-group nmn-hmn in ip access-group nmn-hmn out interface vlan3000 ip access-group nmn-hmn in ip access-group nmn-hmn out   Configure Spanning-Tree   Apply the following configuration to the Dell aggregation switches.\nsw-agg-001 \u0026amp; sw-agg-002 (config)# spanning-tree vlan 1-2,4,4091 priority 61440   Configure OSPF OSPF is a dynamic routing protocol used to exchange routes. It provides reachability from the MTN networks to NMN/Kubernetes networks. The router-id used here is the NMN IP address (VLAN 2 IP).\n  Configure OSPF.\nsw-agg-001 \u0026amp; sw-agg-002 (config)# router ospf 1 router-id 10.252.0.x interface vlan2 ip ospf 1 area 0.0.0.2 interface vlan4 ip ospf 1 area 0.0.0.4   Configure NTP The IP addresses used are the first three worker nodes on the NMN. These can be found in NMN.yaml.\n  Get current NTP configuration.\nsw-agg-001# show running-configuration | grep ntp ntp server 10.252.1.12 ntp server 10.252.1.13 ntp server 10.252.1.14 prefer   Delete any current NTP configuration.\nsw-agg-001# configure terminal sw-agg-001(config)# no ntp server 10.252.1.12 sw-agg-001(config)# no ntp server 10.252.1.13 sw-agg-001(config)# no ntp server 10.252.1.14   Add new NTP server configuration.\nntp server 10.252.1.10 prefer ntp server 10.252.1.11 ntp server 10.252.1.12 ntp source vlan 2   Verify NTP status.\nsw-agg-001# show ntp associations remote refid st t when poll reach delay offset jitter ============================================================================== *10.252.1.10 10.252.1.4 4 u 52 64 3 0.420 -0.262 0.023 10.252.1.11 10.252.1.4 4 u 51 64 3 0.387 -0.225 0.043 10.252.1.12 10.252.1.4 4 u 48 64 3 0.399 -0.222 0.050 * master (synced), # master (unsynced), + selected, - candidate, ~ configured   Configure DNS   Configure DNS.\nThis will point to the unbound DNS server.\nsw-agg-001 \u0026amp; sw-agg-002 (config)# ip name-server 10.92.100.225   Configure SNMP   Configure SNMP.\nsw-agg-001 \u0026amp; sw-agg-002 (config)# snmp-server group cray-reds-group 3 noauth read cray-reds-view snmp-server user testuser cray-reds-group 3 auth md5 testpass1 priv des testpass2 snmp-server view cray-reds-view 1.3.6.1.2 included   Configure Flow Control Configure Edge Port These are ports that are connected to management nodes.\n  Set the worker node and master node configuration.\nRefer to Cable Management Network Servers for cabling specs.\nsw-agg-001 \u0026amp; sw-agg-002 (config)# interface lag 4 multi-chassis no shutdown description w001 no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge exit sw-agg-001 \u0026amp; sw-agg-002 (config)# interface 1/1/7 no shutdown mtu 9198 lag 4 exit   Set the Dell storage port configuration (future use).\nThese will be configured, but the ports will be shut down until needed. These are OCP and PCIe port 2 on storage nodes.\nsw-agg-001 \u0026amp; sw-agg-002 (config)# interface 1/1/7 shutdown mtu 9198 lag 4 exit   Set the Dell LAG configuration.\nsw-agg-001 \u0026amp; sw-agg-002 (config)# interface lag 4 multi-chassis shutdown no routing vlan access 10 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge exit   Configure User Access/Login/Application Node Port  One connection will go to a NMN (VLAN2) access port. This is where the UAN will PXE boot and communicate with internal nodes (see SHCD for UAN cabling). One Bond (two connections) will be going to the MLAG/VSX pair of switches. This will be a trunk port for the CAN connection.    Set the Dell UAN NMN configuration.\nsw-agg-001 \u0026amp; sw-agg-002 (config)# interface 1/1/16 no shutdown mtu 9198 no routing vlan access 2 spanning-tree bpdu-guard spanning-tree port-type admin-edge exit   Set the Dell UAN CAN configuration.\nPort configuration is the same on both switches.\nsw-agg-001 \u0026amp; sw-agg-002 (config)# interface lag 17 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 7 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge exit   Disable iSCSI Disable iSCSI in the configuration.\nsw-agg-001 \u0026amp; sw-agg-002 (config)# no iscsi enable Save Configuration To save a configuration:\nsw-agg-001(config)# exit sw-agg-001# write memory Show Running Configuration To display a running configuration:\nsw-agg-001# show running-config "
},
{
	"uri": "/docs-csm/en-11/install/configure_aruba_management_network_base/",
	"title": "Configure Aruba Management Network Base",
	"tags": [],
	"description": "",
	"content": "Configure Aruba Management Network Base This page provides instructions on how to setup the base network configuration of the Shasta Management network.\nAfter applying the base configuration, all management switches will be accessible to apply the remaining configuration.\nPrerequisites  Console access to all of the switches SHCD available  Configuration The base configuration can be applied once console access to the switches has been established. The purpose of this configuration is to have an IPv6 underlay that provides access to the management switches. The base configuration is running OSPFv3 over IPv6 on VLAN 1, so the switches can dynamically form neighborship, which enables remote access.\n  Apply the admin username and password.\nswitch# conf terminal switch(config)# user admin group administrators password plaintext xxxxxxxx   Set the hostname of the switch.\nUse the name defined in the SHCD to set the hostname.\nswitch(config)# hostname sw-25g04   Enable and configure every uplink or switch to switch link.\nUse SHCD to determine the uplink ports.\nsw-25g04(config)# int 1/1/1 sw-25g04(config-if)# no routing sw-25g04(config-if)# vlan trunk native 1 sw-25g04(config-if)# no shut   Add an IPv6 interface to VLAN 1 and start the OSPv3 process.\nIn addition to this, a unique router-id is required. This is an IPv4 address that will only be used for identifying the router; this is not a routable address. Increment this by 1 for each switch. Other IP addresses may be used for router-IDs if desired.\nsw-25g04(config)# router ospfv3 1 sw-25g04(config-ospfv3-1)# area 0 sw-25g04(config-ospfv3-1)# router-id 172.16.0.1 sw-25g04(config-ospfv3-1)# exit   Add VLAN 1 interface to OSPF area 0.\nsw-25g04(config)# int vlan 1 sw-25g04(config-if-vlan)# ipv6 address autoconfig sw-25g04(config-if-vlan)# ipv6 ospfv3 1 area 0 sw-25g04(config-if-vlan)# exit   Add a unique IPv6 Loopback address, which is the address that we will be remotely connecting to.\nIncrement this address by 1 for every switch.\nsw-25g04(config)# interface loopback 0 sw-25g04(config-loopback-if)# ipv6 address fd01::0/64 sw-25g04(config-loopback-if)# ipv6 ospfv3 1 area 0   Enable remote access via SSH/HTTPS/API.\nsw-25g04(config)# ssh server vrf default sw-25g04(config)# ssh server vrf mgmt sw-25g04(config)# https-server vrf default sw-25g04(config)# https-server vrf mgmt sw-25g04(config)# https-server rest access-mode read-write   View the running configuration.\nThe configuration should look similar to the following:\nsw-25g04(config)# show running Current configuration: ! !Version ArubaOS-CX Virtual.10.05.0020 !export-password: default hostname sw-25g04 user admin group administrators password ciphertext AQBapXDwaGq+GHwyLgj0Eu led locator on ! ! ! ! ssh server vrf default ssh server vrf mgmt vlan 1 interface mgmt no shutdown ip dhcp interface 1/1/1 no shutdown no routing vlan trunk native 1 vlan trunk allowed all interface loopback 0 ipv6 address fd01::1/64 ipv6 ospfv3 1 area 0.0.0.0 interface loopback 1 interface vlan 1 ipv6 address autoconfig ipv6 ospfv3 1 area 0.0.0.0 ! ! ! ! ! router ospfv3 1 router-id 192.168.100.1 area 0.0.0.0 https-server vrf default https-server vrf mgmt   Verify there are now OSPFv3 neighbors.\nsw-25g04# show ipv6 ospfv3 neighbors OSPFv3 Process ID 1 VRF default ================================ Total Number of Neighbors: 1 Neighbor ID Priority State Interface ------------------------------------------------------- 192.168.100.2 1 FULL/BDR vlan1 Neighbor address fe80::800:901:8b4:e152   Connect to the neighbors via the IPv6 loopback that was set earlier.\nsw-25g03# ssh admin@fd01::1   "
},
{
	"uri": "/docs-csm/en-11/install/configure_aruba_leaf_switch/",
	"title": "Configure Aruba Leaf Switch",
	"tags": [],
	"description": "",
	"content": "Configure Aruba Leaf Switch This page describes how Aruba leaf switches are configured.\nLeaf switches are located in air-cooled cabinets and provide connectivity to components in those cabinets. Aruba JL762A 6300M 48G 4SFP56 is the model used.\nPrerequisites  Connectivity to the switch is established. The Configure Aruba Management Network Base procedure has been run. There are two uplinks from the switch to the upstream switch; this is can be an aggregation switch or a spine switch.  Here are example snippets from a leaf switch in the SHCD.\n   Source Source Label Info Destination Label Info Destination Description     sw-smn01 x3000u40-j49 x3105u38-j47 sw-25g01 25g-15m-LC-LC   sw-smn01 x3000u40-j50 x3105u39-j47 sw-25g02 25g-15m-LC-LC    The uplinks are port 49 and 50 on the leaf. They connect to Aggregation switch 1 and 2 on port 47.\nConfigure Uplink The uplink ports are the ports connecting the leaf switches to the upstream switch.\n  Create the LAG.\nsw-leaf-001(config)# interface lag 99 no shutdown no routing vlan trunk native 1 vlan trunk allowed all lacp mode active exit   Add ports to the LAG.\nsw-leaf-001(config)# interface 1/1/49 - 1/1/50 no shutdown mtu 9198 lag 99 exit   Configure VLAN Cray Site Init (CSI) generates the IP addresses used by the system, below are samples only. The VLAN information is located in the network YAML files. Below are examples.\n  View the VLAN interfaces for the leaf switches in the Node Management Network (NMN) and the Hardware Management Network (HMN).\nExample of NMN.yaml:\npit# cat /var/www/ephemeral/prep/${SYSTEM_NAME}/networks/NMN.yaml SNIPPET - ip_address: 10.252.0.4 name: sw-leaf-001 comment: x3000c0w14 aliases: [] name: network_hardware net-name: NMN vlan_id: 2 comment: \u0026#34; gateway: 10.252.0.1 Example of HMN.yaml:\npit# cat /var/www/ephemeral/prep/${SYSTEM_NAME}/networks/HMN.yaml SNIPPET - ip_address: 10.254.0.4 name: sw-leaf-001 comment: x3000c0w14 aliases: [] name: network_hardware net-name: HMN vlan_id: 4 comment: \u0026#34; gateway: 10.254.0.1   Configure NMN VLAN.\nsw-leaf-001(config)# vlan 2 interface vlan2 description RIVER_NMN ip address 10.252.0.4/17 exit   Configure HMN VLAN.\nsw-leaf-001(config)# vlan 4 interface vlan4 description RIVER_HMN ip address 10.254.0.4/17 exit   Configure SNMP SNMP configuration is required for hardware discovery of the HPE Cray EX system.\n  Configure SNMP.\nsnmp-server vrf default snmpv3 user testuser auth md5 auth-pass plaintext testpass1 priv des priv-pass plaintext testpass2   Configure ACL These ACLs are designed to block traffic from the NMN to and from the HMN.\n  Create the access list.\nNOTE: The following are only examples. The IP addresses below need to match what was generated by CSI.\nsw-leaf-001(config)# access-list ip nmn-hmn 10 deny any 10.252.0.0/255.255.128.0 10.254.0.0/255.255.128.0 20 deny any 10.252.0.0/255.255.128.0 10.104.0.0/255.252.0.0 30 deny any 10.254.0.0/255.255.128.0 10.252.0.0/255.255.128.0 40 deny any 10.254.0.0/255.255.128.0 10.100.0.0/255.252.0.0 50 deny any 10.100.0.0/255.252.0.0 10.254.0.0/255.255.128.0 60 deny any 10.100.0.0/255.252.0.0 10.104.0.0/255.252.0.0 70 deny any 10.104.0.0/255.252.0.0 10.252.0.0/255.255.128.0 80 deny any 10.104.0.0/255.252.0.0 10.100.0.0/255.252.0.0 90 permit any any any   Apply ACL to VLANs.\nsw-leaf-001(config)# vlan 2 name RVR_NMN apply access-list ip nmn-hmn in apply access-list ip nmn-hmn out vlan 4 name RVR_HMN apply access-list ip nmn-hmn in apply access-list ip nmn-hmn out   Configure Spanning-Tree The following configuration is applied to Aruba leaf/Aggregation switches.\n  Configure spanning-tree.\nsw-leaf-001(config)# spanning-tree mode rpvst spanning-tree spanning-tree vlan 1,2,4   Configure OSPF OSPF is a dynamic routing protocol used to exchange routes. It provides reachability from the leaf switch to k8s The router-id used here is the NMN IP address. (VLAN 2 IP)\n  Configure OSPF.\nsw-leaf-001(config)# router ospf 1 router-id 10.252.0.x interface vlan2 ip ospf 1 area 0.0.0.2 interface vlan4 ip ospf 1 area 0.0.0.4   Configure NTP The IP addresses used to configure NTP are the first three worker nodes on the NMN. These can be found in the NMN.yaml file.\n  Configure NTP.\nsw-leaf-001(config)# ntp server 10.252.1.7 ntp server 10.252.1.8 ntp server 10.252.1.9 ntp enable   Configure DNS   Configure DNS.\nThis will point to the unbound DNS server.\nsw-leaf-001(config)# ip dns server-address 10.92.100.225   Configure Edge port   Configure the ports that need to be on the HMN, including BMCs/PDUs.\nsw-leaf-001(config)# interface 1/1/35 no shutdown no routing vlan access 4 spanning-tree bpdu-guard spanning-tree port-type admin-edge   Configure ports that need to be on the NMN, including air-cooled compute nodes.\nsw-leaf-001(config)# interface 1/1/35 no shutdown no routing vlan access 2 spanning-tree bpdu-guard spanning-tree port-type admin-edge   Configure Apollo Server Port   Configure iLO BMC port.\nsw-leaf-001(config)# interface 1/1/46 no shutdown no routing vlan trunk native 1 vlan trunk allowed 4 spanning-tree bpdu-guard spanning-tree port-type admin-edge exit   Configure NMN port from OCP card.\ninterface 1/1/14 no shutdown no routing vlan access 2 spanning-tree bpdu-guard spanning-tree port-type admin-edge exit   Save Configuration To save the configuration:\nsw-leaf-001(config)# exit sw-leaf-001# write memory Show Running Configuration To display the running configuration:\nsw-leaf-001# show running-config "
},
{
	"uri": "/docs-csm/en-11/install/configure_aruba_cdu_switch/",
	"title": "Configure Aruba Cdu Switch",
	"tags": [],
	"description": "",
	"content": "Configure Aruba CDU Switch This page describes how Aruba CDU switches are configured.\nCDU switches are located in liquid-cooled cabinets and provide connectivity to MTN (Mountain) components. CDU switches act as leaf switches in the architecture. Aruba JL720A 8360-48XT4 is the model used. They run in a high availability pair and use VSX to provide redundancy.\nPrerequisites  There are two uplinks from each CDU switch to the upstream switch. This is normally a spine switch. Three connections exist between the switches, two of these are used for the Inter switch link (ISL), and one used for the keepalive. Connectivity to the switch is established. The Configure Aruba Management Network Base procedure has been run. The ISL uses 100GB ports and the keepalive uses a 10GB port.  Here are example snippets from a CDU switch in the SHCD.\nThe uplinks are port 51 and 52 on both CDU switches. These go to sw-100g01 and sw-100g2 which are spine 1 and 2.\nThe ISL are ports 49 and 50 on both CDU switches.\nThe Keepalive is port 48.\nNOTE: These are examples only. The installation and cabling may vary. This information is on the 25G_10G tab of the SHCD spreadsheet.\n   Source Source Label Info Destination Label Info Destination Description Notes     sw-100g01 x3105u40-j01 d100u01-j51 d100sw1 100g-30m-AOC    sw-100g02 x3105u41-j01 d100u01-j52 d100sw1 100g-30m-AOC    sw-100g01 x3105u40-j02 d100u02-j51 d100sw2 100g-30m-AOC    sw-100g02 x3105u41-j02 d100u02-j52 d100sw2 100g-30m-AOC    d100sw1 d100u01-j48 d100u02-j48 d100sw2 6ft keepalive   d100sw1 d100u01-j49 d100u02-j49 d100sw2 100g-1m-DAC    d100sw1 d100u01-j50 d100u02-j50 d100sw2 100g-1m-DAC     Configure VSX   Create the keepalive vrf on both switches.\nsw-cdu-001 \u0026amp; sw-cdu-002 (config)# vrf keepalive   Setup the keepalive link.\nThis will require a unique IP address on both switches. The IP address is in its own VRF so this address will not be reachable from anywhere besides the CDU pair.\nsw-cdu-001(config)# int 1/1/48 no shutdown mtu 9198 vrf attach keepalive description VSX keepalive ip address 192.168.255.0/31 sw-cdu-002(config)# int 1/1/48 no shutdown mtu 9198 vrf attach keepalive description VSX keepalive ip address 192.168.255.1/31   Create the ISL lag on both switches.\nsw-cdu-001 \u0026amp; sw-cdu-002 (config)# interface lag 99 no shutdown description ISL link no routing vlan trunk native 1 tag vlan trunk allowed all lacp mode active   Add the ISL ports to the LAG.\nThese are two of the ports connected between the switches.\nsw-cdu-001 \u0026amp; sw-cdu-002 (config)# int 1/1/49-1/1/50 no shutdown mtu 9198 lag 99   Create the VSX instance and setup the keepalive link.\nSW-CDU-001(config)# no ip icmp redirect vsx system-mac 02:01:00:00:01:00 inter-switch-link lag 99 role primary keepalive peer 192.168.255.1 source 192.168.255.0 vrf keepalive linkup-delay-timer 600 vsx-sync vsx-global SW-CDU-002(config)# no ip icmp redirect vsx system-mac 02:01:00:00:01:00 inter-switch-link lag 99 role secondary keepalive peer 192.168.255.0 source 192.168.255.1 vrf keepalive linkup-delay-timer 600 vsx-sync vsx-global   Verify there is an Established VSX session.\nSW-CDU-001 # show vsx brief ISL State : In-Sync Device State : Sync-Primary Keepalive State : Keepalive-Established Device Role : secondary Number of Multi-chassis LAG interfaces : 0   Configure Uplink The uplink ports are the ports connecting the CDU switches to the upstream switch, most likely a spine switch.\n  Create the LAG.\nsw-cdu-001 \u0026amp; sw-cdu-002 (config)# interface lag 99 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed all lacp mode active exit   Add ports to the LAG.\nsw-cdu-001 \u0026amp; sw-cdu-002 (config)# interface 1/1/51 - 1/1/52 no shutdown mtu 9198 lag 99 exit   Configure VLAN Cray Site Init (CSI) generates the IP addresses used by the system, below are samples only. The VLAN information is located in the network YAML files. The following are examples.\n  Configure the VLAN interfaces in for the NMN, HMN, NMN_MTN, and HMN_MTN on the CDU switches.\nNMN configuration:\npit# cat /var/www/ephemeral/prep/${SYSTEM_NAME}/networks/NMN.yaml SNIPPET - ip_address: 10.252.0.5 name: sw-cdu-001 comment: d0w1 aliases: [] - ip_address: 10.252.0.6 name: sw-cdu-002 comment: d0w2 aliases: [] name: network_hardware net-name: NMN vlan_id: 2 comment: \u0026#34; gateway: 10.252.0.1 HMN configuration:\npit# cat /var/www/ephemeral/prep/${SYSTEM_NAME}/networks/HMN.yaml SNIPPET - ip_address: 10.254.0.5 name: sw-cdu-001 comment: d0w1 aliases: [] - ip_address: 10.254.0.6 name: sw-cdu-002 comment: d0w2 aliases: [] name: network_hardware net-name: HMN vlan_id: 4 comment: \u0026#34; gateway: 10.254.0.1 NMN_MTN configuration:\npit# cat /var/www/ephemeral/prep/${SYSTEM_NAME}/networks/NMN_MTN.yaml full_name: Mountain Node Management Network cidr: 10.100.0.0/17 subnets: - full_name: \u0026#34; cidr: ip: 10.100.0.0 mask: - 255 - 255 - 252 - 0 ip_reservations: [] name: cabinet_9000 net-name: \u0026#34; vlan_id: 2000 comment: \u0026#34; gateway: 10.100.0.1 _: \u0026#34; dns_server: \u0026#34; iprange-start: 10.100.0.10 iprange-end: 10.100.3.254 name: NMN_MTN HMN_MTN configuration:\npit# cat /var/www/ephemeral/prep/${SYSTEM_NAME}/networks/HMN_MTN.yaml full_name: Mountain Hardware Management Network cidr: 10.104.0.0/17 subnets: - full_name: \u0026#34; cidr: ip: 10.104.0.0 mask: - 255 - 255 - 252 - 0 ip_reservations: [] name: cabinet_9000 net-name: \u0026#34; vlan_id: 3000 comment: \u0026#34; gateway: 10.104.0.1 _: \u0026#34; dns_server: \u0026#34; iprange-start: 10.104.0.10 iprange-end: 10.104.3.254 name: HMN_MTN NOTE: CSI does not yet generate IP addresses for the CDU switches on VLANs HMN_MTN and NMN_MTN.\n The first CDU switch in the pair will always have an IP address ending in .2 on the HMN_MTN and NMN_MTN networks. The second CDU switch in the pair will always have an IP address ending in .3 on the HMN_MTN and NMN_MTN networks. Both CDU MTN VLAN IP addresses will be at the beginning of the subnet. The gateway will always end in .1 and will be at the beginning of the subnet. Every Mountain Cabinet will get its own HMN and NMN VLAN.  The following is an example of CDU switch IP addressing based on the network .yaml files from above:\n   VLAN CDU1 CDU2 Purpose     2 10.252.0.5/17 10.252.0.6/17 River Node Management   4 10.254.0.5/17 10.254.0.6/17 River Hardware Management   2000 10.100.0.2/22 10.100.0.3/22 Mountain Node Management   3000 10.104.0.2/22 10.104.0.3/22 Mountain Hardware Management    If the system has additional Mountain Cabinets the VLANs will look like the following. This is an example of a system with 3 cabinets.\n   VLAN CDU1 CDU2 Purpose     2000 10.100.0.2/22 10.100.0.3/22 Mountain Node Management   3000 10.104.0.2/22 10.104.0.3/22 Mountain Hardware Management   2001 10.100.4.2/22 10.100.4.3/22 Mountain Node Management   3001 10.104.4.2/22 10.104.4.3/22 Mountain Hardware Management   2002 10.100.8.2/22 10.100.8.3/22 Mountain Node Management   3002 10.104.8.2/22 10.104.8.3/22 Mountain Hardware Management      View the output of the SHCD to prepare to configure the VLANs on the switches.\nThe components in the x1000 cabinet would get their own NMN and HMN VLAN, and components in the x1001 would also get their own NMN and HMN VLAN. The CECs will be on the HMN VLAN of that cabinet.\nFor example:\n  Configure the VLANs on the switches.\nNMN VLAN configuration:\nsw-cdu-001(config)# vlan 2 interface vlan2 ip address 10.252.0.5/17 ip mtu 9198 exit sw-cdu-002(config)# vlan 2 interface vlan2 ip address 10.252.0.6/17 ip mtu 9198 exit HMN VLAN configuration:\nsw-cdu-001(config)# vlan 4 interface vlan4 ip address 10.254.0.5/17 ip mtu 9198 exit sw-cdu-002(config)# vlan 4 interface vlan4 ip address 10.254.0.6/17 ip mtu 9198 exit NMN MTN VLAN configuration:\nsw-cdu-001(config)# vlan 2000 interface vlan2000 description CAB_9000_MTN_NMN ip address 10.100.0.2/22 active-gateway ip mac 02:01:00:00:01:02 active-gateway ip 10.100.0.1 ipv6 address autoconfig ip helper-address 10.92.100.222 exit sw-cdu-002(config)# vlan 2000 interface vlan2000 description CAB_9000_MTN_NMN ip address 10.100.0.3/22 active-gateway ip mac 02:01:00:00:01:02 active-gateway ip 10.100.0.1 ipv6 address autoconfig ip helper-address 10.92.100.222 exit HMN MTN VLAN configuration:\nsw-cdu-001(config)# vlan 3000 interface vlan3000 description CAB_9000_MTN_HMN ip address 10.104.0.2/22 active-gateway ip mac 02:01:00:00:01:02 active-gateway ip 10.104.0.1 ipv6 address autoconfig ip helper-address 10.94.100.222 exit sw-cdu-002(config)# vlan 3000 interface vlan3000 description CAB_9000_MTN_HMN ip address 10.104.0.3/22 active-gateway ip mac 02:01:00:00:01:02 active-gateway ip 10.104.0.1 ipv6 address autoconfig ip helper-address 10.94.100.222 exit   Configure ACL ACLs are designed to block traffic from the NMN to and from the HMN.\n  Create the access list.\nNOTE: These following are examples only; the IP addresses below need to match what was generated by CSI.\nsw-cdu-001 \u0026amp; sw-cdu-002 (config)# access-list ip nmn-hmn 10 deny any 10.252.0.0/255.255.128.0 10.254.0.0/255.255.128.0 20 deny any 10.252.0.0/255.255.128.0 10.104.0.0/255.252.0.0 30 deny any 10.254.0.0/255.255.128.0 10.252.0.0/255.255.128.0 40 deny any 10.254.0.0/255.255.128.0 10.100.0.0/255.252.0.0 50 deny any 10.100.0.0/255.252.0.0 10.254.0.0/255.255.128.0 60 deny any 10.100.0.0/255.252.0.0 10.104.0.0/255.252.0.0 70 deny any 10.104.0.0/255.252.0.0 10.252.0.0/255.255.128.0 80 deny any 10.104.0.0/255.252.0.0 10.100.0.0/255.252.0.0 90 permit any any any   Apply ACL to VLANs.\nsw-cdu-001 \u0026amp; sw-cdu-002 (config)# vlan 2 apply access-list ip nmn-hmn in apply access-list ip nmn-hmn out vlan 4 apply access-list ip nmn-hmn in apply access-list ip nmn-hmn out vlan 2000 apply access-list ip nmn-hmn in apply access-list ip nmn-hmn out vlan 3000 apply access-list ip nmn-hmn in apply access-list ip nmn-hmn out   Configure Spanning-Tree   Apply the following configuration to the Aruba CDU switches.\nIf there are more 2xxx or 3xxx VLANs, add them to the spanning-tree vlan list.\nsw-cdu-001 \u0026amp; sw-cdu-002 (config)# spanning-tree mode rpvst spanning-tree spanning-tree vlan 1,2,4,2000,3000   Verify that each CDU switch is the root bridge for VLANs 2xxx and 3xxx.\n  Configure OSPF   Configure OSPF.\nOSPF is a dynamic routing protocol used to exchange routes. It provides reachability from the MTN networks to NMN/Kubernetes networks. The router-id used here is the NMN IP address (VLAN 2 IP).\nsw-cdu-001 \u0026amp; sw-cdu-002 (config)# router ospf 1 router-id 10.252.0.x interface vlan2 ip ospf 1 area 0.0.0.2 interface vlan4 ip ospf 1 area 0.0.0.4 interface vlan2000 ip ospf 1 area 0.0.0.2 ip ospf passive interface vlan3000 ip ospf 1 area 0.0.0.4 ip ospf passive   Configure NTP   Configure NTP.\nThe IP addresses used are the first three worker nodes on the NMN network. These can be found in NMN.yaml.\nsw-cdu-001 \u0026amp; sw-cdu-002 (config)# ntp server 10.252.1.7 ntp server 10.252.1.8 ntp server 10.252.1.9 ntp enable   Configure DNS   Configure DNS to point to the unbound DNS server.\nsw-cdu-001 \u0026amp; sw-cdu-002 (config)# ip dns server-address 10.92.100.225   Configure Downlink Port Configure LAG for CMMs  This requires updated CMM firmware (version 1.4.20 or later). See Update Firmware with FAS for details on updating CMM firmware with FAS. A static LAG will be configured on the CDU switches. The CDU switches have two cables (10Gb RJ45) connecting to each CMM. This configuration offers increased throughput and redundancy. The CEC will not need to be programmed in order to support the LAG configuration as it was required in previous versions. The updated firmware takes care of this.    Configure ports going to CMM switches.\nThe VLANs used are the cabinet VLANs that are generated from CSI. The Description should be changed to match the cabinet number.\nsw-cdu-001 \u0026amp; sw-cdu-002 (config)# interface lag 1 multi-chassis static no shutdown description CMM_CAB_9000 no routing vlan trunk native 2000 vlan trunk allowed 2000,3000,4091 interface 1/1/1 no shutdown mtu 9198 lag 1 exit   Configure ports going to CECs.\nsw-cdu-001 \u0026amp; sw-cdu-002 (config)# interface 1/1/1 no shutdown mtu 9198 description cec1 no routing vlan access 3000 spanning-tree bpdu-guard spanning-tree port-type admin-edge   Save Configuration To save the configuration:\nsw-cdu-001(config)# exit sw-cdu-001# write memory Show Running Configuration To display the running configuration:\nsw-cdu-001# show running-config "
},
{
	"uri": "/docs-csm/en-11/install/configure_aruba_aggregation_switch/",
	"title": "Configure Aruba Aggregation Switch",
	"tags": [],
	"description": "",
	"content": "Configure Aruba Aggregation Switch This page describes how Aruba aggregation switches are configured.\nManagement nodes and Application nodes will be plugged into aggregation switches.\nSwitch models used: JL635A Aruba 8325-48Y8C\nThey run in a high availability pair and use VSX to provide redundancy.\nPrerequisites   Three connections between the switches, two of these are used for the Inter switch link (ISL), and one used for the keepalive.\n  The ISL uses 100GB ports and the keepalive will be a 25 GB port.\nHere is an example snippet from an aggregation switch on the 25G_10G tab of the SHCD spreadsheet.\n   Source Source Label Info Destination Label Info Destination Description Notes     sw-25g01 x3105u38-j49 x3105u39-j49 sw-25g02 100g-1m-DAC    sw-25g01 x3105u38-j50 x3105u39-j50 sw-25g02 100g-1m-DAC    sw-25g01 x3105u38-j53 x3105u39-j53 sw-25g02 100g-1m-DAC keepalive      Connectivity to the switch is established.\n  The Configure Aruba Management Network Base procedure has been run.\n  Configure VSX   Create the keepalive VRF on both switches.\nsw-agg-001 \u0026amp; sw-agg-002 (config)# vrf keepalive   Set up the keepalive link.\nThis will require a unique IP address on both switches. The IP address is in its own VRF so this address will not be reachable from anywhere besides the aggregation pair.\nsw-agg-001(config)# int 1/1/48 no shutdown vrf attach keepalive description VSX keepalive ip address 192.168.255.0/31 sw-agg-002(config)# int 1/1/48 no shutdown vrf attach keepalive description VSX keepalive ip address 192.168.255.1/31   Create the ISL lag on both switches.\nsw-agg-001 \u0026amp; sw-agg-002 (config)# interface lag 99 no shutdown description ISL link no routing vlan trunk native 1 tag vlan trunk allowed all lacp mode active   Add the ISL ports to the LAG.\nThese are two of the ports connected between the switches.\nsw-agg-001 \u0026amp; sw-agg-002 (config)# int 1/1/31-1/1/32 no shutdown mtu 9198 lag 99   Create the VSX instance and setup the keepalive link.\nsw-agg-001(config)# no ip icmp redirect vsx system-mac 02:01:00:00:01:00 inter-switch-link lag 99 role primary keepalive peer 192.168.255.1 source 192.168.255.0 vrf keepalive linkup-delay-timer 600 vsx-sync vsx-global sw-agg-002(config)# no ip icmp redirect vsx system-mac 02:01:00:00:01:00 inter-switch-link lag 99 role secondary keepalive peer 192.168.255.0 source 192.168.255.1 vrf keepalive linkup-delay-timer 600 vsx-sync vsx-global   Verify there is an Established VSX session.\nsw-agg-001 # show vsx brief ISL State : In-Sync Device State : Sync-Primary Keepalive State : Keepalive-Established Device Role : secondary Number of Multi-chassis LAG interfaces : 0   Configure VLAN Cray Site Init (CSI) generates the IP addresses used by the system, below are samples only. The VLAN information is located in the network YAML files. The following are examples.\n  Check that the aggregation switches have VLAN interfaces in the Node Management Network (NMN) and Hardware Management Network (HMN).\nExample NMN.yaml:\npit# cat /var/www/ephemeral/prep/${SYSTEM_NAME}/networks/NMN.yaml SNIPPET - ip_address: 10.252.0.4 name: sw-agg-001 comment: x3000c0h12s1 aliases: [] - ip_address: 10.252.0.5 name: sw-agg-002 comment: x3000c0h13s1 aliases: [] name: network_hardware net-name: NMN vlan_id: 2 comment: \u0026#34; gateway: 10.252.0.1 Example HMN.yaml:\npit# cat /var/www/ephemeral/prep/${SYSTEM_NAME}/networks/HMN.yaml SNIPPET - ip_address: 10.254.0.4 name: sw-agg-001 comment: x3000c0h12s1 aliases: [] - ip_address: 10.254.0.5 name: sw-agg-002 comment: x3000c0h13s1 aliases: [] name: network_hardware net-name: HMN vlan_id: 4 comment: \u0026#34; gateway: 10.254.0.1 The following is an example of aggregation switch IP addressing based on the network .yaml files above:\n   VLAN Agg01 Agg02 Purpose     2 10.252.0.4/17 10.252.0.5/17 River Node Management   4 10.254.0.4/17 10.254.0.5/17 River Hardware Management      Configure the NMN VLAN.\nsw-agg-001(config)# vlan 2 interface vlan2 vsx-sync active-gateways ip address 10.252.0.2/17 ip mtu 9198 exit sw-agg-002(config)# vlan 2 interface vlan2 ip address 10.252.0.4/17 ip mtu 9198 exit   Configure the HMN VLAN.\nsw-agg-001(config)# vlan 4 interface vlan4 vsx-sync active-gateways ip address 10.254.0.4/17 ip mtu 9198 exit sw-agg-002(config)# vlan 4 interface vlan4 vsx-sync active-gateways ip address 10.254.0.5/17 ip mtu 9198 exit   Configure Uplink The uplink ports are the ports connecting the aggregation switches to the spine switches.\n  Create the LAG.\nsw-agg-001 \u0026amp; sw-agg-002 (config)# interface lag 1 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed all lacp mode active exit   Add ports to the LAG.\nsw-agg-001 \u0026amp; sw-agg-002 (config)# interface 1/1/1 - 1/1/2 no shutdown mtu 9198 lag 1 exit   Configure ACL These ACLs are designed to block traffic from the NMN to and from the HMN.\nOne port is shutdown.\n  Create the access list.\nNOTE: The following are examples only. The IP addresses below need to match what was generated by CSI.\nsw-agg-001 \u0026amp; sw-agg-002 (config)# access-list ip nmn-hmn 10 deny any 10.252.0.0/255.255.128.0 10.254.0.0/255.255.128.0 20 deny any 10.252.0.0/255.255.128.0 10.104.0.0/255.252.0.0 30 deny any 10.254.0.0/255.255.128.0 10.252.0.0/255.255.128.0 40 deny any 10.254.0.0/255.255.128.0 10.100.0.0/255.252.0.0 50 deny any 10.100.0.0/255.252.0.0 10.254.0.0/255.255.128.0 60 deny any 10.100.0.0/255.252.0.0 10.104.0.0/255.252.0.0 70 deny any 10.104.0.0/255.252.0.0 10.252.0.0/255.255.128.0 80 deny any 10.104.0.0/255.252.0.0 10.100.0.0/255.252.0.0 90 permit any any any   Apply ACL to VLANs.\nsw-cdu-001 \u0026amp; sw-cdu-002 (config)# vlan 2 apply access-list ip nmn-hmn in apply access-list ip nmn-hmn out vlan 4 apply access-list ip nmn-hmn in apply access-list ip nmn-hmn out vlan 2000 apply access-list ip nmn-hmn in apply access-list ip nmn-hmn out vlan 3000 apply access-list ip nmn-hmn in apply access-list ip nmn-hmn out   Configure Spanning-Tree The following configuration is applied to Aruba aggregation switches:\nsw-agg-001 \u0026amp; sw-agg-002 (config)# spanning-tree mode rpvst spanning-tree spanning-tree priority 7 spanning-tree vlan 1,2,4,7 Configure OSPF OSPF is a dynamic routing protocol used to exchange routes. It provides reachability from the MTN networks to NMN/Kubernetes networks. The router-id used here is the NMN IP address. (VLAN 2 IP)\nsw-agg-001 \u0026amp; sw-agg-002 (config)# router ospf 1 router-id 10.252.0.x interface vlan2 ip ospf 1 area 0.0.0.2 interface vlan4 ip ospf 1 area 0.0.0.4 Configure NTP The IP addresses used are be the first three worker nodes on the NMN network. These can be found in NMN.yaml.\nsw-agg-001 \u0026amp; sw-agg-002 (config)# ntp server 10.252.1.7 ntp server 10.252.1.8 ntp server 10.252.1.9 ntp enable Configure DNS The following will point to the unbound DNS server.\nsw-agg-001 \u0026amp; sw-agg-002 (config)# ip dns server-address 10.92.100.225 Configure Edge Port These are ports that are connected to management nodes.\n  Set the worker node and master node configuration.\nRefer to Cable Management Network Servers for cabling specs.\nsw-agg-001 \u0026amp; sw-agg-002 (config)# interface lag 4 multi-chassis no shutdown description w001 no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge exit sw-agg-001 \u0026amp; sw-agg-002 (config)# interface 1/1/7 no shutdown mtu 9198 lag 4 exit   Set the Aruba Storage port configuration (future use).\nThese will be configured, but the ports will be shut down until needed. These are OCP and PCIe port 2 on storage nodes.\nsw-agg-001 \u0026amp; sw-agg-002 (config)# interface 1/1/7 shutdown mtu 9198 lag 4 exit   Set the Aruba LAG configuration.\nsw-agg-001 \u0026amp; sw-agg-002 (config)# interface lag 4 multi-chassis shutdown no routing vlan access 10 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge exit   Configure User Access/Login/Application Node Port  One connection will go to a NMN (VLAN2) access port, which is where the UAN will PXE boot and communicate with internal nodes (see SHCD for UAN cabling). One Bond (two connections) will be going to the MLAG/VSX pair of switches. This will be a trunk port for the CAN connection.    Set the Aruba UAN NMN configuration.\nOne port is shutdown.\nsw-agg-001 (config)# interface 1/1/16 no shutdown mtu 9198 no routing vlan access 2 spanning-tree bpdu-guard spanning-tree port-type admin-edge exit sw-agg-002 (config)# interface 1/1/16 shutdown mtu 9198 no routing vlan access 2 spanning-tree bpdu-guard spanning-tree port-type admin-edge exit   Set the Aruba UAN CAN configuration.\nPort Configuration is the same on both switches.\nsw-agg-001 (config)# interface 1/1/16 no shutdown mtu 9198 no routing vlan access 2 spanning-tree bpdu-guard spanning-tree port-type admin-edge exit sw-agg-002 (config)# interface 1/1/16 shutdown mtu 9198 no routing vlan access 2 spanning-tree bpdu-guard spanning-tree port-type admin-edge exit sw-agg-001 \u0026amp; sw-agg-002 (config)# interface 1/1/17 no shutdown mtu 9198 lag 17   Save Configuration To save a configuration:\nsw-agg-001(config)# exit sw-agg-001# write memory Show Running Configuration To show the currently running configuration:\nsw-agg-001# show running-config "
},
{
	"uri": "/docs-csm/en-11/install/configure_administrative_access/",
	"title": "Configure Administrative Access",
	"tags": [],
	"description": "",
	"content": "Configure Administrative Access There are several operations which configure administrative access to different parts of the system. Ensuring that the cray CLI can be used by administrative credentials enables use of many management services via commands. The management nodes can be locked from accidental manipulation by the cray capmc and cray fas commands when the intent is to work on the entire system except the management nodes. The cray scsd command can change the SSH keys, NTP server, syslog server, and BMC/controller passwords.\nTopics:  Configure Keycloak Account Configure the Cray Command Line Interface (cray CLI) Lock Management Nodes Configure BMC and Controller Parameters with SCSD Configure Non-compute Nodes with CFS Upload Olympus BMC Recovery Firmware into TFTP server Next Topic  Note: The procedures in this section of installation documentation are intended to be done in order, even though the topics are administrative or operational procedures. The topics themselves do not have navigational links to the next topic in the sequence.\nDetails \n  Configure Keycloak Account\nUpcoming steps in the installation workflow require an account to be configured in Keycloak for authentication. This can be either a local keycloak account or an external Identity Provider (IdP), such as LDAP. Having an account in keycloak with administrative credentials enables the use of many management services via the cray command.\nSee Configure Keycloak Account \n  Configure the Cray Command Line Interface (cray CLI)\nThe cray command line interface (CLI) is a framework created to integrate all of the system management REST APIs into easily usable commands.\nLater procedures in the installation workflow use the cray command to interact with multiple services. The cray CLI configuration needs to be initialized for the Linux account. The Keycloak user who initializes the CLI configuration needs to be authorized for administrative actions.\nSee Configure the Cray Command Line Interface (cray CLI) \n  Lock Management Nodes\nThe management nodes are unlocked at this point in the installation. Locking them will prevent actions from FAS to update their firmware or CAPMC to power off or do a power reset. Doing any of these by accident will take down a management node. If the management node is a Kubernetes master or worker node, this can have serious negative effects on system operation.\nIf a single node is taken down by mistake, it is possible that things will recover. However, if all management nodes are taken down, or all Kubernetes worker nodes are taken down by mistake, the system is dead and has to be completely restarted. Lock the management nodes now!\nSee Lock and Unlock Nodes \n  Configure BMC and Controller Parameters with SCSD\nNote: If there are no liquid-cooled cabinets present in the HPE Cray EX system, then this step can be skipped.\nThe System Configuration Service (SCSD) allows administrators to set various BMC and controller parameters for components in liquid-cooled cabinets. At this point in the install, SCSD should be used to set the SSH key in the node controllers (BMCs) to enable troubleshooting. If any of the nodes fail to power down or power up as part of the compute node booting process, it may be necessary to look at the logs on the BMC for node power down or node power up.\nSee Configure BMC and Controller Parameters with SCSD \n  Configure Non-compute Nodes with CFS\nNon-compute Nodes (NCN) need to be configured after booting for administrative access, security, and other purposes. The Configuration Framework Service (CFS) is used to apply post-boot configuration in a decoupled, layered manner. Individual software products including CSM provide one or more layers of configuration in a process called \u0026ldquo;NCN personalization\u0026rdquo;.\nSee Configure Non-Compute Nodes with CFS \n  Upload Olympus BMC Recovery Firmware into TFTP server\nThe Olympus hardware (NodeBMCs, ChassisBMCs, RouterBMCs) needs to have recovery firmware loaded to the cray-tftp server in case the BMC loses its firmware. The BMCs are configured to load a recovery firmware from a TFTP server. This procedure does not modify any BMC firmware, but only stages the firmware on the TFTP server for download in the event it is needed.\nSee Load Olympus BMC Recovery Firmware into TFTP server \n  Next Topic\nAfter completing the operational procedures above which configure administrative access, the next step is to validate the health of management nodes and CSM services.\nSee Validate CSM Health\n  "
},
{
	"uri": "/docs-csm/en-11/install/collecting_ncn_mac_addresses/",
	"title": "Collecting NCN Mac Addresses",
	"tags": [],
	"description": "",
	"content": "Collecting NCN MAC Addresses This procedure will detail how to collect the NCN MAC addresses from an HPE Cray EX system. The MAC addresses needed for the Bootstrap MAC, Bond0 MAC0, and Bond0 MAC1 columns in ncn_metadata.csv will be collected.\nThe Bootstrap MAC address will be used for identification of this node during the early part of the PXE boot process before the bonded interface can be established.\nThe Bond0 MAC0 and Bond0 MAC1 are the MAC addresses for the physical interfaces that the node will use for the various VLANs. The Bond0 MAC0 and Bond0 MAC1 should be on the different network cards to establish redundancy for a failed network card. On the other hand, if the node has only a single network card, then MAC1 and MAC0 will still produce a valid configuration if they do reside on the same physical card.\nSections  Collecting NCN MAC Addresses - Sections  Procedure: iPXE Consoles  Requirements MAC Collection   Procedure: Serial Consoles Procedure: Recovering from an incorrect ncn_metadata.csv file    The easy way to do this leverages the NIC-dump provided by the metal-ipxe package. This page will walk-through booting NCNs and collecting their MACs from the ConMan console logs.\n The alternative is to use serial cables (or SSH) to collect the MACs from the switch ARP tables, this can become exponentially difficult for large systems. If this is the only way, please proceed to the bottom of this page.\n Procedure: iPXE Consoles This procedure is faster for those with the LiveCD (CRAY Pre-Install Toolkit). It can be used to quickly boot-check nodes to dump network device information without an operating system. This works by accessing the PCI Configuration Space.\nRequirements  If CSI does not work because of a file requirement, please file a ticket. By default, dnsmasq and ConMan are already running on the LiveCD, but bond0 needs to be configured. dnsmasq needs to serve/listen over bond0, and ConMan needs the BMC information.\n  LiveCD dnsmasq is configured for the bond0/metal network (NMN/HMN/CAN do not matter) BMC MAC addresses already collected LiveCD ConMan is configured for each BMC  For help with either of those, see LiveCD Setup.\nMAC Collection   (Optional) Shim the boot so nodes bail after dumping their network devices.\nRemoving the iPXE script will prevent network booting. Be aware that the nodes may disk boot.\nThis will prevent the nodes from continuing to boot and end in undesired states.\npit# mv /var/www/boot/script.ipxe /var/www/boot/script.ipxe.bak   Verify consoles are active with conman -q. The following command lists all nodes that ConMan is configured for,\npit# conman -q ncn-m002-mgmt ncn-m003-mgmt ncn-s001-mgmt ncn-s002-mgmt ncn-s003-mgmt ncn-w001-mgmt ncn-w002-mgmt ncn-w003-mgmt   Set the nodes to PXE boot and (re)start them.\npit# export USERNAME=root pit# export IPMI_PASSWORD=changeme pit# grep -oP \u0026#34;($mtoken|$stoken|$wtoken)\u0026#34; /etc/dnsmasq.d/statics.conf | sort -u | xargs -t -i ipmitool -I lanplus -U $USERNAME -E -H {} chassis bootdev pxe options=efiboot,persistent pit# grep -oP \u0026#34;($mtoken|$stoken|$wtoken)\u0026#34; /etc/dnsmasq.d/statics.conf | sort -u | xargs -t -i ipmitool -I lanplus -U $USERNAME -E -H {} power off pit# sleep 10 pit# grep -oP \u0026#34;($mtoken|$stoken|$wtoken)\u0026#34; /etc/dnsmasq.d/statics.conf | sort -u | xargs -t -i ipmitool -I lanplus -U $USERNAME -E -H {} power on   Wait for the nodes to netboot. You can follow them with ConMan - the -m option follows the console output in read-only mode or the -j option joins an interactive console session. The available node names were listed in step 2 above. The boot usually starts in less than 3 minutes and log data should start flowing through ConMan, speed depends on how quickly your nodes POST. To see a ConMan help screen for all supported escape sequences use \u0026amp;?.\npit# conman -m ncn-m002-mgmt \u0026lt;ConMan\u0026gt; Connection to console [ncn-m002-mgmt] opened. \u0026lt;\u0026lt; hardware dependent boot log messages \u0026gt;\u0026gt;   Exit ConMan by typing \u0026amp;.\n \u0026lt;\u0026lt; hardware dependent boot log messages \u0026gt;\u0026gt; \u0026amp;. \u0026lt;ConMan\u0026gt; Connection to console [ncn-m002-mgmt] closed. pit#   Print off what has been found in the console logs, this snippet will omit duplicates from multiple boot attempts:\npit# for file in /var/log/conman/*; do echo $file grep -Eoh \u0026#39;(net[0-9] MAC .*)\u0026#39; $file | sort -u | grep PCI \u0026amp;\u0026amp; echo ----- done   Use the output from the previous step to collect 2 MACs to use for bond0, and 2 more to use for bond1 based on the topology.\nThe Bond0 MAC0 must be the first port of the first PCIe card, specifically the port connecting the NCN to the lower spine. For example, if connected to spines01 and 02, this is going to sw-spine-001. If connected to sw-spine-007 and sw-spine-008, then this is sw-spine-007.\nThe 2nd MAC for bond0 is the first port of the 2nd PCIe card, or 2nd port of the first when only one card exists.\nUse the table provided on NCN Networking for referencing commonly seen devices.\nWorker nodes also have the high-speed network cards. If these cards are known, filter their device IDs out from the above output using this snippet:\npit# unset did # clear it if you used it. pit# did=1017 # ConnectX-5 example. pit# for file in /var/log/conman/*; do echo $file grep -Eoh \u0026#39;(net[0-9] MAC .*)\u0026#39; $file | sort -u | grep PCI | grep -Ev \u0026#34;$did\u0026#34; \u0026amp;\u0026amp; echo ----- done To filter out onboard NICs, or site-link cards, omit their device IDs as well. Use the above snippet but add the other IDs:\nThis snippet prints out only mgmt MACs, the did is the HSN and onboard NICs that is being ignored.\npit# unset did # clear it if you used it. pit# did=\u0026#39;(1017|8086|ffff)\u0026#39; pit# for file in /var/log/conman/*; do echo $file grep -Eoh \u0026#39;(net[0-9] MAC .*)\u0026#39; $file | sort -u | grep PCI | grep -Ev \u0026#34;$did\u0026#34; \u0026amp;\u0026amp; echo ----- done   Examine the output from grep to identify the MAC address that make up Bond0 for each management NCN. Use the lowest value MAC address per PCIe card.\n Example: 1 PCIe card with 2 ports for a total of 2 ports per node.\n ----- /var/log/conman/console.ncn-w003-mgmt net2 MAC b8:59:9f:d9:9e:2c PCI.DeviceID 1013 PCI.VendorID 15b3 \u0026lt;-bond0-mac0 (0x2c \u0026lt; 0x2d) net3 MAC b8:59:9f:d9:9e:2d PCI.DeviceID 1013 PCI.VendorID 15b3 \u0026lt;-bond0-mac1 ----- The above output identified MAC0 and MAC1 of the bond as b8:59:9f:d9:9e:2c and b8:59:9f:d9:9e:2d respectively.\n Example: 2 PCIe cards with 2 ports each for a total of 4 ports per node.\n ----- /var/log/conman/console.ncn-w006-mgmt net0 MAC 94:40:c9:5f:b5:df PCI.DeviceID 8070 PCI.VendorID 1077 \u0026lt;-bond0-mac0 (0xdf \u0026lt; 0xe0) net1 MAC 94:40:c9:5f:b5:e0 PCI.DeviceID 8070 PCI.VendorID 1077 (future use) net2 MAC 14:02:ec:da:b9:98 PCI.DeviceID 8070 PCI.VendorID 1077 \u0026lt;-bond0-mac1 (0x98 \u0026lt; 0x99) net3 MAC 14:02:ec:da:b9:99 PCI.DeviceID 8070 PCI.VendorID 1077 (future use) ----- The above output identified MAC0 and MAC1 of the bond as 94:40:c9:5f:b5:df and 14:02:ec:da:b9:99 respectively.\n  Collect the NCN MAC address for the PIT node. This information will be used to populate the MAC addresses for ncn-m001.\npit# cat /proc/net/bonding/bond0 | grep Perm Permanent HW addr: b8:59:9f:c7:12:f2 \u0026lt;-bond0-mac0 Permanent HW addr: b8:59:9f:c7:12:f3 \u0026lt;-bond0-mac1   Update ncn_metadata.csv with the collected MAC addresses for Bond0 from all of the management NCNs.\n  \u0026gt; **NOTE:** Mind the index (3, 2, 1.... ; not 1, 2, 3). For each NCN update the corresponding row in `ncn_metadata` with the values for Bond0 MAC0 and Bond0 MAC1. The Bootstrap MAC should have the same value as the Bond0 MAC0. ``` Xname,Role,Subrole,BMC MAC,Bootstrap MAC,Bond0 MAC0,Bond0 MAC1 x3000c0s9b0n0,Management,Worker,94:40:c9:37:77:26,b8:59:9f:c7:12:f2,b8:59:9f:c7:12:f2,b8:59:9f:c7:12:f3 ^^^^^^^^^^^^^^^^^ ^^^^^^^^^^^^^^^^^ ^^^^^^^^^^^^^^^^^ bond0-mac0 bond0-mac0 bond0-mac1 ``` ```bash pit# vi ncn_metadata.csv ```   If the script.ipxe file was renamed in the first step of this procedure, restore it to its original location.\npit# mv /var/www/boot/script.ipxe.bak /var/www/boot/script.ipxe   Procedure: Serial Consoles Pick out the MAC addresses for the BOND from both the sw-spine-001 and sw-spine-002 switch following the Collecting BMC MAC Addresses procedure.\n NOTE: The node must be booted into an operating system in order for the Bond MAC addresses to appear on the spine switches.\n  A PCIe card with dual-heads may go to either spine switch, meaning MAC0 must be collected from spine-01. Please refer to the cabling diagram or the actual rack (in-person).\n   Follow \u0026ldquo;Metadata BMC\u0026rdquo; on each spine switch that port1 and port2 of the bond is plugged into.\n  Usually the 2nd/3rd/4th/Nth MAC on the PCIe card will be a 0x1 or 0x2 deviation from the first port.\nCollection is quicker if this can be easily confirmed.\n  Procedure: Recovering from an incorrect ncn_metadata.csv file If the ncn_metadata.csv file is incorrect, the NCNs will be unable to deploy. This section details a recovery procedure in case that happens.\n  Remove the incorrectly generated configurations.\nBefore deleting the incorrectly generated configurations, make a backup of them in case they need to be examined at a later time.\n WARNING Ensure that the SYSTEM_NAME environment variable is correctly set. If SYSTEM_NAME is not set the command below could potentially remove the entire prep directory.\npit# export SYSTEM_NAME=eniac  pit# rm -rf /var/www/ephemeral/prep/$SYSTEM_NAME   Manually edit ncn_metadata.csv, replacing the bootstrap MAC address with Bond0 MAC0 address for the afflicted nodes that failed to boot.\n  Re-run csi config init with the required flags.\n  Copy all of the newly generated files into place.\npit# \\ cp -p /var/www/ephemeral/prep/$SYSTEM_NAME/dnsmasq.d/* /etc/dnsmasq.d/* cp -p /var/www/ephemeral/prep/$SYSTEM_NAME/basecamp/* /var/www/ephemeral/configs/ cp -p /var/www/ephemeral/prep/$SYSTEM_NAME/conman.conf /etc/ cp -p /var/www/ephemeral/prep/$SYSTEM_NAME/pit-files/* /etc/sysconfig/network/   Update the CA Cert on the copied data.json file. Provide the path to the data.json, the path to the customizations.yaml file, and the sealed_secrets.key.\npit# csi patch ca \\ --cloud-init-seed-file /var/www/ephemeral/configs/data.json \\ --customizations-file /var/www/ephemeral/prep/site-init/customizations.yaml \\ --sealed-secret-key-file /var/www/ephemeral/prep/site-init/certs/sealed_secrets.key   Restart everything to apply the new configurations:\npit# \\ wicked ifreload all systemctl restart dnsmasq conman basecamp systemctl restart nexus   Ensure system-specific settings generated by CSI are merged into customizations.yaml:\n The yq tool used in the following procedures is available under /var/www/ephemeral/prep/site-init/utils/bin once the SHASTA-CFG repo has been cloned.\n pit# alias yq=\u0026#34;/var/www/ephemeral/prep/site-init/utils/bin/$(uname | awk \u0026#39;{print tolower($0)}\u0026#39;)/yq\u0026#34; pit# yq merge -xP -i /var/www/ephemeral/prep/site-init/customizations.yaml \u0026lt;(yq prefix -P \u0026#34;/var/www/ephemeral/prep/${SYSTEM_NAME}/customizations.yaml\u0026#34; spec)   Follow the workaround instructions for the before-ncn-boot breakpoint.\nReturn to this procedure after applying the workaround instructions.\n  Wipe the disks before relaunching the NCNs.\nSee full wipe from Wipe NCN Disks for Reinstallation.\n  "
},
{
	"uri": "/docs-csm/en-11/install/collecting_bmc_mac_addresses/",
	"title": "Collecting The BMC Mac Addresses",
	"tags": [],
	"description": "",
	"content": "Collecting the BMC MAC Addresses This guide will detail how to collect BMC MAC Addresses from an HPE Cray EX system with configured switches. The BMC MAC Address is the exclusive, dedicated LAN for the onboard BMC.\nResults may vary if an unconfigured switch is being used.\nPrerequisites  There is a configured switch with SSH access or unconfigured with COM access (serial-over-lan/DB-9). Another file is available to record the collected BMC information.  Procedure   Establish an SSH or Connect to Switch over USB-Serial Cable to the leaf switch.\n NOTE: These IP addresses are examples; 10.X.0.4 may or may not match the setup.\n # SSH over METAL MANAGEMENT pit# ssh admin@10.1.0.4 # SSH over NODE MANAGEMENT pit# ssh admin@10.252.0.4 # SSH over HARDWARE MANAGEMENT pit# ssh admin@10.254.0.4 # or.. serial (device name will vary). pit# minicom -b 115200 -D /dev/tty.USB1   Display the MAC addresses for the BMC ports (if known). If they exist on the same VLAN, dump the VLAN to get the MAC addresses.\nIn order to find the ports of the BMCs, cross reference the HMN tab of the SHCD.\n Syntax is for Onyx and Dell EMC devices - please reference the CLI for more information (press ? or tab to assist on-the-fly).\n Print using the VLAN ID:\n# DellOS 10 sw-leaf-001# show mac address-table vlan 4 | except 1/1/52 VlanId\tMac Address\tType\tInterface 4\t00:1e:67:98:fe:2c\tdynamic\tethernet1/1/11 4\ta4:bf:01:38:f0:b1\tdynamic\tethernet1/1/27 4\ta4:bf:01:38:f1:44\tdynamic\tethernet1/1/25 4\ta4:bf:01:48:1e:ac\tdynamic\tethernet1/1/28 4\ta4:bf:01:48:1f:70\tdynamic\tethernet1/1/31 4\ta4:bf:01:48:1f:e0\tdynamic\tethernet1/1/26 4\ta4:bf:01:48:20:03\tdynamic\tethernet1/1/30 4\ta4:bf:01:48:20:57\tdynamic\tethernet1/1/29 4\ta4:bf:01:4d:d9:9a\tdynamic\tethernet1/1/32 Print using the interface and trunk:\n# DellOS 10 sw-leaf-001# show mac address-table interface ethernet 1/1/32 VlanId\tMac Address\tType\tInterface 4\ta4:bf:01:4d:d9:9a\tdynamic\tethernet1/1/32 Print everything:\n# DellOS 10 sw-leaf-001# show mac address-table VlanId\tMac Address\tType\tInterface 4\ta4:bf:01:4d:d9:9a\tdynamic\tethernet1/1/32 .... # Onyx and Aruba sw-leaf-001# show mac-address-table   Ensure the management NCNs are present in the ncn_metadata.csv file.\nThe output from the previous show mac address-table command will display information for all management NCNs that do not have an external connection for their BMC, such as ncn-m001.\nAll of the management NCNs should be present in the ncn_metadata.csv file.\nFill in the Bootstrap MAC, Bond0 MAC0, and Bond0 MAC1 columns with a placeholder value, such as de:ad:be:ef:00:00, as a marker that the correct value is not in this file yet.\nIMPORTANT: Mind the index for each group of nodes (3, 2, 1\u0026hellip;. ; not 1, 2, 3). If storage nodes are ncn-s001 x3000c0s7b0n0, ncn-s002 x3000c0s8b0n0, ncn-s003 x3000c0s9b0n0, then their portion of the file would be ordered x3000c0s9b0n0, x3000c0s8b0n0, x3000c0s7b0n0.\nXname,Role,Subrole,BMC MAC,Bootstrap MAC,Bond0 MAC0,Bond0 MAC1 x3000c0s9b0n0,Management,Storage,a4:bf:01:38:f1:44,de:ad:be:ef:00:00,de:ad:be:ef:00:00,de:ad:be:ef:00:00 x3000c0s8b0n0,Management,Storage,a4:bf:01:48:1f:e0,de:ad:be:ef:00:00,de:ad:be:ef:00:00,de:ad:be:ef:00:00 x3000c0s7b0n0,Management,Storage,a4:bf:01:38:f0:b1,de:ad:be:ef:00:00,de:ad:be:ef:00:00,de:ad:be:ef:00:00 ^^^^^^^^^^^^^^^^^ The column heading must match that shown above for csi to parse it correctly.\n  Collect the BMC MAC address information for the PIT node.\nThe PIT node BMC is not connected to the switch like the other management nodes.\nlinux# export SYSTEM_NAME=eniac linux# export USERNAME=root linux# export IPMI_PASSWORD=changeme linux# ipmitool -I lanplus -U $USERNAME -E -H ${SYSTEM_NAME}-ncn-m001-mgmt lan print | grep \u0026#34;MAC Address\u0026#34; MAC Address : a4:bf:01:37:87:32  NOTE: An Intel node needs to use ipmitool -I lanplus -U $USERNAME -E -H ${SYSTEM_NAME}-ncn-m001-mgmt lan print instead of the above command.\n Add this information for ncn-m001 to the ncn_metadata.csv file. There should be ncn-m003, then ncn-m002, and this new entry for ncn-m001 as the last line in the file.\nx3000c0s1b0n0,Management,Master,a4:bf:01:37:87:32,de:ad:be:ef:00:00,de:ad:be:ef:00:00,de:ad:be:ef:00:00   Verify the ncn_metadata.csv file has a row for every management node in the SHCD.\nThere may be placeholder entries for some MAC addresses.\nSample file showing storage nodes 3,2,1, then worker nodes 3,2,1, then master nodes 3,2,1 with valid BMC MAC addresses, but placeholder values de:ad:be:ef:00:00 for the Bootstrap MAC, Bond0 MAC0, and Bond0 MAC1.\nXname,Role,Subrole,BMC MAC,Bootstrap MAC,Bond0 MAC0,Bond0 MAC1 x3000c0s9b0n0,Management,Storage,a4:bf:01:38:f1:44,de:ad:be:ef:00:00,de:ad:be:ef:00:00,de:ad:be:ef:00:00 x3000c0s8b0n0,Management,Storage,a4:bf:01:48:1f:e0,de:ad:be:ef:00:00,de:ad:be:ef:00:00,de:ad:be:ef:00:00 x3000c0s7b0n0,Management,Storage,a4:bf:01:38:f0:b1,de:ad:be:ef:00:00,de:ad:be:ef:00:00,de:ad:be:ef:00:00 x3000c0s6b0n0,Management,Worker,a4:bf:01:48:1e:ac,de:ad:be:ef:00:00,de:ad:be:ef:00:00,de:ad:be:ef:00:00 x3000c0s5b0n0,Management,Worker,a4:bf:01:48:20:57,de:ad:be:ef:00:00,de:ad:be:ef:00:00,de:ad:be:ef:00:00 x3000c0s4b0n0,Management,Worker,a4:bf:01:48:20:03,de:ad:be:ef:00:00,de:ad:be:ef:00:00,de:ad:be:ef:00:00 x3000c0s3b0n0,Management,Master,a4:bf:01:48:1f:70,de:ad:be:ef:00:00,de:ad:be:ef:00:00,de:ad:be:ef:00:00 x3000c0s2b0n0,Management,Master,a4:bf:01:4d:d9:9a,de:ad:be:ef:00:00,de:ad:be:ef:00:00,de:ad:be:ef:00:00 x3000c0s1b0n0,Management,Master,a4:bf:01:37:87:32,de:ad:be:ef:00:00,de:ad:be:ef:00:00,de:ad:be:ef:00:00   "
},
{
	"uri": "/docs-csm/en-11/install/collect_mac_addresses_for_ncns/",
	"title": "Collect Mac Addresses For NCNs",
	"tags": [],
	"description": "",
	"content": "Collect MAC Addresses for NCNs Now that the PIT node has been booted with the LiveCD and the management network switches have been configured, the actual MAC addresses for the management nodes can be collected. This process will include repetition of some of the steps done up to this point because csi config init will need to be run with the proper MAC addresses and some services will need to be restarted.\nNote: If a reinstall of this software release is being done on this system and the ncn_metadata.csv file already had valid MAC addresses for both BMC and node interfaces before csi config init was run, then this topic could be skipped and instead move to Deploy Management Nodes.\nNote: If a first time install of this software release is being done on this system and the ncn_metadata.csv file already had valid MAC addresses for both BMC and node interfaces before csi config init was run, then this topic could be skipped and instead move to Deploy Management Nodes.\nTopics  Collect the BMC MAC addresses Restart Services after BMC MAC Addresses Collected Collect the NCN MAC addresses Restart Services after NCN MAC Addresses Collected Next Topic  Details 1. Collect the BMC MAC addresses The BMC MAC address can be collected from the switches using knowledge about the cabling of the NMN from the SHCD.\nSee Collecting BMC MAC Addresses.\n2. Restart Services after BMC MAC Addresses Collected The previous step updated ncn_metadata.csv with the BMC MAC Addresses, so several earlier steps need to be repeated.\n  Change into the preparation directory.\npit# cd /var/www/ephemeral/prep   Confirm that the ncn_metadata.csv file in this directory has the new information. There should be no remaining dummy data (de:ad:be:ef:00:00) for the BMC MAC column in the file, but that string may be present for the Bootstrap MAC, Bond0 MAC0, and Bond0 MAC1 columns.\npit# cat ncn_metadata.csv   Remove the incorrectly generated configs. Before deleting the incorrectly generated configs consider making a backup of them, in case they need to be examined at a later time.\n WARNING Ensure that the SYSTEM_NAME environment variable is correctly set.\n pit# export SYSTEM_NAME=eniac pit# echo $SYSTEM_NAME Rename the old directory.\npit# mv /var/www/ephemeral/prep/${SYSTEM_NAME} /var/www/ephemeral/prep/${SYSTEM_NAME}.oldBMC   Copy over the system_config.yaml file from the first attempt at generating the system configuration files.\npit# cp /var/www/ephemeral/prep/${SYSTEM_NAME}.oldBMC/system_config.yaml /var/www/ephemeral/prep/   Generate system configuration again.\nThe needed files should be in the current directory.\npit# ls -1 Expected output looks similar to the following:\napplication_node_config.yaml cabinets.yaml hmn_connections.json ncn_metadata.csv switch_metadata.csv system_config.yaml The system_config.yaml file will make it easier to run the next command because it has the saved information from the command line arguments which were used initially for this command.\npit# csi config init A new directory matching your --system-name argument will now exist in your working directory.\nThese warnings from csi config init for issues in hmn_connections.json can be ignored.\n  The node with the external connection (ncn-m001) will have a warning similar to this because its BMC is connected to the site and not the HMN like the other management NCNs. It can be ignored.\n\u0026quot;Couldn't find switch port for NCN: x3000c0s1b0\u0026quot;   An unexpected component may have this message. If this component is an application node with an unusual prefix, it should be added to the application_node_config.yaml file. Then rerun csi config init. See the procedure to Create Application Node Config YAML\n{\u0026#34;level\u0026#34;:\u0026#34;warn\u0026#34;,\u0026#34;ts\u0026#34;:1610405168.8705149,\u0026#34;msg\u0026#34;:\u0026#34;Found unknown source prefix! If this is expected to be an Application node, please update application_node_config.yaml\u0026#34;,\u0026#34;row\u0026#34;: {\u0026#34;Source\u0026#34;:\u0026#34;gateway01\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u33\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3002\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u48\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j29\u0026#34;}}   If a cooling door is found in hmn_connections.json, there may be a message like the following. It can be safely ignored.\n{\u0026#34;level\u0026#34;:\u0026#34;warn\u0026#34;,\u0026#34;ts\u0026#34;:1612552159.2962296,\u0026#34;msg\u0026#34;:\u0026#34;Cooling door found, but xname does not yet exist for cooling doors!\u0026#34;,\u0026#34;row\u0026#34;: {\u0026#34;Source\u0026#34;:\u0026#34;x3000door-Motiv\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34; \u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u36\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j27\u0026#34;}}     Follow the workaround instructions for the csi-config breakpoint.\n  Copy the interface config files generated earlier by csi config init into /etc/sysconfig/network/.\npit# cp -pv /var/www/ephemeral/prep/${SYSTEM_NAME}/pit-files/* /etc/sysconfig/network/ pit# wicked ifreload all pit# systemctl restart wickedd-nanny \u0026amp;\u0026amp; sleep 5   Check that IP addresses are set for each interface and investigate any failures.\n  Check IP addresses. Do not run tests if these are missing and instead start triaging the issue.\npit# wicked show bond0 vlan002 vlan004 vlan007 bond0 up link: #7, state up, mtu 1500 type: bond, mode ieee802-3ad, hwaddr b8:59:9f:fe:49:d4 config: compat:suse:/etc/sysconfig/network/ifcfg-bond0 leases: ipv4 static granted addr: ipv4 10.1.1.2/16 [static] vlan002 up link: #8, state up, mtu 1500 type: vlan bond0[2], hwaddr b8:59:9f:fe:49:d4 config: compat:suse:/etc/sysconfig/network/ifcfg-vlan002 leases: ipv4 static granted addr: ipv4 10.252.1.4/17 [static] route: ipv4 10.92.100.0/24 via 10.252.0.1 proto boot vlan007 up link: #9, state up, mtu 1500 type: vlan bond0[7], hwaddr b8:59:9f:fe:49:d4 config: compat:suse:/etc/sysconfig/network/ifcfg-vlan007 leases: ipv4 static granted addr: ipv4 10.102.9.5/24 [static] vlan004 up link: #10, state up, mtu 1500 type: vlan bond0[4], hwaddr b8:59:9f:fe:49:d4 config: compat:suse:/etc/sysconfig/network/ifcfg-vlan004 leases: ipv4 static granted addr: ipv4 10.254.1.4/17 [static]   Run tests; inspect failures.\npit# csi pit validate --network     Copy the service config files generated earlier by csi config init for DNSMasq, Metal Basecamp (cloud-init), and Conman.\n  Copy files (files only, -r is expressly not used).\npit# cp -pv /var/www/ephemeral/prep/${SYSTEM_NAME}/dnsmasq.d/* /etc/dnsmasq.d/ pit# cp -pv /var/www/ephemeral/prep/${SYSTEM_NAME}/conman.conf /etc/conman.conf pit# cp -pv /var/www/ephemeral/prep/${SYSTEM_NAME}/basecamp/* /var/www/ephemeral/configs/   Restart all PIT services.\npit# systemctl restart basecamp nexus dnsmasq conman     Follow the workaround instructions for the before-ncn-boot breakpoint.\n  Verify that all BMCs can be pinged.\nNote: It may take about 10 minutes from when dnsmasq is restarted to when the BMCs pick up new DHCP leases.\nThis step will check all management nodes except ncn-m001-mgmt because that has an external connection and could not be booted by itself as the PIT node.\npit# export mtoken=\u0026#39;ncn-m(?!001)\\w+-mgmt\u0026#39; pit# export stoken=\u0026#39;ncn-s\\w+-mgmt\u0026#39; pit# export wtoken=\u0026#39;ncn-w\\w+-mgmt\u0026#39; pit# grep -oP \u0026#34;($mtoken|$stoken|$wtoken)\u0026#34; /etc/dnsmasq.d/statics.conf | sort -u | xargs -t -i ping -c3 {}   3. Collect the NCN MAC addresses Now that the BMC MAC addresses are correct in ncn_metadata.csv and the PIT node services have been restarted, a partial boot of the management nodes can be done to collect the remaining information from the conman console logs on the PIT node using the Procedure: iPXE Consoles\nSee Procedure: iPXE Consoles.\n4. Restart Services after NCN MAC Addresses Collected The previous step updated ncn_metadata.csv with the NCN MAC Addresses for Bootstrap MAC, Bond0 MAC0, and Bond0 MAC1 so several earlier steps need to be repeated.\n  Change into the preparation directory.\npit# cd /var/www/ephemeral/prep   Confirm that the ncn_metadata.csv file in this directory has the new information. There should be no remaining dummy data (de:ad:be:ef:00:00) for columns or rows in the file. Every row should have uniquely different MAC addresses from the other rows.\npit# grep \u0026#34;de:ad:be:ef:00:00\u0026#34; ncn_metadata.csv Expected output looks similar to the following, that is, no lines that still have \u0026ldquo;de:ad:be:ef:00:00\u0026rdquo;:\nDisplay the file and confirm the contents are unique between the different rows.\npit# cat ncn_metadata.csv   Remove the incorrectly generated configs. Before deleting the incorrectly generated configs, consider making a backup of them, in case they need to be examined at a later time.\n WARNING Ensure that the SYSTEM_NAME environment variable is correctly set.\n pit# export SYSTEM_NAME=eniac pit# echo $SYSTEM_NAME Rename the old directory.\npit# mv /var/www/ephemeral/prep/${SYSTEM_NAME} /var/www/ephemeral/prep/${SYSTEM_NAME}.oldNCN   Copy over the system_config.yaml file from the second attempt at generating the system configuration files.\npit# cp /var/www/ephemeral/prep/${SYSTEM_NAME}.oldNCN/system_config.yaml /var/www/ephemeral/prep/   Generate system configuration again.\nCheck for the expected files that should exist be in the current directory.\npit# ls -1 Expected output looks similar to the following:\napplication_node_config.yaml cabinets.yaml hmn_connections.json ncn_metadata.csv switch_metadata.csv system_config.yaml Regenerate the system configuration. The system_config.yaml file contains all of the options that where used to generate the initial system configuration, and can be used in place of specifying CLI flags to CSI.\npit# csi config init A new directory matching your $SYSTEM_NAME environment variable will now exist in your working directory.\nThese warnings from csi config init for issues in hmn_connections.json can be ignored.\n  The node with the external connection (ncn-m001) will have a warning similar to this because its BMC is connected to the site and not the HMN like the other management NCNs. It can be ignored.\n\u0026quot;Couldn't find switch port for NCN: x3000c0s1b0\u0026quot;   An unexpected component may have this message. If this component is an application node with an unusual prefix, it should be added to the application_node_config.yaml file. Then rerun csi config init. See the procedure to Create Application Node Config YAML\n{\u0026#34;level\u0026#34;:\u0026#34;warn\u0026#34;,\u0026#34;ts\u0026#34;:1610405168.8705149,\u0026#34;msg\u0026#34;:\u0026#34;Found unknown source prefix! If this is expected to be an Application node, please update application_node_config.yaml\u0026#34;,\u0026#34;row\u0026#34;: {\u0026#34;Source\u0026#34;:\u0026#34;gateway01\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u33\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3002\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u48\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j29\u0026#34;}}   If a cooling door is found in hmn_connections.json, there may be a message like the following. It can be safely ignored.\n{\u0026#34;level\u0026#34;:\u0026#34;warn\u0026#34;,\u0026#34;ts\u0026#34;:1612552159.2962296,\u0026#34;msg\u0026#34;:\u0026#34;Cooling door found, but xname does not yet exist for cooling doors!\u0026#34;,\u0026#34;row\u0026#34;: {\u0026#34;Source\u0026#34;:\u0026#34;x3000door-Motiv\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34; \u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u36\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j27\u0026#34;}}     Follow the workaround instructions for the csi-config breakpoint.\n  Copy the interface config files generated earlier by csi config init into /etc/sysconfig/network/.\npit# cp -pv /var/www/ephemeral/prep/${SYSTEM_NAME}/pit-files/* /etc/sysconfig/network/ pit# wicked ifreload all pit# systemctl restart wickedd-nanny \u0026amp;\u0026amp; sleep 5   Check that IP addresses are set for each interface and investigate any failures.\n  Check IP addresses. Do not run tests if these are missing and instead start triaging the issue.\npit# wicked show bond0 vlan002 vlan004 vlan007 bond0 up link: #7, state up, mtu 1500 type: bond, mode ieee802-3ad, hwaddr b8:59:9f:fe:49:d4 config: compat:suse:/etc/sysconfig/network/ifcfg-bond0 leases: ipv4 static granted addr: ipv4 10.1.1.2/16 [static] vlan002 up link: #8, state up, mtu 1500 type: vlan bond0[2], hwaddr b8:59:9f:fe:49:d4 config: compat:suse:/etc/sysconfig/network/ifcfg-vlan002 leases: ipv4 static granted addr: ipv4 10.252.1.4/17 [static] route: ipv4 10.92.100.0/24 via 10.252.0.1 proto boot vlan007 up link: #9, state up, mtu 1500 type: vlan bond0[7], hwaddr b8:59:9f:fe:49:d4 config: compat:suse:/etc/sysconfig/network/ifcfg-vlan007 leases: ipv4 static granted addr: ipv4 10.102.9.5/24 [static] vlan004 up link: #10, state up, mtu 1500 type: vlan bond0[4], hwaddr b8:59:9f:fe:49:d4 config: compat:suse:/etc/sysconfig/network/ifcfg-vlan004 leases: ipv4 static granted addr: ipv4 10.254.1.4/17 [static]   Run tests; inspect failures.\npit# csi pit validate --network     Copy the service config files generated earlier by csi config init for DNSMasq, Metal Basecamp (cloud-init), and Conman.\n  Copy files (files only, -r is expressly not used).\npit# cp -pv /var/www/ephemeral/prep/${SYSTEM_NAME}/dnsmasq.d/* /etc/dnsmasq.d/ pit# cp -pv /var/www/ephemeral/prep/${SYSTEM_NAME}/conman.conf /etc/conman.conf pit# cp -pv /var/www/ephemeral/prep/${SYSTEM_NAME}/basecamp/* /var/www/ephemeral/configs/   Update CA Cert on the copied data.json file for Basecamp with the generated certificate in site-init:\npit# csi patch ca \\ --cloud-init-seed-file /var/www/ephemeral/configs/data.json \\ --customizations-file /var/www/ephemeral/prep/site-init/customizations.yaml \\ --sealed-secret-key-file /var/www/ephemeral/prep/site-init/certs/sealed_secrets.key   Restart all PIT services.\npit# systemctl restart basecamp nexus dnsmasq conman     Ensure system-specific settings generated by CSI are merged into customizations.yaml:\n The yq tool used in the following procedures is available under /var/www/ephemeral/prep/site-init/utils/bin once the SHASTA-CFG repo has been cloned.\n pit# alias yq=\u0026#34;/var/www/ephemeral/prep/site-init/utils/bin/$(uname | awk \u0026#39;{print tolower($0)}\u0026#39;)/yq\u0026#34; pit# yq merge -xP -i /var/www/ephemeral/prep/site-init/customizations.yaml \u0026lt;(yq prefix -P \u0026#34;/var/www/ephemeral/prep/${SYSTEM_NAME}/customizations.yaml\u0026#34; spec)   Follow the workaround instructions for the before-ncn-boot breakpoint.\n  Next Topic After completing the collection of BMC MAC addresses and NCN MAC address to update ncn_metadata.csv and restarting the services dependent on correct data in ncn_metadata.csv, the next step is deployment of the management nodes.\nSee Deploy Management Nodes\n"
},
{
	"uri": "/docs-csm/en-11/install/clear_gigabyte_cmos/",
	"title": "Clear Gigabyte Cmos",
	"tags": [],
	"description": "",
	"content": "Clear Gigabyte CMOS Because of a bug in the Gigabyte firmware, the Shasta 1.5 install may negatively impact Gigabyte motherboards when attempting to boot using bonded Mellanox network cards. The result is a board that is unusable until a CMOS clear is physically done via a jumper on the board itself.\nA patched firmware release (newer than C20 BIOS) is expected to be available for a future release of Shasta. It is recommended that Gigabyte users wait for this new firmware before attempting an installation of Shasta 1.5. The procedure to recover the boards is included below.\nClear BIOS settings by jumper  Pull the power cables or blade server from the chassis, and open the system top cover. Move the Clear CMOS Jumper to 2-3, and wait 2 to 3 seconds. Move the Clear CMOS Jumper to 1-2.  Motherboard MZ62-HD0-00/-YF for Gigabyte H262 chassis Motherboard MZ32-AR0-00/-YF for Gigabyte R272 chassis Motherboard MZ92-FS0-00/-YF for Gigabyte R282 chassis "
},
{
	"uri": "/docs-csm/en-11/install/ceph_csi_troubleshooting/",
	"title": "Ceph Csi Troubleshooting",
	"tags": [],
	"description": "",
	"content": "Ceph CSI Troubleshooting If there has been a failure to initialize all Ceph CSI components on ncn-s001, then the storage node cloud-init may need to be rerun.\nTopics:  Verify Ceph CSI Rerun Storage Node cloud-init  Details 1. Verify Ceph CSI Verify that the ceph-csi requirements are in place\n  Log in to ncn-s00 and run this command\nncn-s001# ceph -s If it returns a connection error then assume Ceph is not installed. See Rerun Storage Node cloud-init.\n  Verify all post-Ceph-install tasks have run\nLog in to ncn-s001 and check /etc/cray/ceph for completed task files (ceph_k8s_initialized \u0026amp; csi_initialized).\nncn-s001# ls /etc/cray/ceph/ ceph_k8s_initialized csi_initialized installed kubernetes_nodes.txt tuned Check your results against this example.\nIf any components are missing, See Rerun Storage Node cloud-init.\n  Check to see if k8s ceph-csi prerequisites have been created\nThese commands can be run this from any master, worker node, or ncn-s001.\nncn# kubectl get cm NAME DATA AGE ceph-csi-config 1 3h50m cephfs-csi-sc 1 3h50m kube-csi-sc 1 3h50m sma-csi-sc 1 3h50m sts-rados-config 1 4h ncn# kubectl get secrets | grep csi csi-cephfs-secret Opaque 4 3h51m csi-kube-secret Opaque 2 3h51m csi-sma-secret Opaque 2 3h51m Check your results against the above examples.\nIf any components are missing, see Rerun Storage Node cloud-init.\n  1. Rerun Storage Node cloud-init This procedure will restart the storage node cloud-init process to prepare Ceph for use by the utility storage nodes.\n  Run the following:\nncn-s001# ls /etc/cray/ceph If any files are there they will represent completed stages.\n  If you have a running cluster you will want to edit \u0026lsquo;storage-ceph-cloudinit.sh` on ncn-s001\nncn-s001# vi /srv/cray/scripts/common/storage-ceph-cloudinit.sh Comment out this section:\n#if [ -f \u0026#34;$ceph_installed_file\u0026#34; ]; then # echo \u0026#34;This ceph cluster has been initialized\u0026#34; #else # echo \u0026#34;Installing ceph\u0026#34; # init # mark_initialized $ceph_installed_file #fi   Run the storage-ceph-cloudinit.sh script:\nncn-s001# /srv/cray/scripts/common/storage-ceph-cloudinit.sh Configuring node auditing software Using generic auditing configuration This ceph cluster has been initialized This ceph cluster has already been tuned This ceph radosgw config and initial k8s integration already complete ceph-csi configuration has been already been completed  If your output is like above then that means that all the steps ran.  You can also locate file in /etc/cray/ceph that are created as each step completes.   If the script failed out then you will have more output for the tasks that are being run.    "
},
{
	"uri": "/docs-csm/en-11/install/cable_management_network_servers/",
	"title": "Cable Management Network Servers",
	"tags": [],
	"description": "",
	"content": "Cable Management Network Servers This topic describes nodes in the air-cooled cabinet with diagrams and pictures showing where to find the ports on the nodes and how to cable the nodes to the management network switches.\n HPE Hardware  HPE DL385 HPE DL325 HPE Worker Node Cabling HPE Master Node Cabling HPE Storage Node Cabling HPE UAN Cabling HPE Apollo 6500 XL645D HPE Apollo 6500 XL675D   Gigabyte/Intel Hardware  Worker Node Cabling Master Node Cabling Storage Node Cabling UAN Cabling    HPE Hardware HPE DL385  The OCP Slot is noted (number 7) in the image above.  This is the bottom middle slot to the left of the VGA port. Ports are numbered left-to-right: the far left port is port 1.   The PCIe Slot 1 is on the top left side of the image above (under number 1).  Ports are numbered left-to-right: the far left port is port 1.    HPE DL325  The OCP Slot is noted (number 9) in the image above.  This is the slot on the bottom left of the node. Ports are numbered left-to-right: the far left port is port 1.   The PCIE Slot 1 is on the top left side of the image above (under number 1).  Ports are numbered left-to-right: the far left port is port 1.    HPE Worker Node Cabling    Server Port Management Network Port Speed Use / Configuration     OCP port 1 spine or aggregation pair, switch 1/2 25Gb Management Network NMN/HMN/CAN   OCP port 2 spine or aggregation pair, switch 2/2 25Gb Management Network NMN/HMN/CAN    SHCD Example    hostname Source Destination Destination     wn01 x3000u04ocp-j1 x3000u12-j7 sw-25g01   wn01 x3000u04ocp-j2 x3000u13-j7 sw-25g02    HPE Master Node Cabling Dual Card Installations The table below describes the cabling of dual card configurations. Also read notes in this section to see other possible customer-based configurations.\n   Server Port Management Network Port Speed Use / Configuration     OCP port 1 spine or aggregation pair, switch 1/2 25Gb Management Network NMN/HMN/CAN   OCP port 2 NONE NONE NONE   PCIe Slot 1 port 1 spine or aggr pair, switch 2/2 25Gb Management Network NMN/HMN/CAN   PCIe Slot 1 port 2 NONE (See note below for ncn-m001) NONE Site (See note below for ncn-m001)    SHCD Example    hostname Source Destination Destination     mn01 x3000u01ocp-j1 x3000u12-j1 sw-25g01   mn01 x3000u01s1-j1 x3000u13-j1 sw-25g02    NOTE: Master 1 (ncn-m001) is required to have a site connection for installation and non-CAN system access. This can have several configurations depending on customer requirements/equipment:\n Dual 10/25Gb card configurations as described in the table above should use PCIe Slot 1, Port 2 as a site connection if the customer supports 10/25Gb. If the customer does not support 10/25Gb speeds (or connection type) and requires RJ45 copper or 1Gb, then a new and separate card will be installed on ncn-m001 and that card will provide site connectivity. Another possibility (non-HPE hardware mainly) is that a built-in 1Gb port will be used if available (similar to Shasta v1.3 PoR on Gigabyte hardware).  HPE Storage Node Cabling    Server Port Management Network Port Speed Use / Configuration     OCP port 1 spine or aggregation pair, switch 1/2 25Gb Management Network NMN/HMN/CAN   OCP port 2 spine or aggregation pair, switch 1/2 25Gb Storage SUN (future use)   PCIe Slot 1 port 1 spine or aggregation pair, switch 2/2 25Gb Management Network NMN/HMN/CAN   PCIe Slot 1 port 2 spine or aggregation pair, switch 2/2 25Gb Storage SUN (future use)    SHCD Example    hostname Source Destination Destination     sn01 x3000u17s1-j2 x3000u34-j14 sw-25g02   sn01 x3000u17s1-j1 x3000u34-j8 sw-25g02   sn01 x3000u17ocp-j2 x3000u33-j14 sw-25g01   sn01 x3000u17ocp-j1 x3000u33-j8 sw-25g01   The OCP ports go to the First switch and the PCIe ports go to the Second switch. OCP port 1 and PCIe port 1 form a Bond. OCP port 2 and PCIe port 2 form a Bond.             For systems that include 4 aggregation switches the cabling will look like the following.\nSHCD Example with four aggregation switches.    hostname Source Destination Destination     sn01 x3000u10ocp-j2 x3000u36-j5 sw-25g04   sn01 x3000u10s1-j2 x3000u35-j5 sw-25g03   sn01 x3000u10ocp-j1 x3000u34-j6 sw-25g02   sn01 x3000u10s1-j1 x3000u33-j6 sw-25g01    HPE UAN Cabling    Server Port Management Network Port Speed Use / Configuration     OCP port 1 spine or aggregation pair, switch 1/2 25Gb Management Network NMN   OCP port 2 spine or aggregation pair, switch 1/2 25Gb Management Network CAN bond   PCIe Slot 1 port 1 spine or aggregation pair, switch 2/2 25Gb NONE (Shasta v1.4)   PCIe Slot 1 port 2 spine or aggregation pair, switch 2/2 25Gb Management Network CAN bond    SHCD Example    hostname Source Destination Destination     uan01 x3000u17s1-j2 x3000u34-j14 sw-25g02   uan01 x3000u17s1-j1 x3000u34-j8 sw-25g02   uan01 x3000u17ocp-j2 x3000u33-j14 sw-25g01   uan01 x3000u17ocp-j1 x3000u33-j8 sw-25g01    HPE Apollo 6500 XL645D  The XL645D has two servers in the same chassis. The iLO BMC RJ45 port is a shared network port. Both iLO/BMC traffic and compute node traffic could transit this link.  Isolating this port to iLO/BMC only traffic is not possible within firmware configuration alone. iLO configuration settings must be paired with management switch port settings to ensure only BMC traffic exits the port. The iLO firmware must be set to tag traffic to VLAN 4. The switch port must be set to trunk VLAN 4.   Ports on the OCP card are numbered left-to-right: the far left port is port 1.  Apollo XL645D Cabling (per server)    Server Port Management Network Port Speed Use / Configuration     OCP port 1 1G leaf switch 1Gb Management Network NMN   OCP port 2 None None None   OCP port 3 None None None   OCP port 4 None None None   iLO 1G leaf switch 1Gb Management Network HMN    HPE Apollo 6500 XL675D  Two PCIe slots (chassis slots 21 and 22) are highlighted. One will contain the 1Gb management network card and one will be for the HSN. The iLO BMC RJ45 port is a shared network port. Both iLO/BMC traffic and compute node traffic could transit this link.  Isolating this port to iLO/BMC only traffic is not possible within firmware configuration alone. iLO configuration settings must be paired with management switch port settings to ensure only BMC traffic exits the port. The iLO firmware must be set to tag traffic to VLAN 4. The switch port must be set to trunk VLAN 4.   Ports on the PCIe card are numbered left-to-right: the far left port is port 1.  Apollo XL675D Cabling    Server Port Management Network Port Speed Use / Configuration     PCIe port 1 1G leaf switch 1Gb Management Network NMN   PCIe port 2 None None None   PCIe port 3 None None None   PCIe port 4 None None None   iLO 1G leaf switch 1Gb Management Network HMN    Gigabyte/Intel Hardware Worker Node Cabling    Server Port Management Network Port Speed Use / Configuration     PCIe Slot 1 port 1 spine or aggregation pair, switch 1/2 40Gb Management Network NMN/HMN/CAN   PCIe Slot 1 port 2 spine or aggregation pair, switch 2/2 40Gb Management Network NMN/HMN/CAN    SHCD Example    hostname Source Destination Destination     wn01 x3000u07s1-j1 x3000u24L-j4 sw-smn02   wn01 x3000u07s1-j2 x3000u24R-j4 sw-smn03    NOTE: Cabling of ncn-w001 has changed in Shasta v1.4. Please see ncn-m001 note below.\nMaster Node Cabling    Server Port Management Network Port Speed Use / Configuration     PCIe Slot 1 port 1 spine or aggregation pair, switch 1/2 40Gb Management Network NMN/HMN/CAN   PCIe Slot 1 port 2 spine or aggregation pair, switch 2/2 40Gb Management Network NMN/HMN/CAN   LAN0 port 1 NONE (See note below for ncn-m001) NONE Site (See note below for ncn-m001)    SHCD Example    hostname Source Destination Destination     mn01 x3000u01s1-j1 x3000u24L-j1 sw-smn02   mn01 x3000u01s1-j2 x3000u24R-j1 sw-smn03    NOTE: Master 1 (ncn-m001) is required to have a site connection for installation and non-CAN system access. In Shasta versions \u0026lt;=1.3 this connection was on ncn-w001. This can have several configurations depending on customer requirements/equipment:\n The default configuration for Gigabyte systems uses the built-in 1Gb lan0 port for site connection on ncn-m001. If the customer requires connectivity greater than 1Gb (or a different connection type), then a new and separate card will be installed on ncn-m001 and that card will provide site connectivity.  Storage Node Cabling    Server Port Management Network Port Speed Use / Configuration     PCIe Slot 1 port 1 spine or aggregation pair, switch 1/2 40Gb Management Network NMN/HMN/CAN   PCIe Slot 1 port 2 spine or aggregation pair, switch 2/2 40Gb Management Network NMN/HMN/CAN    SHCD Example    hostname Source Destination Destination     sn01 x3000u13s1-j1 x3000u24L-j7 sw-smn02   sn01 x3000u13s1-j2 x3000u24R-j7 sw-smn03    UAN Cabling    Server Port Management Network Port Speed Use / Configuration     LAN0 port 1 leaf (see note) 1Gb Management Network NMN   PCIe Slot 1 port 1 spine or aggregation pair, switch 1/2 40Gb Management Network CAN bond   PCIe Slot 1 port 2 spine or aggregation pair, switch 2/2 40Gb Management Network CAN bond    SHCD Example    hostname Source Destination Destination     uan01 x3000u27s1-j1 x3000u24L-j10 sw-smn02   uan01 x3000u27s1-j2 x3000u24R-j10 sw-smn03    NOTE that there are a couple configurations possible for LAN0:\n Existing Gigabyte systems on Dell and Mellanox network hardware will use the (existing) Dell leaf port. Any Gigabyte system on Aruba network hardware will use an Aruba 6300 (for the 1Gb port). Optionally a 10/25Gb card could be added in an Aruba hardware system to match the HPE UANs.  "
},
{
	"uri": "/docs-csm/en-11/install/bootstrap_livecd_usb/",
	"title": "Bootstrap Pit Node From Livecd Usb",
	"tags": [],
	"description": "",
	"content": "Bootstrap PIT Node from LiveCD USB The Pre-Install Toolkit (PIT) node needs to be bootstrapped from the LiveCD. There are two media available to bootstrap the PIT node\u0026ndash;the RemoteISO or a bootable USB device. This procedure describes using the USB device. If not using the USB device, see Bootstrap Pit Node from LiveCD Remote ISO.\nThere are 5 overall steps that provide a bootable USB with SSH enabled, capable of installing Shasta v1.5 (or higher).\nTopics  Download and Expand the CSM Release Create the Bootable Media Configuration Payload  Before Configuration Payload Workarounds Generate Installation Files CSI Workarounds Prepare Site Init   Prepopulate LiveCD Daemons Configuration and NCN Artifacts Boot the LiveCD  First Login   Next Topic  Details 1. Download and Expand the CSM Release Fetch the base installation CSM tarball and extract it, installing the contained CSI tool.\n  Set up the Typescript directory as well as the initial typescript. This directory will be returned to for every typescript in the entire CSM installation.\nlinux# cd ~ linux# script -af csm-install-usb.$(date +%Y-%m-%d).txt linux# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39;   The CSM software release should be downloaded and expanded for use.\nImportant: To ensure that the CSM release plus any patches, workarounds, or hotfixes are included follow the instructions in Update CSM Product Stream\nImportant: Download to a location that has sufficient space for both the tarball and the expanded tarball.\nThe rest of this procedure will use the CSM_RELEASE variable and expect to have the contents of the CSM software release tarball plus any patches, workarounds, or hotfixes.\nlinux# CSM_RELEASE=csm-x.y.z linux# echo $CSM_RELEASE linux# tar -zxvf ${CSM_RELEASE}.tar.gz linux# ls -l ${CSM_RELEASE} linux# CSM_PATH=$(pwd)/${CSM_RELEASE} The ISO and other files are now available in the directory from the extracted CSM tar.\n  Install/upgrade the CSI RPM\nlinux# rpm -Uvh --force $(find ${CSM_PATH}/rpm/cray/csm/ -name \u0026#34;cray-site-init-*.x86_64.rpm\u0026#34; | sort -V | tail -1)   Download and install/upgrade the workaround and documentation RPMs. If this machine does not have direct internet access these RPMs will need to be externally downloaded and then copied to this machine.\nImportant: To ensure that the latest workarounds and documentation updates are available, see Check for Latest Workarounds and Documentation Updates\n  Show the version of CSI installed.\nlinux# csi version Expected output looks similar to the following:\nCRAY-Site-Init build signature... Build Commit : b3ed3046a460d804eb545d21a362b3a5c7d517a3-release-shasta-1.4 Build Time : 2021-02-04T21:05:32Z Go Version : go1.14.9 Git Version : b3ed3046a460d804eb545d21a362b3a5c7d517a3 Platform : linux/amd64 App. Version : 1.5.18   Configure zypper with the embedded repository from the CSM release.\nlinux# zypper ar -fG \u0026#34;${CSM_PATH}/rpm/embedded\u0026#34; \u0026#34;${CSM_RELEASE}-embedded\u0026#34;   Install podman or docker to support container tools required to generated sealed secrets.\nPodman RPMs are included in the embedded repository in the CSM release and may be installed in your pre-LiveCD environment using zypper as follows:\n  Install podman and podman-cni-config packages:\nlinux# zypper in --repo ${CSM_RELEASE}-embedded -y podman podman-cni-config   Or you may use rpm -Uvh to install RPMs (and their dependencies) manually from the ${CSM_PATH}/rpm/embedded directory.\nlinux# rpm -Uvh ${CSM_PATH}/rpm/embedded/suse/SLE-Module-Containers/15-SP2/x86_64/update/x86_64/podman-*.x86_64.rpm linux# rpm -Uvh ${CSM_PATH}/rpm/embedded/suse/SLE-Module-Containers/15-SP2/x86_64/update/noarch/podman-cni-config-*.noarch.rpm   Install lsscsi to view attached storage devices.\nlsscsi RPMs are included in the embedded repository in the CSM release and may be installed in your pre-LiveCD environment using zypper as follows:\n  Install lsscsi package:\nlinux# zypper in --repo ${CSM_RELEASE}-embedded -y lsscsi   Or you may use rpm -Uvh to install RPMs (and their dependencies) manually from the ${CSM_PATH}/rpm/embedded directory.\nlinux# rpm -Uvh ${CSM_PATH}/rpm/embedded/suse/SLE-Module-Basesystem/15-SP2/x86_64/product/x86_64/lsscsi-*.x86_64.rpm   Although not strictly required, the procedures for setting up the site-init directory recommend persisting site-init files in a Git repository.\nGit RPMs are included in the embedded repository in the CSM release and may be installed in your pre-LiveCD environment using zypper as follows:\n  Install git package:\nlinux# zypper in --repo ${CSM_RELEASE}-embedded -y git   Or you may use rpm -Uvh to install RPMs (and their dependencies) manually from the ${CSM_PATH}/rpm/embedded directory.\nlinux# rpm -Uvh ${CSM_PATH}/rpm/embedded/suse/SLE-Module-Basesystem/15-SP2/x86_64/update/x86_64/git-core-*.x86_64.rpm linux# rpm -Uvh ${CSM_PATH}/rpm/embedded/suse/SLE-Module-Development-Tools/15-SP2/x86_64/update/x86_64/git-*.x86_64.rpm   2. Create the Bootable Media Cray Site Init will create the bootable LiveCD. Before creating the media, we need to identify which device that is.\n  Identify the USB device.\nThis example shows the USB device is /dev/sdd on the host.\nlinux# lsscsi Expected output looks similar to the following:\n[6:0:0:0] disk ATA SAMSUNG MZ7LH480 404Q /dev/sda [7:0:0:0] disk ATA SAMSUNG MZ7LH480 404Q /dev/sdb [8:0:0:0] disk ATA SAMSUNG MZ7LH480 404Q /dev/sdc [14:0:0:0] disk SanDisk Extreme SSD 1012 /dev/sdd [14:0:0:1] enclosu SanDisk SES Device 1012 - In the above example, we can see our internal disks as the ATA devices and our USB as the disk or enclosu device. Because the SanDisk fits the profile we are looking for, we are going to use /dev/sdd as our disk.\nSet a variable with your disk to avoid mistakes:\nlinux# export USB=/dev/sd\u0026lt;disk_letter\u0026gt;   Format the USB device\nOn Linux using the CSI application:\nlinux# csi pit format $USB ${CSM_PATH}/cray-pre-install-toolkit-*.iso 50000  Note: If the previous command fails with this error message, this indicates that this Linux computer does not have the checkmedia RPM installed. In that case, the RPM can be installed and csi pit format can be run again\nERROR: Unable to validate ISO. Please install checkmedia  Install the missing rpms  linux# zypper in --repo ${CSM_RELEASE}-embedded -y libmediacheck5 checkmedia linux# csi pit format $USB ${CSM_PATH}/cray-pre-install-toolkit-*.iso 50000  On MacOS using the bash script:\nmacos# ./cray-site-init/write-livecd.sh $USB ${CSM_PATH}/cray-pre-install-toolkit-*.iso 50000  NOTE: At this point the USB device is usable in any server with an x86_64 architecture based CPU. The remaining steps help add the installation data and enable SSH on boot.\n   Mount the configuration and persistent data partition:\nlinux# mkdir -pv /mnt/{cow,pitdata} linux# mount -vL cow /mnt/cow \u0026amp;\u0026amp; mount -vL PITDATA /mnt/pitdata   Copy and extract the tarball (compressed) into the USB:\nlinux# cp -v ${CSM_PATH}.tar.gz /mnt/pitdata/ linux# tar -zxvf ${CSM_PATH}.tar.gz -C /mnt/pitdata/   The USB device is now bootable and contains our artifacts. This may be useful for internal or quick usage. Administrators seeking a Shasta installation must continue onto the configuration payload.\n3. Configuration Payload The SHASTA-CFG structure and other configuration files will be prepared, then csi will generate system-unique configuration payload used for the rest of the CSM installation on the USB device.\n Before Configuration Payload Workarounds Generate Installation Files CSI Workarounds Prepare Site Init  3.1 Before Configuration Payload Workarounds Follow the workaround instructions for the before-configuration-payload breakpoint.\n3.2 Generate Installation Files Some files are needed for generating the configuration payload. See these topics in Prepare Configuration Payload if you have not already prepared the information for this system.\n Command Line Configuration Payload Configuration Payload Files   Note: The USB device is usable at this time, but without SSH enabled as well as core services. This means the USB device could be used to boot the system now, and a user can return to this step at another time.\n   At this time see Create HMN Connections JSON for instructions about creating the hmn_connections.json.\n  Change into the preparation directory:\nlinux# mkdir -pv /mnt/pitdata/prep linux# cd /mnt/pitdata/prep   Pull these files into the current working directory:\n application_node_config.yaml (optional - see below) cabinets.yaml (optional - see below) hmn_connections.json ncn_metadata.csv switch_metadata.csv system_config.yaml (see below)   The optional application_node_config.yaml file may be provided for further defining of settings relating to how application nodes will appear in HSM for roles and subroles. See Create Application Node YAML\n  The optional cabinets.yaml file allows cabinet naming and numbering as well as some VLAN overrides. See Create Cabinets YAML.\n  The system_config.yaml is required for a reinstall, because it was created during a previous install. For a first time install, the information in it can be provided as command line arguments to csi config init.\n After gathering the files into this working directory, generate your configurations:\n  If doing a reinstall and the system_config.yaml parameter file is available, then generate the system configuration reusing this parameter file (see avoiding parameters.\nIf not doing a reinstall of Shasta software, then the system_config.yaml file will not be available, so skip the rest of this step.\n  Check for the configuration files. The needed files should be in the current directory.\nlinux# ls -1 Expected output looks similar to the following:\napplication_node_config.yaml cabinets.yaml hmn_connections.json ncn_metadata.csv switch_metadata.csv system_config.yaml   Set an environment variable so this system name can be used in later commands.\nlinux# export SYSTEM_NAME=eniac   Generate the system configuration.\nlinux# csi config init A new directory matching your --system-name argument will now exist in your working directory.\nThese warnings from csi config init for issues in hmn_connections.json can be ignored.\n  The node with the external connection (ncn-m001) will have a warning similar to this because its BMC is connected to the site and not the HMN like the other management NCNs. It can be ignored.\n\u0026quot;Couldn't find switch port for NCN: x3000c0s1b0\u0026quot;   An unexpected component may have this message. If this component is an application node with an unusual prefix, it should be added to the application_node_config.yaml file. Then rerun csi config init. See the procedure to Create Application Node Config YAML\n{\u0026#34;level\u0026#34;:\u0026#34;warn\u0026#34;,\u0026#34;ts\u0026#34;:1610405168.8705149,\u0026#34;msg\u0026#34;:\u0026#34;Found unknown source prefix! If this is expected to be an Application node, please update application_node_config.yaml\u0026#34;,\u0026#34;row\u0026#34;: {\u0026#34;Source\u0026#34;:\u0026#34;gateway01\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u33\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3002\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u48\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j29\u0026#34;}}   If a cooling door is found in hmn_connections.json, there may be a message like the following. It can be safely ignored.\n{\u0026#34;level\u0026#34;:\u0026#34;warn\u0026#34;,\u0026#34;ts\u0026#34;:1612552159.2962296,\u0026#34;msg\u0026#34;:\u0026#34;Cooling door found, but xname does not yet exist for cooling doors!\u0026#34;,\u0026#34;row\u0026#34;: {\u0026#34;Source\u0026#34;:\u0026#34;x3000door-Motiv\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34; \u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u36\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j27\u0026#34;}}     Skip the next step and continue with the CSI Workarounds.\n    If doing a first time install or the system_config.yaml parameter file for a reinstall is not available, generate the system configuration.\nIf doing a first time install, this step is required. If you did the previous step as part of a reinstall, skip this.\n  Check for the configuration files. The needed files should be in the current directory.\nlinux# ls -1 Expected output looks similar to the following:\napplication_node_config.yaml cabinets.yaml hmn_connections.json ncn_metadata.csv switch_metadata.csv   Set an environment variable so this system name can be used in later commands.\nlinux# export SYSTEM_NAME=eniac   Generate the system configuration. See below for an explanation of the command line parameters and some common settings.\nlinux# csi config init \\  --bootstrap-ncn-bmc-user root \\  --bootstrap-ncn-bmc-pass changeme \\  --system-name ${SYSTEM_NAME} \\  --can-cidr 10.103.11.0/24 \\  --can-external-dns 10.103.11.113 \\  --can-gateway 10.103.11.1 \\  --can-static-pool 10.103.11.112/28 \\  --can-dynamic-pool 10.103.11.128/25 \\  --nmn-cidr 10.252.0.0/17 \\  --hmn-cidr 10.254.0.0/17 \\  --ntp-pool time.nist.gov \\  --site-domain dev.cray.com \\  --site-ip 172.30.53.79/20 \\  --site-gw 172.30.48.1 \\  --site-nic p1p2 \\  --site-dns 172.30.84.40 \\  --install-ncn-bond-members p1p1,p10p1 \\  --application-node-config-yaml application_node_config.yaml \\  --cabinets-yaml cabinets.yaml \\  --hmn-mtn-cidr 10.104.0.0/17 \\  --nmn-mtn-cidr 10.100.0.0/17 \\  --bgp-peers aggregation A new directory matching your --system-name argument will now exist in your working directory.\n After generating a configuration, a visual audit of the generated files for network data should be performed.\n Run the command csi config init --help to get more information about the parameters mentioned in the example command above and others which are available.\nNotes about parameters to csi config init:\n The application_node_config.yaml file is optional, but if you have one describing the mapping between prefixes in hmn_connections.csv that should be mapped to HSM subroles, you need to include a command line option to have it used. See Create Application Node YAML. The bootstrap-ncn-bmc-user and bootstrap-ncn-bmc-pass must match what is used for the BMC account and its password for the management NCNs. Set site parameters (site-domain, site-ip, site-gw, site-nic, site-dns) for the information which connects ncn-m001 (the PIT node) to the site. The site-nic is the interface on this node connected to the site. There are other interfaces possible, but the install-ncn-bond-members are typically:  p1p1,p10p1 for HPE nodes p1p1,p1p2 for Gigabyte nodes p801p1,p801p2 for Intel nodes   If you are not using a cabinets-yaml file, set the three cabinet parameters (mountain-cabinets, hill-cabinets, and river-cabinets) to the number of each cabinet which are part of this system. The starting cabinet number for each type of cabinet (for example, starting-mountain-cabinet) has a default that can be overridden. See the csi config init --help For systems that use non-sequential cabinet ID numbers, use cabinets-yaml to include the cabinets.yaml file. This file can include information about the starting ID for each cabinet type and number of cabinets which have separate command line options, but is a way to specify explicitly the id of every cabinet in the system. If you are using a cabinets-yaml file, flags specified on the csi command-line related to cabinets will be ignored. See Create Cabinets YAML. An override to default cabinet IPv4 subnets can be made with the hmn-mtn-cidr and nmn-mtn-cidr parameters. By default, spine switches are used as MetalLB peers. Use --bgp-peers aggregation to use aggregation switches instead. Several parameters (can-gateway, can-cidr, can-static-pool, can-dynamic-pool) describe the CAN (Customer Access network). The can-gateway is the common gateway IP address used for both spine switches and commonly referred to as the Virtual IP address for the CAN. The can-cidr is the IP subnet for the CAN assigned to this system. The can-static-pool and can-dynamic-pool are the MetalLB address static and dynamic pools for the CAN. The can-external-dns is the static IP address assigned to the DNS instance running in the cluster to which requests the cluster subdomain will be forwarded. The can-external-dns IP address must be within the can-static-pool range. Set ntp-pool to a reachable NTP server  These warnings from csi config init for issues in hmn_connections.json can be ignored.\n  The node with the external connection (ncn-m001) will have a warning similar to this because its BMC is connected to the site and not the HMN like the other management NCNs. It can be ignored.\n\u0026quot;Couldn't find switch port for NCN: x3000c0s1b0\u0026quot;   An unexpected component may have this message. If this component is an application node with an unusual prefix, it should be added to the application_node_config.yaml file. Then rerun csi config init. See the procedure to Create Application Node Config YAML\n{\u0026#34;level\u0026#34;:\u0026#34;warn\u0026#34;,\u0026#34;ts\u0026#34;:1610405168.8705149,\u0026#34;msg\u0026#34;:\u0026#34;Found unknown source prefix! If this is expected to be an Application node, please update application_node_config.yaml\u0026#34;,\u0026#34;row\u0026#34;: {\u0026#34;Source\u0026#34;:\u0026#34;gateway01\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u33\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3002\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u48\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j29\u0026#34;}}   If a cooling door is found in hmn_connections.json, there may be a message like the following. It can be safely ignored.\n{\u0026#34;level\u0026#34;:\u0026#34;warn\u0026#34;,\u0026#34;ts\u0026#34;:1612552159.2962296,\u0026#34;msg\u0026#34;:\u0026#34;Cooling door found, but xname does not yet exist for cooling doors!\u0026#34;,\u0026#34;row\u0026#34;: {\u0026#34;Source\u0026#34;:\u0026#34;x3000door-Motiv\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34; \u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u36\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j27\u0026#34;}}     Continue with the next step to apply the csi-config workarounds.\n    3.3 CSI Workarounds Follow the workaround instructions for the csi-config breakpoint.\n3.4 Prepare Site Init Follow the procedures to Prepare Site Init directory for your system.\n4. Prepopulate LiveCD Daemons Configuration and NCN Artifacts Now that the configuration is generated, we can populate the LiveCD with the generated files.\nThis will enable SSH, and other services when the LiveCD starts.\n  Set system name and enter prep directory\nlinux# export SYSTEM_NAME=eniac linux# cd /mnt/pitdata/prep   Use CSI to populate the LiveCD, provide both the mount point and the CSI generated config dir.\nlinux# csi pit populate cow /mnt/cow/ ${SYSTEM_NAME}/ Expected output looks similar to the following:\nconfig------------------------\u0026gt; /mnt/cow/rw/etc/sysconfig/network/config...OK ifcfg-bond0-------------------\u0026gt; /mnt/cow/rw/etc/sysconfig/network/ifcfg-bond0...OK ifcfg-lan0--------------------\u0026gt; /mnt/cow/rw/etc/sysconfig/network/ifcfg-lan0...OK ifcfg-vlan002-----------------\u0026gt; /mnt/cow/rw/etc/sysconfig/network/ifcfg-vlan002...OK ifcfg-vlan004-----------------\u0026gt; /mnt/cow/rw/etc/sysconfig/network/ifcfg-vlan004...OK ifcfg-vlan007-----------------\u0026gt; /mnt/cow/rw/etc/sysconfig/network/ifcfg-vlan007...OK ifroute-lan0------------------\u0026gt; /mnt/cow/rw/etc/sysconfig/network/ifroute-lan0...OK ifroute-vlan002---------------\u0026gt; /mnt/cow/rw/etc/sysconfig/network/ifroute-vlan002...OK CAN.conf----------------------\u0026gt; /mnt/cow/rw/etc/dnsmasq.d/CAN.conf...OK HMN.conf----------------------\u0026gt; /mnt/cow/rw/etc/dnsmasq.d/HMN.conf...OK NMN.conf----------------------\u0026gt; /mnt/cow/rw/etc/dnsmasq.d/NMN.conf...OK mtl.conf----------------------\u0026gt; /mnt/cow/rw/etc/dnsmasq.d/mtl.conf...OK statics.conf------------------\u0026gt; /mnt/cow/rw/etc/dnsmasq.d/statics.conf...OK conman.conf-------------------\u0026gt; /mnt/cow/rw/etc/conman.conf...OK   Set the hostname and print it into the hostname file.\n Do not confuse other administrators and name the LiveCD \u0026ldquo;ncn-m001\u0026rdquo;, please append the \u0026ldquo;-pit\u0026rdquo; suffix which will indicate that the node is booted from the LiveCD.\n linux# echo \u0026#34;${SYSTEM_NAME}-ncn-m001-pit\u0026#34; \u0026gt;/mnt/cow/rw/etc/hostname   Unmount the Overlay, we are done with it\nlinux# umount -v /mnt/cow   Make directories needed for basecamp (cloud-init) and the squashFS images\nlinux# mkdir -pv /mnt/pitdata/configs/ /mnt/pitdata/data/{k8s,ceph}/   Copy basecamp data\nlinux# csi pit populate pitdata ${SYSTEM_NAME} /mnt/pitdata/configs -b Expected output looks similar to the following:\ndata.json---------------------\u0026gt; /mnt/pitdata/configs/data.json...OK   Update CA Cert on the copied data.json file. Provide the path to the data.json, the path to our customizations.yaml, and finally the sealed_secrets.key\nlinux# csi patch ca \\ --cloud-init-seed-file /mnt/pitdata/configs/data.json \\ --customizations-file /mnt/pitdata/prep/site-init/customizations.yaml \\ --sealed-secret-key-file /mnt/pitdata/prep/site-init/certs/sealed_secrets.key   Copy k8s artifacts:\nlinux# csi pit populate pitdata \u0026#34;${CSM_PATH}/images/kubernetes/\u0026#34; /mnt/pitdata/data/k8s/ -kiK Expected output looks similar to the following:\n5.3.18-24.37-default-0.0.6.kernel-----------------\u0026gt; /mnt/pitdata/data/k8s/...OK initrd.img-0.0.6.xz-------------------------------\u0026gt; /mnt/pitdata/data/k8s/...OK kubernetes-0.0.6.squashfs-------------------------\u0026gt; /mnt/pitdata/data/k8s/...OK   Copy Ceph/storage artifacts:\nlinux# csi pit populate pitdata \u0026#34;${CSM_PATH}/images/storage-ceph/\u0026#34; /mnt/pitdata/data/ceph/ -kiC Expected output looks similar to the following:\n5.3.18-24.37-default-0.0.5.kernel-----------------\u0026gt; /mnt/pitdata/data/ceph/...OK initrd.img-0.0.5.xz-------------------------------\u0026gt; /mnt/pitdata/data/ceph/...OK storage-ceph-0.0.5.squashfs-----------------------\u0026gt; /mnt/pitdata/data/ceph/...OK   Quit the typescript session with the exit command and copy the file (csm-install-usb..txt) to the data partition on the USB drive.\nlinux# mkdir -pv /mnt/pitdata/prep/logs linux# exit linux# cp ~/csm-install-usb.*.txt /mnt/pitdata/prep/logs   Unmount the data partition:\nlinux# cd; umount -v /mnt/pitdata   Now the USB device may be reattached to the management node, or if it was made on the management node then it can now reboot into the LiveCD.\n5. Boot the LiveCD Some systems will boot the USB device automatically if no other OS exists (bare-metal). Otherwise the administrator may need to use the BIOS Boot Selection menu to choose the USB device.\nIf an administrator is rebooting a node into the LiveCD, versus booting a bare-metal or wiped node, then efibootmgr will deterministically set the boot order.\nSee the set boot order page for more information on this topic..\n UEFI booting must be enabled to find the USB device\u0026rsquo;s EFI bootloader.\n   Start a typescript on an external system, such as a laptop or Linux system, to record this section of activities done on the console of ncn-m001 via IPMI.\nexternal# script -a boot.livecd.$(date +%Y-%m-%d).txt external# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39; Confirm that the IPMI credentials work for the BMC by checking the power status.\nexternal# export SYSTEM_NAME=eniac external# export USERNAME=root external# export IPMI_PASSWORD=changeme external# ipmitool -I lanplus -U $USERNAME -E -H ${SYSTEM_NAME}-ncn-m001-mgmt chassis power status Connect to the IPMI console.\nexternal# ipmitool -I lanplus -U $USERNAME -E -H ${SYSTEM_NAME}-ncn-m001-mgmt sol activate ncn-m001#   Reboot\nncn-m001# reboot   Watch the shutdown and boot from the ipmitool session to the console terminal. The typescript can be discarded, otherwise if issues arise then it should be submitted with the bug report.\n An integrity check runs before Linux starts by default, it can be skipped by selecting \u0026ldquo;OK\u0026rdquo; in its prompt.\n 5.1 First Login On first login (over SSH or at local console) the LiveCD will prompt the administrator to change the password.\n  The initial password is empty; set the username of root and press return twice:\npit login: root Expected output looks similar to the following:\nPassword: \u0026lt;-------just press Enter here for a blank password You are required to change your password immediately (administrator enforced) Changing password for root. Current password: \u0026lt;------- press Enter here, again, for a blank password New password: \u0026lt;------- type new password Retype new password:\u0026lt;------- retype new password Welcome to the CRAY Pre-Install Toolkit (LiveOS)  NOTE If this password is forgotten, it can be reset by mounting the USB device on another computer. See Reset root Password on LiveCD for information on clearing the password.\n   Disconnect from IPMI console.\nOnce the network is up so that SSH to the node works, disconnect from the IPMI console.\nYou can disconnect from the IPMI console by using the \u0026ldquo;~.\u0026rdquo;, that is, the tilde character followed by a period character.\nLog in via ssh to the node as root.\nexternal# ssh root@${SYSTEM_NAME}-ncn-m001 pit# Note: The hostname should be similar to eniac-ncn-m001-pit when booted from the LiveCD, but it will be shown as \u0026ldquo;pit#\u0026rdquo; in the command prompts from this point onward.\n  6. Configure the Running LiveCD   Mount the data partition\n The data partition is set to fsopt=noauto to facilitate LiveCDs over virtual-ISO mount. USB installations need to mount this manually.\n pit# mount -vL PITDATA   Start a typescript to record this section of activities done on ncn-m001 while booted from the LiveCD.\npit# mkdir -pv /var/www/ephemeral/prep/logs pit# script -af /var/www/ephemeral/prep/logs/booted-csm-livecd.$(date +%Y-%m-%d).txt pit# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39;   Download and install/upgrade the workaround and documentation RPMs.\nIf this machine does not have direct Internet access these RPMs will need to be externally downloaded and then copied to the system.\nImportant: In an earlier step, the CSM release plus any patches, workarounds, or hotfixes were downloaded to a system using the instructions in Check for Latest Workarounds and Documentation Updates. Use that set of RPMs rather than downloading again.\nlinux# wget https://storage.googleapis.com/csm-release-public/shasta-1.5/docs-csm/docs-csm-latest.noarch.rpm linux# wget https://storage.googleapis.com/csm-release-public/shasta-1.5/csm-install-workarounds/csm-install-workarounds-latest.noarch.rpm linux# scp -p docs-csm-*rpm csm-install-workarounds-*rpm ncn-m001:/root linux# ssh ncn-m001 pit# rpm -Uvh --force docs-csm-latest.noarch.rpm pit# rpm -Uvh --force csm-install-workarounds-latest.noarch.rpm   Check the pit-release version.\npit# cat /etc/pit-release Expected output looks similar to the following:\nVERSION=1.4.9 TIMESTAMP=20210309034439 HASH=g1e67449   First login workarounds\nFollow the workaround instructions for the first-livecd-login breakpoint.\n  Start services\npit# systemctl start nexus pit# systemctl start basecamp pit# systemctl start conman   Set shell environment variables.\nThe CSM_RELEASE and CSM_PATH variables will be used later.\npit# cd /var/www/ephemeral pit# export CSM_RELEASE=csm-x.y.z pit# echo $CSM_RELEASE pit# export CSM_PATH=$(pwd)/${CSM_RELEASE} pit# echo $CSM_PATH   Install Goss Tests and Server\nThe following assumes the CSM_PATH environment variable is set to the absolute path of the unpacked CSM release.\npit# rpm -Uvh --force $(find ${CSM_PATH}/rpm/cray/csm/ -name \u0026#34;goss-servers*.rpm\u0026#34; | sort -V | tail -1) pit# rpm -Uvh --force $(find ${CSM_PATH}/rpm/cray/csm/ -name \u0026#34;csm-testing*.rpm\u0026#34; | sort -V | tail -1)   Verify the system:\npit# csi pit validate --network pit# csi pit validate --services   If dnsmasq is dead, restart it with systemctl restart dnsmasq.    In addition, the final output from validating the services should have information about the nexus and basecamp containers/images similar this example.\n CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ff7c22c6c6cb dtr.dev.cray.com/sonatype/nexus3:3.25.0 sh -c ${SONATYPE_... 3 minutes ago Up 3 minutes ago nexus c7638b573b93 dtr.dev.cray.com/cray/metal-basecamp:1.1.0-1de4aa6 5 minutes ago Up 5 minutes ago basecamp   Follow directions in the output from the \u0026lsquo;csi pit validate\u0026rsquo; commands for failed validations before continuing.\n  Next Topic After completing this procedure the next step is to configure the management network switches.\n See Configure Management Network Switches  "
},
{
	"uri": "/docs-csm/en-11/install/bootstrap_livecd_remote_iso/",
	"title": "Bootstrap Pit Node From Livecd Remote Iso",
	"tags": [],
	"description": "",
	"content": "Bootstrap PIT Node from LiveCD Remote ISO The Pre-Install Toolkit (PIT) node needs to be bootstrapped from the LiveCD. There are two media available to bootstrap the PIT node\u0026ndash;the RemoteISO or a bootable USB device. This procedure describes using the RemoteISO. If not using the RemoteISO, see Bootstrap PIT Node from LiveCD USB\nThe installation process is similar to the USB based installation with adjustments to account for the lack of removable storage.\nImportant: Before starting this procedure be sure to complete the procedure to Prepare Configuration Payload for the relevant installation scenario.\nTopics:  Known Compatibility Issues Attaching and Booting the LiveCD with the BMC First Login Configure the Running LiveCD Next Topic  Details 1. Known Compatibility Issues The LiveCD Remote ISO has known compatibility issues for nodes from certain vendors.\n Intel nodes should not attempt to bootstrap using the LiveCD Remote ISO method. Instead use Bootstrap PIT Node from LiveCD USB Gigabyte nodes should not attempt to bootstrap using the LiveCD Remote ISO method. Instead use Bootstrap PIT Node from LiveCD USB  2. Attaching and Booting the LiveCD with the BMC  Warning: If this is a re-installation on a system that still has a USB device from a prior installation, then that USB device must be wiped before continuing. Failing to wipe the USB, if present, may result in confusion. If the USB is still booted, then it can wipe itself using the basic wipe from Wipe NCN Disks for Reinstallation.\n Obtain and attach the LiveCD cray-pre-install-toolkit ISO file to the BMC. Depending on the vendor of the node, the instructions for attaching to the BMC will differ.\n  The CSM software release should be downloaded and expanded for use.\nImportant: To ensure that the CSM release plus any patches, workarounds, or hotfixes are included follow the instructions in Update CSM Product Stream\nThe cray-pre-install-toolkit ISO and other files are now available in the directory from the extracted CSM tar. The ISO will have a name similar to cray-pre-install-toolkit-sle15sp2.x86_64-1.4.10-20210514183447-gc054094.iso\n  Prepare a server on the network to host the cray-pre-install-toolkit ISO.\nThis release of CSM software, the cray-pre-install-toolkit ISO should be placed on a server which the PIT node will be able to contact via http or https.\n HPE nodes can use http or https.  Note: A shorter path name is better than a long path name on the webserver.\n The Cray Pre-Install Toolkit ISO is included in the CSM release tarball. It will have a long filename similar to cray-pre-install-toolkit-sle15sp2.x86_64-1.4.10-20210514183447-gc054094.iso, so pick a shorter name on the webserver.    See the respective procedure below to attach an ISO.\n HPE iLO BMCs [Gigabyte BMCs] Should not use the RemoteISO method. See Bootstrap PIT Node from LiveCD USB [Intel BMCs] Should not use the RemoteISO method. See Bootstrap PIT Node from LiveCD USB    The chosen procedure should have rebooted the server. Observe the server boot into the LiveCD.\n  3. First Login On first login (over SSH or at local console) the LiveCD will prompt the administrator to change the password.\n  The initial password is empty; set the username of root and press return twice.\npit login: root Expected output looks similar to the following:\nPassword: \u0026lt;-------just press Enter here for a blank password You are required to change your password immediately (administrator enforced) Changing password for root. Current password: \u0026lt;------- press Enter here, again, for a blank password New password: \u0026lt;------- type new password Retype new password:\u0026lt;------- retype new password Welcome to the CRAY Pre-Install Toolkit (LiveOS)   4. Configure the Running LiveCD   Set up the Typescript directory as well as the initial typescript. This directory will be returned to for every typescript in the entire CSM installation.\npit# mkdir -pv /var/www/ephemeral/prep/admin pit# pushd !$ pit# script -af csm-install-remoteiso.$(date +%Y-%m-%d).txt pit# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39;   Set up the site-link, enabling SSH to work. You can reconnect with SSH after this step.\n NOTICE REGARDING DHCP If your site\u0026rsquo;s network authority or network administrator has already provisioned an IPv4 address for your master node(s) external NIC(s), then skip this step.\n   Setup Variables.\n# The IPv4 Address for the nodes external interface(s); this will be provided, if not already by the site\u0026#39;s network administrator or network authority. pit# site_ip=172.30.XXX.YYY/20 pit# site_gw=172.30.48.1 pit# site_dns=172.30.84.40 # The actual NIC names for the external site interface; the first onboard or the first 1GBe PCIe (RJ-45). pit# site_nics=\u0026#39;p2p1 p2p2 p2p3\u0026#39; # another example: pit# site_nics=em1   Run the link setup script.\n NOTE : USAGE All of the /root/bin/csi-* scripts are harmless to run without parameters, doing so will dump usage statements.\n pit# /root/bin/csi-setup-lan0.sh $site_ip $site_gw $site_dns $site_nics   (recommended) print lan0, and if it has an IP address, then exit console and log in again using SSH. The SSH connection will provide larger window sizes and better bufferhandling (screen wrapping).\npit# ip a show lan0 pit# exit external# ssh root@${SYSTEM_NAME}-ncn-m001   (recommended) After reconnecting, resume the typescript (the -a appends to an existing script).\npit# pushd /var/www/ephemeral/prep/admin pit# script -af $(ls -tr csm-install-remoteiso* | head -n 1) pit# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39;   Check hostname.\npit# hostnamectl  NOTE If the hostname returned by the hostnamectl command is still pit, then re-run the above script with the same parameters. Otherwise feel free to set the hostname by hand with hostnamectl, please continue to use the -pit suffix to prevent masquerading a PIT node as a real NCN to administrators and automation.\n     Find a local disk for storing product installers.\npit# disk=\u0026#34;$(lsblk -l -o SIZE,NAME,TYPE,TRAN | grep -E \u0026#39;(sata|nvme|sas)\u0026#39; | sort -h | awk \u0026#39;{print $2}\u0026#39; | head -n 1 | tr -d \u0026#39;\\n\u0026#39;)\u0026#34; pit# parted --wipesignatures -m --align=opt --ignore-busy -s /dev/$disk -- mklabel gpt mkpart primary ext4 2048s 100% pit# mkfs.ext4 -L PITDATA \u0026#34;/dev/${disk}1\u0026#34;   Mount local disk, check the output of each command as it goes.\npit# mount -v -L PITDATA pit# pushd /var/www/ephemeral pit/var/www/ephemeral# mkdir -v prep configs data   Download the CSM software release to the PIT node.\nImportant: In an earlier step, the CSM release plus any patches, workarounds, or hotfixes were downloaded to a system using the instructions in Update CSM Product Stream Either copy from that system to the PIT node or set the ENDPOINT variable to URL and use wget.\n  Set helper variables\npit:/var/www/ephemeral# export ENDPOINT=https://arti.dev.cray.com/artifactory/shasta-distribution-stable-local/csm pit:/var/www/ephemeral# export CSM_RELEASE=csm-x.y.z pit:/var/www/ephemeral# export SYSTEM_NAME=eniac   Save the CSM_RELEASE for usage later; all subsequent shell sessions will have this var set.\n# Prepend a new line to assure we add on a unique line and not at the end of another. pit:/var/www/ephemeral# echo -e \u0026#34;\\nCSM_RELEASE=$CSM_RELEASE\u0026#34; \u0026gt;\u0026gt;/etc/environment   Fetch the release tarball.\npit:/var/www/ephemeral# wget ${ENDPOINT}/${CSM_RELEASE}.tar.gz -O /var/www/ephemeral/${CSM_RELEASE}.tar.gz   Expand the tarball on the PIT node.\npit:/var/www/ephemeral# tar -zxvf ${CSM_RELEASE}.tar.gz pit:/var/www/ephemeral# ls -l ${CSM_RELEASE}   Copy the artifacts into place.\npit/var/www/ephemeral# mkdir -pv data/{k8s,ceph} pit/var/www/ephemeral# rsync -a -P --delete ./${CSM_RELEASE}/images/kubernetes/ ./data/k8s/ pit/var/www/ephemeral# rsync -a -P --delete ./${CSM_RELEASE}/images/storage-ceph/ ./data/ceph/    The PIT ISO, Helm charts/images, and bootstrap RPMs are now available in the extracted CSM tar.\n   Install/upgrade the CSI and testing RPMs.\npit:/var/www/ephemeral# rpm -Uvh --force $(find ./${CSM_RELEASE}/rpm/cray/csm/ -name \u0026#34;cray-site-init-*.x86_64.rpm\u0026#34; | sort -V | tail -1) pit:/var/www/ephemeral# rpm -Uvh --force $(find ./${CSM_RELEASE}/rpm/cray/csm/ -name \u0026#34;goss-servers*.rpm\u0026#34; | sort -V | tail -1) pit:/var/www/ephemeral# rpm -Uvh --force $(find ./${CSM_RELEASE}/rpm/cray/csm/ -name \u0026#34;csm-testing*.rpm\u0026#34; | sort -V | tail -1)   Show the version of CSI installed.\npit# csi version Expected output looks similar to the following:\nCRAY-Site-Init build signature... Build Commit : b3ed3046a460d804eb545d21a362b3a5c7d517a3-release-shasta-1.4 Build Time : 2021-02-04T21:05:32Z Go Version : go1.14.9 Git Version : b3ed3046a460d804eb545d21a362b3a5c7d517a3 Platform : linux/amd64 App. Version : 1.5.18   Download and install/upgrade the workaround and documentation RPMs.\nIf this machine does not have direct Internet access, these RPMs will need to be externally downloaded and then copied to the system.\nImportant: In an earlier step, the CSM release plus any patches, workarounds, or hotfixes were downloaded to a system using the instructions in Check for Latest Workarounds and Documentation Updates. Use that set of RPMs rather than downloading again.\nlinux# wget https://storage.googleapis.com/csm-release-public/shasta-1.5/docs-csm/docs-csm-latest.noarch.rpm linux# wget https://storage.googleapis.com/csm-release-public/shasta-1.5/csm-install-workarounds/csm-install-workarounds-latest.noarch.rpm linux# scp -p docs-csm-*rpm csm-install-workarounds-*rpm ncn-m001:/root linux# ssh ncn-m001 pit# rpm -Uvh --force docs-csm-latest.noarch.rpm pit# rpm -Uvh --force csm-install-workarounds-latest.noarch.rpm   Generate configuration files.\nSome files are needed for generating the configuration payload. See these topics in Prepare Configuration Payload if you have not already prepared the information for this system. At this time see Create HMN Connections JSON for instructions about creating the hmn_connections.json.\n Command Line Configuration Payload Configuration Payload Files  Pull these files into the current working directory.\n application_node_config.yaml (optional - see below) cabinets.yaml (optional - see below) hmn_connections.json ncn_metadata.csv switch_metadata.csv system_config.yaml (see below)   The optional application_node_config.yaml file may be provided to further assign application nodes to roles and subroles in the HSM. See Create Application Node YAML\n  The optional cabinets.yaml file allows cabinet naming and numbering as well as some VLAN overrides. See Create Cabinets YAML.\n  The system_config.yaml is required for a reinstall because it was created during a previous install. For a first time install, the information in it can be provided as command line arguments to csi config init.\n   Change into the preparation directory.\nlinux# mkdir -pv /var/www/ephemeral/prep linux# cd /var/www/ephemeral/prep After gathering the files into this working directory, generate your configurations.\n  If doing a reinstall and have the system_config.yaml parameter file available, then generate the system configuration reusing this parameter file (see avoiding parameters.\nIf not doing a reinstall of Shasta software, then the system_config.yaml file will not be available, so skip the rest of this step.\n  Check for the configuration files. The needed files should be in the current directory.\nlinux# ls -1 Expected output looks similar to the following:\napplication_node_config.yaml cabinets.yaml hmn_connections.json ncn_metadata.csv switch_metadata.csv system_config.yaml   Set an environment variable so this system name can be used in later commands.\nlinux# export SYSTEM_NAME=eniac   Generate the system configuration.\nlinux# csi config init A new directory matching your --system-name argument will now exist in your working directory.\nThese warnings from csi config init for issues in hmn_connections.json can be ignored.\n The node with the external connection (ncn-m001) will have a warning similar to this because its BMC is connected to the site and not the HMN like the other management NCNs. It can be ignored.  \u0026quot;Couldn't find switch port for NCN: x3000c0s1b0\u0026quot;  An unexpected component may have this message. If this component is an application node with an unusual prefix, it should be added to the application_node_config.yaml file. Then rerun csi config init. See the procedure to Create Application Node Config YAML  {\u0026#34;level\u0026#34;:\u0026#34;warn\u0026#34;,\u0026#34;ts\u0026#34;:1610405168.8705149,\u0026#34;msg\u0026#34;:\u0026#34;Found unknown source prefix! If this is expected to be an Application node, please update application_node_config.yaml\u0026#34;,\u0026#34;row\u0026#34;: {\u0026#34;Source\u0026#34;:\u0026#34;gateway01\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u33\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3002\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u48\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j29\u0026#34;}}   If a cooling door is found in hmn_connections.json, there may be a message like the following. It can be safely ignored.\n{\u0026#34;level\u0026#34;:\u0026#34;warn\u0026#34;,\u0026#34;ts\u0026#34;:1612552159.2962296,\u0026#34;msg\u0026#34;:\u0026#34;Cooling door found, but xname does not yet exist for cooling doors!\u0026#34;,\u0026#34;row\u0026#34;: {\u0026#34;Source\u0026#34;:\u0026#34;x3000door-Motiv\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34; \u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u36\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j27\u0026#34;}}     Skip the next step and continue with the CSI Workarounds.\n    If doing a first time install or the system_config.yaml parameter file for a reinstall is not available, generate the system configuration.\nIf doing a first time install, this step is required. If you did the previous step as part of a reinstall, skip this.\n  Check for the configuration files. The needed files should be in the current directory.\nlinux# ls -1 Expected output looks similar to the following:\napplication_node_config.yaml cabinets.yaml hmn_connections.json ncn_metadata.csv switch_metadata.csv   Set an environment variable so this system name can be used in later commands.\nlinux# export SYSTEM_NAME=eniac   Generate the system configuration. See below for an explanation of the command line parameters and some common settings.\nlinux# csi config init \\  --bootstrap-ncn-bmc-user root \\  --bootstrap-ncn-bmc-pass changeme \\  --system-name ${SYSTEM_NAME} \\  --can-cidr 10.103.11.0/24 \\  --can-external-dns 10.103.11.113 \\  --can-gateway 10.103.11.1 \\  --can-static-pool 10.103.11.112/28 \\  --can-dynamic-pool 10.103.11.128/25 \\  --nmn-cidr 10.252.0.0/17 \\  --hmn-cidr 10.254.0.0/17 \\  --ntp-pool time.nist.gov \\  --site-domain dev.cray.com \\  --site-ip 172.30.53.79/20 \\  --site-gw 172.30.48.1 \\  --site-nic p1p2 \\  --site-dns 172.30.84.40 \\  --install-ncn-bond-members p1p1,p10p1 \\  --application-node-config-yaml application_node_config.yaml \\  --cabinets-yaml cabinets.yaml \\  --hmn-mtn-cidr 10.104.0.0/17 \\  --nmn-mtn-cidr 10.100.0.0/17 \\  --bgp-peers aggregation A new directory matching your --system-name argument will now exist in your working directory.\n After generating a configuration, a visual audit of the generated files for network data should be performed.\n Run the command csi config init --help to get more information about the parameters mentioned in the example command above and others which are available.\nNotes about parameters to csi config init:\n The application_node_config.yaml file is optional, but if you have one describing the mapping between prefixes in hmn_connections.csv that should be mapped to HSM subroles, you need to include a command line option to have it used. See Create Application Node YAML. The bootstrap-ncn-bmc-user and bootstrap-ncn-bmc-pass must match what is used for the BMC account and its password for the management NCNs. Set site parameters (site-domain, site-ip, site-gw, site-nic, site-dns) for the information which connects ncn-m001 (the PIT node) to the site. The site-nic is the interface on this node connected to the site. There are other interfaces possible, but the install-ncn-bond-members are typically:  p1p1,p10p1 for HPE nodes p1p1,p1p2 for Gigabyte nodes p801p1,p801p2 for Intel nodes   If you are not using a cabinets-yaml file, set the three cabinet parameters (mountain-cabinets, hill-cabinets, and river-cabinets) to the number of each cabinet which are part of this system. The starting cabinet number for each type of cabinet (for example, starting-mountain-cabinet) has a default that can be overridden. See the csi config init --help For systems that use non-sequential cabinet ID numbers, use cabinets-yaml to include the cabinets.yaml file. This file can include information about the starting ID for each cabinet type and number of cabinets which have separate command line options, but is a way to specify explicitly the id of every cabinet in the system. If you are using a cabinets-yaml file, flags specified on the csi command-line related to cabinets will be ignored. See Create Cabinets YAML. An override to default cabinet IPv4 subnets can be made with the hmn-mtn-cidr and nmn-mtn-cidr parameters. By default, spine switches are used as MetalLB peers. Use --bgp-peers aggregation to use aggregation switches instead. Several parameters (can-gateway, can-cidr, can-static-pool, can-dynamic-pool) describe the CAN (Customer Access network). The can-gateway is the common gateway IP address used for both spine switches and commonly referred to as the Virtual IP address for the CAN. The can-cidr is the IP subnet for the CAN assigned to this system. The can-static-pool and can-dynamic-pool are the MetalLB address static and dynamic pools for the CAN. The can-external-dns is the static IP address assigned to the DNS instance running in the cluster to which requests the cluster subdomain will be forwarded. The can-external-dns IP address must be within the can-static-pool range. Set ntp-pool to a reachable NTP server  These warnings from csi config init for issues in hmn_connections.json can be ignored.\n  The node with the external connection (ncn-m001) will have a warning similar to this because its BMC is connected to the site and not the HMN like the other management NCNs. It can be ignored.\n\u0026quot;Couldn't find switch port for NCN: x3000c0s1b0\u0026quot;   An unexpected component may have this message. If this component is an application node with an unusual prefix, it should be added to the application_node_config.yaml file. Then rerun csi config init. See the procedure to Create Application Node Config YAML\n{\u0026#34;level\u0026#34;:\u0026#34;warn\u0026#34;,\u0026#34;ts\u0026#34;:1610405168.8705149,\u0026#34;msg\u0026#34;:\u0026#34;Found unknown source prefix! If this is expected to be an Application node, please update application_node_config.yaml\u0026#34;,\u0026#34;row\u0026#34;: {\u0026#34;Source\u0026#34;:\u0026#34;gateway01\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34;u33\u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3002\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u48\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j29\u0026#34;}}   If a cooling door is found in hmn_connections.json, there may be a message like the following. It can be safely ignored.\n{\u0026#34;level\u0026#34;:\u0026#34;warn\u0026#34;,\u0026#34;ts\u0026#34;:1612552159.2962296,\u0026#34;msg\u0026#34;:\u0026#34;Cooling door found, but xname does not yet exist for cooling doors!\u0026#34;,\u0026#34;row\u0026#34;: {\u0026#34;Source\u0026#34;:\u0026#34;x3000door-Motiv\u0026#34;,\u0026#34;SourceRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;SourceLocation\u0026#34;:\u0026#34; \u0026#34;,\u0026#34;DestinationRack\u0026#34;:\u0026#34;x3000\u0026#34;,\u0026#34;DestinationLocation\u0026#34;:\u0026#34;u36\u0026#34;,\u0026#34;DestinationPort\u0026#34;:\u0026#34;j27\u0026#34;}}     Continue with the next step to apply the csi-config workarounds.\n      \n  CSI Workarounds\nFollow the workaround instructions for the csi-config breakpoint.\n  Copy the interface config files generated earlier by csi config init into /etc/sysconfig/network/ with the first option or use the provided scripts in the second option below.\n  Option 1: Copy PIT files.\npit# cp -pv /var/www/ephemeral/prep/${SYSTEM_NAME}/pit-files/* /etc/sysconfig/network/ pit# wicked ifreload all pit# systemctl restart wickedd-nanny \u0026amp;\u0026amp; sleep 5   Option 2: Set up dnsmasq by hand.\npit# /root/bin/csi-setup-vlan002.sh $nmn_cidr pit# /root/bin/csi-setup-vlan004.sh $hmn_cidr pit# /root/bin/csi-setup-vlan007.sh $can_cidr     Check that IP addresses are set for each interface and investigate any failures.\n  Check IP addresses, do not run tests if these are missing and instead start triage.\npit# wicked show bond0 vlan002 vlan004 vlan007 bond0 up link: #7, state up, mtu 1500 type: bond, mode ieee802-3ad, hwaddr b8:59:9f:fe:49:d4 config: compat:suse:/etc/sysconfig/network/ifcfg-bond0 leases: ipv4 static granted addr: ipv4 10.1.1.2/16 [static] vlan002 up link: #8, state up, mtu 1500 type: vlan bond0[2], hwaddr b8:59:9f:fe:49:d4 config: compat:suse:/etc/sysconfig/network/ifcfg-vlan002 leases: ipv4 static granted addr: ipv4 10.252.1.4/17 [static] route: ipv4 10.92.100.0/24 via 10.252.0.1 proto boot vlan007 up link: #9, state up, mtu 1500 type: vlan bond0[7], hwaddr b8:59:9f:fe:49:d4 config: compat:suse:/etc/sysconfig/network/ifcfg-vlan007 leases: ipv4 static granted addr: ipv4 10.102.9.5/24 [static] vlan004 up link: #10, state up, mtu 1500 type: vlan bond0[4], hwaddr b8:59:9f:fe:49:d4 config: compat:suse:/etc/sysconfig/network/ifcfg-vlan004 leases: ipv4 static granted addr: ipv4 10.254.1.4/17 [static]   Run tests, inspect failures.\npit# csi pit validate --network     Copy the service config files generated earlier by csi config init for DNSMasq, Metal Basecamp (cloud-init), and Conman.\n  Copy files (files only, -r is expressly not used).\npit# cp -pv /var/www/ephemeral/prep/${SYSTEM_NAME}/dnsmasq.d/* /etc/dnsmasq.d/ pit# cp -pv /var/www/ephemeral/prep/${SYSTEM_NAME}/conman.conf /etc/conman.conf pit# cp -pv /var/www/ephemeral/prep/${SYSTEM_NAME}/basecamp/* /var/www/ephemeral/configs/   Enable, and fully restart all PIT services.\npit# systemctl enable basecamp nexus dnsmasq conman pit# systemctl stop basecamp nexus dnsmasq conman pit# systemctl start basecamp nexus dnsmasq conman     Start and configure NTP on the LiveCD for a fallback/recovery server.\npit# /root/bin/configure-ntp.sh   Check that our services are ready and investigate any test failures.\npit# csi pit validate --services   Mount a shim to match the Shasta-CFG steps' directory structure.\npit# mkdir -vp /mnt/pitdata pit# mount -v -L PITDATA /mnt/pitdata   The following procedure will set up customized CA certificates for deployment using Shasta-CFG.\n Prepare Site-Init to create and prepare the site-init directory for your system.    Next Topic After completing this procedure the next step is to configure the management network switches.\n See Configure Management Network Switches  "
},
{
	"uri": "/docs-csm/en-11/install/boot_livecd_virtual_iso/",
	"title": "Boot Livecd Virtual Iso",
	"tags": [],
	"description": "",
	"content": "Boot LiveCD Virtual ISO This page will walk-through booting the LiveCD .iso file directly onto a BMC.\nTopics:  Requirements BMCs' Virtual Mounts  HPE iLO BMCs Gigabyte BMCs   Configuring  Backing up the Overlay COW FS Restoring from an Overlay COW FS Backup    Details Requirements A Cray Pre-Install Toolkit ISO is required for this process. This ISO can be obtained from:\n The Cray Pre-Install Toolkit ISO included in a CSM release tar file. It will have a filename similar to cray-pre-install-toolkit-sle15sp2.x86_64-1.4.10-20210514183447-gc054094.iso  BMCs' Virtual Mounts Most BMCs offer a Web Interface for controlling the node and for providing access to its BIOS and firmware.\nRefer to the following pages based on your node vendor for help mounting an ISO image:\n HPE iLO BMCs Gigabyte  HPE iLO BMCs HPE iLO BMCs allow for booting directly from an HTTP-accessible ISO location.\n Enter the Virtual Media URL, select Boot on Next Reset, and click Insert Media.\n  Next reboot by selecting Reset in the top right power menu.\n  Open the virtual terminal by choosing the HTML5 Console option when clicking the terminal image in the bottom left corner.\n  NOTE It may appear that the boot is stalled at a line of EXT4-fs (loop1): mounted ... or Starting dracut pre-mount hook.... This is the step when it actually begins downloading the ISO\u0026rsquo;s squashfs root file system and can take a few minutes\n Gigabyte BMCs Gigabyte BMCs allow for booting over HTTP.\nNote: Because of problems in the Gigabyte firmware, do not try to boot over NFS or CIFS.\nGo to the BMC settings and setup the remote ISO for your protocol and node.\nWeb Interface\nAccess your BMC\u0026rsquo;s web interface and navigate to Settings -\u0026gt; Media Redirection Settings -\u0026gt; General Settings.\nEnable Remote Media Support and Mount CD/DVD and then fill in the server IP address or DNS name and the path to the server.\n NOTE The Gigabyte URL appears to not allow certain characters and has a limit on path length. You may need to move or rename the ISO to a location with a smaller file name.\n Next navigate to Image Redirection -\u0026gt; Remote Images and click on the Start button to start the Virtual ISO mount.\nFinally, reboot the node and select the Virtual CDROM option from the manual boot options.\nConfiguring  Configuring  Backing up the Overlay COW FS Restoring from an Overlay COW FS Backup    The ISO boots with no password, requiring one be set on first login. Continue the bootstrap process by setting the root password following the procedure First Login.\n NOTE The root OS / directory is writable without persistence. This means that restarting the machine will result in all changes being lost. Before restarting, consider following Backing up the Overlay COW FS and the accompanying Restoring from an Overlay COW FS Backup section.\n Backing up the Overlay COW FS You can backup the writable overlay upper-dir so that your changes are not lost after a reboot or when updating your ISO.\nThis requires that you have a location to which you can scp a tar file as a backup.\ntar czf /run/overlay.tar.gz -C /run/overlayfs/rw . scp /run/overlay.tar.gz \u0026lt;somelocation\u0026gt;  NOTE If you want to reduce the size of the backup you can also delete any squashfs files first, or exclude them in the tar command using --exclude='*.squashfs'. You will then need to re-populate those after you restore your backup\n Restoring from an Overlay COW FS Backup Restore a backed up tar file from the previous command with\nscp \u0026lt;somelocation\u0026gt; /run/overlay.tar.gz tar xf /run/overlay.tar.gz -C /run/overlayfs/rw mount -o remount / If you excluded the squashfs files from the backup, you will also need to repopulate them following the configuration section.\n"
},
{
	"uri": "/docs-csm/en-11/install/aruba_snmp_known_issue_10_06_0010/",
	"title": "Aruba Snmp Known Issue",
	"tags": [],
	"description": "",
	"content": "Aruba SNMP Known Issue Affected Devices 8320/8325/8360 Aruba CX\nAruba Defect CR 153440\nAruba Public Documentation/Images Aruba support portal:\n https://asp.arubanetworks.com/\n Aruba 8325 release notes 10.06.0120:\n https://www.arubanetworks.com/techdocs/AOS-CX/10.06/RN/5200-8179.pdf\n Aruba 8360 release notes 10.06.0120:\n https://www.arubanetworks.com/techdocs/AOS-CX/10.06/RN/5200-8180.pdf\n Where the Issue May Occur During Install During initial network discovery of Nodes, SNMP may not accurately report MAC-address from Aruba Leaf/Spine switches. This will lead to a situation where not all connected devices are discovered as expected. Further troubleshooting would show that the SNMP walk output would not match \u0026lsquo;show mac-address-table\u0026rsquo; command output from the switch.\nSymptom/Scenario The SNMP walk output of OID BRIDGE-MIB::dot1dTpFdbAddress returns fewer MAC addresses than the show mac-address-table command.\nWorkaround Delete the SNMP configuration from the affected switch and reconfigure the SNMP server. Refer to the \u0026ldquo;SNMP configuration\u0026rdquo; section of the Configure Aruba Leaf Switch procedure for more information.\n"
},
{
	"uri": "/docs-csm/en-11/install/8325_mac_learning_hotfix/",
	"title": "Hotfix To Workaround Known Mac-learning Issue With 8325",
	"tags": [],
	"description": "",
	"content": "Hotfix to workaround known mac-learning issue with 8325 Issue description  Aruba CR: 90598\nAffected platform: 8325\nSymptom: MAC learning stops.\nScenario: Under extremely rare DMA stress conditions, anL2 learning thread may timeout and exit preventing future MAC learning.\nWorkaround: Reboot the switch or monitor the L2 thread and restart it with an NAE script.\nFixed in: 10.06.0130, 10.7.0010 and above.\nAruba release notes\n To fix the issue without upgrading software You can run a NAE script on the 8325 platform switches to resolve mac learning issue.\nImportant information  This NAE script creates a bash script in /tmp and runs every 60s The script writes file to storage every 60s (NAE alert file) There are no controls over alert status Event log is created when a problem is detected  BCML2X has quit unexpectedly, attempting to restart...   You can also grep the error from /var/log/messages REST API URI is /rest/v10.04/logs/event?SYSLOG_IDENTIFIER=root\u0026amp;since=yesterday Delete agent and script after upgrading to 10.06.0130+ Monitor eMMC health if you plan on running for a long time Command to run on 8325 switches: show system resource | include utiliz  The file locations in doc-csm  The NAE script is located at: ../docs-csm/upgrade/1.0/scripts/aruba/L2X-Watchdog-creates-bash-script.py Automatic NAE install script is located at: ../docs-csm/upgrade/1.0/scripts/aruba/nae_upload.py  Automated install of NAE script Prerequisites  The nae-upload.py script relies on /etc/hosts file to pull IP addresses of the switch. Without this information the script will not run. You have 8325 in your setup that is running software version below 10.06.0130. Script assumes you are using default username admin for the switch and it will prompt you for the password.  NOTE: The nae-upload.py script automatically detects 8325s and only applies the fix to this platform.\nHow to run the install script   Run the following command:\nncn-m001# ./docs-csm/upgrade/1.0/scripts/aruba/nae_upload.py   Type in your switch password and the script will upload and enable the NAE script.\n  Manual Installation of the NAE script:   Log in to an AOS-CX device via the Web User Interface. Click on the Analytics section on the left, then click on the Scripts button in the top, middle section.\n  On the Scripts page, install the script from your PC to your AOS-CX device by clicking the Upload button on the scripts page and navigating to the file location on your PC.\n  After you have the script on the AOS-CX device, you now need to create an agent. On the Scripts page, you can click the Create Agent button and a Create Agent popup box will appear.\n Give the Agent a name (no spaces). NOTE: You can leave all other default values and click Create.    Navigate you to the Agents page, where you can click on the name of the Agent you made to confirm it is running and no errors are generated.\n The Network Analytics Engine will monitor the switch and automatically fix the mac learning issue.    Known Error Messages Incorrect Password ncn-m001# ./nae_upload.py Switch login password: Traceback (most recent call last): File \u0026#34;./nae_upload.py\u0026#34;, line 57, in \u0026lt;module\u0026gt; platform = system.json() File \u0026#34;/usr/lib/python3.6/site-packages/requests/models.py\u0026#34;, line 898, in json return complexjson.loads(self.text, **kwargs) File \u0026#34;/usr/lib64/python3.6/site-packages/simplejson/__init__.py\u0026#34;, line 518, in loads return _default_decoder.decode(s) File \u0026#34;/usr/lib64/python3.6/site-packages/simplejson/decoder.py\u0026#34;, line 373, in decode raise JSONDecodeError(\u0026#34;Extra data\u0026#34;, s, end, len(s)) simplejson.errors.JSONDecodeError: Extra data: line 1 column 5 - line 1 column 27 (char 4 - 26) Script Already Loaded ncn-m001# ./nae_upload.py Switch login password: L2X-Watchdog NAE script is already installed on sw-spine-001. L2X-Watchdog NAE script is already installed on sw-spine-002. "
},
{
	"uri": "/docs-csm/en-11/",
	"title": "Cray System Management Documentation",
	"tags": [],
	"description": "",
	"content": "Cray System Management Documentation Scope and Audience The documentation included here describes the Cray System Management (CSM) software, how to install or upgrade CSM software, and related supporting operational procedures to manage an HPE Cray EX system. CSM software is the foundation upon which other software product streams for the HPE Cray EX system depend.\nThe CSM installation prepares and deploys a distributed system across a group of management nodes organized into a Kubernetes cluster which uses Ceph for utility storage. These nodes perform their function as Kubernetes master nodes, Kubernetes worker nodes, or utility storage nodes with the Ceph storage.\nSystem services on these nodes are provided as containerized micro-services packaged for deployment via Helm charts. Kubernetes orchestrates these services and schedules them on Kubernetes worker nodes with horizontal scaling. Horizontal scales increases or decreases the number of service instances as demand for them varies, such as when booting many compute nodes or application nodes.\nThis information is intended for system installers, system administrators, and network administrators of the system. It assumes some familiarity with standard Linux and open source tools, such as shell scripts, revision control with git, configuration management with Ansible, YAML, JSON, and TOML file formats, etc.\nTable of Contents   Introduction to CSM Installation\nThis chapter provides an introduction to using the CSM software to manage the HPE Cray EX system which also describes the scenarios for installation and upgrade of CSM software, how product stream updates for CSM are delivered, the operational activities done after installation for on-going management of the HPE Cray EX system, differences between previous release and this release, and conventions used in this documentation.\n  Update CSM Product Stream\nThis chapter explains how to get the CSM product release, any patches, update to the latest set of documenation and any installation workarounds, and check for any Field Notices or Hotfixes.\n  Install CSM\nThis chapter provides an order list of procedures which can be used for CSM software installation or reinstall that indicate when to do operational tasks as part of the installation workflow. Updating software is in another chapter. Installation of the CSM product stream has many steps in multiple procedures which should be done in a specific order. Information about the HPE Cray EX system and the site is used to prepare the configuration payload. The initial node used to bootstrap the installation process is called the PIT node because the Pre-Install Toolkit is installed there. Once the management network switches have been configured, the other management nodes can be deployed with an operating system and the software to create a Kubernetes cluster utilizing Ceph storage. The CSM services provide essential software infrastructure including the API gateway and many micro-services with REST APIs for managing the system. Once administrative access has been configured, the installation of CSM software and nodes can be validated with health checks before doing operational tasks like the check and update of firmware on system components or the preparation of compute nodes.\n  Upgrade CSM\nThis chapter provides an order list of procedures which can be used to update CSM software that indicate when to do operational tasks as part of the software upgrade workflow. There are procedures to prepare the HPE Cray system for the upgrade, and update the management network, the management nodes, and the CSM services. After the upgrade of CSM software, the CSM health checks are used to validate the system before doing any other operational tasks like the check and update of firmware on system components.\n  CSM Operational Activities\nThis chapter provides an unordered set of administrative procedures required to operate an HPE Cray EX system with CSM software and grouped into several major areas:\n CSM Product Management Artifact Management Boot Orchestration Compute Rolling Upgrade Configuration Management Console Management Firmware Management Hardware State Manager Image Management Kubernetes Network Management Node Management Package Repository Management Power Management Resiliency River Endpoint Discovery Service Security And Authentication System Configuration Service System Layout Service System Management Health UAS User And Admin Topics Utility Storage Validate CSM Health    CSM Troubleshooting Information\nThis chapter provides information about some known issues in the system and tips for troubleshooting Kubernetes.\n  CSM Background Information\nThis chapter provides background information about the NCNs (non-compute nodes) which function as management nodes for the HPE Cray EX system. This information is not normally needed to install or upgrade software, but provides background which might be helpful for troubleshooting an installation.\n  Glossary\nThis chapter provides a explanations of terms and acronyms used throughout the rest of this documentation.\n  Copyright and License MIT License\n(C) Copyright [2020] Hewlett Packard Enterprise Development LP\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \u0026ldquo;Software\u0026rdquo;), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED \u0026ldquo;AS IS\u0026rdquo;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
},
{
	"uri": "/docs-csm/en-11/glossary/",
	"title": "Glossary",
	"tags": [],
	"description": "",
	"content": "Glossary Glossary of terms used in CSM documentation.\n Application Node (AN) Baseboard Management Controller (BMC) Blade Switch Controller (sC) Cabinet Cooling Group Cabinet Environmental Controller (CEC) CEC microcontroller (eC) Customer Access Network Chassis Management Module (CMM) Compute Node (CN) Cray Site Init (CSI) Cray System Management (CSM) EX Compute Cabinet EX TDS Cabinet Fabric Floor Standing CDU Hardware Management Network (HMN) High Speed Network (HSN) Kubernetes NCNs LiveCD Management Cabinet Management Nodes NIC Mezzanine Card (NMC) Node Controller (nC) Node Management Network Non-Compute Node (NCN) Power Distribution Unit (PDU) Pre-Install Toolkit (PIT) node Rack-Mounted CDU Rack System Compute Cabinet Rosetta ASIC Service/IO Cabinet Slingshot Slingshot Blade Switch Slingshot Top of Rack (ToR) Switch Shasta Cabling Diagram (SHCD) Supply/Return Cutoff Valves System Management Network (SMNet) System Management Services (SMS) System Management Services (SMS) nodes Top of Rack Switch Controller (sC-ToR) User Access Instance (UAI) User Access Node (UAN) xname  Application Node (AN) An application node (AN) is an NCN which is not providing management functions for the Cray EX system. The AN is not part of the Kubernetes cluster to which management nodes belong. One special type of AN is the UAN (User Access Node), but different systems may have need for other types of ANs, such as:\n nodes which provide a Lustre routing function (LNet router) gateways between HSN and Infiniband data movers between two different network file systems visualization servers other special-purpose nodes  Baseboard Management Controller (BMC) Air-Cooled cabinet COTS servers that include a Redfish-enabled baseboard management controller (BMC) and REST endpoint for API control and management. Either IPMI commands or REST API calls can be used to manage a BMC.\nBlade Switch Controller (sC) The Slingshot blade switch embedded controller (sC) provides a hardware management REST endpoint to monitor environmental conditions and manage the blade power, switch ASIC, FPGA buffer/interfaces, and firmware.\nCabinet Cooling Group A cabinet cooling group is a group of cabinets that are connected to a floor-standing coolant distribution unit (CDU). Management network CDU switches in the CDU aggregate all the node management network (NMN) and hardware management network (HMN) connections for the cabinet group.\nCabinet Environmental Controller (CEC) The Liquid-Cooled cabinet environmental controller (CEC) sets the cabinet\u0026rsquo;s geolocation, monitors environmental sensors, and communicates status to the cooling distribution unit (CDU). The CEC microcontroller (eC) signals the cooling distribution unit (CDU) to start liquid cooling and then enables the DC rectifiers so that a chassis can be powered on. The CEC does not provide a REST endpoint on SMNet, it simply provides the cabinet environmental and CDU status to the CMM for evaluation or action; the CEC takes no action. The CEC firmware is flashed automatically when the CMM firmware is flashed. If there are momentary erroneous signals because of a CEC reset or cable disconnection, the system can ride through these events without issuing an EPO.\nCEC microcontroller (eC) The CEC microcontroller (eC) sets the cabinet\u0026rsquo;s geolocation, monitors the cabinet environmental sensors, and communicates cabinet status to the cooling distribution unit (CDU). The eC does not provide a REST endpoint on SMNet as do other embedded controllers, but simply monitors the cabinet sensors and provides the cabinet environmental and CDU status to the CMMs for evaluation and/or action.\nCustomer Access Network The Customer Access Network (CAN) provides access from outside the customer network to services, noncompute nodes (NCNs), and User Access Nodes (UANs) in the system. This allows for the following:\n Clients outside of the system:  Log in to each of the NCNs and UANs. Access web UIs within the system (e.g. Prometheus, Grafana, and more). Access the Rest APIs within the system. Access a DNS server within the system for resolution of names for the webUI and REST API services. Run Cray CLI commands from outside the system. Access the User Access Instances (UAI).   NCNs and UANs to access systems outside the cluster (e.g. LDAP, license servers, and more). Services within the cluster to access systems outside the cluster.  These nodes and services need an IP address that routes to the customer\u0026rsquo;s network in order to be accessed from outside the network.\nChassis Management Module (CMM) The cabinet chassis management module (CMM) provides a REST endpoint via its chassis controller (cC). The CMM is an embedded controller that monitors and controls all the blades in a chassis. Each chassis supports 8 compute blades and 8 switches and associated rectifiers/PSUs in the rectifier shelf. Power Considerations - Two CMMs in adjacent chassis share power from the rectifier shelf (a shelf connects two adjacent chassis - 0/1, 2/3, 4/5, 6/7). If both CMMs sharing shelf power are both enabling the rectifiers, one of the CMMs can be removed (but only one at a time) without the rectifier shelf powering off. Removing a CMM will shutdown all compute blades and switches in the chassis. Cooling Considerations - Any single CMM in any cabinet can enable CDU cooling. Note that the CDU \u0026ldquo;enable path\u0026rdquo; has vertical control which means CMMs 0/2/4/6 and CEC0 are in a path (half of the cabinet), and CMMs 1/3/5/7 and CEC1 are in another path. Any CMM or CEC in the same half-cabinet path can be removed and CDU cooling will stay enabled as long as the other CMMs/CEC enables CDU cooling.\nCompute Node (CN) The compute node (CN) is where high performance computing application are run. These have hostnames that are of the form \u0026ldquo;nidXXXXXX\u0026rdquo;, that is, \u0026ldquo;nid\u0026rdquo; followed by six digits. where the XXXXXX is a six digit number starting with zero padding.\nCray Site Init (CSI) The Cray Site Init (CSI) program creates, validates, installs, and upgrades a Cray EX system. CSI can prepare the LiveCD for booting the PIT node and then is used from a booted PIT node to do its other functions during an installation. During an upgrade, CSI is installed on one of the nodes to facilitate the CSM software upgrade.\nCray System Management (CSM) Cray System Management (CSM) refers to the product stream which provides the infrastructure to manage a Cray EX system using Kubernetes to manage the containerized workload of layered micro-services with well-defined REST APIs which provide the ability to discover and control the hardware platform, manage configuration of the system, configure the network, boot nodes, gather log and telemetry data, connect API access and user level access to Identity Providers (IdPs), and provide a method for system administrators and end-users to access the Cray EX system.\nEX Compute Cabinet A Liquid-Cooled cabinet is a dense compute cabinet that supports 64 compute blades and 64 high-speed network (HSN) switches.\nEX TDS Cabinet A Liquid-Cooled TDS cabinet is a dense compute cabinet that supports 2-chassis, 16 compute blades and 16 high-speed network (HSN) switches, and includes a rack-mounted 4U coolant distribution unit (MCDU-4U).\nFabric The Slingshot fabric consists of the switches, cables, ports, topology policy, and configuration settings for the Slingshot high-speed network.\nFloor Standing CDU A floor-standing coolant distribution unit (CDU) pumps liquid coolant through a cabinet group or cabinet chilled doors.\nHardware Management Network (HMN) The hardware management network (HMN) includes HMS embedded controllers. This includes chassis controllers (cC), node controllers (nC) and switch controllers (sC), for Liquid-Cooled TDS and Liquid-Cooled systems. For standard rack systems, this includes iPDUs, COTS server BMCs, or any other equipment that requires hardware-management with Redfish. The hardware management network is isolated from all other node management networks. An out-of-band Ethernet management switch and hardware management VLAN is used for customer access and administration of hardware.\nHigh Speed Network (HSN) The High Speed Network (HSN) in an HPE Cray EX system is based on the Slingshot switches.\nKubernetes NCNs The Kubernetes NCNs are the management nodes which are known as Kubernetes master nodes (ncn-mXXX) or Kubernetes worker nodes (ncn-wXXX). The only type of management node which is excluded from this is the utility storage node (ncn-sXXX).\nLiveCD The LiveCD has a complete bootable Linux operating system that can be run from a read-only CD or DVD, a writable USB flash drive, or a hard disk. It is used to bootstrap the installation process for CSM software. It contains the Pre-Install Toolkit (PIT). The node which boots from it during the install is known as the PIT node.\nManagement Cabinet At least one 19-in IEA management cabinet is required for every HPE Cray EX system to support the management non-compute nodes (NCN), system management network, utility storage, and other support equipment. This cabinet serves as the primary customer access point for managing the system.\nManagement Nodes The management nodes are one grouping of NCNs. The management nodes include the master nodes with hostnames of the form of ncn-mXXX, the worker nodes with hostnames of the form ncn-wXXX, and utility storage nodes, with hostnames of the form ncn-sXXX, where the XXX is a three digit number starting with zero padding. The utility storage nodes provide Ceph storage for use by the management nodes. The master nodes provide Kubernetes master functions and have the etcd cluster which provides a datastore for Kubernetes. The worker nodes provide Kubernetes worker functions where most of the containerized workload is scheduled by Kubernetes.\nNIC Mezzanine Card (NMC) The NIC mezzanine card (NMC) attaches to two host port connections on a liquid-cooled compute blade node card and provides the high-speed network (HSN) controllers (NICs). There are typically two or four NICs on each node card. NMCs connect to the rear panel EXAMAX connectors on the compute blade through an internal L0 cable assembly in a single-, dual-, or quad-injection bandwidth configuration depending on the design of the node card.\nNode Controller (nC) Each compute blade node card includes an embedded node controller (nC) and REST endpoint to manage the node environmental conditions, power, HMS nFPGA interface, and firmware.\nNode Management Network The node management network (NMN) communicates with motherboard PCH-style hosts, typically 10GbE Ethernet LAN-on-motherboard (LOM) interfaces. This network supports node boot protocols (DHCP/TFTP/HTTP), in-band telemetry and event exchange, and general access to management REST APIs.\nNon-Compute Node (NCN) Any node which is not a compute node may be called a Non-Compute Node (NCN). The NCNs include management nodes and application nodes.\nPower Distribution Unit (PDU) The cabinet PDU receives 480VAC 3-phase facility power and provides circuit breaker, fuse protection, and EMI filtered power to the rectifier/power supplies that distribute ±190VDC (HVDC) to a chassis. PDUs are passive devices that do not connect to the SMNet.\nPre-Install Toolkit (PIT) node The Pre-Install Toolkit is installed onto the initial node used as the inception node during software installation which is booted from a LiveCD. This is the node that will eventually become ncn-m001. The node running the Pre-Install Toolkit is known as the PIT node during the installation process until it reboots from a normal management node image like the other master nodes.\nEarly in the install process, before the Pre-Install Toolkit has been installed or booted, the documents may still refer to the PIT node. In this case, they are referring to the node which will eventually become the PIT node.\nIn this documentation, PIT node and LiveCD are sometimes used interchangeably.\nRack-Mounted CDU The rack-mounted coolant distribution unit (MCDU-4U) pumps liquid coolant through the Liquid-Cooled TDS cabinet coolant manifolds.\nRack System Compute Cabinet Air-Cooled compute cabinets house a cluster of compute nodes, Slingshot ToR switches, and SMNet ToR switches.\nRosetta ASIC The Rosetta ASIC is a 64-port switch chip that forms the foundation for the Slingshot network. Each port can operate at either 100G or 200G. Each network edge port supports IEEE 802.3 Ethernet, optimized-IP based protocols, and portals (an enhanced frame format that supports higher rates of small messages).\nService/IO Cabinet An Air-Cooled service/IO cabinet houses a cluster of NCN servers, Slingshot ToR switches, and management network ToR switches to support the managed ecosystem storage, network, user access services (UAS), and other IO services such as LNet and gateways.\nSlingshot Slingshot supports L1 and L2 network connectivity between 200Gbs switch ports and L0 connectivity from a single 200Gbs port to two 100Gbs Mellanox ConnectX-5 NICs. Slingshot also supports edge ports and link aggregation groups (LAG) to external storage systems or networks.\n IEEE 802.3cd/bs (200 Gbps) Ethernet over 4 x 50 Gb/s (PAM-4) lanes 200GBASE-DR4, 500m singlemode fiber 200GBASE-SR4, 100m multi-mode fiber 200GBASE-CR4, 3m copper cable IEEE 802.3cd (100 Gbps) Ethernet over 2 x 50 Gb/s (PAM-4) lanes 100GBASE-SR2, 100m multimode fiber 100GBASE-CR2, 3m copper cable IEEE 802.3 2018 100 Gbps Ethernet over 4 x 25 Gb/s (NRZ) lanes 100GBASE-CR4, 5m copper cable 100GBASE-SR4, 100m multi-mode fiber Optimized Ethernet and HPC fabric formats Lossy and lossless delivery Flow control, 802.1x (PAUSE), 802.1p (PFC), credit based flow control on fabric links, fine-grain flow control on host links and edge ports, Link level retry, low latency FEC, Ethernet physical interfaces:  Slingshot Blade Switch The Liquid-Cooled cabinet blade switch supports one switch ASIC and 48 fabric ports. Eight connectors on the rear panel connect orthogonally to each compute blade then to NIC mezzanine cards (NMCs) inside the compute blade. Each rear panel EXAMAX connector supports two switch ports (a total of 16 fabric ports per blade). Twelve QSFP-DD cages on the front panel (4 fabric ports per QSFP-DD cage), fan out 48 external fabric ports to other switches. The front-panel top ports support passive electrical cables (PEC) or active optical cables (AOC). The front-panel bottom ports support only PECs for proper cooling in the blade enclosure.\nSlingshot Top of Rack (ToR) Switch A standard cabinet can support one, two, or four, rack-mounted Slingshot ToR switches. Each switch supports a total of 64 fabric ports. 32 QSFP-DD connectors on the front panel connect 64 ports to the fabric. All front-panel connectors support either passive electrical cables (PEC) or active optical cables (AOC).\nShasta Cabling Diagram (SHCD) The Shasta Cabling Diagram (SHCD) is a multiple tab spreadsheet prepared by HPE Cray Manufacturing with information about the components in an HPE Cray EX system. This document has much information about the system. Included in the SHCD are a configuration summary with revision history, floor layout plan, type and location of components in the air-cooled cabinets, type and location of components in the liquid-cooled cabinets, device diagrams for switches and nodes in the cabinets, list of source and destination of every HSN cable, list of source and destination of every cable connected to the spine switches, list of source and destination of every cable connected to the NMN, list of source and destination of every cable connected to the HMN. list of cabling for the KVM, and routing of power to the PDUs.\nSupply/Return Cutoff Valves Manual coolant supply and return shutoff valves at the top of each cabinet can be closed to isolate a single cabinet from the other cabinets in the cooling group for maintenance. If the valves are closed during operation, the action automatically causes the CMMs to remove ±190VDC from each chassis in the cabinet because of the loss of coolant pressure.\nSystem Management Network (SMNet) The system management network (SMNet) is a dedicated out-of-band (OOB) spine-leaf topology Ethernet network that interconnects all the nodes in the system to management services.\nSystem Management Services (SMS) System Management Services (SMS) leverages open REST APIs, Kubernetes container orchestration, and a pool of commercial off-the-shelf (COTS) servers to manage the system. The management server pool, custom Redfish-enabled embedded controllers, iPDU controllers, and server BMCs are unified under a common software platform that provides 3 levels of management: Level 1 HaaS, Level 2 IaaS, and Level 3 PaaS.\nSystem Management Services (SMS) nodes System Management Services (SMS) nodes provide access to the entire management cluster and Kubernetes container orchestration.\nTop of Rack Switch Controller (sC-ToR) The Air-Cooled cabinet HSN ToR switch embedded controller (sC-ToR) provides a hardware management REST endpoint to monitor the ToR switch environmental conditions and manage the switch power, HSN ASIC, and FPGA interfaces.\nUser Access Instance (UAI) The User Access Instance (UAI) is a lightweight, disposable platform that runs under Kubernetes orchestration on worker nodes. The UAI provides a single user containerized environment for users on a Cray Ex system to develop, build, and execute their applications on the Cray EX compute node. See UAN for another way for users to gain access.\nUser Access Node (UAN) The User Access Node (UAN) is an NCN, but is really one of the special types of Application nodes. The UAN provides a traditional multi-user Linux environment for users on a Cray Ex system to develop, build, and execute their applications on the Cray EX compute node. See UAI for another way for users to gain access. Some sites refer to their UANs as Login nodes.\nxname Component names (xnames) identify the geolocation for hardware components in the HPE Cray EX system. Every component is uniquely identified by these component names. Some, like the system cabinet number or the CDU number, can be changed by site needs. There is no geolocation encoded within the cabinet number, such as an X-Y coordinate system to relate to the floor layout of the cabinets. Other component names refer to the location within a cabinet and go down to the port on a card or switch or the socket holding a processor or a memory DIMM location. See Component Names (xnames).\n"
},
{
	"uri": "/docs-csm/en-11/background/ncn_operating_system_releases/",
	"title": "NCN Operating System Releases",
	"tags": [],
	"description": "",
	"content": "NCN Operating System Releases The NCNs define their products per image layer:\n Management node SquashFS images are always SLE_HPC (SuSE High Performance Computing) Utility Storage nodes Ceph Images are always SLE_HPC (SuSE High Performance Computing) with SES (SuSE Enterprise Storage)  The sles-release RPM is uninstalled for NCNs, and instead, the sle_HPC-release RPM is installed. These both provide the same files, but differ for os-release and /etc/product.d/baseproduct.\nThe ses-release RPM is installed on top of the sle_HPC-release RPM in the Ceph images.\nThe following example shows the two product files for a utility storage node booted from the Ceph image. This node is capable of high performance computing and serving enterprise storage.\nncn-s# ls -l /etc/products.d/ total 5 lrwxrwxrwx 1 root root 12 Jan 1 06:43 baseproduct -\u0026gt; SLE_HPC.prod -rw-r--r-- 1 root root 1587 Oct 21 15:27 ses.prod -rw-r--r-- 1 root root 2956 Jun 10 2020 SLE_HPC.prod ncn-s# grep \u0026#39;\u0026lt;summary\u0026#39; /etc/products.d/*.prod /etc/products.d/ses.prod: \u0026lt;summary\u0026gt;SUSE Enterprise Storage 7\u0026lt;/summary\u0026gt; /etc/products.d/SLE_HPC.prod: \u0026lt;summary\u0026gt;SUSE Linux Enterprise High Performance Computing 15 SP2\u0026lt;/summary\u0026gt; Kubernetes nodes will report SLE HPC only, which is reflected in the kubectl output.\nncn# kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME ncn-m002 Ready master 128m v1.18.6 10.252.1.14 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP2 5.3.18-24.37-default containerd://1.3.4 ncn-m003 Ready master 127m v1.18.6 10.252.1.13 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP2 5.3.18-24.37-default containerd://1.3.4 ncn-w001 Ready \u0026lt;none\u0026gt; 90m v1.18.6 10.252.1.12 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP2 5.3.18-24.37-default containerd://1.3.4 ncn-w002 Ready \u0026lt;none\u0026gt; 88m v1.18.6 10.252.1.11 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP2 5.3.18-24.37-default containerd://1.3.4 ncn-w003 Ready \u0026lt;none\u0026gt; 82m v1.18.6 10.252.1.10 \u0026lt;none\u0026gt; SUSE Linux Enterprise High Performance Computing 15 SP2 5.3.18-24.37-default containerd://1.3.4 "
},
{
	"uri": "/docs-csm/en-11/background/ncn_packages/",
	"title": "NCN Packages",
	"tags": [],
	"description": "",
	"content": "NCN Packages The management nodes boot from images which have many (RPM) packages installed. Lists of the packages for these images are generated on running nodes. A list of images can be collected by running a zypper command on one of the storage, worker, or master nodes.\nKubernetes Images The Kubernetes image is used to boot the master nodes and worker nodes.\nCollection ncn-w002# zypper --disable-repositories se --installed-only | grep i+ | tr -d \u0026#39;|\u0026#39; | awk \u0026#39;{print $2}\u0026#39; The List  SLE_HPC SLE_HPC-release acpid apparmor-profiles arptables at autoyast2 autoyast2-installation base bc bind-utils biosdevname ceph-common cfs-state-reporter cloud-init conntrack-tools cpupower crash cray-cos-release cray-cps-utils cray-dvs-kmp-default cray-dvs-service cray-heartbeat cray-lustre-client cray-lustre-client-devel cray-lustre-client-kmp-default cray-network-config cray-node-identity cray-orca cray-power-button cray-sat-podman craycli-wrapper createrepo_c dnsmasq dosfstools dracut-kiwi-live dracut-metal-mdsquash dump e2fsprogs ebtables emacs ethtool expect fping gdb genders git-core glibc gnuplot gperftools gptfdisk grub2 haproxy hpe-csm-scripts hplip ipmitool iproute2-bash-completion ipset ipvsadm irqbalance java-1_8_0-ibm java-1_8_0-ibm-plugin kdump kdumpid keepalived kernel-mft-mlnx-kmp-default kernel-source kernel-syms kexec-tools kmod-bash-completion kubeadm kubectl kubelet less libcanberra-gtk0 libecpg6 libpq5 libunwind-devel ltrace lvm2 man mariadb mariadb-tools mdadm minicom mlocate nmap numactl open-lldp patterns-base-base pdsh perf perl-MailTools perl-doc pixz podman podman-cni-config postgresql postgresql-contrib postgresql-docs postgresql-server python python-urlgrabber python3-Jinja2 python3-MarkupSafe python3-click python3-colorama python3-curses python3-ipaddr python3-lxml python3-netaddr python3-pexpect python3-pip python3-rpm python3-sip rarpd rasdaemon rsync rsyslog rzsz screen slingshot-network-config smartmontools socat spire-agent squashfs squid strace sudo sysstat systemtap tar tcpdump unixODBC usbutils vim wget wireshark xfsdump yast2-add-on yast2-bootloader yast2-country yast2-network yast2-services-manager yast2-users yum-metadata-parser zip  CEPH The Ceph image is used to boot the utility storage nodes.\nCollection ncn-s002# zypper --disable-repositories se --installed-only | grep i+ | tr -d \u0026#39;|\u0026#39; | awk \u0026#39;{print $2}\u0026#39; The List  SLE_HPC SLE_HPC-release acpid apparmor-profiles arptables at autoyast2 autoyast2-installation base bc bind-utils biosdevname ceph-mds ceph-mgr ceph-mon ceph-osd ceph-radosgw cfs-state-reporter cloud-init cpupower crash cray-cos-release cray-dvs-kmp-default cray-dvs-service cray-heartbeat cray-lustre-client cray-lustre-client-devel cray-lustre-client-kmp-default cray-network-config cray-node-identity cray-power-button craycli-wrapper createrepo_c dnsmasq dosfstools dracut-kiwi-live dracut-metal-mdsquash dump e2fsprogs ebtables emacs ethtool expect fping gdb genders git-core glibc gnuplot gperftools gptfdisk grub2 haproxy hpe-csm-scripts hplip ipmitool iproute2-bash-completion ipset irqbalance java-1_8_0-ibm java-1_8_0-ibm-plugin kdump kdumpid keepalived kernel-mft-mlnx-kmp-default kernel-source kernel-syms kexec-tools kmod-bash-completion kubectl less libcanberra-gtk0 libecpg6 libpq5 libunwind-devel ltrace lvm2 man mariadb mariadb-tools mdadm minicom mlocate netcat-openbsd nmap numactl open-lldp patterns-base-base pdsh perf perl-MailTools perl-doc pixz podman podman-cni-config postgresql postgresql-contrib postgresql-docs postgresql-server python python-urlgrabber python3-Jinja2 python3-MarkupSafe python3-boto3 python3-click python3-colorama python3-curses python3-ipaddr python3-lxml python3-netaddr python3-pexpect python3-pip python3-rpm python3-sip rarpd rasdaemon rsync rsyslog rzsz screen slingshot-network-config smartmontools socat spire-agent squashfs squid strace sudo sysstat systemtap tar tcpdump unixODBC usbutils vim wget wireshark xfsdump yast2-add-on yast2-bootloader yast2-country yast2-network yast2-services-manager yast2-users yum-metadata-parser zip  "
},
{
	"uri": "/docs-csm/en-11/background/ncn_images/",
	"title": "NCN Images",
	"tags": [],
	"description": "",
	"content": "NCN Images The management nodes boot from NCN images which are created from layers on top of a common base image. The common image is customized with a Kubernetes layer for the master nodes and worker nodes. The common image is customized with a storage-ceph layer for the utility storage nodes..\nTopics:  Overview of NCN Images LiveCD Server  Details Overview of NCN Images There are several flavors of NCN images, each share a common base image. When booting NCNs an admin or user will need to choose between stable (Release) and unstable (pre-release/dev) images.\n For details on how these images behave and inherit from the base and common images, see [node-image-docs][1].\n In short, each image (i.e. Kubernetes and storage-ceph) inherit from the non-compute-common layer. Operationally these are all that matter; the common layer, Kubernetes layer, Ceph layer, and any other new images.\nTo boot an NCN, you need 3 artifacts for each node-type (kubernetes-manager/worker, ceph):\n The Kubernetes SquashFS ([stable][4] or [unstable][5])  initrd-img-[RELEASE].xz $version-[RELEASE].kernel kubernetes-[RELEASE].squashfs   The CEPH SquashFS ([stable][6] or [unstable][7])  initrd-img-[RELEASE].xz $version-[RELEASE].kernel storage-ceph-[RELEASE].squashfs    LiveCD Server   View the current ephemeral data payload:\npit# ls -l /var/www total 8 drwxr-xr-x 1 dnsmasq tftp 4096 Dec 17 21:20 boot drwxr-xr-x 7 root root 4096 Dec 2 04:45 ephemeral pit# ls -l /var/www/ephemeral/data/* /var/www/ephemeral/data/ceph: total 4 drwxr-xr-x 2 root root 4096 Dec 17 21:42 0.0.7 /var/www/ephemeral/data/k8s: total 4 drwxr-xr-x 2 root root 4096 Dec 17 21:26 0.0.8   Setup the \u0026ldquo;booting repositories\u0026rdquo;:\npit# set-sqfs-links.sh Mismatching kernels! The discovered artifacts will deploy an undesirable stack. mkdir: created directory \u0026#39;ncn-m001\u0026#39; /var/www/ncn-m001 /var/www \u0026#39;kernel\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/5.3.18-24.37-default-0.0.8.kernel\u0026#39; \u0026#39;initrd.img.xz\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/initrd.img-0.0.8.xz\u0026#39; \u0026#39;filesystem.squashfs\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/kubernetes-0.0.8.squashfs\u0026#39; /var/www mkdir: created directory \u0026#39;ncn-m002\u0026#39; /var/www/ncn-m002 /var/www \u0026#39;kernel\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/5.3.18-24.37-default-0.0.8.kernel\u0026#39; \u0026#39;initrd.img.xz\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/initrd.img-0.0.8.xz\u0026#39; \u0026#39;filesystem.squashfs\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/kubernetes-0.0.8.squashfs\u0026#39; /var/www mkdir: created directory \u0026#39;ncn-m003\u0026#39; /var/www/ncn-m003 /var/www \u0026#39;kernel\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/5.3.18-24.37-default-0.0.8.kernel\u0026#39; \u0026#39;initrd.img.xz\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/initrd.img-0.0.8.xz\u0026#39; \u0026#39;filesystem.squashfs\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/kubernetes-0.0.8.squashfs\u0026#39; /var/www mkdir: created directory \u0026#39;ncn-w002\u0026#39; /var/www/ncn-w002 /var/www \u0026#39;kernel\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/5.3.18-24.37-default-0.0.8.kernel\u0026#39; \u0026#39;initrd.img.xz\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/initrd.img-0.0.8.xz\u0026#39; \u0026#39;filesystem.squashfs\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/kubernetes-0.0.8.squashfs\u0026#39; /var/www mkdir: created directory \u0026#39;ncn-w003\u0026#39; /var/www/ncn-w003 /var/www \u0026#39;kernel\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/5.3.18-24.37-default-0.0.8.kernel\u0026#39; \u0026#39;initrd.img.xz\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/initrd.img-0.0.8.xz\u0026#39; \u0026#39;filesystem.squashfs\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/k8s/0.0.8/kubernetes-0.0.8.squashfs\u0026#39; /var/www mkdir: created directory \u0026#39;ncn-s001\u0026#39; /var/www/ncn-s001 /var/www \u0026#39;kernel\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/ceph/0.0.7/5.3.18-24.37-default-0.0.7.kernel\u0026#39; \u0026#39;initrd.img.xz\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/ceph/0.0.7/initrd.img-0.0.7.xz\u0026#39; \u0026#39;filesystem.squashfs\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/ceph/0.0.7/storage-ceph-0.0.7.squashfs\u0026#39; /var/www mkdir: created directory \u0026#39;ncn-s002\u0026#39; /var/www/ncn-s002 /var/www \u0026#39;kernel\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/ceph/0.0.7/5.3.18-24.37-default-0.0.7.kernel\u0026#39; \u0026#39;initrd.img.xz\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/ceph/0.0.7/initrd.img-0.0.7.xz\u0026#39; \u0026#39;filesystem.squashfs\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/ceph/0.0.7/storage-ceph-0.0.7.squashfs\u0026#39; /var/www mkdir: created directory \u0026#39;ncn-s003\u0026#39; /var/www/ncn-s003 /var/www \u0026#39;kernel\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/ceph/0.0.7/5.3.18-24.37-default-0.0.7.kernel\u0026#39; \u0026#39;initrd.img.xz\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/ceph/0.0.7/initrd.img-0.0.7.xz\u0026#39; \u0026#39;filesystem.squashfs\u0026#39; -\u0026gt; \u0026#39;../ephemeral/data/ceph/0.0.7/storage-ceph-0.0.7.squashfs\u0026#39; /var/www   View the currently set links\npit# ls -l /var/www/ncn-* boot: total 1552 -rw-r--r-- 1 root root 166634 Dec 17 13:21 graffiti.png -rw-r--r-- 1 dnsmasq tftp 700480 Dec 17 13:25 ipxe.efi -rw-r--r-- 1 dnsmasq tftp 700352 Dec 15 09:35 ipxe.efi.stable -rw-r--r-- 1 root root 6157 Dec 15 05:12 script.ipxe -rw-r--r-- 1 root root 6284 Dec 17 13:21 script.ipxe.rpmnew ephemeral: total 32 drwxr-xr-x 2 root root 4096 Dec 6 22:18 configs drwxr-xr-x 4 root root 4096 Dec 7 04:29 data drwx------ 2 root root 16384 Dec 2 04:25 lost+found drwxr-xr-x 4 root root 4096 Dec 3 02:31 prep drwxr-xr-x 2 root root 4096 Dec 2 04:45 static ncn-m001: total 4 lrwxrwxrwx 1 root root 53 Dec 26 06:11 filesystem.squashfs -\u0026gt; ../ephemeral/data/k8s/0.0.8/kubernetes-0.0.8.squashfs lrwxrwxrwx 1 root root 47 Dec 26 06:11 initrd.img.xz -\u0026gt; ../ephemeral/data/k8s/0.0.8/initrd.img-0.0.8.xz lrwxrwxrwx 1 root root 61 Dec 26 06:11 kernel -\u0026gt; ../ephemeral/data/k8s/0.0.8/5.3.18-24.37-default-0.0.8.kernel ncn-m002: total 4 lrwxrwxrwx 1 root root 53 Dec 26 06:11 filesystem.squashfs -\u0026gt; ../ephemeral/data/k8s/0.0.8/kubernetes-0.0.8.squashfs lrwxrwxrwx 1 root root 47 Dec 26 06:11 initrd.img.xz -\u0026gt; ../ephemeral/data/k8s/0.0.8/initrd.img-0.0.8.xz lrwxrwxrwx 1 root root 61 Dec 26 06:11 kernel -\u0026gt; ../ephemeral/data/k8s/0.0.8/5.3.18-24.37-default-0.0.8.kernel ncn-m003: total 4 lrwxrwxrwx 1 root root 53 Dec 26 06:11 filesystem.squashfs -\u0026gt; ../ephemeral/data/k8s/0.0.8/kubernetes-0.0.8.squashfs lrwxrwxrwx 1 root root 47 Dec 26 06:11 initrd.img.xz -\u0026gt; ../ephemeral/data/k8s/0.0.8/initrd.img-0.0.8.xz lrwxrwxrwx 1 root root 61 Dec 26 06:11 kernel -\u0026gt; ../ephemeral/data/k8s/0.0.8/5.3.18-24.37-default-0.0.8.kernel ncn-s001: total 4 lrwxrwxrwx 1 root root 56 Dec 26 06:11 filesystem.squashfs -\u0026gt; ../ephemeral/data/ceph/0.0.7/storage-ceph-0.0.7.squashfs lrwxrwxrwx 1 root root 48 Dec 26 06:11 initrd.img.xz -\u0026gt; ../ephemeral/data/ceph/0.0.7/initrd.img-0.0.7.xz lrwxrwxrwx 1 root root 62 Dec 26 06:11 kernel -\u0026gt; ../ephemeral/data/ceph/0.0.7/5.3.18-24.37-default-0.0.7.kernel ncn-s002: total 4 lrwxrwxrwx 1 root root 56 Dec 26 06:11 filesystem.squashfs -\u0026gt; ../ephemeral/data/ceph/0.0.7/storage-ceph-0.0.7.squashfs lrwxrwxrwx 1 root root 48 Dec 26 06:11 initrd.img.xz -\u0026gt; ../ephemeral/data/ceph/0.0.7/initrd.img-0.0.7.xz lrwxrwxrwx 1 root root 62 Dec 26 06:11 kernel -\u0026gt; ../ephemeral/data/ceph/0.0.7/5.3.18-24.37-default-0.0.7.kernel ncn-s003: total 4 lrwxrwxrwx 1 root root 56 Dec 26 06:11 filesystem.squashfs -\u0026gt; ../ephemeral/data/ceph/0.0.7/storage-ceph-0.0.7.squashfs lrwxrwxrwx 1 root root 48 Dec 26 06:11 initrd.img.xz -\u0026gt; ../ephemeral/data/ceph/0.0.7/initrd.img-0.0.7.xz lrwxrwxrwx 1 root root 62 Dec 26 06:11 kernel -\u0026gt; ../ephemeral/data/ceph/0.0.7/5.3.18-24.37-default-0.0.7.kernel ncn-w002: total 4 lrwxrwxrwx 1 root root 53 Dec 26 06:11 filesystem.squashfs -\u0026gt; ../ephemeral/data/k8s/0.0.8/kubernetes-0.0.8.squashfs lrwxrwxrwx 1 root root 47 Dec 26 06:11 initrd.img.xz -\u0026gt; ../ephemeral/data/k8s/0.0.8/initrd.img-0.0.8.xz lrwxrwxrwx 1 root root 61 Dec 26 06:11 kernel -\u0026gt; ../ephemeral/data/k8s/0.0.8/5.3.18-24.37-default-0.0.8.kernel ncn-w003: total 4 lrwxrwxrwx 1 root root 53 Dec 26 06:11 filesystem.squashfs -\u0026gt; ../ephemeral/data/k8s/0.0.8/kubernetes-0.0.8.squashfs lrwxrwxrwx 1 root root 47 Dec 26 06:11 initrd.img.xz -\u0026gt; ../ephemeral/data/k8s/0.0.8/initrd.img-0.0.8.xz lrwxrwxrwx 1 root root 61 Dec 26 06:11 kernel -\u0026gt; ../ephemeral/data/k8s/0.0.8/5.3.18-24.37-default-0.0.8.kernel   "
},
{
	"uri": "/docs-csm/en-11/background/ncn_mounts_and_file_systems/",
	"title": "NCN Mounts And File Systems",
	"tags": [],
	"description": "",
	"content": "NCN Mounts and File Systems The management nodes use drive storage for persistence and block storage. This page outlines reference information for these disks, their partition tables, and their management.\nTopics:  What Controls Partitioning? Plan of Record / Baseline  Problems When Above/Below Baseline Worker Nodes with ETCD  Disable Luks Expand the RAID     Disk Layout Quick-Reference Tables OverlayFS and Persistence  OverlayFS Example Persistent Directories  Layering - Upperdir and Lowerdir(s) Layering Real World Example   OverlayFS Control  Reset Toggles Reset On Next Boot Reset on Every Boot Re-sizing the Persistent Overlay Thin Overlay Feature     SystemD MetalFS Old/Retired FS-Labels  Details What Controls Partitioning? Partitioning is controlled by two aspects:\n dracut; this selects disks and builds their partition tables and/or LVM storage. cloud-init; this manages standalone partitions or volumes, as well as high-level object storage.  Plan of Record / Baseline    Node Type No. of \u0026ldquo;small\u0026rdquo; disks (0.5 TiB) No. of \u0026ldquo;large\u0026rdquo; disks (1.9 TiB)     k8s-master nodes 3 0   k8s-worker nodes 2 1   ceph-storage nodes 2 3+    Disks are chosen by dracut. Kubernetes and storage nodes use different dracut modules.\n First, two disks for the OS are chosen from the pool of \u0026ldquo;small\u0026rdquo; disks Second, one disk is selected for the ephemeral data  Problems When Above/Below Baseline The master nodes and worker nodes use the same artifacts, and thus have the same dracut modules assimilating disks. Therefore, it is important to beware of:\n k8s-master nodes with one or more extra \u0026ldquo;large\u0026rdquo; disk(s); these disks help but are unnecessary ceph-storage nodes do not run the same dracut modules because they have different disk demands  Worker Nodes with ETCD k8s-worker nodes with 1 or more extra \u0026ldquo;small\u0026rdquo; disk(s); these disks are confusing and unnecessary and can be disabled easily.\nDisable Luks  NOTE This is broken, use the expand RAID option instead. (MTL-1309)\n All NCNs (master/worker/storage) have the same kernel parameters, but are not always necessary. This method works by toggling the dependency for the metal ETCD module, disabling LUKs will disable ETCD bare-metal creation.\n  Disable LUKs for each worker node, thus disabling the metal ETCD module:\n During Bootstrap (on the pit node): sed -i \u0026#39;s/disk-opts rd.luks /disk-opts rd.luks=0 /g\u0026#39; /var/www/ncn-w*/script.ipxe  During runtime with csi: csi handoff bss-update-param rd.luks=0     Rebuild the node\n Run the basic wipe if the node was already booted (re)boot the node    Expand the RAID This option simply expands the RAID to consume the extra disks, leaving none behind for the metal ETCD module to find.\n  Set metal.disks equal to the number of \u0026ldquo;small\u0026rdquo; disks in the node(s), this will reserve them for the RAID and prevent any other partitioning from happening on them.\n During Bootstrap (on the pit node): sed -i \u0026#39;s/disk-opts /disk-opts metal.disks=3 /g\u0026#39; /var/www/ncn-w*/script.ipxe  During runtime with csi: csi handoff bss-update-param metal.disks=3     Change the RAID type, or leave it as default (mirror)\n During Bootstrap (on the pit node): sed -i \u0026#39;s/disk-opts /disk-opts metal.md-level=stripe /g\u0026#39; /var/www/ncn-w*/script.ipxe  During runtime with csi: csi handoff bss-update-param metal.md-level=stripe     Rebuild the node\n Run the basic wipe if the node was already booted (re)boot the node    Disk Layout Quick-Reference Tables The table below represents all recognizable FS labels on any given management node, varying slightly by node role (Kubernetes master or Kubernetes worker).\n   k8s-master k8s-worker storage-ceph FS Label Partitions Device Partition Size OverlayFS Work Order(s) Memo     ✅ ✅ ✅ BOOTRAID /metal/recovery 2 small disks in RAID1 500 MiB ❌ Present since Shasta-Preview 1    ✅ ✅ ✅ SQFSRAID /run/initramfs/live 2 small disks in RAID1 25 GiB ✅ CASM-1885 squashfs should compress our images to about 1/3rd their uncompressed size. (20G → 6.6G) On pepsi\u0026rsquo;s ncn-w001, we are at about 20G of non-volatile data storage needed.   ✅ ✅ ✅ ROOTRAID /run/initramfs/overlayfs 2 small disks in RAID1 150 GiB ✅ Present since Shasta-Preview 1 The persistent image file is loaded from this partition, when the image file is loaded the underlying drive is lazily unmounted (umount -l) so that when the overlay closes the disk follows suit.   ✅ ✅ ✅ AUX /dev/md/AUX (Not Mounted) 2 small disks in RAID1 150 GiB ❌ MTL-1308 Auxiliary RAID array for cloud-init to use.   ❌ ❌ ✅ CEPHETC /etc/ceph LVM 10 GiB ❌ MTL-1308    ❌ ❌ ✅ CEPHVAR /var/lib/ceph LVM 60 GiB ❌ MTL-1308    ❌ ❌ ✅ CONTAIN /run/containers LVM 60 GiB ❌ MTL-1308    ✅ ✅ ❌ CRAYSDU /var/lib/sdu LVM 100 GiB ❌ MTL-1292    ❌ ✅ ❌ CONRUN /run/containerd Ephemeral 75 GiB ❌ MTL-916 On pepsi ncn-w001, we have less than 200G of operational storage for this.   ❌ ✅ ❌ CONLIB /run/lib-containerd Ephemeral 25% ✅ MTL-892 CASMINST-255    ✅ ❌ ❌ ETCDLVM /run/lib-etcd Ephemeral 32 GiB ✅ CASMPET-338    ✅ ❌ ❌ K8SLET /var/lib/kubelet Ephemeral 25% ❌ MTL-892 CASMINST-255     The above table\u0026rsquo;s rows with overlayFS map their \u0026ldquo;Mount Paths\u0026rdquo; to the \u0026ldquo;Upper Directory\u0026rdquo; in the table below:\n The \u0026ldquo;OverlayFS Name\u0026rdquo; is the name used in fstab and seen in the output of mount.\n    OverlayFS Name Upper Directory Lower Directory (or more)     etcd_overlayfs /run/lib-etcd /var/lib/etcd   containerd_overlayfs /run/lib-containerd /var/lib/containerd     For notes on previous/old labels, scroll to the bottom.\n OverlayFS and Persistence There are a few overlays used for NCN image boots. These enable two critical functions; changes to data and new data will persist between reboots, and RAM (memory) is freed because we are using our block-devices (SATA/PCIe).\n ROOTRAID is the persistent root overlayFS, it commits and saves all changes made to the running OS and it stands on a RAID1 mirror. CONLIB is a persistent overlayFS for containerd, it commits and saves all new changes while allowing read-through to pre-existing (baked-in) data from the squashFS. ETCDK8S is a persistent overlayFS for etcd, it works like the CONLIB overlayFS however this exists in an encrypted LUKS2 partition.  OverlayFS Example  Helpful commands\u0026hellip; the overlayFS organization can be best viewed with these three commands:\n lsblk, lsblk -f will show how the RAIDs and disks are mounted losetup -a will show where the squashFS is mounted from mount | grep ' / ' will show you the overlay being layered atop the squashFS   Let us pick apart the SQFSRAID and ROOTRAID overlays.\n /run/rootfsbase is the SquashFS image itself /run/initramfs/live is the squashFS\u0026rsquo;s storage array, where one or more squashFS can live /run/initramfs/overlayfs is the overlayFS storage array, where the persistent directories live /run/overlayfs and /run/ovlwork are symlinks to `/run/initramfs/overlayfs/overlayfs-SQFSRAID-$(blkid -s UUID -o value /dev/disk/by-label/SQFSRAID) and the neighboring work directory Admin note: The \u0026ldquo;work\u0026rdquo; directory is where the operating system processes data. It is the interim where data passes between RAM and persistent storage.  Using the above bullets, one may be able to better understand the machine output below:\nncn-m002# mount | grep \u0026#39; / \u0026#39; LiveOS_rootfs on / type overlay (rw,relatime,lowerdir=/run/rootfsbase,upperdir=/run/overlayfs,workdir=/run/ovlwork) ^^^R/O^SQUASHFS IMAGE^^^|^^^ R/W PERSISTENCE ^^^|^^^^^^INTERIM^^^^^^ ^^^R/O^SQUASHFS IMAGE^^^|^^^ R/W PERSISTENCE ^^^|^^^^^^INTERIM^^^^^^ ^^^R/O^SQUASHFS IMAGE^^^|^^^ R/W PERSISTENCE ^^^|^^^^^^INTERIM^^^^^^ ncn-m002# losetup -a /dev/loop1: [0025]:74858 (/run/initramfs/thin-overlay/meta) /dev/loop2: [0025]:74859 (/run/initramfs/thin-overlay/data) /dev/loop0: [2430]:100 (/run/initramfs/live/LiveOS/filesystem.squashfs)  The THIN OVERLAY is the transient space the system uses behind the scenes to allow data to live in RAM as it is written to disk. The THIN part of the overlay is the magic, using THIN overlays means the kernel will automatically clear free blocks.\n Below is the layout of what a persistent system looks like. Note, this means that persistent capacity is there, but administrators should beware of reset toggles on unfamiliar systems. There are toggles to reset overlays that are, by default, toggled off (so data persistence be default is safe but one should not assume).\nncn-m002# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT loop0 7:0 0 3.8G 1 loop /run/rootfsbase loop1 7:1 0 30G 0 loop └─live-overlay-pool 254:2 0 300G 0 dm loop2 7:2 0 300G 0 loop └─live-overlay-pool 254:2 0 300G 0 dm sda 8:0 1 447.1G 0 disk ├─sda1 8:1 1 476M 0 part │ └─md127 9:127 0 476M 0 raid1 ├─sda2 8:2 1 92.7G 0 part │ └─md126 9:126 0 92.6G 0 raid1 /run/initramfs/live └─sda3 8:3 1 279.4G 0 part └─md125 9:125 0 279.3G 0 raid1 /run/initramfs/overlayfs sdb 8:16 1 447.1G 0 disk ├─sdb1 8:17 1 476M 0 part │ └─md127 9:127 0 476M 0 raid1 ├─sdb2 8:18 1 92.7G 0 part │ └─md126 9:126 0 92.6G 0 raid1 /run/initramfs/live └─sdb3 8:19 1 279.4G 0 part └─md125 9:125 0 279.3G 0 raid1 /run/initramfs/overlayfs sdc 8:32 1 447.1G 0 disk └─ETCDLVM 254:0 0 447.1G 0 crypt └─etcdvg0-ETCDK8S 254:1 0 32G 0 lvm /run/lib-etcd Persistent Directories Not all directories are persistent!\nOnly the following directories are persistent by default:\n etc home root srv tmp var /run/containerd /run/lib-containerd /run/lib-etcd /run/lib/kubelet  More directories can be added, but mileage varies. The initial set is actually managed by dracut, when using a reset toggle the above list is \u0026ldquo;reset/cleared\u0026rdquo;. If more directories are added, they will be eradicated when enabling a reset toggle.\nThese are all provided through the Overlay from /run/overlayfs:\nncn-m001:/run/overlayfs # ls -l total 0 drwxr-xr-x 8 root root 290 Oct 15 22:41 etc drwxr-xr-x 3 root root 18 Oct 15 22:41 home drwx------ 3 root root 39 Oct 13 16:53 root drwxr-xr-x 3 root root 18 Oct 5 19:16 srv drwxrwxrwt 2 root root 85 Oct 16 14:50 tmp drwxr-xr-x 8 root root 76 Oct 13 16:52 var  Remember: /run/overlayfs is a symbolic link to the real disk /run/initramfs/overlayfs/*.\n Layering - Upperdir and Lowerdir(s) The file system the user is working on is really two layered file systems (overlays).\n The lower layer is the SquashFS image itself, read-only, which provides all that we need to run. The upper layer is the OverlayFS, read-write, which does a bit-wise xor with the lower-layer Anything in the upper-layer takes precedence by default.   There are fancier options for overlays, such as multiple lower-layers, copy-up (lower-layer precedence), and opaque (removing a directory in the upper layer hides it in the lower layer). You can read more [here|https://www.kernel.org/doc/html/latest/filesystems/overlayfs.html#inode-properties]\n Layering Real World Example Let us take /root for example, we can see in the upper-dir (the overlay) we have these files:\nThe upper-dir has these files:\nncn-m001# ls -l /run/overlayfs/root/ total 4 -rw------- 1 root root 252 Nov 4 18:23 .bash_history drwxr-x--- 4 root root 37 Nov 4 04:35 .kube drwx------ 2 root root 29 Oct 21 21:57 .ssh Then in the squashFS image (lower-dir) we have these\u0026hellip;\nncn-m001# ls -l /run/rootfsbase/root/ total 1 -rw------- 1 root root 0 Oct 19 15:31 .bash_history drwxr-xr-x 2 root root 3 May 25 2018 bin drwx------ 3 root root 26 Oct 21 22:07 .cache drwx------ 2 root root 3 May 25 2018 .gnupg drwxr-xr-x 4 root root 57 Oct 19 15:23 inst-sys drwxr-xr-x 2 root root 33 Oct 19 15:33 .kbd drwxr-xr-x 5 root root 53 Oct 19 15:34 spire drwx------ 2 root root 70 Oct 21 21:57 .ssh -rw-r--r-- 1 root root 172 Oct 26 15:25 .wget-hsts  Notice how the .bash_history file in the lower-dir is 0 bytes, but it is 252 bytes in the upperdir? Notice the .kube dir exists in the upper, but not the lower?  Finally, looking at /root we see the magic:\nncn-m001# ls -l /root total 5 -rw------- 1 root root 252 Nov 4 18:23 .bash_history drwxr-xr-x 2 root root 3 May 25 2018 bin drwx------ 3 root root 26 Oct 21 22:07 .cache drwx------ 2 root root 3 May 25 2018 .gnupg drwxr-xr-x 4 root root 57 Oct 19 15:23 inst-sys drwxr-xr-x 2 root root 33 Oct 19 15:33 .kbd drwxr-x--- 4 root root 37 Nov 4 04:35 .kube drwxr-xr-x 5 root root 53 Oct 19 15:34 spire drwx------ 1 root root 29 Oct 21 21:57 .ssh -rw-r--r-- 1 root root 172 Oct 26 15:25 .wget-hsts  Notice how .bash_history matches the upper-dir? Notice how .kube exists here?  The take-away here is: any change done to /root/ will persist through /run/overlayfs/root and will take precedence to the squashFS image root.\nOverlayFS Control These features or toggles are passable on the kernel command line, and change the behavior of the overlayFS.\nReset Toggles The overlay FS provides a few reset toggles to clear out the persistence directories without reinstall.\nThe toggles require rebooting.\nReset On Next Boot The preferred way to reset persistent storage is to use the overlayFS reset toggle.\nModify the boot command line on the PXE server, adding this\n# Reset the overlay on boot rd.live.overlay.reset=1 Once reset, you may want to enable persistence again. Simply revert your change and the next reboot will persist.\n# Cease resetting the overlayFS rd.live.overlay.reset=0 Reset on Every Boot There are two options one can leave enabled to accomplish this:\n rd.live.overlay.reset=1 will eradicate/recreate the overlay every reboot. rd.live.overlayr.readonly=1 will clear the overlay on every reboot.  For long-term usage, rd.live.overlay.readonly=1 should be added to the command line.\nThe reset=1 toggle is usually used to fix a problematic overlay. If you want to refresh and purge the overlay completely, then use rd.live.overlay.reset.\n# Authorize METAL to purge metal.no-wipe=0 rd.live.overlay.reset=1  Note: metal.no-wipe=1 does not protect against rd.live.overlay.reset, metal.no-wipe is not a feature of dmsquash-live.\n Re-sizing the Persistent Overlay  Default Size: 300 GiB File System: XFS  The overlay can be resized to fit a variety of needs or use cases. The size is provided directly on the command line. Any value can be provided, but it must be in megabytes.\nIf you are resetting the overlay on a deployed node, you will need to also set rd.live.overlay.reset=1.\nIt is recommended to set the size before deployment. There is a linkage between the metal-dracut module and the live-module that makes this inflexible.\n# Use a 300 GiB overlayFS (default) rd.live.overlay.size=307200 # Use a 1 TiB overlayFS rd.live.overlay.size=1000000 Thin Overlay Feature The persistent overlayFS leverages newer, \u0026ldquo;thin\u0026rdquo; overlays that support discards and that will free blocks that are not claimed by the file system. This means that memory is free/released when the file system does not claim it anymore.\nThin overlays can be disabled, and instead classic DM Snapshots can be used to manage the overlay. This will use more RAM. It is not recommended, because dmraid is not included in the initrd.\n# Enable (default) rd.live.overlay.thin=1 # Disable (not recommended; undesirable RAM waste) rd.live.overlay.thin=0 SystemD MetalFS The metalfs systemd service will try to mount any metal created partitions.\nThis runs against the /run/initramfs/overlayfs/fstab.metal when it exists. This file is dynamically created by most metal dracut modules.\nThe service will continuously attempt to mount the partitions, if problems arise please stop the service:\nncn# systemctl stop metalfs Old/Retired FS-Labels Deprecated FS labels/partitions from Shasta 1.3.X (no longer in Shasta 1.4.0 and onwards).\n   FS Label Partitions Nodes Device Size on Disk Work Order Memo     K8SKUBE /var/lib/kubelet ncn-w001, ncn-w002 Ephemeral Max/Remainder CASMPET-338 CASMPET-342 No longer mounted/used in shasta-1.4   K8SEPH /var/lib/cray/k8s_ephemeral ncn-w001, ncn-w002 Ephemeral Max/Remainder CASMPET-338 CASMPET-342 No longer mounted/used in shasta-1.4   CRAYINSTALL /var/cray/vfat ncn-w001, ncn-w002 Ephemeral 12 GiB CASMPET-338 CASMPET-342 No longer mounted/used in shasta-1.4   CRAYVBIS /var/cray/vbis ncn-w001, ncn-w002 Ephemeral 900 GiB CASMPET-338 CASMPET-342 No longer mounted/used in shasta-1.4   CRAYNFS /var/lib/nfsroot/nmd ncn-w001, ncn-w002 Ephemeral 12 GiB CASMPET-338 CASMPET-342 No longer mounted/used in shasta-1.4    "
},
{
	"uri": "/docs-csm/en-11/background/ncn_networking/",
	"title": "NCN Networking",
	"tags": [],
	"description": "",
	"content": "NCN Networking Non-compute nodes and compute nodes have different network interfaces used for booting, this topic focuses on the network interfaces for management nodes.\nTopics:  NCN Network Interfaces Device Naming Vendor and Bus ID Identification  Details NCN Network Interfaces The following table includes information about the different NCN network interfaces:\n   Name Type MTU     mgmt0 Slot 1 on the SMNet card. 9000   mgmt1 Slot 2 on the SMNet card, or slot 1 on the 2nd SMNet card. 9000   bond0 LACP Link Agg. of mgmt0 and mgmt1, or mgmt0 and mgmt2 on dual-bonds (when bond1 is present). 9000   bond1 LACP Link Agg. of mgmt1 and mgmt3. 9000   lan0 Externally facing interface. 1500   lan1 Yet-another externally facing interface, or anything (unused). 1500   hsn0 High-speed network interface. 9000   hsnN+1 Yet-another high-speed network interface. 9000   vlan002 Virtual LAN for managing nodes 1500   vlan004 Virtual LAN for managing hardware 1500   vlan007 Virtual LAN for the customer access network 1500    These interfaces can be observed on a live NCN with the following command.\nncn# ip link Device Naming The underlying naming relies on [BIOSDEVNAME][1], this helps conform device naming into a smaller set of possible names. It also helps show us when driver issues occur, if a non-BIOSDEVNAME interface appears then METAL can/should receive a triage report/bug.\nThe MAC based udev rules set the interfaces during initial boot in iPXE. When a node boots, iPXE will dump the PCI busses and sort network interfaces into 3 buckets:\n mgmt: internal/management network connection hsn: high-speed connection lan: external/site-connection  The source code for the rule generation is in [metal-ipxe][1], but for technical information on the PCI configuration/reading please read on.\nVendor and Bus ID Identification The initial boot of an NCN sets interface udev rules because it has no discovery method yet.\nThe information needed is:\n PCI Vendor IDs for devices/cards to be used on the Management network. PCI Device IDs for the devices/cards to be used on the High-Speed Network.  The 16-bit Vendor ID is allocated by the PCI-SIG (Peripheral Component Interconnect Special Interest Group).\nThe information belongs to the first 4 bytes of the PCI header, and admin can obtain it using lspci or your preferred method for reading the PCI bus.\nlspci | grep -i ethernet lspci | grep c6:00.0 The Device and Vendor IDs are used in iPXE for bootstrapping the nodes, this allows generators to swap IDs out for certain systems until smarter logic can be added to cloud-init.\nThe following table includes popular vendor and device IDs.\n The bolded numbers are the defaults that live in metal-ipxe\u0026rsquo;s boot script.\n    Vendor Model Device ID Vendor ID     Intel Corporation Ethernet Connection X722 37d2 8086   Intel Corporation 82576 1526 8086   Mellanox Technologies ConnectX-4 1013 15b3   Mellanox Technologies ConnectX-5 1017 15b3   Giga-Byte Intel Corporation I350 1521 8086   QLogic Corporation FastLinQ QL41000 8070 1077    "
},
{
	"uri": "/docs-csm/en-11/background/",
	"title": "Background Information",
	"tags": [],
	"description": "",
	"content": "CSM Background Information This document provides background information about the NCNs (non-compute nodes) which function as management nodes for the HPE Cray EX system. This information is not normally needed to install software, but provides background which might be helpful for troubleshooting an installation.\nTopics:  Cray Site Init Files Certificate Authority NCN Images NCN Boot Workflow NCN Networking NCN Mounts and File Systems NCN Packages NCN Operating System Releases cloud-init Basecamp Configuration  Details Cray Site Init Files The Cray Site Init (csi) command has several files which describe pre-configuration data needed during the installation process:\n application_node_config.yaml cabinets.yaml hmn_connections.json ncn_metadata.csv switch_metadata.csv  In addition, after running csi with those pre-config files, csi creates an output system_config.yaml file which can be passed to csi when reinstalling this software release.\nSee Cray Site Init Files for more information about these files.\nCertificate Authority While a system is being installed for the first time, a certificate authority (CA) is needed. This can be generated for a system, or one can be supplied from a customer intermediate CA. Outside of a new installation, there is no supported method to rotate or change the platform CA in this release.\nFor more information about these topics, see Certificate Authority\n \u0026ldquo;Overview\u0026rdquo; \u0026ldquo;Use Default Platform Generated CA\u0026rdquo; \u0026ldquo;Customize Platform Generated CA\u0026rdquo; \u0026ldquo;Use an External CA\u0026rdquo;  NCN Images The management nodes boot from NCN images which are created as layers on top of a common base image. The common image is customized with a Kubernetes layer for the master nodes and worker nodes. The common image is also customized with a storage-ceph layer for the utility storage nodes. Three artifacts are needed to boot the management nodes.\nSee NCN Images\nNCN Boot Workflow The boot workflow for management nodes (NCNs) is different from compute nodes or application nodes. They can PXE boot over the network or from local storage.\nSee NCN Boot Workflow for these topics\n How can I tell if I booted via disk or PXE? Set BMCs to DHCP Set Boot Order  Setting Order Trimming Boot Order Examples Reverting Changes Locating USB Device    NCN Networking Non-compute nodes and compute nodes have different network interfaces used for booting. The NCN network interfaces, device naming, and vendor and bus identification are described in this topic.\n NCN Networking  NCN Mounts and File Systems The management nodes have specific file systems and mounts and use overlayfs.\nSee NCN Mounts and File Systems\nNCN Packages The management nodes boot from images which have many (RPM) packages installed. The packages installed differ between the Kubernetes master and worker nodes versus the utility storage nodes.\n NCN Packages  NCN Operating System Releases All management nodes have an operating system based on SLE_HPC (SuSE High Performance Computing).\n NCN Operating System Releases  cloud-init Basecamp Configuration Metal Basecamp is a cloud-init DataSource available on the LiveCD. Basecamp\u0026rsquo;s configuration file offers many inputs for various cloud-init scripts embedded within the NCN images.\n cloud-init Basecamp Configuration  "
},
{
	"uri": "/docs-csm/en-11/background/cray_site_init_files/",
	"title": "Cray Site Init Files",
	"tags": [],
	"description": "",
	"content": "Cray Site Init Files This page describes administrative knowledge around the pre-config files to csi or the output files from csi.\n Information for collecting certain files starts in Configuration Payload\n  application_node_config.yaml cabinets.yaml hmn_connections.json ncn_metadata.csv switch_metadata.csv  Topics:  Save-File / Avoiding Parameters  Details Save-File / Avoiding Parameters A system_config.yaml file may be provided by the administrator that will omit the need for specifying parameters on the command line.\n This file is dumped in the generated configuration after every csi config init call. The new dumped file serves as a fingerprint for re-generating the same configuration.\n Here is an example file\napplication-node-config-yaml: application_node_config.yaml bgp-asn: \u0026#34;65533\u0026#34; bgp-peers: spine bootstrap-ncn-bmc-pass: admin bootstrap-ncn-bmc-user: admin cabinets-yaml: cabinets.yaml can-bootstrap-vlan: 7 can-cidr: 10.102.9.0/24 can-dynamic-pool: 10.102.9.128/25 can-gateway: 10.102.9.20 can-external-dns: 10.102.9.113 can-gw: 10.102.9.20 can-static-pool: 10.102.9.112/28 ceph-cephfs-image: dtr.dev.cray.com/cray/cray-cephfs-provisioner:0.1.0-nautilus-1.3 ceph-rbd-image: dtr.dev.cray.com/cray/cray-rbd-provisioner:0.1.0-nautilus-1.3 chart-repo: http://helmrepo.dev.cray.com:8080 docker-image-registry: dtr.dev.cray.com help: false hill-cabinets: 0 hmn-bootstrap-vlan: 4 hmn-cidr: 10.254.0.0/17 hmn-connections: hmn_connections.json hsn-cidr: 10.250.0.0/16 install-ncn: ncn-m001 install-ncn-bond-members: p801p1,p801p2 ipv4-resolvers: 8.8.8.8, 9.9.9.9 management-net-ips: 0 manifest-release: \u0026#34; mountain-cabinets: 0 mtl-cidr: 10.1.1.0/16 ncn-metadata: ncn_metadata.csv ncn-mgmt-node-auditing-enabled: false nmn-bootstrap-vlan: 2 nmn-cidr: 10.252.0.0/17 ntp-pool: time.nist.gov river-cabinets: 1 rpm-repository: https://packages.nmn/repository/shasta-master site-dns: 172.30.84.40 site-domain: dev.cray.com site-gw: 172.30.48.1 site-ip: 172.30.53.153/20 site-nic: em1 starting-hill-cabinet: 9000 starting-mountain-cabinet: 5000 starting-mountain-nid: 1000 starting-river-cabinet: 3000 starting-river-nid: 1 supernet: true switch-metadata: switch_metadata.csv system-name: eniac upstream_ntp_server: time.nist.gov v2-registry: https://registry.nmn/ versioninfo: major: \u0026#34;1\u0026#34; minor: \u0026#34;5\u0026#34; fixvr: \u0026#34;20\u0026#34; gitversion: f771260cc9b478782b2e7fa597090637916fd4ef gitcommit: f771260cc9b478782b2e7fa597090637916fd4ef-release-shasta-1.5 builddate: \u0026#34;2021-02-18T00:41:38Z\u0026#34; goversion: go1.14.9 compiler: gc platform: linux/amd64 "
},
{
	"uri": "/docs-csm/en-11/background/ncn_boot_workflow/",
	"title": "NCN Boot Workflow",
	"tags": [],
	"description": "",
	"content": "NCN Boot Workflow Non-compute nodes boot two ways:\n Network/PXE booting Disk Booting  Topics:  Determine the Current Boot Order Reasons to Change the Boot Order After CSM Install Determine if NCNs Booted via Disk or PXE Set BMCs to DHCP Boot Order Overview Setting Order Trimming Boot Order Example Boot Orders Reverting Changes Locating USB Device  Determine the Current Boot Order Under normal operations, the NCNs use the following boot order:\n PXE (to ensure the NCN is booting with desired images and configuration) Disk (fallback in the event that PXE services are unavailable)  Reasons to Change the Boot Order After CSM Install After the CSM install is complete, it is usually not necessary to change the boot order. Having PXE first and disk as a fallback works in the majority of situations.\nIt may be desirable to change the boot order under these circumstances:\n testing disk-backed booting booting from a USB or remote ISO testing or deploying other customizations  Determine if NCNs Booted via Disk or PXE There are two different methods for determining whether a management node is booted using disk or PXE. The method to use will vary depending on the system environment.\n  Check kernel parameters.\nncn# cat /proc/cmdline If it starts with kernel, then the node network booted. If it starts with BOOT_IMAGE=(, then it disk booted.\n  Check output from efibootmgr.\nncn# efibootmgr The BootCurrent value should be matched to the list beneath to see if it lines up with a networking option or a cray sd*) option for disk boots.\nncn# efibootmgr BootCurrent: 0016 \u0026lt;---- BootCurrent Timeout: 2 seconds BootOrder: 0000,0011,0013,0014,0015,0016,0017,0005,0007,0018,0019,001A,001B,001C,001D,001E,001F,0020,0021,0012 Boot0000* cray (sda1) Boot0001* UEFI: Built-in EFI Shell Boot0005* UEFI: PXE IP4 Mellanox Network Adapter - B8:59:9F:1D:D8:4E Boot0007* UEFI: PXE IP4 Mellanox Network Adapter - B8:59:9F:1D:D8:4F Boot0010* UEFI: AMI Virtual CDROM0 1.00 Boot0011* cray (sdb1) Boot0012* UEFI: Built-in EFI Shell Boot0013* UEFI OS Boot0014* UEFI OS Boot0015* UEFI: AMI Virtual CDROM0 1.00 Boot0016* UEFI: SanDisk \u0026lt;--- Matches here Boot0017* UEFI: SanDisk, Partition 2 Boot0018* UEFI: HTTP IP4 Intel(R) I350 Gigabit Network Connection Boot0019* UEFI: PXE IP4 Intel(R) I350 Gigabit Network Connection Boot001A* UEFI: HTTP IP4 Mellanox Network Adapter - B8:59:9F:1D:D8:4E Boot001B* UEFI: HTTP IP4 Mellanox Network Adapter - B8:59:9F:1D:D8:4F Boot001C* UEFI: HTTP IP4 Intel(R) I350 Gigabit Network Connection Boot001D* UEFI: PXE IP4 Intel(R) I350 Gigabit Network Connection Boot001E* UEFI: PXE IP6 Intel(R) I350 Gigabit Network Connection Boot001F* UEFI: PXE IP6 Intel(R) I350 Gigabit Network Connection Boot0020* UEFI: PXE IP6 Mellanox Network Adapter - B8:59:9F:1D:D8:4E Boot0021* UEFI: PXE IP6 Mellanox Network Adapter - B8:59:9F:1D:D8:4F   Set BMCs to DHCP If you are reinstalling a system, the BMCs for the NCNs may be set to static IP addressing. We check /var/lib/misc/dnsmasq.leases for setting up the symlinks for the artifacts each node needs to boot. So if your BMCs are set to static, those artifacts will not get set up correctly. You can set them back to DHCP by using a command such as:\nncn# export USERNAME=root ncn# export IPMI_PASSWORD=changeme ncn# for h in $( grep mgmt /etc/dnsmasq.d/statics.conf | grep -v m001 | awk -F \u0026#39;,\u0026#39; \u0026#39;{print $2}\u0026#39; ) do ipmitool -U $USERNAME -I lanplus -H $h -E lan set 1 ipsrc dhcp done Some BMCs need a cold reset in order to pick up this change fully:\nncn# export USERNAME=root ncn# export IPMI_PASSWORD=changeme ncn# for h in $( grep mgmt /etc/dnsmasq.d/statics.conf | grep -v m001 | awk -F \u0026#39;,\u0026#39; \u0026#39;{print $2}\u0026#39; ) do ipmitool -U $USERNAME -I lanplus -H $h -E mc reset cold done Boot Order Overview  ipmitool can set and edit boot order; it works better for some vendors based on their BMC implementation efibootmgr speaks directly to the node\u0026rsquo;s UEFI; it can only be ignored by new BIOS activity   NOTE Cloud-init will set boot order when it runs, but this does not always work with certain hardware vendors. An administrator can invoke the cloud-init script at /srv/cray/scripts/metal/set-efi-bbs.sh on any NCN. Find the script here, on GitHub.\n Setting Order This section gives the procedure for setting the boot order on NCNs and the PIT node.\nSetting the boot order with efibootmgr will ensure that the desired network interfaces and disks are in the proper order for booting.\nThe commands are the same for all hardware vendors, except where noted.\n  Create a list of the desired IPv4 boot devices.\nFollow the section corresponding to the hardware manufacturer of the system:\n  Gigabyte Technology\nncn# efibootmgr | grep -iP \u0026#39;(pxe ipv?4.*adapter)\u0026#39; | tee /tmp/bbs1   Hewlett-Packard Enterprise\nncn# efibootmgr | grep -i \u0026#39;port 1\u0026#39; | grep -i \u0026#39;pxe ipv4\u0026#39; | tee /tmp/bbs1   Intel Corporation\nncn# efibootmgr | grep -i \u0026#39;ipv4\u0026#39; | grep -iv \u0026#39;baseboard\u0026#39; | tee /tmp/bbs1     Create a list of the cray disk boot devices.\nncn# efibootmgr | grep cray | tee /tmp/bbs2   Set the boot order to first PXE boot, with disk boot as the fallback options.\nncn# efibootmgr -o $(cat /tmp/bbs* | awk \u0026#39;!x[$0]++\u0026#39; | sed \u0026#39;s/^Boot//g\u0026#39; | tr -d \u0026#39;*\u0026#39; | awk \u0026#39;{print $1}\u0026#39; | tr -t \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39; | sed \u0026#39;s/,$//\u0026#39;) | grep -i bootorder   Set all of the desired boot options to be active.\nncn# cat /tmp/bbs* | awk \u0026#39;!x[$0]++\u0026#39; | sed \u0026#39;s/^Boot//g\u0026#39; | tr -d \u0026#39;*\u0026#39; | awk \u0026#39;{print $1}\u0026#39; | xargs -r -t -i efibootmgr -b {} -a   After following the steps above on a given NCN, that NCN will now use the desired Shasta boot order.\nThis is the end of the Setting Boot Order procedure.\nTrimming Boot Order This section gives the procedure for removing unwanted entries from the boot order on NCNs and the PIT node.\nThis section will only advise on removing other PXE entries. There are too many vendor-specific entries beyond disks and NICs to cover in this section (e.g. BIOS entries, iLO entries, etc.).\nIn this case, the instructions are the same regardless of node type (management, storage, or worker):\n  Make lists of the unwanted boot entries.\n  Gigabyte Technology\nncn# efibootmgr | grep -ivP \u0026#39;(pxe ipv?4.*)\u0026#39; | grep -iP \u0026#39;(adapter|connection|nvme|sata)\u0026#39; | tee /tmp/rbbs1 ncn# efibootmgr | grep -iP \u0026#39;(pxe ipv?4.*)\u0026#39; | grep -i connection | tee /tmp/rbbs2   Hewlett-Packard Enterprise\n NOTE This does not trim HSN Mellanox cards; these should disable their OpROMs using the high speed network snippet(s).\n ncn# efibootmgr | grep -vi \u0026#39;pxe ipv4\u0026#39; | grep -i adapter |tee /tmp/rbbs1 ncn# efibootmgr | grep -iP \u0026#39;(sata|nvme)\u0026#39; | tee /tmp/rbbs2   Intel Corporation\nncn# efibootmgr | grep -vi \u0026#39;ipv4\u0026#39; | grep -iP \u0026#39;(sata|nvme|uefi)\u0026#39; | tee /tmp/rbbs1 ncn# efibootmgr | grep -i baseboard | tee /tmp/rbbs2     Remove them.\nncn# cat /tmp/rbbs* | awk \u0026#39;!x[$0]++\u0026#39; | sed \u0026#39;s/^Boot//g\u0026#39; | awk \u0026#39;{print $1}\u0026#39; | tr -d \u0026#39;*\u0026#39; | xargs -r -t -i efibootmgr -b {} -B   Your boot menu should be trimmed down to contain only relevant entries.\nThis is the end of the Trimming Boot Order procedure.\nExample Boot Orders   Master node (with onboard NICs enabled)\nncn-m# efibootmgr BootCurrent: 0009 Timeout: 2 seconds BootOrder: 0004,0000,0007,0009,000B,000D,0012,0013,0002,0003,0001 Boot0000* cray (sda1) Boot0001* UEFI: Built-in EFI Shell Boot0002* UEFI OS Boot0003* UEFI OS Boot0004* cray (sdb1) Boot0007* UEFI: PXE IP4 Intel(R) I350 Gigabit Network Connection Boot0009* UEFI: PXE IP4 Mellanox Network Adapter - B8:59:9F:34:89:62 Boot000B* UEFI: PXE IP4 Mellanox Network Adapter - B8:59:9F:34:89:63 Boot000D* UEFI: PXE IP4 Intel(R) I350 Gigabit Network Connection Boot0012* UEFI: PNY USB 3.1 FD PMAP Boot0013* UEFI: PNY USB 3.1 FD PMAP, Partition 2   Storage node (with onboard NICs enabled)\nncn-s# efibootmgr BootNext: 0005 BootCurrent: 0006 Timeout: 2 seconds BootOrder: 0007,0009,0000,0002 Boot0000* cray (sda1) Boot0001* UEFI: Built-in EFI Shell Boot0002* cray (sdb1) Boot0005* UEFI: PXE IP4 Intel(R) I350 Gigabit Network Connection Boot0007* UEFI: PXE IP4 Mellanox Network Adapter - B8:59:9F:34:88:76 Boot0009* UEFI: PXE IP4 Mellanox Network Adapter - B8:59:9F:34:88:77 Boot000B* UEFI: PXE IP4 Intel(R) I350 Gigabit Network Connection   Worker node (with onboard NICs enabled)\nncn-w# efibootmgr BootNext: 0005 BootCurrent: 0008 Timeout: 2 seconds BootOrder: 0007,0009,000B,0000,0002 Boot0000* cray (sda1) Boot0001* UEFI: Built-in EFI Shell Boot0002* cray (sdb1) Boot0005* UEFI: PXE IP4 Intel(R) I350 Gigabit Network Connection Boot0007* UEFI: PXE IP4 Mellanox Network Adapter - 98:03:9B:AA:88:30 Boot0009* UEFI: PXE IP4 Mellanox Network Adapter - B8:59:9F:34:89:2A Boot000B* UEFI: PXE IP4 Mellanox Network Adapter - B8:59:9F:34:89:2B Boot000D* UEFI: PXE IP4 Intel(R) I350 Gigabit Network Connection   Reverting Changes This procedure is only needed if you wish to revert boot order changes.\nReset the BIOS. Refer to vendor documentation for resetting the BIOS or attempt to reset the BIOS with ipmitool\n NOTE When using ipmitool against a machine remotely, it requires more arguments:\nlinux# USERNAME=root linux# IPMI_PASSWORD=CHANGEME linux# ipmitool -I lanplus -U $USERNAME -E -H \u0026lt;bmc-hostname\u0026gt;    Reset BIOS with ipmitool\nncn# ipmitool chassis bootdev none options=clear-cmos   Set next boot with ipmitool\nncn# ipmitool chassis bootdev pxe options=efiboot,persistent   Boot to BIOS for checkout of boot devices\nncn# ipmitool chassis bootdev bios options=efiboot   This is the end of the Reverting Changes procedure.\nLocating USB Device This procedure explains how to identify USB devices on NCNs.\nSome nodes very obviously display which device is the USB, other nodes (such as Gigabyte) do not.\nParsing the output of efibootmgr can be helpful in determining which device is your USB device. One can and should use tools such as lsblk, blkid, or kernel (/proc) as well if one knows how. As an example, one can sometimes match up ls -l /dev/disk/by-partuuid with efibootmgr -v.\n  Display the current UEFI boot selections.\nncn# efibootmgr BootCurrent: 0015 Timeout: 1 seconds BootOrder: 000E,000D,0011,0012,0007,0005,0006,0008,0009,0000,0001,0002,000A,000B,000C,0003,0004,000F,0010,0013,0014 Boot0000* Enter Setup Boot0001 Boot Device List Boot0002 Network Boot Boot0003* Launch EFI Shell Boot0004* UEFI HTTPv6: Network 00 at Riser 02 Slot 01 Boot0005* UEFI HTTPv6: Intel Network 00 at Baseboard Boot0006* UEFI HTTPv4: Intel Network 00 at Baseboard Boot0007* UEFI IPv4: Intel Network 00 at Baseboard Boot0008* UEFI IPv6: Intel Network 00 at Baseboard Boot0009* UEFI HTTPv6: Intel Network 01 at Baseboard Boot000A* UEFI HTTPv4: Intel Network 01 at Baseboard Boot000B* UEFI IPv4: Intel Network 01 at Baseboard Boot000C* UEFI IPv6: Intel Network 01 at Baseboard Boot000D* UEFI HTTPv4: Network 00 at Riser 02 Slot 01 Boot000E* UEFI IPv4: Network 00 at Riser 02 Slot 01 Boot000F* UEFI IPv6: Network 00 at Riser 02 Slot 01 Boot0010* UEFI HTTPv6: Network 01 at Riser 02 Slot 01 Boot0011* UEFI HTTPv4: Network 01 at Riser 02 Slot 01 Boot0012* UEFI IPv4: Network 01 at Riser 02 Slot 01 Boot0013* UEFI IPv6: Network 01 at Riser 02 Slot 01 Boot0014* UEFI Samsung Flash Drive 1100 Boot0015* UEFI Samsung Flash Drive 1100 Boot0018* UEFI SAMSUNG MZ7LH480HAHQ-00005 S45PNA0M838871 Boot1001* Enter Setup   Set next boot entry.\nIn the example above, our device is 0014 or 0015. We will guess it is the first one, and can correct this on-the-fly in POST. Notice the lack of \u0026ldquo;Boot\u0026rdquo; in the ID number given, we want Boot0014 so we pass 0014 to efibootmgr:\nncn-m# efibootmgr -n 0014   Verify the BootNext device is what you selected:\nncn-m# efibootmgr | grep -i bootnext BootNext: 0014   Now the UEFI Samsung Flash Drive will boot next.\n Note There are duplicates in the list. During boot, the EFI boot manager will select the first one. If you find that the first one is false, false entries can be deleted with efibootmgr -b 0014 -d.\n   This is the end of the Locating USB Device procedure.\n"
},
{
	"uri": "/docs-csm/en-11/background/ncn_bios/",
	"title": "Non-compute Node Bios",
	"tags": [],
	"description": "",
	"content": "Non-Compute Node BIOS This page denotes BIOS settings that are desirable for non-compute nodes.\n NOTE Any tunables in this page are in the interest of performance and stability. If either of those facets seem to be infringed by any of the content on this page, please contact Cray System Management for reconciliation.\n  NOTE The table below declares desired settings; unlisted settings should remain at vendor-default. This table may be expanded as new settings are adjusted.\n    Common Name Common Value Description Value Rationale Common Menu Location     Intel® Hyper-Threading (e.g. HT) Enabled Enables two-threads per physical core. Leverage the full performance of the CPU, the higher thread-count assists with parallel tasks within the processor(s). Within the Processor or the PCH Menu.   Intel® Virtualization Technology (e.g. VT-x, VT) and AMD Virtualization Technology (e.g. AMD-V) Enabled Enables Virtual Machine extensions. Provides added CPU support for hypervisors and more for the virtualized plane within Shasta. Within the Processor or the PCH Menu.   PXE Retry Count 1 or 2 (default: 1) Attempts done on a single boot-menu option (note: 2 should be set for systems with unsolved network congestion). If networking is working nominally, then the interface either works or does not. Retrying the same NIC should not work, if it does then there are networking problems that need to be addressed. Within the Networking Menu, and then under Network Boot.   PXE Timeout 5 Seconds (or less, never more) The time that the PXE ROM will wait for a DHCP handshake to complete before moving on to the next boot device. If DHCP is working nominally, then the DHCP handshake should not take longer than 5 seconds. This timeout could be increased where networking faults cannot be reconciled, but ideally this should be tuned to 3 or 2 seconds.    Continuous Boot Disabled Whether boot-group (e.g. all network devices, or all disk devices) should continuously retry. This prevents fall-through to the fallback disks. We want deterministic nodes in Shasta, if the boot fails the first tier we want the node to try the next tier of boot mediums before failing at a shell or menu for intervention.      NOTE PCIe options can be found in PCIe : Setting Expected Values.\n "
},
{
	"uri": "/docs-csm/en-11/background/certificate_authority/",
	"title": "Certificate Authority",
	"tags": [],
	"description": "",
	"content": "Certificate Authority While a system is being installed for the first time, a certificate authority (CA) is needed. This can be generated for a system, or one can be supplied from a customer intermediate CA. Outside of a new installation, there is no supported method to rotate or change the platform CA in this release.\nTopics:  Overview Use Default Platform Generated CA Customize Platform Generated CA Use External CA  Overview At install time, a PKI certificate authority (CA) can either be generated for a system, or a customer can opt to supply their own (intermediate) CA.\n Outside of a new installation, there is currently no supported method to rotate (change) the platform CA. The ability to rotate CAs is anticipated as part of a future release.\n Sealed Secrets, part of shasta-cfg, are used by the installation process to inject CA material in an encrypted form. Vault (cray-vault instance) ultimately sources and stores the CA from a K8S secret (result of decrypting the corresponding Sealed Secret).\nThe resulting CA will be used to sign multiple workloads on the platform (Ingress, mTLS for PostgreSQL Clusters, Spire, \u0026hellip;).\n Management of Sealed Secrets should ideally take place on a secure workstation.\n Use Default Platform Generated CA In shasta-cfg, there is a Sealed Secret generator named platform_ca. By default, the customizations.yaml file will contain a generation template to use this generator, and will create a sealed secret named generated-platform-ca-1. The cray-vault overrides in customizations.yaml contain a) a templated reference to expand the generated-platform-ca-1 Sealed Secret and b) directives instructing vault to load the CA material on start-up \u0026ndash; ultimately initializing a HashiCorp Vault PKI Engine instance with the material.\n Note: the intermediate CA gets installed into Vault, not the root CA (as generated). Use of a root CA is not recommended.\n The resulting default configuration (prior to seeding customizations) should look like the following customizations.yaml snippet:\nspec: ... kubernetes: sealed_secrets: ... gen_platform_ca_1: generate: name: generated-platform-ca-1 data: - type: platform_ca args: root_days: 3651 int_days: 3650 root_cn: \u0026#34;Platform CA\u0026#34; int_cn: \u0026#34;Platform CA - L1\u0026#34; services: ... cray-vault: sealedSecrets: - \u0026#34;{{ kubernetes.sealed_secrets.gen_platform_ca_1 | toYaml }}\u0026#34; pki: customCA: enabled: true secret: generated-platform-ca-1 private_key: int_ca.key certificate: int_ca.crt ca_bundle: root_ca.crt ...  The platform_ca generator will produce RSA CAs with a 3072-bit modulus, using SHA256 as the base signature algorithm.\n Customize Platform Generated CA The platform_ca generator inputs can be customized, if desired. Notably, the root_days, int_days, root_cn, and int_cn fields can be modified. While the shasta-cfg documentation on the use of generators supplies additional detail, the *_days settings control the validity period and the *_cn settings control the common name value for the resulting CA certificates. Ensure the Sealed Secret name reference in spec.kubernetes.services.cray-vault.sealedSecrets is updated if you opt to use a different name.\n Outside of a new installation, there is currently no supported method to rotate (change) the platform CA. Please set validity periods accordingly. The ability to rotate CAs is anticipated as part of a future release.\n Use External CA The static_platform_ca generator, part of shasta-cfg, can be used to supply an external CA private key, certificate, and associated upstream CAs that form the trust chain. The generator will attempt to prevent you from supplying a root CA. You must also supply the entire trust chain up to the root CA certificate.\n Outside of a new installation, there is currently no supported method to rotate (change) the platform CA. Please ensure validity periods are set accordingly for external CAs you use in this process. The ability to rotate CAs is anticipated as part of a future release.\n Here is an example customizations.yaml snippet illustrating the generator input to inject a static CA:\nspec: ... kubernetes: sealed_secrets: ... external_platform_ca_1: generate: name: external-platform-ca-1 data: - type: static_platform_ca args: key: |------BEGIN PRIVATE KEY----- MIIG/gIBADANBgkqhkiG9w0BAQEFAASCBugwggbkAgEAAoIBgQDvhzXCUmGalTDo uswnppXbM+E+OwU79xvaZBsiGEDPpERPZfizpSO3/6IWnYvCUCrb1V4rIhkSKGYq LLVMhmEkfiEImDnx+ksbZau3/w23ogP4qj+BpbTRF707//IOfXgRSD1Q+mVQ7MVo crOt8e/hR4DqZjbkWOrw9pdrfvV159o6x9RVpip33BkAtDzONYApY6ePhzS1BFmo I9R0zMGNeVpy7I2m47YUwpyGAWjRoof0P2BFHX7vdEoJE/TWAlbbiqlM9OHmR85J I/O0MwP63C2Eqn9HajbF1GPVw2IvGN6fE3THtmVDVwxD17cFsKxtVl8gMHljkw9V I+U5piuIfDPvaCoUIC3hlv7jsQs9j52LyZZF3sOKP3xsGG4a5ThqK08EKEgrFovg MYsQrt8aSx7o/7K6IzDOD9QVf7dmkFVxlbPGAjR6nlQ5aW7gFEOAr1CbbZFS+lKi KGjHGraIv93MTqqToE7yRJ6Sv0yP7U9clCi6MNi89AWFfZDkLAsCAwEAAQKCAYAW R61odeE+T8JM45M53PTzfs/kyfiiq0mb9tPPSBI/Pjhcak/H5gR8iPq6v8zQNkTG TgKEYJeUaM2X/rCefaFrk4/fDMnXCEEUO1DNvJu6CQf1iWB+3rsC+AJSImyRjHou oVmSvrfN3zg9ju3HsElv2wbSxs80TlEMOOO8zAJpBTf3X78QeHRa0c5BkoJVbASP 1QUxBJKSg+UTDsIkWydl0XPoXLiQXX4CUFfe3yKw3T1oKrz5sNSt0VNRpNmRToY3 s96Teuv2iBUnN4UciuFajgjlP0Wt2YvntWoYcwJ7mOjwo6Ru5IXdPMeLBx/xKeLF j2SnPiozSAg2OV8G+yffOIcV7598s2Jh9LpgEX0S2NWPdSrjp33IWM9clivzQXaV fFZtFcb3dkrXTt2jVuj6hQR5dsVMC/D/sfORPuAudejmUkAYmozTI9vgcOJpWw3h AT8KBZ6xR3ifr3/GwJk9eosFMeLCTnUprhgbMzM9sde31NOzgYPhiPrN4GJRp4EC gcEA+e3m7HNrSY766GOaiYwiVdzLftL7i6Ie0QTHqJLLESu2/XyxuoML6IRXc+Df A/HVtuwJMqxEe3APvOcwS/Qs6qnPhh0WNz9vJ+3D/uo7Om3cbIR8J6QlsQID9Kas /OAOqxcbtedkkiDSzVM1SPzNh+R85FBDK2xBM433Eu9xET0V8YZegT99SWg72l8+ M37/EhGvtyQpYpY8lYs8pI3Xj7IRLt+jkPKu59uDdATMvVntOMheddpTwYW7XdUI M67VAoHBAPVYodD9Hoe5AcUBrahM7trGzAw3z8fom5lf/wmzJ6Mow8lgH6tliwCs 4NS5PR45olONhK7o7vd/PXvzP1QSIHLNbInveCH29O0ZmBasDlF/eDT+Hcdzq0sw YWUR+9mX5kNS3DuZaWy6f2PDQC+mzPn1yxGmwL2yW0sY6ExfKjmFVSjqG7Mt/oMo BriKaANd3ctge3aRm2MHniXOPq+jC2Zq1rRopWgWIWDzchQsyl4e6iHs5s80nQsE R9nrC6CfXwKBwQDMlwLB7HmW7YRXV7HZhu1UfDnYx71CwKOZVuBaDlBM7gwN1VVn 6H6HCE7OfPYStJTN+MpOwNYOdd1sNZRDmM5sCjXnA0h8UWEcvnYC5ps1aVlXO9ym VqjEDXJPg2F4X7GiPHhin9ikBlqJ2eN0q/1TkKbr/wf9M9Dr8vqedYOJKQgdfnE+ PErDHKBiUjUI0pzanb/Jm8CFA5b0k9ZAnhwndQy74jZzITYsdnVVM9il6EdYhC1P LDoD4QVP+mOMa0ECgcEA0ZCKb4O1j0Kk000ysx47q53A7vLBRUVXmzOXGgbwZXpN efXkNze9+q6wQKOVI/sgv3OTEQAgFkGWGAjXYA03sDftbQiiOYjC/r8s3LjMZiqW V9VzREl11/yURIuO7vbDlV/yg+nvVhMa+vDtI4a7cQrVENe5rI7rUgMNcSacX5OX ASKu1GcGDaujyf9XBwEnkS9xZf7LllQMbshzXPzMoQfDK0hzeKvmiPSIzdjQZoLL hHzhTb3oIl/eq7IMNX/LAoHAYuVeWbSXROyXITXrYcYMwgtYjjUWThQmrLQImJjj HDUNMqq8w8OaQsV+JpZ0lwukeYst3d8vH8Eb4UczUaR+oJpBeEmXjXCGYG4Ec1EQ H72VrrZoJowoqORDSp88h+akcF6+vPJPuNC/Ea7+eAeiYqgxOX5nc2uLjZxBt4OC AhKMY5mnBN2pfAkGVpuyUw3dqGctTSCT0jnxvFPXpldgdAmXi2NTPqPd0IzmLKNG jja1TCeqn9XRTy+EArf1bYi+ -----END PRIVATE KEY----- cert: |------BEGIN CERTIFICATE----- MIIEZTCCAs2gAwIBAgIJAKnqv1FyMOp/MA0GCSqGSIb3DQEBCwUAMFsxDzANBgNV BAoMBlNoYXN0YTERMA8GA1UECwwIUGxhdGZvcm0xGjAYBgNVBAMMEVJvb3QgR2Vu ZXJhdGVkIENBMRkwFwYDVQQDDBBQbGF0Zm9ybSBSb290IENBMB4XDTIwMDcwMTIz MjU1MVoXDTIwMDcxMTIzMjU1MVowJDEPMA0GA1UECgwGU2hhc3RhMREwDwYDVQQL DAhQbGF0Zm9ybTCCAaIwDQYJKoZIhvcNAQEBBQADggGPADCCAYoCggGBAO+HNcJS YZqVMOi6zCemldsz4T47BTv3G9pkGyIYQM+kRE9l+LOlI7f/ohadi8JQKtvVXisi GRIoZiostUyGYSR+IQiYOfH6Sxtlq7f/DbeiA/iqP4GltNEXvTv/8g59eBFIPVD6 ZVDsxWhys63x7+FHgOpmNuRY6vD2l2t+9XXn2jrH1FWmKnfcGQC0PM41gCljp4+H NLUEWagj1HTMwY15WnLsjabjthTCnIYBaNGih/Q/YEUdfu90SgkT9NYCVtuKqUz0 4eZHzkkj87QzA/rcLYSqf0dqNsXUY9XDYi8Y3p8TdMe2ZUNXDEPXtwWwrG1WXyAw eWOTD1Uj5TmmK4h8M+9oKhQgLeGW/uOxCz2PnYvJlkXew4o/fGwYbhrlOGorTwQo SCsWi+AxixCu3xpLHuj/srojMM4P1BV/t2aQVXGVs8YCNHqeVDlpbuAUQ4CvUJtt kVL6UqIoaMcatoi/3cxOqpOgTvJEnpK/TI/tT1yUKLow2Lz0BYV9kOQsCwIDAQAB o2MwYTAPBgNVHRMBAf8EBTADAQH/MA4GA1UdDwEB/wQEAwIBBjAdBgNVHQ4EFgQU uNa6qcbJsHdxo6k8kaR5o53DNbIwHwYDVR0jBBgwFoAU/SFNwDBMcAYWBC2SCsDf OyZJbEMwDQYJKoZIhvcNAQELBQADggGBAD8O1Vg9WLFem0RZiZWjtXiNOTZmaksE +a49CE7yGqyETljlVOvbkTUTr4eJnzq2prYJUF8QavSBs38OahcxkTU2GOawZa09 hFc1aBiGSPAxTxJqdHV+G3QZcce1CG2e9VyrxqNudosNRNBEPMOsgg4LpvlRqMfm QhPEJcfvVaCopDZBFXLBPxqmt9BckWFmTSsK09xnrCE/40YD69hdUQ6USJaz9/cd UfNm0HIugRUMvFUP2ytdJmbV+1YQbfVsFrKU4aClrMg+ECX83od5N1TUNQwMePLh IizLGoGDF353eRVKxlzyI724Ni9W82rMW66TQdA7vU6liItHYrhDmcZ+mK2R0F5B ZuYjsLf/BCQ1uDv/bsVG40ogjH/eI/qfhRIzbgVVTF74uKG97pOakp2iQaG9USFd 9/s6ouQQXfkDZ2a/vzs8SBD4eIx7vmeABPRqlHTE8VzohxugxMbJNMdZRPGrEeH6 uddqVNpMH9ehQtsDdt0nmfVIy9/An3BKFw== -----END CERTIFICATE----- ca_bundle: |------BEGIN CERTIFICATE----- MIIEezCCAuOgAwIBAgIJAMjuQjQKUpUtMA0GCSqGSIb3DQEBCwUAMFsxDzANBgNV BAoMBlNoYXN0YTERMA8GA1UECwwIUGxhdGZvcm0xGjAYBgNVBAMMEVJvb3QgR2Vu ZXJhdGVkIENBMRkwFwYDVQQDDBBQbGF0Zm9ybSBSb290IENBMB4XDTIwMDcwMTIz MjU1MVoXDTIwMDcxMTIzMjU1MVowWzEPMA0GA1UECgwGU2hhc3RhMREwDwYDVQQL DAhQbGF0Zm9ybTEaMBgGA1UEAwwRUm9vdCBHZW5lcmF0ZWQgQ0ExGTAXBgNVBAMM EFBsYXRmb3JtIFJvb3QgQ0EwggGiMA0GCSqGSIb3DQEBAQUAA4IBjwAwggGKAoIB gQDQ0DTdZmqCOfrWb8KTXJ0hT1r2G51rRE5eAp8d/PoVCgV1gg5h1+jbiv3yYd2R BgM/CPZPvEJaL03wR1gO9NiGEXh1ALd8+yv1O1VRKNb6JuB5cPZFHE3Z8El6aGMc zrqN1ZekRPrZMM1W5Iw78olOMZvsxYw0ZIJqfKOWYB9jYUNM1KohHVj65f/HD/Em kC+9VFhepRV9z21q6fBU13bMz6/NlW19omvbTMwrVSPbYi2nSzqOfi00GXmVh/9Q WElBrAeiGLOsjWkeQ8sFF8ab4SSvzLAAilyQqkBhz2jIxB4L7iG+b9KEgVLeOoMH 1Rs7RhduOMEQypZGVA/vsu/86/5ctM1Cu60mZP+s5B7oT2rwypz0ihLiVCaDCcS5 lDK7PPT5GxZPD8TAqX0SgtaxJnSB/RzavGPSS7efFvlWXh18frwlwa+FgOnyCw1/ qR3BHarcZX9XZivBQSupxQAaUNPMlk0N4wYi6oWrmf21zwd7NtZAinxC2F98J1sn sK8CAwEAAaNCMEAwDwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAQYwHQYD VR0OBBYEFP0hTcAwTHAGFgQtkgrA3zsmSWxDMA0GCSqGSIb3DQEBCwUAA4IBgQAp ApgLdQBK6fZ7CWlEWwXSKxcjv3akuSqf1NXfn/J9e1rAqqyYoTDE9DXG9dYHL9OA p78KLsLy9fQmrLMmjacXw49bpXDG6XN1WLJfhgQg3j7lXvOvXyxynOgKDtBlroiU nMoK+or9lF2lBIuY34GPyZCL/+vB8s1tu0dGBDgHMUL8/k5d27sdGZZUljC7CgcC k+ABrv19IygDpZpZ6m5N27xajnKpJSjXOfpMCPdhCuNRMgMTX6x8bxZzVAx9ogQ8 16ZzAziB4iMXeCggaY/+YnoEstzTDPXB8FuqeGEVt63Y9ZA7NgWYvVExtKFGGhOL lnEhCLjQyu6/LgOJNfNM9EofaE/IU+i0talgFA+ygSChmYdXzFJn4EfAY9XbwEwV Pw+NHbkpv82jIpc+mopuMRdDO5OyFb+IGkn7ITUFE9N+u97oz2PjD5nQ/Z5DGjBu y3sefnrlqaRanHYkmOnOBTwImPSq8RE8eJP2aRrnu+2YrnoACXxS+XWUXtNhXJ4= -----END CERTIFICATE----- services: ... cray-vault: sealedSecrets: - \u0026#34;{{ kubernetes.sealed_secrets.external_platform_ca_1 | toYaml }}\u0026#34; pki: customCA: enabled: true secret: external-platform-ca-1 private_key: int_ca.key certificate: int_ca.crt ca_bundle: ca_bundle.crt ...  Only RSA-based CAs with 3072- or 4096-bit moduli, using RSA256 as a signature/digest algorithm have been tested/are supported. Also note, the generator does not support password-protected private keys.\n "
},
{
	"uri": "/docs-csm/en-11/background/cloud-init_basecamp_configuration/",
	"title": "Cloud-init Basecamp Configuration",
	"tags": [],
	"description": "",
	"content": "cloud-init Basecamp Configuration Metal Basecamp is a cloud-init DataSource available on the LiveCD. Basecamp\u0026rsquo;s configuration file offers many inputs for various cloud-init scripts baked into the NCN images.\nThis page details what those settings are.\n Basecamp  Config Files Purging Basecamp   CAN CEPH  Certificate Authority RADOS Gateway Wiping   DNS  Resolution Configuration Static Fallback   Kubernetes NTP Node Auditing  Generally these settings are determined by the cray-site-init tool. See csi config --help for more information. Manual adjustments typically are for debug and development.\nBasecamp Config Files  The cloud-init configuration file is located at /var/www/ephemeral/configs/data.json. The basecamp server configuration file is located at /var/www/ephemeral/configs/server.yaml The static artifact directory served by basecamp can be leveraged at /var/www/ephemeral/static   NOTE The jq tool is provided on the LiveCD to facilitate viewing JSON files like these.\n Purging Basecamp If the desire to reset basecamp to defaults comes up, you can do so by following these commands.\npit# systemctl stop basecamp pit# podman rm basecamp pit# podman rmi basecamp pit# rm -f /var/www/ephemeral/configs/server.yaml pit# systemctl start basecamp Basecamp is now entirely fresh.\nCAN Customer Access Network.\n Key: can-gw\ndata:\n{ // ... \u0026#34;can-gw\u0026#34;: \u0026#34;10.102.9.20\u0026#34;, // ... }  Key: can-if\ndata:\n{ // ... \u0026#34;can-if\u0026#34;: \u0026#34;vlan007\u0026#34;, // ... }  CEPH  Key: num_storage_nodes\ndata:\n{ // ... \u0026#34;num_storage_nodes\u0026#34;: \u0026#34;3\u0026#34;, // ... }  Certificate Authority  Key: ca-certs\ndata:\n{ // ... \u0026#34;ca-certs\u0026#34;: {\u0026#34;remove-defaults\u0026#34;:false,\u0026#34;trusted\u0026#34;:[\u0026#34;-----BEGIN CERTIFICATE-----\\nM,\u0026#34;]} // ... }  RADOS Gateway  Key: rgw-virtual-ip\ndata:\n{ // ... \u0026#34;rgw-virtual-ip\u0026#34;: \u0026#34;10.252.1.3\u0026#34;, // ... }  Wiping  Key: wipe-ceph-osds\ndata:\n{ // ... \u0026#34;wipe-ceph-osds\u0026#34;: \u0026#34;yes\u0026#34;, // ... }  DNS cloud-init modifications to DNS.\nResolution Configuration Paves over bootstrap provisions by adjusting /etc/sysconfig/network/config to match the dns-server value. Updates /etc/resolv.conf by invoking netconfig update -f.\n script: /srv/cray/scripts/metal/set-dns-config.sh\n  Key: dns-server\ndata:\n{ // ... \u0026#34;dns-server\u0026#34;: \u0026#34;10.92.100.225 10.252.1.4\u0026#34;, // ... }  Key: domain\ndata:\n{ // ... \u0026#34;domain\u0026#34;: \u0026#34;nmn hmn\u0026#34;, // ... }  Static Fallback Safety-net script for installing static-fallback resolution when Kubernetes is offline.\n script: /srv/cray/scripts/metal/set-host-records.sh\n Key: host_records\ndata:\n{ // ... \u0026#34;host_records\u0026#34;: [ { \u0026#34;aliases\u0026#34;: [ \u0026#34;ncn-s003.nmn\u0026#34;, \u0026#34;ncn-s003\u0026#34; ], \u0026#34;ip\u0026#34;: \u0026#34;10.252.1.4\u0026#34; }, { \u0026#34;aliases\u0026#34;: [ \u0026#34;ncn-s003.mtl\u0026#34; ], \u0026#34;ip\u0026#34;: \u0026#34;10.1.1.2\u0026#34; }, { \u0026#34;aliases\u0026#34;: [ \u0026#34;ncn-s003.hmn\u0026#34;, // ... // ... } Kubernetes  Key: k8s_virtual_ip\ndata:\n{ // ... \u0026#34;k8s_virtual_ip\u0026#34;: \u0026#34;10.252.1.2\u0026#34;, // ... }   Key: first_master_hostname\ndata:\n{ // ... \u0026#34;first_master_hostname\u0026#34;: \u0026#34;ncn-m002\u0026#34;, // ... }  NTP cloud-init modifications to NTP.\n script: /srv/cray/scripts/metal/set-ntp-config.sh\n  Key: ntp_peers\ndata:\n{ // ... \u0026#34;ntp_peers\u0026#34;: \u0026#34;ncn-m003 ncn-w001 ncn-s001 ncn-s002 ncn-s003 ncn-m002 ncn-w003 ncn-m001 ncn-w002\u0026#34;, // ... }  Key: ntp_local_nets\ndata:\n{ // ... \u0026#34;ntp_local_nets\u0026#34;: \u0026#34;10.252.0.0/17 10.254.0.0/17\u0026#34;, // ... }  Key: upstream_ntp_server\n WARNING at this time, multiple upstream-NTP servers cannot be specified.\n data:\n{ // ... \u0026#34;upstream_ntp_server\u0026#34;: \u0026#34;time.nist.gov\u0026#34;, // ... }  Node Auditing  Key: ncn-mgmt-node-auditing-enabled\ndata:\n{ // ... \u0026#34;ncn-mgmt-node-auditing-enabled\u0026#34;: false, // ... }  "
},
{
	"uri": "/docs-csm/en-11/release_notes/",
	"title": "Cray System Management - Release Notes",
	"tags": [],
	"description": "",
	"content": "Cray System Management (CSM) - Release Notes What’s New Bug Fixes Known Issues  Incorrect_output_for_bos_command_rerun: When a Boot Orchestration Service (BOS) session fails, it may output a message in the Boot Orchestration Agent (BOA) log associated with that session. This output contains a command that instructs the user how to re-run the failed session. It will only contain the nodes that failed during that session. The command is faulty, and this issue addresses correcting it. Cfs_session_stuck_in_pending: Under some circumstances, Configuration Framework Service (CFS) sessions can get stuck in a pending state, never completing and potentially blocking other sessions. This addresses cleaning up those sessions. The branch parameter in CFS configurations may not work, and setting it will instead return an error. Continue setting the git commit hash instead.  "
},
{
	"uri": "/docs-csm/en-11/readme/",
	"title": "Cray System Management - Readme",
	"tags": [],
	"description": "",
	"content": "Cray System Management (CSM) - README The documentation included here describes how to install or upgrade the Cray System Management (CSM) software and related supporting operational procedures. CSM software is the foundation upon which other software product streams for the HPE Cray EX system depend.\nThis documentation is in Markdown format. Although much of it can be viewed with any text editor, a richer experience will come from using a tool which can render the Markdown to show different font sizes, the use of bold and italics formatting, inclusion of diagrams and screen shots as image files, and to follow navigational links within a topic file and to other files.\nThere are many tools which can render the Markdown format to get these advantages. Any Internet search for Markdown tools will provide a long list of these tools. Some of the tools are better than others at displaying the images and allowing you to follow the navigational links.\nThe exploration of the CSM documentation begins with the Cray System Management Documentation which introduces topics related to CSM software installation, upgrade, and operational use. Notice that the previous sentence had a link to the _index.md file for the Cray System Management Documentation. If the link does not work, then a better Markdown viewer is needed.\n"
},
{
	"uri": "/docs-csm/en-11/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/docs-csm/en-11/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]